self.eof_index = len(string)
return self.eof_index
has_previous_line = self.start_line > 1 if has_previous_line: line_to_check = string_lines.line_number_to_line(self.start_line - 1) self._mark_disabled(line_to_check, scope_start_string=True) if self.is_disabled: return
print("", file=out) print("{} violations total".format(self.total_violations), file=out)
return False
return None
start_delim_index = template.find(start_delim, start_index, close_char_index) if 0 <= start_delim_index < open_char_index: return None
match = uncommented_line_start_index_regex.search(template, line_start_index) if match is None: return None elif match.start() < start_index: return start_index else: return match.start()
return start_index
elif "+" not in argument: if argument.endswith('.el') or argument.endswith('.$el'): return True return False
if last_expression is not None: results.violations.append(ExpressionRuleViolation( rule, last_expression ))
start_index = end_triple_quote_match.start()
if node.attr == 'format': self.contains_format_call = True else: self.generic_visit(node)
self.format_caller_node = node.func.value
self.interpolates_text_or_html = True
else: self.generic_visit(node)
if is_caller_html_or_text is False: self.results.violations.append(ExpressionRuleViolation( Rules.python_requires_html_or_text, self.node_to_expression(node.func) ))
if file_name == os.path.basename(__file__): return results
if root_node is not None: visitor = OuterFormatVisitor(file_contents, results) visitor.visit(root_node) results.prepare_results(file_contents, line_comment_delim=self.LINE_COMMENT_DELIM)
visitor = AllNodeVisitor(python_code, results) visitor.visit(root_node)
python_block_regex = re.compile(r'<%\s(?P<code>.*?)%>', re.DOTALL)
filters_regex = re.compile(r'\|([.,\w\s]*)\}') filters_match = filters_regex.search(expression.expression)
match_type = match_type.group()[6:-1].lower() if match_type in html_types: context_type = 'html' elif match_type not in javascript_types: context_type = 'unknown'
uncommented_start_index = self._uncommented_start_index(mako_template, start_index) if uncommented_start_index != start_index: start_index = uncommented_start_index continue
start_index = start_index + len(start_delim)
start_index = expression.end_index
epilog += " http://edx.readthedocs.org/projects/edx-developer-guide/en/latest/conventions/safe_templates.html#safe-template-linter\n"
from django.core import management
message = "<script>alert('XSS');</script>" x = "<string>{}</strong>".format(message)
if len(results.violations) != len(rules): for violation in results.violations: print("Found violation: {}".format(violation.rule))
<%block name="requirejs"> {expression} </%block>
<script type="{}"> ${{x | n, dump_js_escaped_json}} </script>
course_id=course_overview.id
return self.func(*args)
msg = "No merge commit for {commit} in {branch}!".format( commit=commit, branch=branch, ) raise DoesNotExist(msg, commit, branch)
username = email.split("@")[0] try: email = people[username]['email'] except KeyError: pass
args.date = parse_datestring(args.date).date()
startup.enable_microsites() directories = LOOKUP['main'].directories self.assertEqual(len([directory for directory in directories if 'external_module' in directory]), 1)
STATICFILES_STORAGE = 'openedx.core.lib.django_require.staticstorage.OptimizedCachedRequireJsStorage'
TEST_ROOT = REPO_ROOT / "test_root" LOG_DIR = (TEST_ROOT / "log").abspath()
STATIC_ROOT = (TEST_ROOT / "staticfiles" / "lms").abspath()
os.environ['REQUIRE_BUILD_PROFILE_OPTIMIZE'] = 'none'
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
STATIC_ROOT_BASE = ENV_TOKENS.get('STATIC_ROOT_BASE', None) if STATIC_ROOT_BASE: STATIC_ROOT = path(STATIC_ROOT_BASE)
DEFAULT_COURSE_ABOUT_IMAGE_URL = ENV_TOKENS.get('DEFAULT_COURSE_ABOUT_IMAGE_URL', DEFAULT_COURSE_ABOUT_IMAGE_URL)
MEDIA_ROOT = ENV_TOKENS.get('MEDIA_ROOT', MEDIA_ROOT) MEDIA_URL = ENV_TOKENS.get('MEDIA_URL', MEDIA_URL)
PLATFORM_TWITTER_ACCOUNT = ENV_TOKENS.get('PLATFORM_TWITTER_ACCOUNT', PLATFORM_TWITTER_ACCOUNT) PLATFORM_FACEBOOK_ACCOUNT = ENV_TOKENS.get('PLATFORM_FACEBOOK_ACCOUNT', PLATFORM_FACEBOOK_ACCOUNT)
SOCIAL_MEDIA_FOOTER_URLS = ENV_TOKENS.get('SOCIAL_MEDIA_FOOTER_URLS', SOCIAL_MEDIA_FOOTER_URLS)
EDXMKTG_LOGGED_IN_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_LOGGED_IN_COOKIE_NAME', EDXMKTG_LOGGED_IN_COOKIE_NAME) EDXMKTG_USER_INFO_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_USER_INFO_COOKIE_NAME', EDXMKTG_USER_INFO_COOKIE_NAME)
PAID_COURSE_REGISTRATION_CURRENCY = ENV_TOKENS.get('PAID_COURSE_REGISTRATION_CURRENCY', PAID_COURSE_REGISTRATION_CURRENCY)
PAYMENT_REPORT_GENERATOR_GROUP = ENV_TOKENS.get('PAYMENT_REPORT_GENERATOR_GROUP', PAYMENT_REPORT_GENERATOR_GROUP)
BULK_EMAIL_ROUTING_KEY_SMALL_JOBS = LOW_PRIORITY_QUEUE
THEME_NAME = ENV_TOKENS.get('THEME_NAME', None) COMPREHENSIVE_THEME_DIR = path(ENV_TOKENS.get('COMPREHENSIVE_THEME_DIR', COMPREHENSIVE_THEME_DIR))
MKTG_URL_LINK_MAP.update(ENV_TOKENS.get('MKTG_URL_LINK_MAP', {}))
MOBILE_STORE_URLS = ENV_TOKENS.get('MOBILE_STORE_URLS', MOBILE_STORE_URLS)
TIME_ZONE = ENV_TOKENS.get('TIME_ZONE', TIME_ZONE)
for app in ENV_TOKENS.get('ADDL_INSTALLED_APPS', []): INSTALLED_APPS += (app,)
GIT_REPO_DIR = ENV_TOKENS.get('GIT_REPO_DIR', '/edx/var/edxapp/course_repos') GIT_IMPORT_STATIC = ENV_TOKENS.get('GIT_IMPORT_STATIC', True)
if "TRACKING_IGNORE_URL_PATTERNS" in ENV_TOKENS: TRACKING_IGNORE_URL_PATTERNS = ENV_TOKENS.get("TRACKING_IGNORE_URL_PATTERNS")
SSL_AUTH_EMAIL_DOMAIN = ENV_TOKENS.get("SSL_AUTH_EMAIL_DOMAIN", "MIT.EDU") SSL_AUTH_DN_FORMAT_STRING = ENV_TOKENS.get("SSL_AUTH_DN_FORMAT_STRING", "/C=US/ST=Massachusetts/O=Massachusetts Institute of Technology/OU=Client CA v1/CN={0}/emailAddress={1}")
VIDEO_CDN_URL = ENV_TOKENS.get('VIDEO_CDN_URL', {})
NOTIFICATION_EMAIL_CSS = ENV_TOKENS.get('NOTIFICATION_EMAIL_CSS', NOTIFICATION_EMAIL_CSS) NOTIFICATION_EMAIL_EDX_LOGO = ENV_TOKENS.get('NOTIFICATION_EMAIL_EDX_LOGO', NOTIFICATION_EMAIL_EDX_LOGO)
CSRF_COOKIE_SECURE = ENV_TOKENS.get('CSRF_COOKIE_SECURE', False)
FIELD_OVERRIDE_PROVIDERS = tuple(ENV_TOKENS.get('FIELD_OVERRIDE_PROVIDERS', []))
AWS_QUERYSTRING_AUTH = AUTH_TOKENS.get('AWS_QUERYSTRING_AUTH', True) AWS_S3_CUSTOM_DOMAIN = AUTH_TOKENS.get('AWS_S3_CUSTOM_DOMAIN', 'edxuploads.s3.amazonaws.com')
FILE_UPLOAD_STORAGE_BUCKET_NAME = ENV_TOKENS.get('FILE_UPLOAD_STORAGE_BUCKET_NAME', FILE_UPLOAD_STORAGE_BUCKET_NAME) FILE_UPLOAD_STORAGE_PREFIX = ENV_TOKENS.get('FILE_UPLOAD_STORAGE_PREFIX', FILE_UPLOAD_STORAGE_PREFIX)
DATABASES = AUTH_TOKENS['DATABASES']
DATADOG = AUTH_TOKENS.get("DATADOG", {}) DATADOG.update(ENV_TOKENS.get("DATADOG", {}))
ANALYTICS_SERVER_URL = ENV_TOKENS.get("ANALYTICS_SERVER_URL") ANALYTICS_API_KEY = AUTH_TOKENS.get("ANALYTICS_API_KEY", "")
ANALYTICS_DATA_URL = ENV_TOKENS.get("ANALYTICS_DATA_URL", ANALYTICS_DATA_URL) ANALYTICS_DATA_TOKEN = AUTH_TOKENS.get("ANALYTICS_DATA_TOKEN", ANALYTICS_DATA_TOKEN)
ANALYTICS_DASHBOARD_URL = ENV_TOKENS.get("ANALYTICS_DASHBOARD_URL", ANALYTICS_DASHBOARD_URL) ANALYTICS_DASHBOARD_NAME = ENV_TOKENS.get("ANALYTICS_DASHBOARD_NAME", PLATFORM_NAME + " Insights")
MAILCHIMP_NEW_USER_LIST_ID = ENV_TOKENS.get("MAILCHIMP_NEW_USER_LIST_ID")
ZENDESK_USER = AUTH_TOKENS.get("ZENDESK_USER") ZENDESK_API_KEY = AUTH_TOKENS.get("ZENDESK_API_KEY")
EDX_API_KEY = AUTH_TOKENS.get("EDX_API_KEY")
STUDENT_FILEUPLOAD_MAX_SIZE = ENV_TOKENS.get("STUDENT_FILEUPLOAD_MAX_SIZE", STUDENT_FILEUPLOAD_MAX_SIZE)
VERIFY_STUDENT = AUTH_TOKENS.get("VERIFY_STUDENT", VERIFY_STUDENT)
GRADES_DOWNLOAD_ROUTING_KEY = HIGH_MEM_QUEUE
FINANCIAL_REPORTS = ENV_TOKENS.get("FINANCIAL_REPORTS", FINANCIAL_REPORTS)
ORA2_FILE_PREFIX = ENV_TOKENS.get("ORA2_FILE_PREFIX", ORA2_FILE_PREFIX)
SOCIAL_AUTH_PIPELINE_TIMEOUT = ENV_TOKENS.get('SOCIAL_AUTH_PIPELINE_TIMEOUT', 600)
THIRD_PARTY_AUTH_OLD_CONFIG = AUTH_TOKENS.get('THIRD_PARTY_AUTH', None)
THIRD_PARTY_AUTH_CUSTOM_AUTH_FORMS = AUTH_TOKENS.get('THIRD_PARTY_AUTH_CUSTOM_AUTH_FORMS', {})
INVOICE_CORP_ADDRESS = ENV_TOKENS.get('INVOICE_CORP_ADDRESS', INVOICE_CORP_ADDRESS) INVOICE_PAYMENT_INSTRUCTIONS = ENV_TOKENS.get('INVOICE_PAYMENT_INSTRUCTIONS', INVOICE_PAYMENT_INSTRUCTIONS)
COURSE_CATALOG_VISIBILITY_PERMISSION = ENV_TOKENS.get( 'COURSE_CATALOG_VISIBILITY_PERMISSION', COURSE_CATALOG_VISIBILITY_PERMISSION ) COURSE_ABOUT_VISIBILITY_PERMISSION = ENV_TOKENS.get( 'COURSE_ABOUT_VISIBILITY_PERMISSION', COURSE_ABOUT_VISIBILITY_PERMISSION )
ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT = ENV_TOKENS.get('ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT', 60)
SEARCH_ENGINE = "search.elastic.ElasticSearchEngine"
FACEBOOK_API_VERSION = AUTH_TOKENS.get("FACEBOOK_API_VERSION") FACEBOOK_APP_SECRET = AUTH_TOKENS.get("FACEBOOK_APP_SECRET") FACEBOOK_APP_ID = AUTH_TOKENS.get("FACEBOOK_APP_ID")
LTI_AGGREGATE_SCORE_PASSBACK_DELAY = ENV_TOKENS.get( 'LTI_AGGREGATE_SCORE_PASSBACK_DELAY', LTI_AGGREGATE_SCORE_PASSBACK_DELAY )
MICROSITE_BACKEND = ENV_TOKENS.get("MICROSITE_BACKEND", MICROSITE_BACKEND) MICROSITE_TEMPLATE_BACKEND = ENV_TOKENS.get("MICROSITE_TEMPLATE_BACKEND", MICROSITE_TEMPLATE_BACKEND) MICROSITE_DATABASE_TEMPLATE_CACHE_TTL = ENV_TOKENS.get( "MICROSITE_DATABASE_TEMPLATE_CACHE_TTL", MICROSITE_DATABASE_TEMPLATE_CACHE_TTL )
MAX_BOOKMARKS_PER_COURSE = ENV_TOKENS.get('MAX_BOOKMARKS_PER_COURSE', MAX_BOOKMARKS_PER_COURSE)
STUDENTMODULEHISTORYEXTENDED_OFFSET = ENV_TOKENS.get( 'STUDENTMODULEHISTORYEXTENDED_OFFSET', STUDENTMODULEHISTORYEXTENDED_OFFSET )
if ENV_TOKENS.get('AUDIT_CERT_CUTOFF_DATE', None): AUDIT_CERT_CUTOFF_DATE = dateutil.parser.parse(ENV_TOKENS.get('AUDIT_CERT_CUTOFF_DATE'))
if FEATURES.get('ENABLE_CSMH_EXTENDED'): INSTALLED_APPS += ('coursewarehistoryextended',)
APP_UPGRADE_CACHE_TIMEOUT = ENV_TOKENS.get('APP_UPGRADE_CACHE_TIMEOUT', APP_UPGRADE_CACHE_TIMEOUT)
PLATFORM_NAME = "Your Platform Name Here" CC_MERCHANT_NAME = PLATFORM_NAME COPYRIGHT_YEAR = "2015"
FEATURES = { 'DISPLAY_DEBUG_INFO_TO_STAFF': True,
'ENABLE_DISCUSSION_SERVICE': True, 'ENABLE_TEXTBOOK': True,
'ENABLE_DISCUSSION_HOME_PANEL': False,
'AUTH_USE_OPENID': False, 'AUTH_USE_CERTIFICATES': False, 'AUTH_USE_OPENID_PROVIDER': False, 'AUTH_USE_SHIB': False, 'AUTH_USE_CAS': False,
'SHIB_DISABLE_TOS': False,
'ENABLE_OAUTH2_PROVIDER': False,
'ENABLE_XBLOCK_VIEW_ENDPOINT': False,
'ENABLE_CORS_HEADERS': False,
'COURSES_ARE_BROWSABLE': True,
'RESTRICT_ENROLL_BY_REG_METHOD': False,
'RUN_AS_ANALYTICS_SERVER_ENABLED': False,
'USE_YOUTUBE_OBJECT_API': False,
'ENABLE_STUDENT_HISTORY_VIEW': True,
'ENABLE_FEEDBACK_SUBMISSION': False,
'ENABLE_DEBUG_RUN_PYTHON': False,
'ENABLE_SERVICE_STATUS': False,
'USE_CUSTOM_THEME': False,
'AUTOPLAY_VIDEOS': False,
'ENABLE_INSTRUCTOR_BACKGROUND_TASKS': True,
'INDIVIDUAL_DUE_DATES': False,
'CUSTOM_COURSES_EDX': False,
'ENABLE_VERIFIED_CERTIFICATES': False,
'AUTOMATIC_AUTH_FOR_TESTING': False,
'ENABLE_SHOPPING_CART': False,
'STORE_BILLING_INFO': False,
'ENABLE_PAID_COURSE_REGISTRATION': False,
'ENABLE_COSMETIC_DISPLAY_PRICE': False,
'AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING': False,
'MAX_ENROLLMENT_INSTR_BUTTONS': 200,
'ENABLE_S3_GRADE_DOWNLOADS': False,
'ENFORCE_PASSWORD_POLICY': True,
'ALLOW_COURSE_STAFF_GRADE_DOWNLOADS': False,
'ENABLE_MAX_FAILED_LOGIN_ATTEMPTS': True,
'SQUELCH_PII_IN_LOGS': True,
'EMBARGO': False,
'ALLOW_WIKI_ROOT_ACCESS': True,
'USE_MICROSITES': False,
'ENABLE_THIRD_PARTY_AUTH': False,
'ENABLE_MKTG_SITE': False,
'PREVENT_CONCURRENT_LOGINS': True,
'ADVANCED_SECURITY': True,
'ALWAYS_REDIRECT_HOMEPAGE_TO_DASHBOARD_FOR_AUTHENTICATED_USER': True,
'ENABLE_COURSE_SORTING_BY_START_DATE': True,
'ENABLE_MOBILE_REST_API': False,
'ENABLE_COMBINED_LOGIN_REGISTRATION': False,
'ENABLE_MKTG_EMAIL_OPT_IN': False,
'ALLOW_AUTOMATED_SIGNUPS': False,
'DISPLAY_ANALYTICS_ENROLLMENTS': True,
'ENABLE_FOOTER_MOBILE_APP_LINKS': False,
'ENABLE_EDXNOTES': False,
'MILESTONES_APP': False,
'ORGANIZATIONS_APP': False,
'ENABLE_PREREQUISITE_COURSES': False,
'MODE_CREATION_FOR_TESTING': False,
'ENABLE_COURSEWARE_SEARCH': False,
'ENABLE_DASHBOARD_SEARCH': False,
'LOG_POSTPAY_CALLBACKS': True,
'ENABLE_VIDEO_BEACON': False,
'ENABLE_ONLOAD_BEACON': False,
'LICENSING': False,
'CERTIFICATES_HTML_VIEW': False,
'CERTIFICATES_INSTRUCTOR_GENERATION': False,
'ENABLE_COURSE_DISCOVERY': False,
'ENABLE_SOFTWARE_SECURE_FAKE': False,
'ENABLE_TEAMS': True,
'ENABLE_VIDEO_BUMPER': False,
'SHOW_BUMPER_PERIODICITY': 7 * 24 * 3600,
'ENABLE_SPECIAL_EXAMS': False,
'ENABLE_OPENBADGES': False,
'ENABLE_DISABLING_XBLOCK_TYPES': True,
'ENABLE_MAX_SCORE_CACHE': True,
'ENABLE_LTI_PROVIDER': False,
'SHOW_LANGUAGE_SELECTOR': False,
'ENABLE_CSMH_EXTENDED': False,
'ENABLE_READING_FROM_MULTIPLE_HISTORY_TABLES': True,
ASSET_IGNORE_REGEX = r"(^\._.*$)|(^\.DS_Store$)|(^.*~$)"
DEFAULT_GROUPS = []
GENERATE_PROFILE_SCORES = False
COMPREHENSIVE_THEME_DIR = ""
GEOIP_PATH = REPO_ROOT / "common/static/data/geoip/GeoIP.dat" GEOIPV6_PATH = REPO_ROOT / "common/static/data/geoip/GeoIPv6.dat"
STATUS_MESSAGE_PATH = ENV_ROOT / "status_message.json"
'edxmako.shortcuts.marketing_link_context_processor',
'shoppingcart.context_processor.user_has_cart_context_processor',
'edxmako.shortcuts.microsite_footer_context_processor',
'context_processors.doc_url',
'debug': False
AUTHENTICATION_BACKENDS = ( 'ratelimitbackend.backends.RateLimitModelBackend', )
STATIC_GRAB = False DEV_CONTENT = True
TRACK_MAX_EVENT = 50000
TRACKING_IGNORE_URL_PATTERNS = [r'^/event', r'^/login', r'^/heartbeat', r'^/segmentio/event', r'^/performance']
from xmodule.modulestore.inheritance import InheritanceMixin from xmodule.modulestore import prefer_xmodules from xmodule.x_module import XModuleMixin
XBLOCK_MIXINS = (LmsBlockMixin, InheritanceMixin, XModuleMixin, EditInfoMixin)
XBLOCK_SELECT_FUNCTION = prefer_xmodules
XBLOCK_FIELD_DATA_WRAPPERS = ()
'python_bin': None, 'user': 'sandbox',
'limits': { 'CPU': 1, },
COURSES_WITH_UNSAFE_CODE = []
DEBUG = False USE_TZ = True SESSION_COOKIE_SECURE = False SESSION_SAVE_EVERY_REQUEST = False SESSION_SERIALIZER = 'django.contrib.sessions.serializers.PickleSerializer'
CMS_BASE = 'localhost:8001'
SITE_ID = 1 SITE_NAME = "example.com" HTTPS = 'on' ROOT_URLCONF = 'lms.urls'
CONTACT_MAILING_ADDRESS = ''
EDX_PLATFORM_REVISION = dealer.git.Backend(path=REPO_ROOT).revision
EDX_PLATFORM_REVISION = 'unknown'
STATIC_URL = '/static/' STATIC_ROOT = ENV_ROOT / "staticfiles"
MEDIA_ROOT = '/edx/var/edxapp/media/' MEDIA_URL = '/media/'
LANGUAGES_BIDI = ("he", "ar", "fa", "ur", "fa-ir", "rtl")
LANGUAGES = ( ('en', u'English'), ('rtl', u'Right-to-Left Test Language'),
MESSAGE_STORAGE = 'django.contrib.messages.storage.session.SessionStorage'
PAID_COURSE_REGISTRATION_CURRENCY = ['usd', '$']
PAYMENT_REPORT_GENERATOR_GROUP = 'shoppingcart_report_access'
EDXNOTES_PUBLIC_API = 'http://localhost:8120/api/v1' EDXNOTES_INTERNAL_API = 'http://localhost:8120/api/v1'
PARENTAL_CONSENT_AGE_LIMIT = 13
FOOTER_OPENEDX_URL = "http://open.edx.org"
FOOTER_ORGANIZATION_IMAGE = "images/logo.png"
FOOTER_CACHE_TIMEOUT = 30 * 60
FOOTER_BROWSER_CACHE_MAX_AGE = 5 * 60
CREDIT_NOTIFICATION_CACHE_TIMEOUT = 5 * 60 * 60
simplefilter('ignore')
'openedx.core.djangoapps.safe_sessions.middleware.SafeSessionMiddleware',
#'django.contrib.auth.middleware.AuthenticationMiddleware', 'cache_toolbox.middleware.CacheBackedAuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
'openedx.core.djangoapps.user_api.middleware.UserTagsEventContextMiddleware',
'corsheaders.middleware.CorsMiddleware', 'cors_csrf.middleware.CorsCSRFMiddleware', 'cors_csrf.middleware.CsrfCrossDomainCookieMiddleware', 'django.middleware.csrf.CsrfViewMiddleware',
'lang_pref.middleware.LanguagePreferenceMiddleware',
'dark_lang.middleware.DarkLangMiddleware',
'django.middleware.locale.LocaleMiddleware',
'ratelimitbackend.middleware.RateLimitMiddleware', 'edxmako.middleware.MakoMiddleware',
'session_inactivity_timeout.middleware.SessionInactivityTimeout',
'django.middleware.clickjacking.XFrameOptionsMiddleware',
'courseware.middleware.RedirectUnenrolledMiddleware',
'microsite_configuration.middleware.MicrositeSessionCookieDomainMiddleware',
X_FRAME_OPTIONS = 'ALLOW'
P3P_HEADER = 'CP="Open EdX does not have a P3P policy."'
PIPELINE_COMPILE_INPLACE = True
PIPELINE_DISABLE_WRAPPER = True
PIPELINE_UGLIFYJS_BINARY = 'node_modules/.bin/uglifyjs'
'edx-ui-toolkit/js/utils/global-loader.js', 'edx-ui-toolkit/js/utils/string-utils.js', 'edx-ui-toolkit/js/utils/html-utils.js',
'js/vendor/requirejs/require.js', 'js/RequireJS-namespace-undefine.js', 'js/vendor/URI.min.js', 'common/js/vendor/backbone.js', 'edx-pattern-library/js/modernizr-custom.js',
"spec", "spec_helpers",
"xmodule_js",
REQUIRE_BASE_URL = "./"
REQUIRE_JS = "js/vendor/requirejs/require.js"
REQUIRE_STANDALONE_MODULES = {}
REQUIRE_DEBUG = False
REQUIRE_EXCLUDE = ("build.txt",)
CELERY_IMPORTS = ( 'openedx.core.djangoapps.programs.tasks.v1.tasks', )
CELERYD_HIJACK_ROOT_LOGGER = False
BULK_EMAIL_DEFAULT_FROM_EMAIL = 'no-reply@example.com'
BULK_EMAIL_EMAILS_PER_TASK = 100
BULK_EMAIL_DEFAULT_RETRY_DELAY = 30
BULK_EMAIL_MAX_RETRIES = 5
BULK_EMAIL_INFINITE_RETRY_CAP = 1000
BULK_EMAIL_ROUTING_KEY = HIGH_PRIORITY_QUEUE
BULK_EMAIL_ROUTING_KEY_SMALL_JOBS = LOW_PRIORITY_QUEUE
BULK_EMAIL_JOB_SIZE_THRESHOLD = 100
BULK_EMAIL_LOG_SENT_EMAILS = False
BULK_EMAIL_RETRY_DELAY_BETWEEN_SENDS = 0.02
EMAIL_OPTIN_MINIMUM_AGE = PARENTAL_CONSENT_AGE_LIMIT
'API': 'https://www.youtube.com/iframe_api',
'METADATA_URL': 'https://www.googleapis.com/youtube/v3/videos/',
'openedx.core.djangoapps.common_views',
'simple_history',
'config_models',
'service_status',
'status',
'edxmako', 'pipeline', 'static_replace',
'contentserver',
'openedx.core.djangoapps.theming',
'openedx.core.djangoapps.site_configuration',
'courseware', 'student',
'support',
'external_auth', 'django_openid_auth',
'provider', 'provider.oauth2', 'edx_oauth2_provider',
'oauth2_provider',
#'wiki.plugins.notifications', 'course_wiki.plugins.markdownedx',
'django_comment_client', 'django_comment_common', 'discussion_api', 'notes',
'splash',
'datadog',
'rest_framework', 'openedx.core.djangoapps.user_api',
'shoppingcart',
'notification_prefs',
'course_modes',
'enrollment',
'lms.djangoapps.verify_student',
'dark_lang',
'microsite_configuration',
'rss_proxy',
'reverification',
'monitoring',
'course_action_state',
'django_countries',
'mobile_api', 'social.apps.django_app.default',
'survey',
'openedx.core.djangoapps.content.course_overviews', 'openedx.core.djangoapps.content.course_structures', 'lms.djangoapps.course_blocks',
'course_structure_api',
'mailing',
'corsheaders', 'cors_csrf',
'openedx.core.djangoapps.credit',
'lms.djangoapps.teams',
'openedx.core.djangoapps.bookmarks',
'openedx.core.djangoapps.programs',
'openedx.core.djangoapps.self_paced',
'openedx.core.djangoapps.credentials',
'milestones',
'gating.apps.GatingConfig',
'statici18n',
'openedx.core.djangoapps.coursetalk',
'openedx.core.djangoapps.api_admin',
'verified_track_content',
'learner_dashboard',
'badges',
MIGRATION_MODULES = { 'social.apps.django_app.default': 'social.apps.django_app.default.south_migrations' }
CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_SECURE = False
'WHAT_IS_VERIFIED_CERT': 'verified-certificate',
SOCIAL_MEDIA_FOOTER_NAMES = [ "facebook", "twitter", "youtube", "linkedin", "google_plus", "reddit", ]
SOCIAL_MEDIA_FOOTER_URLS = {}
MOBILE_STORE_URLS = { 'apple': '#', 'google': '#' }
XDOMAIN_PROXY_CACHE_TIMEOUT = 60 * 15
REGISTRATION_EMAIL_PATTERNS_ALLOWED = None
BADGR_API_TOKEN = None BADGR_BASE_URL = "http://localhost:8005" BADGR_ISSUER_SLUG = "example-issuer" BADGR_TIMEOUT = 10
GRADES_DOWNLOAD_ROUTING_KEY = HIGH_MEM_QUEUE
ORA2_FILE_PREFIX = None
FILE_UPLOAD_STORAGE_BUCKET_NAME = 'edxuploads' FILE_UPLOAD_STORAGE_PREFIX = 'submissions_attachments'
'submissions', 'openassessment', 'openassessment.assessment', 'openassessment.fileupload', 'openassessment.workflow', 'openassessment.xblock',
'edxval',
'edx_proctoring',
'organizations',
try: imp.find_module(app_name) except ImportError: try: __import__(app_name) except ImportError: continue INSTALLED_APPS += (app_name,)
ADVANCED_SECURITY_CONFIG = {}
INVOICE_CORP_ADDRESS = "Please place your corporate address\nin this configuration" INVOICE_PAYMENT_INSTRUCTIONS = "This is where you can\nput directions on how people\nbuying registration codes"
COUNTRIES_OVERRIDE = { "TW": "Taiwan", 'XK': _('Kosovo'), }
COURSE_CATALOG_VISIBILITY_PERMISSION = 'see_exists'
COURSE_ABOUT_VISIBILITY_PERMISSION = 'see_exists'
ENROLLMENT_COURSE_DETAILS_CACHE_TIMEOUT = 60
NOTES_DISABLED_TABS = ['course_structure', 'tags']
CDN_VIDEO_URLS = {}
ONLOAD_BEACON_SAMPLE_RATE = 0.0
ACCOUNT_VISIBILITY_CONFIGURATION = { "default_visibility": "all_users",
"shareable_fields": [ 'username', 'profile_image', 'country', 'time_zone', 'language_proficiencies', 'bio', 'account_privacy', 'accomplishments_shared', ],
"public_fields": [ 'username', 'profile_image', 'account_privacy', ],
ECOMMERCE_PUBLIC_URL_ROOT = None ECOMMERCE_API_URL = None ECOMMERCE_API_SIGNING_KEY = None ECOMMERCE_API_TIMEOUT = 5 ECOMMERCE_SERVICE_WORKER_USERNAME = 'ecommerce_worker'
CHECKPOINT_PATTERN = r'(?P<checkpoint_name>[^/]+)'
FIELD_OVERRIDE_PROVIDERS = ()
MODULESTORE_FIELD_OVERRIDE_PROVIDERS = ()
HOMEPAGE_COURSE_MAX = None
CREDIT_TASK_DEFAULT_RETRY_DELAY = 30
CREDIT_TASK_MAX_RETRIES = 5
SECRET_KEY = 'dev key'
CREDIT_PROVIDER_SECRET_KEYS = {}
CREDIT_PROVIDER_TIMESTAMP_EXPIRATION = 15 * 60
CREDIT_HELP_LINK_URL = "#"
LTI_USER_EMAIL_DOMAIN = 'lti.example.com'
PUBLIC_RSA_KEY = None PRIVATE_RSA_KEY = None
NOTIFICATION_EMAIL_CSS = "templates/credit_notifications/credit_notification.css" NOTIFICATION_EMAIL_EDX_LOGO = "templates/credit_notifications/edx-logo-header.png"
CCX_MAX_STUDENTS_ALLOWED = 200
FINANCIAL_ASSISTANCE_MIN_LENGTH = 800 FINANCIAL_ASSISTANCE_MAX_LENGTH = 2500
MAX_BOOKMARKS_PER_COURSE = 100
MOBILE_APP_USER_AGENT_REGEXES = [ r'edX/org.edx.mobile', ]
APP_UPGRADE_CACHE_TIMEOUT = 3600
DEPRECATED_ADVANCED_COMPONENT_TYPES = []
DEFAULT_SITE_ID = 1
AFFILIATE_COOKIE_NAME = 'affiliate_id'
from yaml import Loader, SafeLoader
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
SSL_AUTH_EMAIL_DOMAIN = "MIT.EDU" SSL_AUTH_DN_FORMAT_STRING = "/C=US/ST=Massachusetts/O=Massachusetts Institute of Technology/OU=Client CA v1/CN={0}/emailAddress={1}"
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
ENV_TOKENS = convert_tokens(ENV_TOKENS)
if 'FEATURES' in ENV_TOKENS: del ENV_TOKENS['FEATURES']
vars().update(ENV_TOKENS)
SESSION_COOKIE_NAME = str(SESSION_COOKIE_NAME)
BULK_EMAIL_ROUTING_KEY_SMALL_JOBS = LOW_PRIORITY_QUEUE
for app in ADDL_INSTALLED_APPS: INSTALLED_APPS += (app,)
AUTH_TOKENS = convert_tokens(AUTH_TOKENS)
GRADES_DOWNLOAD_ROUTING_KEY = HIGH_MEM_QUEUE
FEATURES['ENABLE_MKTG_SITE'] = True FEATURES['USE_MICROSITES'] = True
'debug_toolbar_mongo',
HOSTNAME_MODULESTORE_DEFAULT_MAPPINGS = { 'preview\.': 'draft-preferred' }
SECRET_KEY = 'dev key'
for database_name in DATABASES: DATABASES[database_name]['ATOMIC_REQUESTS'] = False
from util.testing import patch_testcase, patch_sessions patch_testcase() patch_sessions()
MONGO_PORT_NUM = int(os.environ.get('EDXAPP_TEST_MONGO_PORT', '27017')) MONGO_HOST = os.environ.get('EDXAPP_TEST_MONGO_HOST', 'localhost')
FEATURES['DISABLE_START_DATES'] = True
FEATURES['ENABLE_S3_GRADE_DOWNLOADS'] = True FEATURES['ALLOW_COURSE_STAFF_GRADE_DOWNLOADS'] = True
FEATURES['EMBARGO'] = True
PARENTAL_CONSENT_AGE_LIMIT = 13
TEST_RUNNER = 'openedx.core.djangolib.nose.NoseTestSuiteRunner'
TEST_ROOT = path("test_root") STATIC_ROOT = TEST_ROOT / "staticfiles"
GITHUB_REPO_ROOT = ENV_ROOT / "data"
MOCK_STAFF_GRADING = True MOCK_PEER_GRADING = True
STATICFILES_STORAGE = 'pipeline.storage.NonPackagingPipelineStorage'
PIPELINE_JS_COMPRESSOR = None
MIGRATION_MODULES = NoOpMigrationModules()
FEATURES['ENABLE_CSMH_EXTENDED'] = True INSTALLED_APPS += ('coursewarehistoryextended',)
'default': { 'BACKEND': 'django.core.cache.backends.dummy.DummyCache', },
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
filterwarnings('ignore', message='No request passed to the backend, unable to rate-limit')
simplefilter('ignore')
FEATURES['ENFORCE_PASSWORD_POLICY'] = False FEATURES['ENABLE_MAX_FAILED_LOGIN_ATTEMPTS'] = False FEATURES['SQUELCH_PII_IN_LOGS'] = False FEATURES['PREVENT_CONCURRENT_LOGINS'] = False FEATURES['ADVANCED_SECURITY'] = False PASSWORD_MIN_LENGTH = None PASSWORD_COMPLEXITY = {}
OIDC_COURSE_HANDLER_CACHE_TIMEOUT = 0
FEATURES['ENABLE_PAYMENT_FAKE'] = True
from random import choice from string import letters, digits, punctuation RANDOM_SHARED_SECRET = ''.join( choice(letters + digits + punctuation) for x in range(250) )
'WHAT_IS_VERIFIED_CERT': 'verified-certificate',
for static_dir in STATICFILES_DIRS: try: _, data_dir = static_dir except ValueError: data_dir = static_dir
LETTUCE_SERVER_PORT = 8003 XQUEUE_PORT = 8040 YOUTUBE_PORT = 8031 LTI_PORT = 8765 VIDEO_SOURCE_PORT = 8777
'django.contrib.auth.hashers.SHA1PasswordHasher', 'django.contrib.auth.hashers.MD5PasswordHasher',
VERIFY_STUDENT["SOFTWARE_SECURE"] = { "API_ACCESS_KEY": "BBBBBBBBBBBBBBBBBBBB", "API_SECRET_KEY": "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC", }
FEATURES['ENABLE_EDXNOTES'] = True
FEATURES['ENABLE_TEAMS'] = True
FEATURES['ENABLE_COURSEWARE_SEARCH'] = True
FEATURES['ENABLE_DASHBOARD_SEARCH'] = True
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine"
FEATURES['ENABLE_LTI_PROVIDER'] = True INSTALLED_APPS += ('lti_provider',) AUTHENTICATION_BACKENDS += ('lti_provider.users.LtiBackend',)
FEATURES['ORGANIZATIONS_APP'] = True
FEATURES['ENABLE_FINANCIAL_ASSISTANCE_FORM'] = True
from openedx.core.lib.block_structure.transformer_registry import TransformerRegistry TransformerRegistry.USE_PLUGIN_MANAGER = False
OAUTH2_PROVIDER_APPLICATION_MODEL = 'oauth2_provider.Application'
DEBUG = True
REQUIRE_DEBUG = False
STATICFILES_STORAGE = 'pipeline.storage.PipelineCachedStorage'
INSTALLED_APPS += ('django_extensions',)
GITHUB_REPO_ROOT = (TEST_ROOT / "data").abspath() LOG_DIR = (TEST_ROOT / "log").abspath()
DEBUG = True
PIPELINE_JS_COMPRESSOR = None
XQUEUE_INTERFACE['url'] = 'http://localhost:8040'
EDXNOTES_PUBLIC_API = 'http://localhost:8042/api/v1' EDXNOTES_INTERNAL_API = 'http://localhost:8042/api/v1'
FEATURES['MILESTONES_APP'] = True
FEATURES['ENABLE_OAUTH2_PROVIDER'] = True
FEATURES['ENABLE_PREREQUISITE_COURSES'] = True
FEATURES['ENABLE_COURSE_DISCOVERY'] = True
FEATURES['ENABLE_EDXNOTES'] = True
FEATURES['ENABLE_TEAMS'] = True
FEATURES['LICENSING'] = True
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
FEATURES['ENFORCE_PASSWORD_POLICY'] = False FEATURES['ENABLE_MAX_FAILED_LOGIN_ATTEMPTS'] = False FEATURES['SQUELCH_PII_IN_LOGS'] = False FEATURES['PREVENT_CONCURRENT_LOGINS'] = False FEATURES['ADVANCED_SECURITY'] = False
FEATURES['ENABLE_COURSEWARE_SEARCH'] = True
FEATURES['ENABLE_DASHBOARD_SEARCH'] = True
FEATURES['ENABLE_OPENBADGES'] = True
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine" MOCK_SEARCH_BACKING_FILE = ( TEST_ROOT / "index_file.dat" ).abspath()
SECRET_KEY = "very_secret_bok_choy_key"
FEATURES['ENABLE_CSMH_EXTENDED'] = True INSTALLED_APPS += ('coursewarehistoryextended',)
try:
del DEFAULT_FILE_STORAGE MEDIA_ROOT = "/edx/var/edxapp/uploads"
CELERY_ALWAYS_EAGER = True HTTPS = 'off'
ANALYTICS_DASHBOARD_URL = None
)
PIPELINE_JS_COMPRESSOR = None
REQUIRE_DEBUG = DEBUG
FEATURES['COURSES_ARE_BROWSEABLE'] = True HOMEPAGE_COURSE_MAX = 9
FEATURES['ENABLE_SOFTWARE_SECURE_FAKE'] = True
VERIFY_STUDENT["SOFTWARE_SECURE"] = { "API_ACCESS_KEY": "BBBBBBBBBBBBBBBBBBBB", "API_SECRET_KEY": "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC", }
SEARCH_SKIP_ENROLLMENT_START_DATE_FILTERING = True
if os.path.isfile(join(dirname(abspath(__file__)), 'private.py')):
MODULESTORE = convert_module_store_setting_if_needed(MODULESTORE)
DEBUG = True SITE_NAME = 'localhost:{}'.format(LETTUCE_SERVER_PORT)
import logging logging.basicConfig(filename=TEST_ROOT / "log" / "lms_acceptance.log", level=logging.ERROR)
logging.getLogger().setLevel(logging.ERROR)
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
FEATURES['ENABLE_PAYMENT_FAKE'] = True
FEATURES['ENABLE_SPECIAL_EXAMS'] = True
FEATURES['AUTOMATIC_VERIFY_STUDENT_IDENTITY_FOR_TESTING'] = True
USE_I18N = True
INSTALLED_APPS += ('lettuce.django',) LETTUCE_APPS = ('courseware', 'instructor')
LETTUCE_SELENIUM_CLIENT = os.environ.get('LETTUCE_SELENIUM_CLIENT', 'local')
try:
SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine"
import uuid SECRET_KEY = uuid.uuid4().hex
MIGRATION_MODULES = {}
CACHE_TIMEOUT = 0
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
VIRTUAL_UNIVERSITIES = []
META_UNIVERSITIES = {'UTx': ['UTAustinX']}
CELERY_ALWAYS_EAGER = True
LMS_SEGMENT_KEY = os.environ.get('SEGMENT_KEY')
try:
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
from .aws import * import os from django.core.exceptions import ImproperlyConfigured
if db != 'read_replica': DATABASES[db].update(get_db_overrides(db))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')
APP.config_from_object('django.conf:settings') APP.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
if settings.DEBUG or settings.FEATURES.get('ENABLE_DJANGO_ADMIN_SITE'): admin.autodiscover()
urlpatterns = ( '',
url(r'^user_api/', include('openedx.core.djangoapps.user_api.legacy_urls')),
url(r'^submit_feedback$', 'util.views.submit_feedback'),
url(r'^api/enrollment/v1/', include('enrollment.urls')),
url(r'^search/', include('search.urls')),
url(r'^api/course_structure/', include('course_structure_api.urls', namespace='course_structure_api')),
url(r'^api/courses/', include('course_api.urls')),
url(r'^api/user/', include('openedx.core.djangoapps.user_api.urls')),
url(r'^api/bookmarks/', include('openedx.core.djangoapps.bookmarks.urls')),
url(r'^api/profile_images/', include('openedx.core.djangoapps.profile_images.urls')),
url(r'^api/val/v0/', include('edxval.urls')),
url(r'^lang_pref/session_language', 'lang_pref.views.update_session_language', name='session_language'),
url(r'^api-admin/', include('openedx.core.djangoapps.api_admin.urls', namespace='api_admin')),
'packages': ('openassessment',),
if settings.FEATURES["ENABLE_SYSADMIN_DASHBOARD"]: urlpatterns += ( url(r'^sysadmin/', include('dashboard.sysadmin_urls')), )
urlpatterns += ( url(r'^404$', 'static_template_view.views.render', {'template': '404.html'}, name="404"), )
url(r'^press/([_a-zA-Z0-9-]+)$', 'static_template_view.views.render_press_release', name='press_release'),
for key, value in settings.MKTG_URL_LINK_MAP.items(): if value is None: continue
if key == "ROOT" or key == "COURSES": continue
template = key.lower() if '.' not in template: template = "%s.%s" % (template, settings.STATIC_TEMPLATE_VIEW_DEFAULT_FILE_EXTENSION)
if settings.FEATURES["USE_CUSTOM_THEME"]: template = "theme-" + template
urlpatterns += (url(r'^%s$' % key.lower(), 'static_template_view.views.render', {'template': template}, name=value),)
if settings.WIKI_ENABLED: from wiki.urls import get_pattern as wiki_pattern from django_notify.urls import get_pattern as notify_pattern
url( r'^courses/{course_key}/xblock/{usage_key}/view/(?P<view_name>[^/]*)$'.format( course_key=settings.COURSE_ID_PATTERN, usage_key=settings.USAGE_ID_PATTERN, ), 'courseware.module_render.xblock_view', name='xblock_view', ),
url( r'^courses/{}/survey$'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.course_survey', name='course_survey', ),
url( r'^courses/{}/progress/(?P<student_id>[^/]*)/$'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.progress', name='student_progress', ),
url( r'^courses/{}/instructor$'.format( settings.COURSE_ID_PATTERN, ), 'instructor.views.instructor_dashboard.instructor_dashboard_2', name='instructor_dashboard', ),
url( r'^courses/{}/lti_rest_endpoints/'.format( settings.COURSE_ID_PATTERN, ), 'courseware.views.views.get_course_lti_endpoints', name='lti_rest_endpoints', ),
url( r'^account/', include('student_account.urls') ),
url( r'^u/(?P<username>[\w.@+-]+)$', 'student_profile.views.learner_profile', name='learner_profile', ),
url( r'^courses/{}/edxnotes'.format( settings.COURSE_ID_PATTERN, ), include('edxnotes.urls'), name='edxnotes_endpoints', ),
if settings.FEATURES.get('EMBARGO'): urlpatterns += ( url(r'^embargo/', include('embargo.urls')), )
urlpatterns += ( url(r'^survey/', include('survey.urls')), )
if settings.FEATURES.get('AUTOMATIC_AUTH_FOR_TESTING'): urlpatterns += ( url(r'^auto_auth$', 'student.views.auto_auth'), )
if settings.FEATURES.get('ENABLE_OAUTH2_PROVIDER'): urlpatterns += ( url( r'^oauth2/login/$', auth_exchange.views.LoginWithAccessTokenView.as_view(), name="login_with_access_token" ), )
urlpatterns += ( url(r'^certificates/', include('certificates.urls', app_name="certificates", namespace="certificates")),
url(r'^api/certificates/', include('lms.djangoapps.certificates.apis.urls', namespace='certificates_api')),
urlpatterns += ( url(r'^xdomain_proxy.html$', 'cors_csrf.views.xdomain_proxy', name='xdomain_proxy'), )
if settings.FEATURES.get("ENABLE_LTI_PROVIDER"): urlpatterns += ( url(r'^lti_provider/', include('lti_provider.urls')), )
urlpatterns += url(r'^template/(?P<template>.+)$', 'openedx.core.djangoapps.debug.views.show_reference_template'),
handler404 = 'static_template_view.views.render_404' handler500 = 'static_template_view.views.render_500'
urlpatterns += ( url(r'^404$', handler404), url(r'^500$', handler500), )
urlpatterns += ( url(r'^api/', include('edx_proctoring.urls')), )
from .celery import APP as CELERY_APP
from safe_lxml import defuse_xml_libs defuse_xml_libs()
from django.core.wsgi import get_wsgi_application
if settings.FEATURES.get('ENABLE_THIRD_PARTY_AUTH', False): enable_third_party_auth()
if settings.COMPREHENSIVE_THEME_DIR: enable_comprehensive_theme(settings.COMPREHENSIVE_THEME_DIR)
microsite.enable_microsites_pre_startup(log)
microsite.enable_microsites(log)
if settings.LMS_SEGMENT_KEY: analytics.write_key = settings.LMS_SEGMENT_KEY
set_runtime_service('instructor', InstructorService())
if getattr(settings, "THEME_NAME", "") == "": settings.THEME_NAME = None return
theme_root = settings.ENV_ROOT / "themes" / settings.THEME_NAME
settings.DEFAULT_TEMPLATE_ENGINE['DIRS'].insert(0, theme_root / 'templates') edxmako.paths.add_lookup('main', theme_root / 'templates', prepend=True)
settings.STATICFILES_DIRS.append( (u'themes/{}'.format(settings.THEME_NAME), theme_root / 'static') )
settings.LOCALE_PATHS = (theme_root / 'conf/locale',) + settings.LOCALE_PATHS
super(LmsSearchResultProcessorTestCase, self).setUp() self.build_course()
course_org_filter = microsite.get_value('course_org_filter') if course_org_filter: field_dictionary['org'] = course_org_filter
if not course_org_filter: org_filter_out_set = microsite.get_all_orgs() if org_filter_out_set: exclude_dictionary['org'] = list(org_filter_out_set)
video = self.store.get_item(child_to_move_location)
video = self.store.get_item(child_to_move_location) self.assertEqual( old_parent_location, video.get_parent().location.for_branch(None) )
metric_tag_fields = [ 'course_id', 'group_id', 'pinned', 'closed', 'anonymous', 'anonymous_to_peers', 'endorsed', 'read' ]
initializable_fields = updatable_fields + ['thread_type', 'context']
from .comment import Comment from .thread import Thread from .user import User from .commentable import Commentable
self.save() response = perform_request( 'get', url, retrieve_params, metric_action='model.retrieve', metric_tags=self._metric_tags, )
if text_message is None: text_message = html_to_text(html_message)
course_email = cls( course_id=course_id, sender=sender, subject=subject, html_message=html_message, text_message=text_message, template_name=template_name, from_addr=from_addr, )
user = models.ForeignKey(User, db_index=True, null=True) course_id = CourseKeyField(max_length=255, db_index=True)
COURSE_EMAIL_MESSAGE_BODY_TAG = '{{message_body}}'
if 'user_id' in context and 'course_id' in context: message_body = substitute_keywords_with_data(message_body, context)
return wrap_message(result)
course_id = CourseKeyField(max_length=255, db_index=True, unique=True)
email_enabled = models.BooleanField(default=False)
return u"Course '{}': Instructor Email {}Enabled".format(self.course_id.to_deprecated_string(), not_en)
require_course_email_auth = models.BooleanField(default=True)
actions = None
name = self.cleaned_data.get("name").strip() or None
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
pass
email.to_option = next( ( t_type for t_type in ( target.target_type for target in email.targets.all() ) if t_type in EMAIL_TARGETS ), SEND_TO_MYSELF ) email.save()
pass
from __future__ import unicode_literals
self.assertTrue(BulkEmailFlag.feature_enabled(self.course.id))
bad_id = SlashSeparatedCourseKey(u'Broken{}'.format(self.course.id.org), 'hello', self.course.id.run + '_typo')
self.assertFalse(form.is_valid())
self.assertFalse(form.is_valid())
form_data = {'course_id': self.course.id.run, 'email_enabled': True} form = CourseAuthorizationAdminForm(data=form_data) self.assertFalse(form.is_valid())
cet = CourseEmailTemplate.objects.get(name=None) self.assertIsNotNone(cet)
cet = CourseEmailTemplate.objects.get(name=None) self.assertIsNotNone(cet)
cet = CourseEmailTemplate.objects.get(name='foo') self.assertIsNotNone(cet)
cet = CourseEmailTemplate.objects.get(name='foo') self.assertIsNotNone(cet)
form = CourseEmailTemplateForm(form_data) self.assertFalse(form.is_valid())
call_command("loaddata", "course_email_template.json")
self.assertEqual(len(mail.outbox), 1) self.assertEqual(mail.outbox[0].to[0], self.instructor.email)
response = self.client.get(url) email_section = '<div class="vert-left send-email" id="section-send-email">' self.assertIn(email_section, response.content)
call_command("loaddata", "course_email_template.json")
self.assertContains(response, "Email is not enabled for this course.", status_code=403)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
unicode_user = UserFactory(first_name=u'Ⓡⓞⓑⓞⓣ', last_name=u'ՇﻉรՇ') CourseEnrollmentFactory.create(user=unicode_user, course_id=self.course.id) self.students.append(unicode_user)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
long_name = u"Финансовое программирование и политика, часть 1: макроэкономические счета и анализ"
self.assertEqual(len(encoded_unexpected_from_addr), 318) self.assertEqual(len(escaped_encoded_unexpected_from_addr), 324) self.assertEqual(len(unexpected_from_addr), 137)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
response = self.client.post(self.send_mail_url, test_email) self.assertEquals(json.loads(response.content), self.success_content)
call_command("loaddata", "course_email_template.json")
template = CourseEmailTemplate.get_template() self.assertIsNotNone(template.html_template) self.assertIsNotNone(template.plain_template)
self.assertFalse(BulkEmailFlag.feature_enabled(course_id))
self.assertTrue(BulkEmailFlag.feature_enabled(course_id))
cauth = CourseAuthorization(course_id=course_id, email_enabled=False) cauth.save()
self.assertTrue(BulkEmailFlag.feature_enabled(course_id))
call_command("loaddata", "course_email_template.json")
self.assertEquals(parent_status.get('total'), total) self.assertEquals(parent_status.get('action_name'), action_name)
self._test_email_address_failures(SMTPDataError(554, "Email address is blacklisted"))
self._test_email_address_failures(SESAddressBlacklistedError(554, "Email address is blacklisted"))
self._test_email_address_failures(SESIllegalAddressError(554, "Email address is illegal"))
self._test_email_address_failures(SESLocalAddressCharacterError(554, "Email address contains a bad character"))
self._test_email_address_failures(SESDomainEndsWithDotError(554, "Email address ends with a dot"))
course_image = u'在淡水測試.jpg' self.course = CourseFactory.create(course_image=course_image)
self._create_students(num_emails - 1)
self.assertTrue(retry.called) (__, kwargs) = retry.call_args exc = kwargs['exc'] self.assertIsInstance(exc, SMTPDataError)
with self.assertRaisesRegexp(ValueError, r"(?i)course not found"): perform_delegate_email_batches(entry.id, course_id, task_input, "action_name")
SINGLE_EMAIL_FAILURE_ERRORS = (
LIMITED_RETRY_ERRORS = ( SMTPConnectError, SMTPServerDisconnected, AWSConnectionError, )
BULK_EMAIL_FAILURE_ERRORS = (
user_id = entry.requester.id task_id = entry.task_id
course = get_course(course_id)
targets = email_obj.targets.all() global_email_context = _get_course_email_context(course)
if total_recipients <= settings.BULK_EMAIL_JOB_SIZE_THRESHOLD: routing_key = settings.BULK_EMAIL_ROUTING_KEY_SMALL_JOBS
log.info("Send-email task %s for email %s: succeeded", current_task_id, email_id) update_subtask_status(entry_id, current_task_id, new_subtask_status)
log.warning("Send-email task %s for email %s: being retried", current_task_id, email_id)
log.info("Send-email task %s for email %s: returning status %s", current_task_id, email_id, new_subtask_status) return new_subtask_status.to_dict()
num_optout = len(optouts) to_list = [recipient for recipient in to_list if recipient['email'] not in optouts] return to_list, num_optout
course_name = re.sub(r"[^\w.-]", '_', course_id.course)
__, encoded_from_addr = forbid_multi_line_headers('from', from_addr, 'utf-8')
escaped_encoded_from_addr = escape(encoded_from_addr) if len(escaped_encoded_from_addr) >= 320 and truncate: from_addr = format_address(course_name)
parent_task_id = InstructorTask.objects.get(pk=entry_id).task_id task_id = subtask_status.task_id total_recipients = len(to_list) recipient_num = 0 total_recipients_successful = 0 total_recipients_failed = 0 recipients_info = Counter()
from_addr = course_email.from_addr if course_email.from_addr else \ _get_source_address(course_email.course_id, course_title)
course_email_template = course_email.get_template() try: connection = get_connection() connection.open()
email_context = {'name': '', 'email': ''} email_context.update(global_email_context)
plaintext_msg = course_email_template.render_plaintext(course_email.text_message, email_context) html_msg = course_email_template.render_htmltext(course_email.html_message, email_context)
email_msg = EmailMultiAlternatives( course_email.subject, plaintext_msg, from_addr, [email], connection=connection ) email_msg.attach_alternative(html_msg, 'text/html')
recipients_info[email] += 1 to_list.pop()
subtask_status.increment(retried_nomax=1, state=RETRY) return _submit_for_retry( entry_id, email_id, to_list, global_email_context, exc, subtask_status, skip_retry_max=True )
subtask_status.increment(failed=num_pending, state=FAILURE) return subtask_status, exc
subtask_status.increment(state=SUCCESS) return subtask_status, None
connection.close()
countdown = ((2 ** retry_index) * base_delay) * random.uniform(.75, 1.25)
update_subtask_status(entry_id, task_id, subtask_status)
try: return self.matches[state] except IndexError: return None
current_commit_id = get_commit_id(def_ms.courses[reload_dir]) log.debug('commit_id="%s"', commit_id) log.debug('current_commit_id="%s"', current_commit_id)
pass
if not consumer: consumer = LtiConsumer.objects.get( consumer_key=consumer_key, )
if instance_guid and not consumer.instance_guid: consumer.instance_guid = instance_guid consumer.save() return consumer
lti_user = create_lti_user(lti_user_id, lti_consumer)
switch_user(request, lti_user, lti_consumer)
pass
raise PermissionDenied()
REQUIRED_PARAMETERS = [ 'roles', 'context_id', 'oauth_version', 'oauth_consumer_key', 'oauth_signature', 'oauth_signature_method', 'oauth_timestamp', 'oauth_nonce', 'user_id' ]
params = get_required_parameters(request.POST) if not params: return HttpResponseBadRequest() params.update(get_optional_parameters(request.POST))
try: lti_consumer = LtiConsumer.get_or_supplement( params.get('tool_consumer_instance_guid', None), params['oauth_consumer_key'] ) except LtiConsumer.DoesNotExist: return HttpResponseForbidden()
if not SignatureValidator(lti_consumer).verify(request): return HttpResponseForbidden()
authenticate_lti_user(request, params['user_id'], lti_consumer)
store_outcome_parameters(params, request.user, lti_consumer)
from courseware.views.views import render_xblock return render_xblock(request, unicode(usage_key), check_if_enrolled=False)
headers = {"Content-Type": request.META['CONTENT_TYPE']} result, __ = self.endpoint.validate_request(url, method, body, headers) return result
usage_key = request_params['usage_key'] course_key = request_params['course_key']
outcomes, __ = OutcomeService.objects.get_or_create( lis_outcome_service_url=result_service, lti_consumer=lti_consumer )
response = None log.exception("Outcome Service: Error when sending result.")
if response.status_code != 200: log.error( "Outcome service response: Unexpected status code %s", response.status_code ) return False
from __future__ import unicode_literals
from __future__ import unicode_literals
SignatureValidator.verify = MagicMock(return_value=True) self.consumer = models.LtiConsumer( consumer_name='consumer', consumer_key=LTI_DEFAULT_PARAMS['oauth_consumer_key'], consumer_secret='secret' ) self.consumer.save()
self.assertIn( 'oauth_body_hash="00hq6RNueFa8QiEjhep5cJRHWAI%3D"', prepped_req.headers['Authorization'] )
assignments = outcomes.get_assignments_for_problem( problem_descriptor, user_id, course_key ) for assignment in assignments: assignment.version_number += 1 assignment.save() return assignments
context = { "request": get_request_or_stub() }
serialized_course_team['pk'] = self.course_team.pk serialized_course_team.pop('membership', None)
serialized_course_team['content'] = { 'text': self.content_text() }
FIELD_BLACKLIST = ['last_activity_at', 'team_size']
sort_order = 'name' topics = get_alphabetical_topics(course)
topics_data = self._serialize_and_paginate( TopicsPagination, topics, request, BulkTeamCountTopicSerializer, {'course_id': course.id}, ) topics_data["sort_order"] = sort_order
serializer_ctx["request"] = request
paginator = pagination_cls() page = paginator.paginate_queryset(queryset, request)
serializer = serializer_cls(page, context=serializer_ctx, many=True)
authentication_classes = (OAuth2Authentication, SessionAuthentication) permission_classes = (permissions.IsAuthenticated,) serializer_class = CourseTeamSerializer
course_module = modulestore().get_course(course_key) if course_module is None: return Response(status=status.HTTP_404_NOT_FOUND) result_filter.update({'course_id': course_key})
queryset = queryset.order_by('name')
'user_message': _(u"The ordering {ordering} is not supported").format(ordering=order_by_input),
if not modulestore().has_course(course_key): return Response(status=status.HTTP_404_NOT_FOUND)
page_kwarg = self.kwargs.get(self.paginator.page_query_param) page_query_param = self.request.query_params.get(self.paginator.page_query_param) return page_kwarg or page_query_param or 1
memberships = list(CourseTeamMembership.get_memberships(team_id=team_id))
course_module = modulestore().get_course(course_id)
'user_message': _(u"The ordering {ordering} is not supported").format(ordering=ordering),
course_module = modulestore().get_course(course_id) if course_module is None: return Response(status=status.HTTP_404_NOT_FOUND)
from ...search_indexes import CourseTeamIndexer
from __future__ import unicode_literals
self.user = UserFactory.create(password=self.test_password) self.teams_url = reverse('teams_dashboard', args=[self.course.id])
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id) self.client.login(username=self.user.username, password=self.test_password)
with self.assertNumQueries(18): self.client.get(self.teams_url)
for topic_id in range(self.NUM_TOPICS): team = CourseTeamFactory.create( name=u"Team for topic {}".format(topic_id), course_id=self.course.id, topic_id=topic_id, )
team.add_user(self.user)
with self.assertNumQueries(24): self.client.get(self.teams_url)
course_one_team = CourseTeamFactory.create(name="Course one team", course_id=self.course.id, topic_id=1)
course_one_team.add_user(self.user)
response = self.client.get(course_one_teams_url) self.assertIn('"teams": {"count": 1', response.content)
cls.create_and_enroll_student( courses=[cls.test_course_1, cls.test_course_2], username='student_enrolled_both_courses_other_team' )
cls.create_and_enroll_student( courses=[cls.test_course_2], username='student_enrolled_public_profile' ) profile = cls.users['student_enrolled_public_profile'].profile profile.year_of_birth = 1970 profile.save()
cls.create_and_enroll_student( courses=[cls.test_course_2], username='student_enrolled_other_course_not_on_team' )
team_list = self.get_teams_list(user=user, expected_status=200, data=course_one_data) self.assertEqual(team_list['count'], 0)
self.solar_team.add_user(self.users[user])
team_list = self.get_teams_list(user=user, expected_status=200, data=course_one_data) self.assertEqual(team_list['count'], 1)
team_list = self.get_teams_list(user=user, expected_status=200, data=course_two_data) self.assertEqual(team_list['count'], 0)
self.verify_expected_team_id(new_teams[0], 'the-best-team') self.verify_expected_team_id(new_teams[1], 'the-best-team') self.assertNotEqual(new_teams[0]['id'], new_teams[1]['id'])
self.verify_expected_team_id(new_teams[2], 'a-really-long-team-n')
self.post_create_membership( 200, self.build_membership_data(user, self.solar_team), user=user )
self.verify_expected_team_id(team, 'fully-specified-team') del team['id']
del team['date_created'] del team['discussion_topic_id']
team_membership = team['membership'] del team['membership']
self.assertEqual(len(team_membership), 1) member = team_membership[0]['user'] self.assertEqual(member['username'], creator)
team = self.post_create_team(data=self.build_team_data( name="New team", course=self.test_course_1, description="Another fantastic team", ), user=user)
result = self.get_team_detail(self.solar_team.team_id, 200, {'expand': 'user'}) self.verify_expanded_private_user(result['membership'][0]['user'])
serializer = None
self.assertIsNotNone(current_last_activity)
self.assertEqual(self.team_membership11.last_activity_at, current_last_activity)
PLATFORM_CLASSES = {IOS.NAME: IOS, Android.NAME: Android}
import datetime
courses = [ course_with_prereq, CourseFactory.create(start=self.NEXT_WEEK, mobile_available=True), CourseFactory.create(visible_to_staff_only=True, mobile_available=True), CourseFactory.create(start=self.LAST_WEEK, mobile_available=True, visible_to_staff_only=False), ]
for course in courses: self.enroll(course.id)
return self.client.patch(url, data=kwargs.get('data', None))
self.api_response(data={"last_visited_module_id": unicode(initial_unit.location)})
return self._get_course_info(request, course)
return self._get_course_info(request, course)
'id': course_id, 'name': course_overview.display_name, 'number': course_overview.display_number_with_default, 'org': course_overview.display_org_with_default,
'start': course_overview.start, 'start_display': course_overview.start_display, 'start_type': course_overview.start_type, 'end': course_overview.end,
'subscription_id': course_overview.clean_id(padding_char='_'),
'courseware_access': has_access( request.user, 'load_mobile', course_overview ).to_json(),
MobileApiConfig(video_profiles="mobile_low,mobile_high,youtube").save()
course_outline = self.api_response().data course_outline[0]['summary'].pop("id") self.assertEqual(course_outline[0]['summary'], expected_output)
MobileApiConfig(video_profiles="mobile_low,youtube").save()
MobileApiConfig(video_profiles="youtube,mobile_high").save()
add_user_to_cohort(cohorts[cohort_index], self.user.username)
remove_user_from_cohort(cohorts[cohort_index], self.user.username)
video_outline = self.api_response().data self.assertEqual(len(video_outline), 0)
self.user.is_staff = True self.user.save() video_outline = self.api_response().data self.assertEqual(len(video_outline), 2)
'name': block.display_name_with_default_escaped, 'category': block.category, 'id': unicode(block.location)
video_data = local_cache['course_videos'].get(video_descriptor.edx_video_id, {})
default_encoded_video = {}
elif video_descriptor.html5_sources: video_url = video_descriptor.html5_sources[0] else: video_url = video_descriptor.source
duration = video_data.get('duration', None) size = default_encoded_video.get('file_size', 0)
transcripts_info = video_descriptor.get_transcripts_info() transcript_langs = video_descriptor.available_translations(transcripts_info, verify_assets=False)
from __future__ import unicode_literals
from __future__ import unicode_literals
from datetime import timedelta
self.login_and_enroll() self.logout()
other = UserFactory.create() self.client.login(username=other.username, password='test') self.enroll() self.logout()
self.login() self.api_response(expected_response_code=404, username=other.username)
if role: role(self.course.id).add_users(self.user)
response = self.api_response()
self.assertNotIn("\"/static/", response.content)
underlying_updates = modulestore().get_item(updates_usage_key) underlying_content = underlying_updates.items[0]['content'] if new_format else underlying_updates.data self.assertIn("\"/static/", underlying_content)
response = self.api_response() self.assertNotIn('\'/static/', response.data['handouts_html'])
response = self.api_response() self.assertIn("/courses/{}/jump_to_id/".format(self.course.id), response.data['handouts_html'])
response = self.api_response() self.assertIn("/courses/{}/".format(self.course.id), response.data['handouts_html'])
return Response({'handouts_html': None})
msg += _('Failed in authenticating {username}, error {error}\n').format( username=euser, error=err ) continue
msg = _('All ok!')
msg += _('Email address must end in {domain}').format(domain="@{0}".format(email_domain)) return msg
output = StringIO.StringIO() import_log_handler = logging.StreamHandler(output) import_log_handler.setLevel(logging.DEBUG)
for logger in loggers: logger.setLevel(logging.NOTSET) logger.removeHandler(import_log_handler)
_('Git Commit'), _('Last Change'), _('Last Editor')],
mongo_db = { 'host': 'localhost', 'user': '', 'password': '', 'db': 'xlog', }
if not request.user.is_staff: raise Http404 cilset = CourseImportLog.objects.order_by('-created')
parser.add_argument('repository_url') parser.add_argument('--directory_path', action='store') parser.add_argument('--repository_branch', action='store')
if not os.path.isdir(self.git_repo_dir / 'edx4edx'): os.mkdir(self.git_repo_dir / 'edx4edx')
call_command('git_add_course', self.TEST_REPO, directory_path=self.git_repo_dir / 'edx4edx_lite', repository_branch=self.TEST_BRANCH)
if not os.path.isdir(repo_dir): os.mkdir(repo_dir) self.addCleanup(shutil.rmtree, repo_dir)
with self.assertRaises(GitImportErrorRemoteBranchMissing): git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', 'asdfasdfasdf')
git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', self.TEST_BRANCH) def_ms = modulestore() self.assertIsNotNone(def_ms.get_course(self.TEST_BRANCH_COURSE))
git_import.add_repo(self.TEST_REPO, repo_dir / 'edx4edx_lite', self.TEST_BRANCH)
repo_dir = self.git_repo_dir if not os.path.isdir(repo_dir): os.mkdir(repo_dir) self.addCleanup(shutil.rmtree, repo_dir)
output = StringIO.StringIO() test_log_handler = logging.StreamHandler(output) test_log_handler.setLevel(logging.DEBUG) glog = git_import.log glog.addHandler(test_log_handler)
MESSAGE = _('The specified remote branch is not available.')
MESSAGE = _('Unable to switch to specified branch. Please check your branch name.')
output = StringIO.StringIO() import_log_handler = logging.StreamHandler(output) import_log_handler.setLevel(logging.DEBUG)
for logger in loggers: logger.setLevel(logging.NOTSET) logger.removeHandler(import_log_handler)
mongouri = 'mongodb://{user}:{password}@{host}:{port}/{db}'.format(**mongo_db)
course = def_ms.courses.get(course_path, None)
course = def_ms.get_course(SlashSeparatedCourseKey('MITx', 'edx4edx', 'edx4edx'))
response = self._add_edx4edx() self.assertIn(GitImportErrorNoDir(settings.GIT_REPO_DIR).message, response.content.decode('UTF-8'))
response = self.client.get(reverse('sysadmin_courses')) self.assertNotRegexpMatches(response.content, table_re)
response = self._add_edx4edx() self.assertRegexpMatches(response.content, table_re)
self.assertIn('/gitlogs/MITx/edx4edx/edx4edx', response.content)
import_logs = CourseImportLog.objects.all() import_logs.delete()
def_ms = modulestore() course = def_ms.get_course(SlashSeparatedCourseKey('MITx', 'edx4edx', 'edx4edx')) CourseStaffRole(course.id).add_users(self.user)
from django.core.cache import cache from django.test.utils import override_settings from lang_pref import LANGUAGE_KEY
from edx_oauth2_provider.tests import IDTokenTestCase, UserInfoTestCase
cache.clear()
CourseFactory.create(emit_signals=True)
value = anonymous_id_for_user(data['user'], None) return value
language = UserPreference.get_value(data['user'], LANGUAGE_KEY)
if not language: language = settings.LANGUAGE_CODE
if values is not None: course_ids = list(set(course_ids) & set(values))
def _get_courses_with_access_type(self, user, access_type):
if not GlobalStaff().has_user(user): course_keys = [course_key for course_key in course_keys if has_access(user, access_type, course_key)]
if data.get('essential'): return super(IDTokenHandler, self).claim_instructor_courses(data) else: return None
if data.get('essential'): return super(IDTokenHandler, self).claim_staff_courses(data) else: return None
content_type, __ = mimetypes.guess_type(template)
resp = self.client.get(url, HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME) self.assertContains(resp, settings.MICROSITE_CONFIGURATION['test_microsite']['email_from_address'])
data = dict(self.data.items()) self.cleaned_data['confirmed'] = data['confirmed'] = 'true' self.data = data is_valid = False
from .index import * from .certificate import * from .enrollments import * from .refund import * from .programs import IssueProgramCertificatesView
import logging
return login_required(inner)
SupportStaffRole().remove_users(self.admin) response = self.client.get('/support/') self.assertTrue(response.status_code, 302)
self.client.logout() response = self.client.get(url)
redirect_url = "{login_url}?next={original_url}".format( login_url=reverse("signin_user"), original_url=url, ) self.assertRedirects(response, redirect_url)
for url_name in self.EXPECTED_URL_NAMES: self.assertContains(response, reverse(url_name))
response = self.client.get(reverse("support:certificates")) self.assertContains(response, "userFilter: ''")
if 'course_id' in data and data['course_id'] is None:
from django.core.urlresolvers import reverse from django.test import TestCase import mock from edx_oauth2_provider.tests.factories import AccessTokenFactory, ClientFactory
url(r'^{}/all_sequential_open_distrib$'.format(settings.COURSE_ID_PATTERN), 'class_dashboard.views.all_sequential_open_distrib', name="all_sequential_open_distrib"),
url(r'^{}/problem_grade_distribution/(?P<section>\d+)$'.format(settings.COURSE_ID_PATTERN), 'class_dashboard.views.section_problem_grade_distrib', name="section_problem_grade_distrib"),
url(r'^get_students_opened_subsection$', 'class_dashboard.dashboard_data.get_students_opened_subsection', name="get_students_opened_subsection"),
url(r'^get_students_problem_grades$', 'class_dashboard.dashboard_data.get_students_problem_grades', name="get_students_problem_grades"),
url(r'^post_metrics_data_csv_url', 'class_dashboard.dashboard_data.post_metrics_data_csv', name="post_metrics_data_csv"),
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_sequential_open_distrib(course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_problem_grade_distrib(course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_id) if has_instructor_access_for_class(request.user, course_key): try: data = dashboard_data.get_d3_section_grade_distrib(course_key, section)
MAX_SCREEN_LIST_LENGTH = 250
for row in db_query: curr_problem = course_id.make_usage_key_from_deprecated_string(row['module_state_key'])
if curr_problem in prob_grade_distrib: prob_grade_distrib[curr_problem]['grade_distrib'].append((row['grade'], row['count_grade']))
total_student_count[curr_problem] = total_student_count.get(curr_problem, 0) + row['count_grade']
db_query = models.StudentModule.objects.filter( course_id__exact=course_id, module_type__exact="sequential", ).values('module_state_key').annotate(count_sequential=Count('module_state_key'))
sequential_open_distrib = {} for row in db_query: row_loc = course_id.make_usage_key_from_deprecated_string(row['module_state_key']) sequential_open_distrib[row_loc] = row['count_sequential']
for row in db_query: row_loc = course_id.make_usage_key_from_deprecated_string(row['module_state_key']) if row_loc not in prob_grade_distrib: prob_grade_distrib[row_loc] = { 'max_grade': 0, 'grade_distrib': [], }
course = modulestore().get_course(course_id, depth=4)
if child.location.category == 'problem': c_problem += 1 stack_data = []
label = "P{0}.{1}.{2}".format(c_subsection, c_unit, c_problem)
if child.location in prob_grade_distrib:
problem_info = prob_grade_distrib[child.location]
problem_name = own_metadata(child).get('display_name', '')
student_count_percent = 0 if total_student_count.get(child.location, 0) > 0: student_count_percent = count_grade * 100 / total_student_count[child.location]
stack_data.append({ 'color': percent, 'value': count_grade, 'tooltip': tooltip, 'module_url': child.location.to_deprecated_string(), })
course = modulestore().get_course(course_id, depth=2)
for section in course.get_children(): curr_section = {} curr_section['display_name'] = own_metadata(section).get('display_name', '') data = [] c_subsection = 0
for subsection in section.get_children(): c_subsection += 1 subsection_name = own_metadata(subsection).get('display_name', '')
tooltip = { 'type': 'subsection', 'num_students': num_students, 'subsection_num': c_subsection, 'subsection_name': subsection_name }
course = modulestore().get_course(course_id, depth=4)
grade_distrib = get_problem_set_grade_distrib(course_id, problem_set)
for problem in problem_set: stack_data = []
for student in students[0:MAX_SCREEN_LIST_LENGTH + 1]: results.append({ 'name': student['student__profile__name'], 'username': student['student__username'], })
del results[-1] max_exceeded = True
filename = sanitize_filename(' '.join(tooltip.split(' ')[3:]))
del results[-1] max_exceeded = True
if data_type == 'subsection': for tooltip_dict in tooltips[index]: num_students = tooltip_dict['num_students'] subsection = tooltip_dict['subsection_name'] results.append(['', subsection, num_students])
results.append(['', label, problem_name, count_grade, student_count_percent, percent])
self.assertEquals(2, len(response_results)) self.assertEquals(True, response_max_exceeded)
self.assertEquals(2, len(response_results)) self.assertEquals(True, response_max_exceeded)
self.assertEquals(USER_COUNT + 1, len(response.content.splitlines()))
self.assertEquals(3, len(response.content.splitlines()))
self.assertEquals(4, len(response.content.splitlines()))
SUBSCRIBE_BATCH_SIZE = 1000
for batch in chunk(formated_data, SUBSCRIBE_BATCH_SIZE): result = mailchimp.listBatchSubscribe(id=list_id, batch=batch, double_optin=False, update_existing=True)
segments = mailchimp.listStaticSegments(id=list_id) for seg in segments: if seg['name'].startswith('random'): mailchimp.listStaticSegmentDel(id=list_id, seg_id=seg['id'])
emails = list(emails)
if DEBUG_ACCESS: log.debug(*args, **kwargs)
return start
def get_context(self): context = super(TodaysDate, self).get_context() context['date'] = '' return context
if enrollment_mode is None and is_active is None: return True
return is_active and enrollment_mode in CourseMode.UPSELL_TO_VERIFIED_MODES
inheritable = InheritanceMixin.fields.keys() if name in inheritable: for ancestor in _lineage(block): if self.get_override(ancestor, name) is not NOTSET: return False
module_state_key = LocationKeyField(max_length=255, db_index=True, db_column='module_id') student = models.ForeignKey(User, db_index=True)
state = models.TextField(null=True, blank=True)
'student_id': self.student_id, 'module_state_key': self.module_state_key, 'state': str(self.state)[:20],
created = models.DateTimeField(db_index=True) state = models.TextField(null=True, blank=True) grade = models.FloatField(null=True, blank=True) max_grade = models.FloatField(null=True, blank=True)
student_module__in=[module.id for module in student_modules]
if settings.FEATURES.get('ENABLE_READING_FROM_MULTIPLE_HISTORY_TABLES'): history_entries += StudentModuleHistory.objects.prefetch_related('student_module').filter( student_module__in=student_modules ).order_by('-id')
if not settings.FEATURES.get('ENABLE_CSMH_EXTENDED'): post_save.connect(save_history, sender=StudentModule)
field_name = models.CharField(max_length=64, db_index=True)
value = models.TextField(default='null')
usage_id = LocationKeyField(max_length=255, db_index=True)
module_type = BlockTypeKeyField(max_length=64, db_index=True)
required_content = milestones_helpers.get_required_content(course, user)
gated_content = gating_api.get_gated_content(course, user)
if not user_must_complete_entrance_exam(request, user, course): required_content = [content for content in required_content if not content == course.entrance_exam_id]
display_id = slugify(chapter.display_name_with_default_escaped) local_hide_from_toc = False if required_content: if unicode(chapter.location) not in required_content: local_hide_from_toc = True
if chapter.hide_from_toc or local_hide_from_toc: continue
if gated_content and unicode(section.location) in gated_content: continue if section.hide_from_toc: continue
timed_exam_attempt_context = None try: timed_exam_attempt_context = get_attempt_status_summary( user.id, unicode(course.id), unicode(section.location) )
log.exception(ex)
section_context.update({ 'proctoring': timed_exam_attempt_context, })
log.exception("Error in get_module") return None
score_bucket = get_score_bucket(grade, max_grade)
_fulfill_content_milestones( user, course_id, descriptor.location, )
SCORE_CHANGED.send( sender=None, points_possible=event['max_value'], points_earned=event['value'], user_id=user_id, course_id=unicode(course_id), usage_id=unicode(descriptor.location) )
module.runtime = inner_system inner_system.xmodule_instance = module
block_wrappers = []
block_wrappers.append(partial( replace_static_urls, getattr(descriptor, 'data_dir', None), course_id=course_id, static_asset_path=static_asset_path or descriptor.static_asset_path ))
block_wrappers.append(partial(replace_course_urls, course_id))
if position is not None: try: position = int(position) except (ValueError, TypeError): log.exception('Non-integer %r passed as position.', position) position = None
if has_access(user, u'staff', descriptor.location, course_id): system.error_descriptor_class = ErrorDescriptor else: system.error_descriptor_class = NonStaffErrorDescriptor
for key in ['xqueue_header', 'xqueue_body']: if key not in data: raise Http404
data.update({'queuekey': header['lms_key']})
try: instance.handle_ajax(dispatch, data) instance.save() except: log.exception("error processing ajax call") raise
if descriptor_orig_usage_key is not None: tracking_context['module']['original_usage_key'] = unicode(descriptor_orig_usage_key) tracking_context['module']['original_usage_version'] = unicode(descriptor_orig_version)
log.debug("No module %s for user %s -- access denied?", usage_key, user) raise Http404
files = request.FILES or {} error_msg = _check_files_limits(files) if error_msg: return JsonResponse({'success': error_msg}, status=413)
try: course_key = CourseKey.from_string(course_id) except InvalidKeyError: raise Http404
newrelic.agent.add_custom_parameter('course_id', unicode(course_key)) newrelic.agent.add_custom_parameter('org', unicode(course_key.org))
except NotFoundError: log.exception("Module indicating to user that request doesn't exist") raise Http404
except ProcessingError as err: log.warning("Module encountered an error while processing AJAX call", exc_info=True) return JsonResponse({'success': err.args[0]}, status=200)
except Exception: log.exception("error executing xblock handler") raise
if len(inputfiles) > settings.MAX_FILEUPLOADS_PER_INPUT: msg = 'Submission aborted! Maximum %d files may be submitted at once' % \ settings.MAX_FILEUPLOADS_PER_INPUT return msg
for inputfile in inputfiles:
from __future__ import division
cache_key = u"{}".format(course.id)
max_scores_cache.fetch_from_remote(field_data_cache.scorable_locations)
for section_format, sections in grading_context['graded_sections'].iteritems(): format_scores = [] for section in sections: section_descriptor = section['section_descriptor'] section_name = section_descriptor.display_name_with_default_escaped
if not should_grade_section: should_grade_section = any( descriptor.location.to_deprecated_string() in submissions_scores for descriptor in section['xmoduledescriptors'] )
if should_grade_section: scores = []
graded = False
course.set_grading_policy(course.grading_policy) grade_summary = course.grader.grade(totaled_scores, generate_random_scores=settings.GENERATE_PROFILE_SCORES)
grade_summary['percent'] = round(grade_summary['percent'] * 100 + 0.05) / 100
grade_summary['raw_scores'] = raw_scores
descending_grades = sorted(grade_cutoffs, key=lambda x: grade_cutoffs[x], reverse=True) for possible_grade in descending_grades: if percentage >= grade_cutoffs[possible_grade]: letter_grade = possible_grade break
max_scores_cache.fetch_from_remote(field_data_cache.scorable_locations)
gated_content = gating_api.get_gated_content(course, student)
for chapter_module in course_module.get_display_items(): if chapter_module.hide_from_toc: continue
with outer_atomic(): if section_module.hide_from_toc or unicode(section_module.location) in gated_content: continue
if weight is None or raw_total == 0: return (raw_correct, raw_total) return (float(raw_correct) * weight / raw_total, float(weight))
return (None, None)
if total is None: return (None, None) else: max_scores_cache.set(problem_descriptor.location, total)
request.session = {} gradeset = grade(student, request, course, keep_raw_scores) yield student, gradeset, ""
raise CoursewareAccessException(access_response)
if not ((user.id and CourseEnrollment.is_enrolled(user, course.id)) or has_access(user, 'staff', course)): raise UserNotEnrolled(course.id)
field_data_cache = FieldDataCache([], course.id, request.user) about_module = get_module( request.user, request, loc, field_data_cache, log_if_not_found=False, wrap_xmodule_display=False, static_asset_path=course.static_asset_path, course=course )
field_data_cache = FieldDataCache([], course.id, user)
key = lambda course: course.sorting_score courses = sorted(courses, key=key)
return u"//{}/{}/{}".format(settings.CMS_BASE, page, unicode(course.id))
return u"//{}/{}/{}".format(settings.CMS_BASE, page, block.location)
section_descriptor = modulestore().get_item(section_key, depth=3)
if name == 'due': return None if name == 'start' and block.category != 'course': return None
self._raise_unless_scope_is_allowed(key)
assert key.user_id == self.user.id
assert key.user_id == self.user.id
saved_fields.extend(key.field_name for key in set_many_data)
assert key.user_id == self.user.id
assert key.user_id == self.user.id
assert key.user_id == self.user.id
title = ugettext_noop("Textbooks") is_collection = True is_default = False
title = ugettext_noop('Discussion') priority = None is_default = False
course_tab_list += _get_dynamic_tabs(course, user) return course_tab_list
modulestore = XMLModuleStore( data_dir, default_class=None, source_dirs=source_dirs )
validators = ( traverse_tree, )
print "======== Roundtrip diff: ========="
correct_map = CorrectMap() if 'correct_map' in state_dict: correct_map.set_dict(state_dict['correct_map'])
correct = 0 for key in correct_map: correct += correct_map.get_npoints(key)
if cls.test_course_key not in [c.id for c in courses]: import_course_from_xml( store, ModuleStoreEnum.UserID.mgmt_command, DATA_DIR, XML_COURSE_DIRS, create_if_not_present=True )
if isinstance(module, DiscussionDescriptor) and 'discussion_id' not in items: items['discussion_id'] = module.discussion_id
inherited_metadata_filter_list = list(filtered_metadata.keys()) inherited_metadata_filter_list.extend(INHERITED_FILTER_LIST)
pipe_results = False if filename == '-': filename = mktemp() pipe_results = True
from __future__ import unicode_literals
if not user: user = AnonymousUser()
if isinstance(obj, CourseDescriptor): return _has_access_course(user, action, obj)
if isinstance(obj, XBlock): return _has_access_descriptor(user, action, obj, course_key)
raise TypeError("Unknown object type in has_access(): '{0}'" .format(type(obj)))
course_key = courselike.id
if user is not None and user.is_authenticated(): if CourseEnrollmentAllowed.objects.filter(email=user.email, course_id=course_key): return ACCESS_GRANTED
return ACCESS_GRANTED
merged_access = descriptor.merged_group_access if False in merged_access.values(): log.warning("Group access check excludes all students, access will be denied.", exc_info=True) return ACCESS_DENIED
user_groups = {} for partition, groups in partition_groups: user_groups[partition.id] = partition.scheme.get_group_for_user( course_key, user, partition, )
if not all(user_groups.get(partition.id) in groups for partition, groups in partition_groups): return ACCESS_DENIED
return ACCESS_GRANTED
return has_access(user, action, xmodule.descriptor, course_key)
API_DATADOG_SAMPLE_RATE = 0.1
if state == {}: continue
finish_time = time() self._ddog_histogram(evt_time, 'get_many.blks_out', block_count) self._ddog_histogram(evt_time, 'get_many.response_time', (finish_time - evt_time) * 1000)
if self.user is not None and self.user.username == username: user = self.user else: user = User.objects.get(username=username)
return
student_module.save(force_update=True)
if created: self._ddog_increment(evt_time, 'set_many.state_created') else: self._ddog_increment(evt_time, 'set_many.state_updated')
self._ddog_histogram(evt_time, 'set_many.fields_in', len(state))
num_new_fields_set = num_fields_after - num_fields_before self._ddog_histogram(evt_time, 'set_many.fields_set', num_new_fields_set)
num_fields_updated = max(0, len(state) - num_new_fields_set) self._ddog_histogram(evt_time, 'set_many.fields_updated', num_fields_updated)
student_module.save(force_update=True)
finish_time = time() self._ddog_histogram(evt_time, 'delete_many.response_time', (finish_time - evt_time) * 1000)
if not history_entries: raise self.DoesNotExist()
if state is not None: state = json.loads(state)
if state == {}: state = None
REQUIREMENTS_DISPLAY_MODES = CourseMode.CREDIT_MODES + [CourseMode.VERIFIED]
if user_must_complete_entrance_exam(request, user, course): return redirect(reverse('courseware', args=[unicode(course.id)]))
if request.user.is_authenticated() and survey.utils.must_answer_survey(course, user): return redirect(reverse('course_survey', args=[unicode(course.id)]))
url_to_enroll = reverse(course_about, args=[course_id]) if settings.FEATURES.get('ENABLE_MKTG_SITE'): url_to_enroll = marketing_link('COURSES')
context['last_accessed_courseware_url'] = None if SelfPacedConfiguration.current().enable_course_home_improvements: context['last_accessed_courseware_url'] = get_last_accessed_courseware(course, request, user)
context['disable_student_access'] = True
return _("{currency_symbol}{price}").format(currency_symbol=currency_symbol, price=price)
return _('Free')
return redirect(reverse('about_course', args=[unicode(course_key)]))
in_cart = False reg_then_add_to_cart_link = ""
registration_price = CourseMode.min_course_price_for_currency( course_key, settings.PAID_COURSE_REGISTRATION_CURRENCY[0] ) course_price = get_cosmetic_display_price(course, registration_price) can_add_course_to_cart = _is_shopping_cart_enabled and registration_price
can_enroll = bool(has_access(request.user, 'enroll', course)) invitation_only = course.invitation_only is_course_full = CourseEnrollment.objects.is_course_full(course)
active_reg_button = not(registered or is_course_full or not can_enroll)
pre_requisite_courses = get_prerequisite_courses_display(course)
overview = CourseOverview.get_from_id(course.id)
if survey.utils.must_answer_survey(course, request.user): return redirect(reverse('course_survey', args=[unicode(course.id)]))
student = request.user
if not has_access_on_students_profiles: raise Http404 try: student = User.objects.get(id=student_id) except (ValueError, User.DoesNotExist): raise Http404
student = User.objects.prefetch_related("groups").get(id=student.id)
enrollment_mode, is_active = CourseEnrollment.enrollment_mode_for_user(student, course_key) show_generate_cert_btn = ( is_active and CourseMode.is_eligible_for_certificate(enrollment_mode) and certs_api.cert_generation_enabled(course_key) )
if not (settings.FEATURES.get("ENABLE_CREDIT_ELIGIBILITY", False) and is_credit_course(course_key)): return None
enrollment = CourseEnrollment.get_enrollment(student, course_key) if enrollment and enrollment.mode not in REQUIREMENTS_DISPLAY_MODES: return None
non_eligible_statuses = ['failed', 'declined']
elif any(requirement['status'] in non_eligible_statuses for requirement in requirement_statuses): eligibility_status = "not_eligible"
else: eligibility_status = "partial_eligible"
if (student_username != request.user.username) and (not staff_access): raise PermissionDenied
if not course.course_survey_name: return redirect(redirect_url)
try: course = get_course_with_access(request.user, 'load', course_key, check_if_enrolled=check_if_enrolled) except UserNotEnrolled: raise Http404("Course not found.")
block, _ = get_module_by_usage_id( request, unicode(course_key), unicode(usage_key), disable_staff_debug_info=True, course=course )
username = data['username'] if request.user.username != username: return HttpResponseForbidden()
return HttpResponseBadRequest(u'Could not parse request JSON.')
return HttpResponseBadRequest(u'Could not parse request course key.')
return HttpResponseBadRequest(u'The field {} is required.'.format(err.message))
return HttpResponse(status=status.HTTP_500_INTERNAL_SERVER_ERROR)
raise
self.request.user = self.effective_user
if not self._is_masquerading_as_student(): raise Http404('No {block_type} found with name {url_name}'.format( block_type=block_type, url_name=url_name, ))
self.section = modulestore().get_item(self.section.location, depth=None) self.field_data_cache.add_descriptor_descendents(self.section, depth=None)
self.section = get_module_for_descriptor( self.effective_user, self.request, self.section, self.field_data_cache, self.course_key, self.position, course=self.course, )
if self.section.default_tab: courseware_context['default_tab'] = self.section.default_tab
if position != seq_module.position: seq_module.position = position
if is_staff: user = User.objects.get(email=email) user.is_staff = True user.save()
self._login(staff_email, staff_password, should_succeed=False, err_msg_check="Your password has expired due to password policy on this account")
self._update_password(staff_email, "updated") self._login(staff_email, "updated")
resp = self.client.post('/password_reset_confirm/{0}-{1}/'.format(uidb36, token), { 'new_password1': 'bar', 'new_password2': 'bar' }, follow=True)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
self.assertIn( err_msg, resp.content )
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
user = User.objects.get(email=staff_email) token = default_token_generator.make_token(user) uidb36 = int_to_base36(user.id)
course = store.get_course(course_key) self.enroll(course, True)
items = store.get_items(course_key)
for descriptor in items:
store.get_items(SlashSeparatedCourseKey('abc', 'def', 'ghi'), qualifiers={'category': 'vertical'})
cls.course_hidden_visibility = CourseFactory.create( display_name='Hidden_course', org='TestMicrositeX', catalog_visibility=CATALOG_VISIBILITY_NONE, emit_signals=True, )
cls.course_with_visibility = CourseFactory.create( display_name='visible_course', org='TestMicrositeX', course="foo", catalog_visibility=CATALOG_VISIBILITY_CATALOG_AND_ABOUT, emit_signals=True, )
self.assertContains(resp, 'Robot_Super_Course')
self.assertContains(resp, 'visible_course')
self.assertNotContains(resp, 'Robot_Course_Outside_Microsite')
self.assertNotContains(resp, 'Hidden_course')
self.assertContains(resp, 'This is a Test Microsite footer')
self.assertNotContains(resp, '<section class="university-partners university-partners2x6">')
self.assertNotContains(resp, 'Explore free courses from')
self.assertNotContains(resp, 'Robot_Super_Course')
self.assertContains(resp, 'Robot_Course_Outside_Microsite')
self.assertNotContains(resp, 'This is a Test Microsite footer')
resp = self.client.get(reverse('dashboard'), HTTP_HOST=settings.MICROSITE_TEST_HOSTNAME) self.assertContains(resp, 'Robot_Super_Course') self.assertNotContains(resp, 'Robot_Course_Outside_Microsite')
resp = self.client.get(reverse('dashboard')) self.assertNotContains(resp, 'Robot_Super_Course') self.assertContains(resp, 'Robot_Course_Outside_Microsite')
self.coach = AdminFactory.create(password="test") self.client.login(username=self.coach.username, password="test")
role = CourseCcxCoachRole(self.course.id) role.add_users(self.coach) self.request_factory = RequestFactory()
self.assertTrue(access.has_ccx_coach_role(self.coach, ccx_locator))
self.setup_user() self.assertFalse(access.has_ccx_coach_role(self.user, ccx_locator))
CourseEnrollment.enroll(student, ccx_locator)
request = self.request_factory.get(reverse('about_course', args=[unicode(ccx_locator)])) request.user = student mako_middleware_process_request(request)
CourseEnrollmentFactory(user=self.student, course_id=self.course.id)
self.assertTrue( bool(access.has_staff_access_to_preview_mode(self.global_staff, obj=self.course, course_key=course_key)) )
CourseEnrollmentFactory(user=self.student, course_id=self.course.id)
user = StaffFactory.create(course_key=course.id) self.assertTrue(access._has_access_course(user, 'enroll', course))
fulfill_course_milestone(pre_requisite_course.id, user) self.assertTrue(access._has_access_course(user, 'view_courseware_with_prerequisites', course))
self._install_masquerade(self.course_staff) self.assertEqual( 'student', access.get_user_role(self.course_staff, self.course_key) )
self._install_masquerade(self.course_instructor) self.assertEqual( 'student', access.get_user_role(self.course_instructor, self.course_key) )
if user_attr_name == 'user_anonymous': user = AnonymousUserFactory() else: user = getattr(self, user_attr_name) user = User.objects.get(id=user.id)
num_queries = 1
num_queries = 2
UserCourseTagFactory( user=self.student, course_id=self.course.id, key='xblock.partition_service.partition_{0}'.format(self.partition.id), value=str(user_tag) )
self.assertIn('<button class="{} inactive progress-0 nav-item"'.format(self.ICON_CLASSES[user_tag]), content) for tooltip in self.TOOLTIPS[user_tag]: self.assertIn(tooltip, content)
for visible in self.VISIBLE_CONTENT[user_tag]: self.assertIn(visible, content)
VISIBLE_CONTENT = [ ['class=&#34;problems-wrapper'], ['Some HTML for group 1'] ]
super(TestSplitTestVert, self).setUp()
VISIBLE_CONTENT = [ ['class=&#34;problems-wrapper'], ['Some HTML for group 1'] ]
super(TestVertSplitTestVert, self).setUp()
course.position = 2 course.save()
import json from functools import partial import factory from factory.django import DjangoModelFactory
with super(TestNavigation, cls).setUpClassAndTestData(): cls.test_course = CourseFactory.create() cls.test_course_proctored = CourseFactory.create() cls.course = CourseFactory.create()
resp = self.client.get(reverse('dashboard')) self.assertEquals(resp.status_code, 200)
time.sleep(2)
self.assertRedirects(resp, settings.LOGIN_REDIRECT_URL + '?next=' + reverse('dashboard'))
self.assertIn(REG_STR, resp.content)
self.xml_data = "about page 463139"
resp = self.client.get(url) self.assertEqual(resp.status_code, 200) self.assertIn("Course is full", resp.content)
result = self.enroll(self.course) self.assertFalse(result)
self.assertNotIn(REG_STR, resp.content)
self.assertNotIn(REG_STR, resp.content)
self.assertIn(REG_STR, resp.content)
now = datetime.datetime.now(pytz.UTC) tomorrow = now + datetime.timedelta(days=1) nextday = tomorrow + datetime.timedelta(days=1)
self.assertNotIn(REG_STR, resp.content)
self.assertNotIn('<span class="important-dates-item-text">$10</span>', resp.content)
CourseEnrollment.enroll(self.user, self.course.id)
self.assertIn('<span class="important-dates-item-text">$10</span>', resp.content)
CourseEnrollment.enroll(self.user, course.id)
self.coach = coach = AdminFactory.create(password="test") self.client.login(username=coach.username, password="test")
ccx = CcxFactory(course_id=self.course.id, coach=self.coach) ccx_locator = CCXLocator.from_course_locator(self.course.id, unicode(ccx.id))
self.assertGreater(exam_score * 100, 50)
chaos_user = UserFactory() locked_toc = self._return_table_of_contents() for toc_section in self.expected_locked_toc: self.assertIn(toc_section, locked_toc)
locked_toc = self._return_table_of_contents() for toc_section in self.expected_locked_toc: self.assertIn(toc_section, locked_toc)
self.client.logout() staff_user = StaffFactory(course_key=self.course.id) staff_user.is_staff = True self.client.login(username=staff_user.username, password='test')
self.request.user = staff_user unlocked_toc = self._return_table_of_contents() for toc_section in self.expected_unlocked_toc: self.assertIn(toc_section, unlocked_toc)
self._assert_chapter_loaded(self.course, self.chapter)
module = get_module( user, request, problem.scope_ids.usage_id, field_data_cache, )._xmodule module.system.publish(problem, 'grade', grade_dict)
self.assertEqual(orphan_sequential.location.block_type, root.location.block_type) self.assertEqual(orphan_sequential.location.block_id, root.location.block_id)
from django.test import TestCase from nose.plugins.attrib import attr
store.get_items(SlashSeparatedCourseKey('a', 'b', 'c'), qualifiers={'category': 'vertical'})
tab = tab_class(tab_dict=dict_tab)
self.assertEqual(tab.name, expected_name)
self.assertEqual(tab.link_func(self.course, self.reverse), expected_link)
self.assertEqual(tab.tab_id, expected_tab_id)
self.assertTrue(tab.validate(dict_tab)) if invalid_dict_tab: with self.assertRaises(xmodule_tabs.InvalidTabsException): tab.validate(invalid_dict_tab)
self.check_get_and_set_methods(tab)
self.check_tab_json_methods(tab)
self.check_tab_equality(tab, dict_tab)
return tab
tab_content = get_static_tab_contents(request, course, tab) self.assertIn(self.course.id.to_deprecated_string(), tab_content) self.assertIn('static_tab', tab_content)
self.xml_data = "static 463139" self.xml_url = "8e4cce2b4aaf4ba28b1220804619e41f"
instructor = InstructorFactory(course_key=self.course.id) self.client.logout() self.client.login(username=instructor.username, password='test')
unique_tab_types = [ CoursewareTab.type, CourseInfoTab.type, 'textbooks', 'pdf_textbooks', 'html_textbooks', ]
{'type': unique_tab_type}, {'type': unique_tab_type},
self.set_up_books(1)
self.course.tabs = self.all_valid_tab_list
for i, tab in enumerate(xmodule_tabs.CourseTabList.iterate_displayable( self.course, inline_collections=False )): self.assertEquals(tab.type, self.course.tabs[i].type)
self.assertIn( {'type': 'html_textbooks'}, list(xmodule_tabs.CourseTabList.iterate_displayable(self.course, inline_collections=False)), )
self.course.html_textbooks = [] self.assertNotIn( {'type': 'html_textbooks'}, list(xmodule_tabs.CourseTabList.iterate_displayable(self.course, inline_collections=False)), )
self.assertEquals(xmodule_tabs.CourseTabList.get_tab_by_type(self.course.tabs, tab.type), tab)
self.assertEquals(xmodule_tabs.CourseTabList.get_tab_by_id(self.course.tabs, tab.tab_id), tab)
CATEGORY = "vertical" DATA = '' METADATA = {} MODEL_DATA = {'data': '<some_module></some_module>'}
modulestore().request_cache = None modulestore().metadata_inheritance_cache_subsystem = None
self.users = [ UserFactory.create() for dummy0 in range(self.USER_COUNT) ]
self.release_languages('fa')
response = self.client.get('/?preview-lang=fa-ir') self.assert_tag_has_attr(response.content, "html", "lang", "fa-ir")
response = self.client.get('/?clear-lang') self.assert_tag_has_attr(response.content, "html", "lang", site_lang)
RegistrationFactory(user=self.user)
UserProfileFactory(user=self.user)
self.client = Client()
self.url = reverse('dashboard') self.site_lang = settings.LANGUAGE_CODE
self.release_languages('ar, es-419')
response = self.client.get(self.url) self.assert_tag_has_attr(response.content, "html", "lang", self.site_lang)
self.release_languages('ar, es-419')
_upload_sjson_file(good_sjson, self.item_descriptor.location)
_upload_file(self.srt_file, self.item_descriptor.location, os.path.split(self.srt_file.name)[1])
_upload_file(en_translation, self.item_descriptor.location, en_translation_filename)
_upload_file(self.srt_file, self.item_descriptor.location, uk_translation_filename)
request = Request.blank('/translation/uk') response = self.item.transcript(request=request, dispatch='translation/uk') self.assertEqual(response.status, '404 Not Found')
request = Request.blank('') response = self.item_descriptor.studio_transcript(request=request, dispatch='translation') self.assertEqual(response.status, '400 Bad Request')
request = Request.blank('') response = self.item_descriptor.studio_transcript(request=request, dispatch='translation/uk') self.assertEqual(response.status, '400 Bad Request')
with self.assertRaises(NotFoundError): self.item.get_transcript(transcripts)
self.item.youtube_id_1_0 = None with self.assertRaises(ValueError): self.item.get_transcript(transcripts)
self.course_key = SlashSeparatedCourseKey('edX', 'toy', '2012_Fall')
self.course = modulestore().get_course(self.course.id)
self.chapter = self.store.get_item(self.chapter.location)
for section in self.chapter.get_children(): section.visible_to_staff_only = True self.store.update_item(section, ModuleStoreEnum.UserID.test)
self.assertTrue(CourseEnrollment.is_enrolled(self.global_staff, self.course.id))
mako_middleware_process_request(request) response = views.course_about(request, unicode(course.id)) self.assertEqual(response.status_code, 200) self.assertNotIn(in_cart_span, response.content)
mako_middleware_process_request(request)
mock_user = MagicMock() mock_user.is_authenticated.return_value = False self.assertEqual(views.user_groups(mock_user), [])
self.assertEqual(views.get_cosmetic_display_price(self.course, registration_price), "$99")
self.assertEqual(views.get_cosmetic_display_price(self.course, registration_price), "$10")
self.assertEqual(views.get_cosmetic_display_price(self.course, registration_price), "Free")
self.verify_end_date('edX/toy/TT_2012_Fall')
self.verify_end_date("edX/test_end/2012_Fall", "Sep 17, 2015")
self.verify_end_date("edX/test_about_blob_end_date/2012_Fall", "Learning never ends")
admin = AdminFactory()
self.assertFalse('Invalid' in response.content)
admin = AdminFactory()
admin = AdminFactory.create()
state_client.set( username=admin.username, block_key=usage_key, state={'field_a': 'x', 'field_b': 'y'} )
self.assertContains(response, checkbox_html, html=True) self.assertContains(response, org_name_string)
self.assertNotContains(response, checkbox_html, html=True)
self.assertEqual(response.status_code, 200) self.assertIn('Financial Assistance Application', response.content)
self.assertIn(str(verified_course_audit_track), response.content) for course in ( non_verified_course, verified_course_verified_track, verified_course_deadline_passed, unenrolled_course ): self.assertNotIn(str(course), response.content)
request.user = self.user
mako_middleware_process_request(request)
self.assertEqual(response.status_code, 302) self.assertEqual( response.url, reverse('courseware', args=[course_id]) )
course = self.set_up_course(due_date_display_format=None) text = self.get_text(course) self.assertIn(self.time_with_tz, text)
course = self.set_up_course(due_date_display_format=u"") text = self.get_text(course) self.assertNotIn("due ", text)
mako_middleware_process_request(self.request) return views.progress(self.request, course_id=unicode(course.id), student_id=self.user.id).content
mako_middleware_process_request(self.request) self.request.user = self.user
self.assertIn("2013-SEPTEMBER-16", text)
self.assertIn("2015-JULY-17", text)
mako_middleware_process_request(self.request)
self.assertNotIn(malicious_code, resp.content)
self.course = CourseFactory.create(default_store=default_store)
course = CourseFactory.create(default_store=default_store) not_enrolled_user = UserFactory.create() self.request.user = AdminFactory.create()
CreditCourse.objects.create(course_key=course.id, enabled=True)
CreditProvider.objects.create( provider_id="ASU", enable_integration=True, provider_url="https://credit.example.com/request" )
set_credit_requirements(course.id, requirements)
CertificateGenerationConfiguration(enabled=True).save() resp = views.progress(self.request, course_id=unicode(self.course.id)) self.assertNotContains(resp, 'Request Certificate')
CertificateGenerationConfiguration(enabled=True).save()
certs_api.set_cert_generation_enabled(self.course.id, True)
certificates[0]['is_active'] = False self.store.update_item(self.course, self.user.id)
CertificateGenerationConfiguration(enabled=True).save()
certs_api.set_cert_generation_enabled(self.course.id, True)
self.assertFalse(views.is_course_passed(self.course, None, self.student, self.request))
self.assertTrue(views.is_course_passed(self.course, None, self.student, self.request))
self.assertFalse(views.is_course_passed(self.course, None, self.student, self.request))
self.assertTrue(views.is_course_passed(self.course, None, self.student, self.request))
resp = self.client.post('/courses/def/abc/in_valid/generate_user_cert') self.assertEqual(resp.status_code, HttpResponseBadRequest.status_code) self.assertIn("Course is not valid", resp.content)
resp = self.client.post('/courses/def/generate_user_cert') self.assertEqual(resp.status_code, 404)
has_children = False
mako_middleware_process_request(request)
mako_middleware_process_request(request)
mako_middleware_process_request(request)
self.assertIn("example_source.mp4", self.item_descriptor.render(STUDENT_VIEW).content)
'sources': [u'example.mp4', u'example.webm', u'http://www.meowmix.com'],
'sources': [u'example.mp4', u'example.webm'] + [video['url'] for video in encoded_videos],
cases = [ dict(case_data, edx_video_id=""), dict(case_data, edx_video_id="vid-v1:12345"), ]
result = self.get_result(allow_cache_miss) self.verify_result_with_val_profile(result)
result = self.get_result(allow_cache_miss) self.verify_result_with_fallback_and_youtube(result)
COURSE_SLUG = "100" COURSE_NAME = "test_course"
enrollment_exists = CourseEnrollment.objects.filter( user=self.user, course_id=self.course.id ).exists() self.assertFalse(enrollment_exists)
self.coach = coach = AdminFactory.create(password="test") self.client.login(username=coach.username, password="test")
ccx = CcxFactory(course_id=self.course.id, coach=self.coach) ccx_locator = CCXLocator.from_course_locator(self.course.id, unicode(ccx.id))
self.xml_data = "course info 463139"
self.assert_request_status_code(302, reverse('logout'))
user = User.objects.get(email=email) self.assertFalse(user.is_active) return user
url = reverse('activate', kwargs={'key': activation_key}) self.assert_request_status_code(200, url) self.assertTrue(User.objects.get(email=email).is_active)
response_dict = {(answer_key_prefix + k): v for k, v in responses.items()} resp = self.client.post(modx_url, response_dict)
multi_db = True COURSE_SLUG = "100" COURSE_NAME = "test_course"
self.refresh_course() return problem
if not hasattr(self, 'chapter'): self.chapter = ItemFactory.create( parent_location=self.course.location, category='chapter' )
self.refresh_course() return section
sections_list = [] for chapter in self.get_progress_summary(): sections_list.extend(chapter['sections'])
hw_section = next(section for section in sections_list if section.get('url_name') == hw_url_name) return [s.earned for s in hw_section['scores']]
multi_db = True
self.hw1_names = ['h1p1', 'h1p2'] self.hw2_names = ['h2p1', 'h2p2'] self.hw3_names = ['h3p1', 'h3p2']
student_module = StudentModule.objects.filter( course_id=self.course.id, student=self.student_user ) baseline = BaseStudentModuleHistory.get_history(student_module) self.assertEqual(len(baseline), 3)
self.show_question_answer('p1')
csmh = BaseStudentModuleHistory.get_history(student_module) self.assertEqual(len(csmh), 3)
max_scores_cache.fetch_from_remote([location_to_cache]) self.assertIsNone(max_scores_cache.get(location_to_cache)) self.check_grade_percent(0.33)
max_scores_cache.fetch_from_remote([location_to_cache]) self.assertIsNotNone(max_scores_cache.get(location_to_cache)) self.check_grade_percent(0.33)
mock_get_scores.assert_called_with( self.course.id.to_deprecated_string(), anonymous_id_for_user(self.student_user, self.course.id) )
self.submit_question_answer('H1P1', {'2_1': 'Correct', '2_2': 'Correct'}) self.check_grade_percent(0.25)
credit_course = CreditCourse.objects.create( course_key=self.course.id, enabled=True, )
CreditProvider.objects.create( provider_id="ASU", enable_integration=True, provider_url="https://credit.example.com/request", )
set_credit_requirements(self.course.id, requirements)
multi_db = True
self.refresh_course()
multi_db = True
self.correct_responses[name] = self.SCHEMATIC_CORRECT self.incorrect_responses[name] = self.SCHEMATIC_INCORRECT
self.refresh_course()
self.correct_responses[name] = expect self.incorrect_responses[name] = self.CUSTOM_RESPONSE_INCORRECT
self.refresh_course()
self.correct_responses[name] = self.COMPUTED_ANSWER_CORRECT self.incorrect_responses[name] = self.COMPUTED_ANSWER_INCORRECT
self.refresh_course()
empty_distribution = grades.answer_distributions(self.course.id)
self.submit_question_answer('p1', {'2_1': u'ⓤⓝⓘⓒⓞⓓⓔ'}) self.submit_question_answer('p2', {'2_1': 'Correct'})
self.submit_question_answer('p1', {'2_1': u'Correct'}) self.submit_question_answer('p2', {'2_1': u'Correct'})
self.submit_question_answer('p1', {'2_1': u'Correct'})
self.submit_question_answer('p1', {'2_1': 'Incorrect'})
empty_distribution = grades.answer_distributions(self.course.id)
self.submit_question_answer('p1', {'2_1': u'Correct'})
prb1 = StudentModule.objects.get( course_id=self.course.id, student=self.student_user )
self.submit_question_answer('p2', {'2_1': u'Incorrect'})
UserCourseTagFactory( user=self.student_user, course_id=self.course.id, key='xblock.partition_service.partition_{0}'.format(self.partition.id), value=str(user_partition_group) )
self.add_dropdown_to_section(vertical_1.location, 'H2P1_GROUP1', 1).location.html_id()
self.submit_question_answer('H1P1', {'2_1': 'Correct', '2_2': 'Incorrect'})
homework_1_score = 1.0 / 2 homework_2_score = (1.0 + 2.0) / 4 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
homework_1_score = 1.0 / 2 homework_2_score = 1.0 / 1 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
self.add_dropdown_to_section(vertical_1.location, 'H2P1_GROUP1', 1).location.html_id()
homework_1_score = 1.0 / 2 homework_2_score = 0.0 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
homework_1_score = 1.0 / 2 homework_2_score = 1.0 / 1 self.check_grade_percent(round((homework_1_score + homework_2_score) / 2, 2))
users_state = {}
users_state = self._get_users_state()
users_state_after_post = self._post_words(['word1', 'word2'])
users_state_before_fail = self._get_users_state()
users_state_after_post = self._post_words( ['word1', 'word2', 'word3'])
current_users_state = self._get_users_state() self._check_response(users_state_before_fail, current_users_state)
url = self._reverse_urls(['courseware'], course)[0] self.assert_request_status_code(302, url)
for url in urls: self.assert_request_status_code(404, url)
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) self.assert_request_status_code(200, url)
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) self.assert_request_status_code(200, url)
self.login(self.enrolled_user)
self._check_non_staff_light(self.course) self._check_non_staff_dark(self.course) self._check_non_staff_light(self.test_course) self._check_non_staff_dark(self.test_course)
self.enroll(self.course, True) self.enroll(self.test_course, True)
self._check_non_staff_light(self.test_course) self._check_non_staff_dark(self.test_course) self._check_staff(self.course)
self._check_staff(self.course) self._check_staff(self.test_course)
self.login(self.unenrolled_user) self.assertFalse(self.enroll(self.course)) self.assertTrue(self.enroll(self.test_course))
self.logout() self.login(self.instructor_user) self.assertTrue(self.enroll(self.course))
self.logout() self.login(self.global_staff_user) self.assertTrue(self.enroll(self.course))
self.assertFalse(has_access(self.normal_student, 'load', self.content, self.course.id))
self.assertTrue(has_access(self.beta_tester, 'load', self.content, self.course.id))
answer_objs = SurveyAnswer.objects.filter( user=self.user, form=self.survey )
primary_course = CourseFactory.create(org=primary, emit_signals=True) alternate_course = CourseFactory.create(org=alternate, emit_signals=True)
no_courses = get_courses(user, org=primary) self.assertEqual(no_courses, [])
microsite_courses = get_courses(user, org=alternate) self.assertTrue( all(course.org == alternate_course.org for course in microsite_courses) )
course_about = get_course_about_section(self.request, self.course, 'short_description') self.assertEqual(course_about, "A course about toys.")
self.verify_staff_debug_present(True)
self.update_masquerade(role='student') self.verify_staff_debug_present(False)
self.update_masquerade(role='staff') self.verify_staff_debug_present(True)
self.verify_show_answer_present(True)
self.update_masquerade(role='student') self.verify_show_answer_present(False)
self.update_masquerade(role='staff') self.verify_show_answer_present(True)
self.login_staff() response = self.get_course_info_page() self.assertEqual(response.status_code, 200) content = response.content self.assertIn("OOGIE BLOOGIE", content)
self.login_student() self.submit_answer('Correct', 'Correct') self.assertEqual(self.get_progress_detail(), u'2/2')
self.login_staff() self.assertEqual(self.get_progress_detail(), u'0/2')
self.update_masquerade(role='student', user_name=self.student_user.username) self.assertEqual(self.get_progress_detail(), u'2/2')
self.submit_answer('Correct', 'Incorrect') self.assertEqual(self.get_progress_detail(), u'1/2')
self.get_courseware_page() self.assertEqual(self.get_progress_detail(), u'2/2')
self.update_masquerade(role='staff') self.assertEqual(self.get_progress_detail(), u'0/2')
self.login_student() self.assertEqual(self.get_progress_detail(), u'2/2')
self.login_staff() content = self.get_course_info_page().content self.assertIn("OOGIE BLOOGIE", content)
self.update_masquerade(role='student', user_name=self.student_user.username) content = self.get_course_info_page().content self.assertIn("OOGIE BLOOGIE", content)
group_id, user_partition_id = get_masquerading_group_info(self.test_user, self.course.id) self.assertIsNone(group_id) self.assertIsNone(user_partition_id)
group_id, user_partition_id = get_masquerading_group_info(self.test_user, self.course.id) self.assertEqual(group_id, 1) self.assertEqual(user_partition_id, 0)
CreditCourse.objects.create(course_key=self.course.id, enabled=True)
self.user = UserFactory.create(username=self.USERNAME, password=self.PASSWORD) self.user.profile.name = self.USER_FULL_NAME self.user.profile.save()
self.enrollment = CourseEnrollmentFactory( user=self.user, course_id=self.course.id, mode="verified" )
response = self._get_progress_page()
credit_api.set_credit_requirement_status( self.user.username, self.course.id, "grade", "grade", status="satisfied", reason={"final_grade": 0.95} )
credit_api.set_credit_requirement_status( self.user.username, self.course.id, "reverification", "midterm", status="failed", reason={} )
classes = ('credit-eligibility', 'eligibility-heading') method = self.assertContains if is_requirement_displayed else self.assertNotContains
staticfiles.finders.get_finder.cache_clear()
self.assertContains(resp, "super-ugly") self.assertContains(resp, "This file is only for demonstration, and is horrendous!")
before_finders = list(settings.STATICFILES_FINDERS) before_dirs = list(settings.STATICFILES_DIRS)
import datetime import pytz
ItemFactory.create( parent=parent, category='discussion', display_name='released', start=self.now, )
ItemFactory.create( parent=parent, category='discussion', display_name='scheduled', start=self.future, )
self_paced_course, self_paced_section = self.setup_course(**course_options) beta_tester = BetaTesterFactory(course_key=self_paced_course.id)
self.assertTrue(self_paced_course.self_paced) self.assertEqual(self_paced_course.start, one_month_from_now) self.assertIsNone(self_paced_section.start)
self.assertFalse(has_access(self.non_staff_user, 'load', self_paced_course))
self.assertTrue(has_access(beta_tester, 'load', self_paced_course)) self.assertTrue(has_access(beta_tester, 'load', self_paced_section, self_paced_course.id))
modules = get_accessible_discussion_modules(course, self.non_staff_user) self.assertTrue( all(module.display_name == 'released' for module in modules) )
import unittest from nose.plugins.attrib import attr
with self.assertRaises(KeyError): data.get('block', 'foo')
multi_db = True
return 'problem'
@skip("Not supported by DjangoXBlockUserStateClient") def test_iter_blocks_deleted_block(self): pass
self.animal_partition.groups.pop() self.color_partition.groups.pop()
self.staff = StaffFactory.create(course_key=self.course.id)
return
self.set_group_access(self.chapter_location, {self.animal_partition.id: [self.dog_group.id]}) self.check_access(self.red_cat, self.vertical_location, False)
self.set_user_partitions(self.vertical_location, []) self.check_access(self.red_cat, self.vertical_location, True)
self.set_user_partitions(self.vertical_location, [split_test_partition, self.animal_partition]) self.check_access(self.red_cat, self.vertical_location, False)
expected_url = reverse( "about_course", args=[self.course.id.to_deprecated_string()] )
multi_db = True
with self.assertNumQueries(1): self.field_data_cache = FieldDataCache( [mock_descriptor([mock_field(Scope.user_state, 'a_field')])], course_id, self.user )
with self.assertNumQueries(0): self.assertEquals('a_value', self.kvs.get(user_state_key('a_field')))
with self.assertNumQueries(0): self.assertRaises(KeyError, self.kvs.get, user_state_key('not_a_field'))
for key in kv_dict: self.kvs.set(key, 'test_value')
multi_db = True
with self.assertNumQueries(0): self.field_data_cache = FieldDataCache([mock_descriptor()], course_id, self.user) self.kvs = DjangoKeyValueStore(self.field_data_cache)
with self.assertNumQueries(1): self.field_data_cache = FieldDataCache([self.mock_descriptor], course_id, self.user) self.kvs = DjangoKeyValueStore(self.field_data_cache)
with self.assertNumQueries(len(kv_dict)): self.kvs.set_many(kv_dict) for key in kv_dict: self.assertEquals(self.kvs.get(key), kv_dict[key])
self.mock_module = MagicMock() self.mock_module.id = 1 self.dispatch = 'score_update'
html = module.render(STUDENT_VIEW).content
self.assertIn('/courses/' + self.course_key.to_deprecated_string() + '/jump_to_id/vertical_test', html)
request.POST['queuekey'] = fake_key self.mock_module.handle_ajax.assert_called_once_with(self.dispatch, request.POST)
self.assertEquals(render.get_score_bucket(11, 10), 'incorrect') self.assertEquals(render.get_score_bucket(-1, 10), 'incorrect')
self.assertIs(descriptor._unwrapped_field_data, original_field_data) self.assertIsNot(descriptor._unwrapped_field_data, descriptor._field_data)
for user in [UserFactory(), UserFactory(), UserFactory()]: render.get_module_for_descriptor( user, request, descriptor, field_data_cache, course.id, course=course )
self.assertIsInstance(descriptor._field_data, LmsFieldData)
self.assertIsInstance( descriptor._field_data._authored_data._source, OverrideFieldData )
self.assertIs( descriptor._field_data._authored_data._source.fallback, descriptor._unwrapped_field_data )
self.mock_module = MagicMock() self.mock_module.id = 1 self.dispatch = 'score_update'
mock_file.name = name return mock_file
self.assertNotIn('proctoring', section_actual)
self.field_data_cache = FieldDataCache.cache_for_descriptor_descendents( self.course_key, self.request.user, self.toy_course, depth=2 )
self.assertEqual(html.count("</script>"), 1)
self.child_module = self._get_module(course.id, child_descriptor, child_descriptor.location)
PER_STUDENT_ANONYMIZED_DESCRIPTORS = set( class_ for (name, class_) in XModuleDescriptor.load_classes() if not issubclass(class_, PER_COURSE_ANONYMIZED_DESCRIPTORS) )
descriptor.bind_for_student = partial(xblock_class.bind_for_student, descriptor)
'5afe5d9bb03796557ee2614f5c9611fb', self._get_anonymous_id(CourseKey.from_string(course_id), descriptor_class)
'e3b0b940318df9c14be59acb08e78af5', self._get_anonymous_id(SlashSeparatedCourseKey('MITx', '6.00x', '2012_Fall'), descriptor_class)
'f82b5416c9f54b5ce33989511bb5ef2e', self._get_anonymous_id(SlashSeparatedCourseKey('MITx', '6.00x', '2013_Spring'), descriptor_class)
user2 = UserFactory.create() module.descriptor.bind_for_student(module.system, user2.id)
self.assertFalse(runtime.user_is_beta_tester) self.assertEqual(runtime.days_early_for_beta, 5)
def setUp(self): super(TestFilteredChildren, self).setUp() self.users = {number: UserFactory() for number in USER_NUMBERS}
if isinstance(block, XModuleDescriptor):
if isinstance(block, XModuleDescriptor):
self.children_for_user = { user: [ ItemFactory(category=child_type, parent=self.parent).scope_ids.usage_id for child_type in BLOCK_TYPES ] for user in self.users.itervalues() }
def setUp(self): super(TestDisabledXBlockTypes, self).setUp()
self.assertEqual(len(all_gradesets), 5)
self.assertFalse(all_gradesets[student3]) self.assertFalse(all_gradesets[student4])
self.assertTrue(all_gradesets[student1]) self.assertTrue(all_gradesets[student2]) self.assertTrue(all_gradesets[student5])
max_scores_cache.set(self.locations[0], 1) self.assertEqual(max_scores_cache.num_cached_updates(), 1)
max_scores_cache.push_to_remote()
max_scores_cache = MaxScoresCache("test_max_scores_cache") max_scores_cache.fetch_from_remote(self.locations)
self.assertEqual(max_scores_cache.num_cached_from_remote(), 1)
multi_db = True
module = get_module( user, request, problem.scope_ids.usage_id, field_data_cache, )._xmodule module.system.publish(problem, 'grade', grade_dict)
XBLOCK_REMOVED_HTML_ELEMENTS = [ '<div class="wrap-instructor-info"', ]
MASQUERADE_SETTINGS_KEY = 'masquerade_settings'
MASQUERADE_DATA_KEY = 'masquerade_data'
self.course_key = course_key self.role = role self.user_partition_id = user_partition_id self.group_id = group_id self.user_name = user_name
_DELETED_SENTINEL = object()
self.set(key, _DELETED_SENTINEL)
world.disable_jquery_animations()
world.css_click(css_selector='.chapter', index=1) subsection_css = 'a[href*="Test_Subsection_2/"]'
world.css_click(subsection_css)
subsection_css = 'a[href*="Test_Subsection_2/"]' world.css_click(subsection_css)
factory_dict = PROBLEM_DICT['multiple choice'] problem_xml = factory_dict['factory'].build_xml(**factory_dict['kwargs'])
world.ItemFactory.create( parent_location=parent_location, category='problem', display_name=display_name, data=problem_xml )
world.wait_for_ajax_complete()
assert world.is_css_present('.error_message', wait_time=0)
assert not world.is_css_present('iframe', wait_time=0)
assert not world.is_css_present('.link_lti_new_window', wait_time=0)
assert world.css_visible('iframe') check_lti_iframe_content("This is LTI tool. Success.")
assert len(world.browser.windows) == 1 alert = world.browser.get_alert() alert.accept() check_no_alert()
world.wait_for( lambda _: len(world.browser.windows) == 2, timeout=5, timeout_msg="Timed out waiting for the LTI window to appear." )
check_lti_popup(parent_window)
check_lti_iframe_content("Wrong LTI signature")
world.clear_courses()
world.scenario_dict['COURSE'] = world.CourseFactory.create( org='edx', number=course, display_name='Test Course', metadata=metadata, grading_policy=grading_policy, )
user = BetaTesterFactory(course_key=course_descriptor.id) normal_student = UserFactory() instructor = InstructorFactory(course_key=course_descriptor.id)
if has_access(user, 'load', course_descriptor): world.enroll_user(user, course_descriptor.id)
windows = world.browser.windows assert_equal(len(windows), 2)
tabs = [] expected_tabs = [u'LTI | Test Section | {0} Courseware | edX'.format(TEST_COURSE_NAME), u'TEST TITLE']
world.clear_courses()
world.scenario_dict['COURSE'] = world.CourseFactory.create( org='edx', number=course, display_name='Test Course' )
world.scenario_dict['CHAPTER'] = world.ItemFactory.create( parent_location=world.scenario_dict['COURSE'].location, category='chapter', display_name='Test Chapter',
create_course(step, course)
world.create_user('robot', 'test') user = User.objects.get(username='robot')
def course_id(course_num): return world.scenario_dict['COURSE'].id.replace(course=course_num)
world.wait_for_ajax_complete()
first_addend = random.randint(-100, 100) second_addend = 10 - first_addend
if correctness == 'incorrect': second_addend += random.randint(1, 10)
pass
category_name = "problem" return world.ItemFactory.create( parent_location=section_location(course), category=category_name, display_name=str(problem_type), data=problem_xml, metadata=metadata )
if problem_type in ("radio_text", "checkbox_text"): selector_template = "input#{}_2_{input}" else: selector_template = "input#input_{}_2_{input}"
assert world.is_css_present(sel)
return sel
assert element.value.strip() == expected
add_problem_to_course(world.scenario_dict['COURSE'].number, problem_type, problem_settings)
visit_scenario_item('SECTION')
world.xqueue.config['default'] = response_dict
input_problem_answer(step, problem_type, correctness)
check_problem(step)
world.wait_for_ajax_complete()
world.browser.execute_script("window.scrollTo(0,1024)") assert world.is_css_present("button.check.is-disabled")
world.wait_for_ajax_complete()
label_css = 'button.show span.show-label' world.wait_for(lambda _: world.css_has_text(label_css, label_name))
score_css = 'div.problem-progress' expected_text = '({})'.format(score) world.wait_for(lambda _: world.css_has_text(score_css, expected_text))
assert correctness in ['correct', 'incorrect', 'unanswered'] assert problem_type in PROBLEM_DICT
for sel in PROBLEM_DICT[problem_type][correctness]: if bool(isnt_marked):
if has_expected: break
assert has_expected
world.mongo_client.fsync()
from . import signals
self.course_key = course_key
self.user = user
self._has_staff_access = None
update_course_in_cache.apply_async([unicode(course_key)], countdown=0)
COURSE_BLOCK_ACCESS_TRANSFORMERS = [ library_content.ContentLibraryTransformer(), start_date.StartDateTransformer(), user_partitions.UserPartitionTransformer(), visibility.VisibilityTransformer(), ]
parents = block_structure.get_parents(block_key)
block_structure.set_transformer_block_field( block_key, cls, cls.MERGED_VISIBLE_TO_STAFF_ONLY, ( all_parents_visible_to_staff_only or block_structure.get_xblock(block_key).visible_to_staff_only ) )
if usage_info.has_staff_access: return
parents = block_structure.get_parents(block_key) min_all_parents_start_date = min( cls.get_merged_start_date(block_structure, parent_key) for parent_key in parents ) if parents else None
block_start = get_field_on_block(block_structure.get_xblock(block_key), 'start') if min_all_parents_start_date is None: merged_start_value = block_start or DEFAULT_START_DATE
merged_start_value = min_all_parents_start_date
merged_start_value = max(min_all_parents_start_date, block_start)
if usage_info.has_staff_access: return
previous_count = len(selected) block_keys = LibraryContentModule.make_selection(selected, library_children, max_count, mode) selected = block_keys['selected']
self._publish_events(block_structure, block_key, previous_count, max_count, block_keys) all_selected_children.update(usage_info.course_key.make_usage_key(s[0], s[1]) for s in selected)
block_structure.remove_block_if( check_child_removal )
child_to_group = { xblock.group_id_to_child.get(unicode(group.id), None): group.id for group in partition_for_this_block.groups }
for child_location in xblock.children: child = block_structure.get_xblock(child_location) group = child_to_group.get(child_location, None) child.group_access[partition_for_this_block.id] = [group] if group else []
block_structure.remove_block_if( lambda block_key: block_key.block_type == 'split_test', keep_descendants=True, )
SplitTestTransformer.collect(block_structure)
root_block = block_structure.get_xblock(block_structure.root_block_usage_key) user_partitions = getattr(root_block, 'user_partitions', []) or [] block_structure.set_transformer_data(cls, 'user_partitions', user_partitions)
if not user_partitions: return
self._access = {}
xblock_group_access = get_field_on_block(xblock, 'group_access', default_value={})
merged_parent_group_ids = None
merged_parent_group_ids = set()
xblock_partition_access = set(xblock_group_access.get(partition.id) or []) or None
merged_group_ids = _MergedGroupAccess._intersection(xblock_partition_access, merged_parent_group_ids)
if merged_group_ids is not None: self._access[partition.id] = merged_group_ids
if partition_id not in user_groups: return False
elif user_groups[partition_id].id in allowed_group_ids: continue
else: return False
return True
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
self.password = 'test' self.user = UserFactory.create(password=self.password) self.staff = UserFactory.create(password=self.password, is_staff=True)
course = modulestore().get_item(block_map['course'].location) course.children.remove(block_key) block_map['course'] = update_block(course)
for parent_ref in parents: parent_block = modulestore().get_item(block_map[parent_ref].location) parent_block.children.append(block_key) block_map[parent_ref] = update_block(parent_block)
for child_hierarchy in block_hierarchy.get('#children', []): self.add_parents(child_hierarchy, block_map)
for block_hierarchy in course_hierarchy: self.build_xblock(block_hierarchy, block_map, parent=None)
for block_hierarchy in course_hierarchy: self.add_parents(block_hierarchy, block_map)
parents_map = [[], [0], [0], [1], [1], [2], [2, 4]]
self.course = CourseFactory.create()
self.xblock_keys = [self.course.location]
for i, parents_index in enumerate(self.parents_map): if i == 0:
self._check_results( test_user, expected_user_accessible_blocks, blocks_with_differing_access, transformers, )
self._check_results(self.staff, set(range(len(self.parents_map))), {}, transformers)
block_structure_result = xblock_key in block_structure has_access_result = bool(has_access(user, 'load', self.get_block(i), course_key=self.course.id))
self.assertEquals( block_structure_result, i in expected_accessible_blocks, "block_structure return value {0} not equal to expected value for block {1} for user {2}".format( block_structure_result, i, user.username ) )
self.groups = [] for group_num in range(1, num_groups + 1): self.groups.append(Group(group_num, 'Group ' + unicode(group_num)))
self.setup_groups_partitions() self.user_partition = self.user_partitions[0]
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
self.setup_cohorts(self.course)
self.setup_groups_partitions(num_user_partitions=3)
self.setup_cohorts(self.course)
AccessTestData(expected_access=True), AccessTestData(xblock_access={1: None}, expected_access=True), AccessTestData(xblock_access={1: []}, expected_access=True),
AccessTestData(partition_groups={1: 3, 2: 3}, xblock_access={1: [1, 2], 2: [1, 2]}),
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {1}}], expected_access=True),
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {}}]), AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {3}}]),
AccessTestData(partition_groups={1: 1, 2: 2}, merged_parents_list=[{1: {3}}, {1: {1}}], expected_access=True),
AccessTestData(partition_groups={1: 1, 2: 2}, xblock_access={1: [3]}, merged_parents_list=[{1: {1}}]), AccessTestData(partition_groups={1: 1, 2: 2}, xblock_access={1: [2]}, merged_parents_list=[{1: {1}}]),
AccessTestData( partition_groups={1: 1, 2: 2}, xblock_access={1: [1]}, merged_parents_list=[{1: {3}}, {1: {1}}], expected_access=True, ),
block = self.course
if xblock_access is not None: block.group_access = xblock_access update_block(self.course)
for ind, merged_parent in enumerate(merged_parents_list): converted_object = _MergedGroupAccess([], block, []) converted_object._access = merged_parent merged_parents_list[ind] = converted_object
for partition_id, group_id in user_partition_groups.iteritems(): user_partition_groups[partition_id] = self.groups[group_id - 1]
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
user_groups = _get_user_partition_groups( self.course.id, [self.split_test_user_partition], self.user ) self.assertEquals(len(user_groups), 1)
self.assertFalse(self.course.visible_to_staff_only) orig_block_structure = get_course_blocks(self.user, self.course_usage_key) self.assertFalse( VisibilityTransformer.get_visible_to_staff_only(orig_block_structure, self.course_usage_key) )
self.course.visible_to_staff_only = True self.store.update_item(self.course, self.user.id)
mode = models.CharField(max_length=100, default='', blank=True) image = models.ImageField(upload_to='badge_classes', validators=[validate_badge_image])
help_text=_( u"Badge images must be square PNG files. The file size should be under 250KB." ), upload_to='course_complete_badges', validators=[validate_badge_image]
if self.default and CourseCompleteImageConfiguration.objects.filter(default=True).exclude(id=self.id): raise ValidationError(_(u"There can be only one default image."))
return cls.objects.get(default=True).icon
admin.site.register(CourseEventBadgesConfiguration, ConfigurationModelAdmin)
slug = hashlib.sha256(slug + unicode(badge_class.course_id)).hexdigest()
slug = hashlib.sha256(slug).hexdigest()
EXAMPLE_SLUG = '15bb687e0c59ef2f0a49f6838f511bf4ca6c566dd45da6293cabbd9369390e1a'
from __future__ import unicode_literals
from __future__ import unicode_literals
continue
from __future__ import unicode_literals
return
return
CourseEnrollment.enroll(user, course_key=course.location.course_key) self.assertFalse(user.badgeassertion_set.all())
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
self.assertFalse(user.badgeassertion_set.all())
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
self.courses.append([CourseFactory().location.course_key for _i in range(3)])
user=user, course_id=course.location.course_key, status=CertificateStatuses.downloadable
self.assertFalse(user.badgeassertion_set.all())
if i + 1 == len(course_keys): self.assertTrue(badge_class.get_for_user(user)) else: self.assertFalse(badge_class.get_for_user(user))
self.client.login(username=self.user.username, password='test')
self.check_assertion_structure(assertion, response['results'][0])
self.check_assertion_structure(assertion, response['results'][0])
alt_class = BadgeClassFactory.create( slug=badge_class.slug, issuing_component=badge_class.issuing_component, course_id=CourseFactory.create().location.course_key ) BadgeAssertionFactory.create(user=self.user, badge_class=alt_class)
for dummy in range(6): BadgeAssertionFactory.create()
self.assertEqual(len(response['results']), expected_length) unused_class = self.create_badge_class(check_course, slug='unused_slug', issuing_component='unused_component')
self.assertEqual(len(response['results']), 0)
self.check_assertion_structure(assertion, response['results'][0])
course_id = None
course_id = None
course_id = CourseKeyField.Empty
badge_class = BadgeClass.get_badge_class( slug='new_slug', issuing_component='new_component', description=None, criteria=None, display_name=None, image_file_handle=None, create=False ) self.assertIsNone(badge_class)
from course_wiki.plugins.markdownedx.wiki_plugin import ExtendMarkdownPlugin
from markdown.util import etree, AtomicString
md.inlinePatterns.add('mathjax', MathJaxPattern(), '<escape')
from markdown.util import etree
for key, value in configs: self.setConfig(key, value)
width = self.ext.config['bliptv_width'][0] height = self.ext.config['bliptv_height'][0] return flash_object(url, width, height)
width = self.ext.config['dailymotion_width'][0] height = self.ext.config['dailymotion_height'][0] return flash_object(url, width, height)
width = self.ext.config['gametrailers_width'][0] height = self.ext.config['gametrailers_height'][0] return flash_object(url, width, height)
width = self.ext.config['metacafe_width'][0] height = self.ext.config['metacafe_height'][0] return flash_object(url, width, height)
width = self.ext.config['veoh_width'][0] height = self.ext.config['veoh_height'][0] return flash_object(url, width, height)
width = self.ext.config['vimeo_width'][0] height = self.ext.config['vimeo_height'][0] return flash_object(url, width, height)
width = self.ext.config['youtube_width'][0] height = self.ext.config['youtube_height'][0] return flash_object(url, width, height)
urlpath = None article = None
root = get_or_create_root()
urlpath.delete()
_("This is the wiki for **{organization}**'s _{course_name}_.").format( organization=course.display_org_with_default, course_name=course.display_name_with_default_escaped, )
resp = self.client.get(course_wiki_page, follow=False, HTTP_REFERER=referer) self.assertEqual(resp.status_code, 302)
resp = self.client.get(course_wiki_page, follow=False) self.assertEqual(resp.status_code, 302)
resp = self.client.get(course_wiki_page, follow=True) target_url, __ = resp.redirect_chain[-1] self.assertTrue(reverse('signin_user') in target_url)
self.assertContains(response, "super-ugly")
if slug_is_numerical(slug): slug = slug + "_"
ancestors = urlpath.cached_ancestors
try: get_course_overview_with_access(request.user, 'load', course_id) return redirect("/courses/{course_id}/wiki/{path}".format(course_id=course_id.to_deprecated_string(), path=wiki_path)) except Http404: pass
if not view_func.__module__.startswith('wiki.'): return
if not request.user.is_authenticated(): return redirect(reverse('signin_user'), next=request.path)
course_path = "/courses/{}".format(course_id.to_deprecated_string())
return redirect('about_course', course_id.to_deprecated_string())
if not settings.FEATURES.get('ALLOW_WIKI_ROOT_ACCESS', False): raise PermissionDenied()
request.grant_type = None
request.user = request.client.user
request.grant_type = grant_type request.user = user
sale_order_dict = dict((feature, getattr(purchased_course.order, feature)) for feature in sale_order_features)
order_item_dict = dict((feature, getattr(purchased_course, feature, None)) for feature in order_item_features)
if coupon_redemption.exists(): coupon_codes = [redemption.coupon.code for redemption in coupon_redemption] order_item_dict.update({'coupon_code': ", ".join(coupon_codes)})
sale_dict = dict((feature, getattr(invoice, feature)) for feature in sale_features)
for data in generated_certificates: data['report_run_date'] = report_run_date
meta_features = [] for feature in features: if 'meta.' in feature: meta_key = feature.split('.')[1] meta_features.append((feature, meta_key))
meta_dict = json.loads(profile.meta) if profile.meta else {} for meta_feature, meta_key in meta_features: student_dict[meta_feature] = meta_dict.get(meta_key)
student_dict['cohort'] = next( (cohort.name for cohort in student.course_groups.all() if cohort.course_id == course_key), "[unassigned]" )
run = problem_key.run if not run: problem_key = course_key.make_usage_key_from_deprecated_string(problem_location) if problem_key.course_key != course_key: return []
if csv_type is not None: try: redemption_set = registration_code.registrationcoderedemption_set redeemed_by = redemption_set.get(registration_code=registration_code).redeemed_by course_registration_dict['redeemed_by'] = redeemed_by.email except ObjectDoesNotExist: pass
mock_problem_key = Mock(return_value=u'') mock_problem_key.course_key = self.course_key with patch.object(UsageKey, 'from_string') as patched_from_string: patched_from_string.return_value = mock_problem_key
mock_results = MagicMock(return_value=[result_factory(n) for n in range(5)]) with patch.object(StudentModule, 'objects') as patched_manager: patched_manager.filter.return_value = mock_results
patched_from_string.assert_called_once_with(mock_problem_location) patched_manager.filter.assert_called_once_with( course_id=self.course_key, module_state_key=mock_problem_key )
self.assertEqual(userreport['city'], "None") self.assertEqual(userreport['country'], "")
item = order.orderitem_set.all().select_subclasses()[0] coupon_redemption = CouponRedemption.objects.select_related('coupon').filter(order=order)
item = order.orderitem_set.all().select_subclasses()[0]
mode = CourseModeFactory.create() mode.course_id = self.course.id mode.min_price = 1 mode.save()
_EASY_CHOICE_FEATURES = ('gender', 'level_of_education') _OPEN_CHOICE_FEATURES = ('year_of_birth',)
self.type = None self.data = None self.choices_display_names = None
choices = [(short, full) for (short, full) in raw_choices] + [('no_data', 'No Data')]
if None in distribution:
distribution['no_data'] = profiles.filter( **{feature: None} ).count()
super(SurveyForm, self).save(*args, **kwargs)
self.clear_user_answers(user) SurveyAnswer.save_answers(self, user, answers, course_key)
tree = etree.fromstring(u'<div>{}</div>'.format(html))
course_key = CourseKeyField(max_length=255, db_index=True, null=True)
value = answers[name] defaults = {"field_value": value} if course_key: defaults['course_key'] = course_key
answer.field_value = value answer.course_key = course_key answer.save()
existing_answers = survey.get_answers(user=user).get(user.id, {})
array_val = request.POST.getlist(key) answers[key] = request.POST[key] if len(array_val) == 0 else ','.join(array_val)
redirect_url = answers['_redirect_url'] if '_redirect_url' in answers else reverse('dashboard')
filtered_answers = {} for answer_key in answers.keys(): if answer_key in allowed_field_names: filtered_answers[answer_key] = escape(answers[answer_key])
"redirect_url": redirect_url,
from __future__ import unicode_literals
self.password = 'abc' self.student = UserFactory.create(username='student', email='student@test.com', password=self.password)
self.assertIn(self.test_form, resp.content)
answer_objs = SurveyAnswer.objects.filter( user=self.student, form=self.survey )
survey.save_user_answers(self.student, self.student_answers_update, self.course_id)
survey.save_user_answers(self.student, self.student_answers_update2, self.course_id)
all_answers = survey.get_answers(limit_num_users=1) self.assertEquals(len(all_answers.keys()), 1)
survey = SurveyForm.get(course_descriptor.course_survey_name)
answered_survey = SurveyAnswer.do_survey_answers_exist(survey, user) return not answered_survey and not has_staff_access
from commerce import signals
error_summary = _("An error occurred while creating your receipt.")
log.exception( "Unexpected exception while attempting to initiate refund for user [%s], course [%s]", course_enrollment.user.id, course_enrollment.course_id, )
log.warning("User [%s] was not authorized to initiate a refund for user [%s] " "upon unenrollment from course [%s]", request_user.id, unenrolled_user.id, course_key_str) return []
raise exc
log.info( "Refund successfully opened for user [%s], course [%s]: %r", unenrolled_user.id, course_key_str, refund_ids, )
log.warning("Could not send email notification for refund.", exc_info=True)
log.debug("No refund opened for user [%s], course [%s]", unenrolled_user.id, course_key_str)
tags = list(tags or []) tags.append('LMS')
tags = list(set(tags))
payload = json.dumps(data)
raise NotImplementedError("Unable to send refund processing emails to microsite teams.")
from django.conf import settings from django.contrib.auth.models import User from django.db import models, migrations
from __future__ import unicode_literals
from __future__ import unicode_literals
authentication_classes = (EnrollmentCrossDomainSessionAuth, OAuth2AuthenticationAllowInactiveUser) permission_classes = (IsAuthenticated,)
log.exception( 'Failed to handle marketing opt-in flag: user="%s", course="%s"', user.username, course_key )
honor_mode = CourseMode.mode_for_course(course_key, CourseMode.HONOR) audit_mode = CourseMode.mode_for_course(course_key, CourseMode.AUDIT)
default_enrollment_mode = audit_mode or honor_mode
try: response_data = api.baskets.post({ 'products': [{'sku': default_enrollment_mode.sku}], 'checkout': True, })
response = JsonResponse(payment_data)
msg = Messages.ORDER_COMPLETED.format(order_number=response_data['order']['number']) log.debug(msg) response = DetailResponse(msg)
self.reset_tracker()
self.assertTrue(mock_audit_log.called)
if is_completed: msg = Messages.ORDER_COMPLETED.format(order_number=TEST_ORDER_NUMBER) self.assertResponseMessage(response, msg) else: self.assertResponsePaymentData(response)
self.user.is_active = user_is_active
self.user.is_active = user_is_active
with mock_create_basket(expect_called=False): response = self._post_to_view()
for course_mode in CourseMode.objects.filter(course_id=self.course.id): course_mode.sku = None course_mode.save()
self.assertEqual(response.status_code, 200) msg = Messages.NO_ECOM_API.format(username=self.user.username, course_id=self.course.id) self.assertResponseMessage(response, msg)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course.id))
self.assertEqual(response.status_code, 406) msg = Messages.NO_DEFAULT_ENROLLMENT_MODE.format(course_id=self.course.id) self.assertResponseMessage(response, msg)
for course_mode in CourseMode.objects.filter(course_id=self.course.id): course_mode.sku = '' course_mode.save()
CourseEnrollment.enroll(self.user, self.course.id) self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course.id))
log.warning('Failed to retrieve CourseOverview for [%s]. Using empty course name.', course_id) return None
VerificationDeadline.set_deadline(self.id, self.verification_deadline, is_explicit=True)
queryset = CourseMode.objects.all()
pass
if upgrade_deadline is not None and verification_deadline < upgrade_deadline: raise serializers.ValidationError( 'Verification deadline must be after the course mode upgrade deadlines.')
response = self.client.put(self.path, json.dumps(expected), content_type=JSON_CONTENT_TYPE)
self.assertIsNone(VerificationDeadline.deadline_for_course(self.course.id))
verification_deadline = datetime(year=2020, month=12, day=31, tzinfo=pytz.utc) expiration_datetime = datetime.now(pytz.utc) response, expected = self._get_update_response_and_expected_data(expiration_datetime, verification_deadline)
self.assertEqual(response.status_code, 200)
actual = json.loads(response.content) self.assertEqual(actual, expected)
self.assertEqual(VerificationDeadline.deadline_for_course(self.course.id), verification_deadline)
response, __ = self._get_update_response_and_expected_data(None, None) self.assertEqual(response.status_code, 200)
self.assertFalse(CourseMode.objects.filter(id=self.course_mode.id).exists())
course_modes = CourseMode.objects.filter(course_id=course.id) actual = [course_mode.mode_display_name for course_mode in course_modes] self.assertListEqual(actual, ['Verified Certificate', 'Honor Certificate'])
httpretty.register_uri( httpretty.POST, '{}/baskets/1/'.format(TEST_API_URL), status=200, body='{}', adding_headers={'Content-Type': JSON} )
del post_data[post_key] expected_pattern = r"<title>(\s+)Receipt"
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
message = 'foo: bar="baz", qux="quux"' self.assertTrue(mock_log.info.called_with(message))
default_response = None
method = None
mock_refund_seat.reset_mock() self.send_signal(skip_refund=True) self.assertFalse(mock_refund_seat.called)
mock_refund_seat.reset_mock() self.course_enrollment.refundable = mock.Mock(return_value=False) self.send_signal() self.assertFalse(mock_refund_seat.called)
self.send_signal() self.assertTrue(mock_refund_seat.called) self.assertEqual(mock_refund_seat.call_args[0], (self.course_enrollment, self.student))
mock_get_request_user.return_value = AnonymousUser() mock_refund_seat.reset_mock() self.send_signal() self.assertFalse(mock_refund_seat.called)
STATUS = Choices('created', 'ready', 'submitted', 'must_retry', 'approved', 'denied') user = models.ForeignKey(User, db_index=True)
face_image_url = models.URLField(blank=True, max_length=255) photo_id_image_url = models.URLField(blank=True, max_length=255)
receipt_id = models.CharField( db_index=True, default=generateUUID, max_length=255, )
reviewing_user = models.ForeignKey( User, db_index=True, default=None, null=True, related_name="photo_verifications_reviewed" )
reviewing_service = models.CharField(blank=True, max_length=255)
error_msg = models.TextField(blank=True)
error_code = models.CharField(blank=True, max_length=50)
active_attempts = cls.objects.filter(user=user, status='ready').order_by('-created_at') if active_attempts: return active_attempts[0] else: return None
status = 'pending'
if attempt.status == 'denied': status = 'must_reverify'
if deadline is None: return candidates[0]
for verification in candidates: if verification.active_at_datetime(deadline): return verification
self.name = self.user.profile.name self.status = "ready" self.save()
if self.status == "approved": return
s3_key = self._generate_s3_key("photo_id") s3_key.set_contents_from_string(encrypt_and_encode(img_data, aes_key))
self.photo_id_key = rsa_encrypted_aes_key.encode('base64') self.save()
category_msgs = msg_dict[category] for category_msg in category_msgs: msg.append(message_dict[(category, category_msg)])
log.error('PhotoVerification: Error parsing this error message: %s', self.error_msg) return _("There was an error verifying your ID photos.")
receipt_id = self.receipt_id if override_receipt_id is None else override_receipt_id
photo_id_url = ( self.image_url("photo_id") if copy_id_photo_from is None else self.image_url("photo_id", override_receipt_id=copy_id_photo_from.receipt_id) )
deadline_is_explicit = models.BooleanField(default=False)
history = HistoricalRecords()
url( r'^reverify/{course_id}/{usage_id}/$'.format( course_id=settings.COURSE_ID_PATTERN, usage_id=settings.USAGE_ID_PATTERN ), views.InCourseReverifyView.as_view(), name="verify_student_incourse_reverify" ),
SKIP_STEPS = [ INTRO_STEP, ]
FIRST_TIME_VERIFY_MSG = 'first-time-verify' VERIFY_NOW_MSG = 'verify-now' VERIFY_LATER_MSG = 'verify-later' UPGRADE_MSG = 'upgrade' PAYMENT_CONFIRMATION_MSG = 'payment-confirmation'
VERIFICATION_DEADLINE = "verification" UPGRADE_DEADLINE = "upgrade"
course_key = CourseKey.from_string(course_id) course = modulestore().get_course(course_key)
if course is None: log.warn(u"Could not find course with ID %s.", course_id) raise Http404
redirect_url = embargo_api.redirect_if_blocked( course_key, user=request.user, ip_address=get_ip(request), url=request.path ) if redirect_url: return redirect(redirect_url)
redirect_response = self._redirect_if_necessary( message, already_verified, already_paid, is_enrolled, course_key, user_is_trying_to_pay, request.user, relevant_course_mode.sku ) if redirect_response is not None: return redirect_response
contribution_amount = request.session.get( 'donation_for_course', {} ).get(unicode(course_key), '')
request.session['attempting_upgrade'] = (message == self.UPGRADE_MSG)
verification_good_until = self._verification_valid_until(request.user)
if relevant_course_mode.sku: processors = ecommerce_api_client(request.user).payment.processors.get() else: processors = [settings.CC_PROCESSOR_NAME]
if not already_paid: url = reverse('verify_student_upgrade_and_verify', kwargs=course_kwargs)
url = reverse('verify_student_start_flow', kwargs=course_kwargs)
url = reverse('verify_student_verify_now', kwargs=course_kwargs)
ecommerce_service = EcommerceService() if ecommerce_service.is_enabled(user): url = ecommerce_service.checkout_page_url(sku)
if url is not None: return redirect(url)
all_modes, unexpired_modes = CourseMode.all_and_unexpired_modes_for_courses([course_key])
for mode in unexpired_modes[course_key]: if mode.min_price > 0 and not CourseMode.is_credit_mode(mode): return mode
for mode in all_modes[course_key]: if mode.min_price > 0 and not CourseMode.is_credit_mode(mode): return mode
return None
remove_steps |= set([self.INTRO_STEP])
if photo_verifications: return photo_verifications[0].expiration_datetime.strftime(date_format)
result = api.baskets.post({ 'products': [{'sku': course_mode.sku}], 'checkout': True, 'payment_processor_name': processor })
return result.get('payment_data')
payment_data = checkout_with_ecommerce_service( request.user, course_id, current_mode, request.POST.get('processor') )
payment_data = payment_data['payment_form_data']
params, response = self._validate_parameters(request, bool(initial_verification)) if response is not None: return response
if "full_name" in params: response = self._update_full_name(request.user, params["full_name"]) if response is not None: return response
face_image, photo_id_image, response = self._decode_image_data( params["face_image"], params.get("photo_id_image") )
if photo_id_image is not None: initial_verification = None
attempt = self._submit_attempt(request.user, face_image, photo_id_image, initial_verification)
redirect_url = get_redirect_url(params["course_key"], params["checkpoint"]) return JsonResponse({"url": redirect_url})
params = { param_name: request.POST[param_name] for param_name in [ "face_image", "photo_id_image", "course_key", "checkpoint", "full_name" ] if param_name in request.POST }
if "face_image" not in params: msg = _("Missing required parameter face_image") return None, HttpResponseBadRequest(msg)
face_image = decode_image_data(face_data)
photo_id_image = ( decode_image_data(photo_id_data) if photo_id_data is not None else None )
attempt.upload_face_image(face_image)
attempt.mark_ready() attempt.submit(copy_id_photo_from=initial_verification)
log.exception("Could not send notification email for initial verification for user %s", user.id)
log.error("Unable to add Credit requirement status for user with id %d", attempt.user.id)
#if not sig_valid:
if access_key != settings.VERIFY_STUDENT["SOFTWARE_SECURE"]["API_ACCESS_KEY"]: return HttpResponseBadRequest("Access key invalid")
icrv_status_emails = IcrvStatusEmailsConfiguration.current() if icrv_status_emails.enabled and checkpoints: user_id = attempt.user.id course_key = checkpoints[0].course_id related_assessment_location = checkpoints[0].checkpoint_location
self._track_reverification_events('edx.bi.reverify.started', user.id, course_id, checkpoint.checkpoint_name)
if force_must_retry: attempt.status = 'must_retry'
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
VerificationCheckpoint.get_or_create_verification_checkpoint(course_key, related_assessment_location)
from openedx.core.djangoapps.credit.api import set_credit_requirement_status
set_credit_requirement_status( user.username, course_key, 'reverification', checkpoint.checkpoint_location, status='declined' )
MIN_PRICE = 1438
resp = self.client.get(self.urls['course_modes_choose'], follow=True) self.assertRedirects(resp, self.urls['verify_student_start_flow'])
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertContains(resp, 'payment-button')
assert_roundtrip("12345678901234561234567890123456123456789012345601") assert_roundtrip("")
assert_equals(len(base64.urlsafe_b64encode(encrypted_aes_key)), 344)
response = self._get_page(payment_flow, course.id) self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.MAKE_PAYMENT_STEP ) self._assert_requirements_displayed(response, [])
response = self._get_page(payment_flow, course.id) self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.MAKE_PAYMENT_STEP ) self._assert_requirements_displayed(response, [])
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
course = self._create_course("verified") self._enroll(course.id, "verified") response = self._get_page('verify_student_verify_now', course.id)
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.FACE_PHOTO_STEP )
self._assert_requirements_displayed(response, [ PayAndVerifyView.PHOTO_ID_REQ, PayAndVerifyView.WEBCAM_REQ, ])
response = self._get_page( 'verify_student_verify_now', course.id, expected_status_code=302 ) self._assert_redirects_to_dashboard(response)
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.PAYMENT_CONFIRMATION_STEP, )
self._assert_requirements_displayed(response, [ PayAndVerifyView.PHOTO_ID_REQ, PayAndVerifyView.WEBCAM_REQ, ])
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS + PayAndVerifyView.VERIFICATION_STEPS, PayAndVerifyView.MAKE_PAYMENT_STEP, )
self._assert_steps_displayed( response, PayAndVerifyView.PAYMENT_STEPS, PayAndVerifyView.PAYMENT_CONFIRMATION_STEP, )
response = self._get_page( 'verify_student_upgrade_and_verify', course.id, expected_status_code=302 ) self._assert_redirects_to_verify_start(response, course.id)
response = self._get_page( 'verify_student_upgrade_and_verify', course.id, expected_status_code=302 ) self._assert_redirects_to_dashboard(response)
course = self._create_course("verified") response = self._get_page(payment_flow, course.id) self._assert_contribution_amount(response, "")
response = self._get_page(payment_flow, course.id) self._assert_contribution_amount(response, "")
course = self._create_course("verified") self._set_contribution("12.34", course.id)
response = self._get_page(payment_flow, course.id) self._assert_contribution_amount(response, "12.34")
self._set_deadlines(course.id, upgrade_deadline=deadline, verification_deadline=deadline)
self._enroll(course.id, "verified")
self._enroll(course.id, "verified")
response = self._get_page("verify_student_verify_now", course.id) self.assertNotContains(response, "Verification is no longer available")
self.assertEqual(data['course_mode_slug'], "verified")
self._enroll(course.id, "verified")
response = self._get_page(payment_flow, course.id, expected_status_code=302) self.assertRedirects(response, redirect_url)
mode = CourseMode.objects.get(course_id=course_key, mode_slug=mode_slug) mode.expiration_datetime = upgrade_deadline mode.save()
VerificationDeadline.set_deadline(course_key, verification_deadline)
course = self._create_course("verified", sku='nonempty-sku') self._enroll(course.id)
self.assertNotEqual(httpretty.last_request().headers, {})
httpretty.register_uri( httpretty.POST, "{}/baskets/".format(TEST_API_URL), body=json.dumps({'payment_data': expected_payment_data}), content_type="application/json", )
actual_payment_data = checkout_with_ecommerce_service( user, 'dummy-course-key', course_mode, 'test-processor' )
self.assertTrue(mock_audit_log.called)
response = self.client.get(reverse('verify_student_create_order'), create_order_post_data) self.assertEqual(response.status_code, 405)
self._submit_photos( face_image=self.IMAGE_DATA, photo_id_image=self.IMAGE_DATA )
attempt = SoftwareSecurePhotoVerification.objects.get(user=self.user) self.assertEqual(attempt.status, "submitted")
self._assert_user_name(self.user.profile.name)
self._submit_photos( face_image=self.IMAGE_DATA, photo_id_image=self.IMAGE_DATA, full_name=self.FULL_NAME )
self._assert_user_name(self.FULL_NAME)
self._submit_photos(expected_status_code=400) self._assert_confirmation_email(False)
httpretty.register_uri(httpretty.POST, "https://verify.example.com/submit/")
self._submit_photos( face_image=self.IMAGE_DATA + "4567", photo_id_image=self.IMAGE_DATA + "8910", ) initial_data = self._get_post_data()
self._submit_photos(face_image=self.IMAGE_DATA + "1112") reverification_data = self._get_post_data()
self.assertEqual(initial_data["PhotoIDKey"], reverification_data["PhotoIDKey"])
initial_photo_response = requests.get(initial_data["UserPhoto"]) self.assertEqual(initial_photo_response.status_code, 200)
self._submit_photos( face_image=self.IMAGE_DATA + "9999", photo_id_image=self.IMAGE_DATA + "1111", ) two_photo_reverification_data = self._get_post_data()
self.assertNotEqual(initial_data["PhotoIDKey"], two_photo_reverification_data["PhotoIDKey"])
params = { 'photo_id_image': self.IMAGE_DATA } response = self._submit_photos(expected_status_code=400, **params) self.assertEqual(response.content, "Missing required parameter face_image")
response = self._submit_photos(expected_status_code=400, face_image=self.IMAGE_DATA) self.assertEqual( response.content, "Photo ID image is required if the user does not have an initial verification attempt." )
self._submit_photos( face_image=self.IMAGE_DATA, photo_id_image=self.IMAGE_DATA, ) attempt = SoftwareSecurePhotoVerification.objects.get(user=self.user) attempt.photo_id_key = "dummy_photo_id_key" attempt.save()
self._submit_photos(face_image=self.IMAGE_DATA)
self.assertEqual(len(mail.outbox), 1) self.assertEqual("Verification photos received", mail.outbox[0].subject)
self.assertEqual(len(mail.outbox), 0)
self.assertEqual(len(mail.outbox), 0) user_status = VerificationStatus.objects.filter(user=self.user).count() self.assertEqual(user_status, 0)
IcrvStatusEmailsConfiguration.objects.create(enabled=True) self.create_reverification_xblock()
self.create_reverification_xblock()
subject = "Re-verification Status" mock_send_email.assert_called_once_with(self.user.id, subject, ANY)
checkpoint = VerificationCheckpoint(course_id=self.course_id, checkpoint_location=reverification.location) checkpoint.save()
checkpoint.add_verification_attempt(self.attempt)
VerificationStatus.add_verification_status(checkpoint, self.user, "submitted")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.deny("error") self._assert_can_reverify()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self._assert_can_reverify()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
self._assert_can_reverify()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self._assert_cannot_reverify()
CourseEnrollment.enroll(self.user, self.course_key, mode="verified")
analytics_patcher = patch('lms.djangoapps.verify_student.views.analytics') self.mock_tracker = analytics_patcher.start() self.addCleanup(analytics_patcher.stop)
response = self.client.get(self._get_url(self.course_key, "invalid_checkpoint")) self.assertEqual(response.status_code, 404)
response = self._submit_photos(self.course_key, self.reverification_location, self.IMAGE_DATA) self.assertEqual(response.status_code, 400)
status = VerificationStatus.get_user_status_at_checkpoint( self.user, self.course_key, self.reverification_location ) self.assertEqual(status, "submitted")
with check_mongo_calls(1): ver_block = modulestore().get_item(self.reverification.location)
self.assertIsNotNone(ver_block)
from datetime import timedelta, datetime import json
photo_id_key = data_dict["PhotoIDKey"].decode("base64") user_photo_key = data_dict["UserPhotoKey"].decode("base64")
assert_raises(VerificationException, attempt.submit) assert_raises(VerificationException, attempt.approve) assert_raises(VerificationException, attempt.deny)
attempt.mark_ready() assert_equals(attempt.status, "ready")
assert_raises(VerificationException, attempt.approve) assert_raises(VerificationException, attempt.deny)
attempt.status = "must_retry" attempt.system_error("System error") attempt.approve() attempt.status = "must_retry" attempt.deny(DENY_ERROR_MSG)
attempt.status = "submitted" attempt.deny(DENY_ERROR_MSG) attempt.status = "submitted" attempt.approve()
assert_raises(VerificationException, attempt.submit)
assert_raises(VerificationException, attempt.submit)
attempt = self.create_and_submit() assert_equals(attempt.status, "submitted")
with patch('lms.djangoapps.verify_student.models.requests.post', new=mock_software_secure_post_error): attempt = self.create_and_submit() assert_equals(attempt.status, "must_retry")
with patch('lms.djangoapps.verify_student.models.requests.post', new=mock_software_secure_post_unavailable): attempt = self.create_and_submit() assert_equals(attempt.status, "must_retry")
assert_is_none(SoftwareSecurePhotoVerification.active_for_user(user))
attempt = SoftwareSecurePhotoVerification(user=user) attempt.mark_ready() assert_equals(attempt, SoftwareSecurePhotoVerification.active_for_user(user))
user2 = UserFactory.create() user2.save() assert_is_none(SoftwareSecurePhotoVerification.active_for_user(user2))
attempt_2 = SoftwareSecurePhotoVerification(user=user) attempt_2.mark_ready() assert_equals(attempt_2, SoftwareSecurePhotoVerification.active_for_user(user))
attempt_3 = SoftwareSecurePhotoVerification( user=user, created_at=attempt_2.created_at + timedelta(days=1) ) attempt_3.save()
assert_equals(attempt_2, SoftwareSecurePhotoVerification.active_for_user(user))
attempt_3.mark_ready() assert_equals(attempt_3, SoftwareSecurePhotoVerification.active_for_user(user))
for status in ["created", "ready", "denied"]: attempt.status = status attempt.save() assert_false(SoftwareSecurePhotoVerification.user_has_valid_or_pending(user), status)
for status in ["submitted", "must_retry", "approved"]: attempt.status = status attempt.save() assert_true(SoftwareSecurePhotoVerification.user_has_valid_or_pending(user), status)
user = UserFactory.create() status = SoftwareSecurePhotoVerification.user_status(user) self.assertEquals(status, ('none', ''))
attempt = SoftwareSecurePhotoVerification(user=user) attempt.status = 'approved' attempt.save()
attempt2 = SoftwareSecurePhotoVerification(user=user) attempt2.status = 'denied' attempt2.error_msg = '[{"photoIdReasons": ["Not provided"]}]' attempt2.save()
attempt.delete() status = SoftwareSecurePhotoVerification.user_status(user) self.assertEquals(status, ('must_reverify', "No photo ID was provided."))
before = attempt.created_at - timedelta(seconds=1) self.assertFalse(attempt.active_at_datetime(before))
after_created = attempt.created_at + timedelta(seconds=1) self.assertTrue(attempt.active_at_datetime(after_created))
expiration = attempt.created_at + timedelta(days=settings.VERIFY_STUDENT["DAYS_GOOD_FOR"]) before_expiration = expiration - timedelta(seconds=1) self.assertTrue(attempt.active_at_datetime(before_expiration))
after = expiration + timedelta(seconds=1) self.assertFalse(attempt.active_at_datetime(after))
query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(now, query) self.assertIs(result, None)
query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(None, query) self.assertIs(result, None)
attempt = SoftwareSecurePhotoVerification.objects.create(user=user)
before = attempt.created_at - timedelta(seconds=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(before, query) self.assertIs(result, None)
after_created = attempt.created_at + timedelta(seconds=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(after_created, query) self.assertEqual(result, attempt)
query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(None, query) self.assertEqual(result, attempt)
after = expiration + timedelta(seconds=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(after, query) self.assertIs(result, None)
second_attempt = SoftwareSecurePhotoVerification.objects.create(user=user)
deadline = second_attempt.created_at + timedelta(days=1) query = SoftwareSecurePhotoVerification.objects.filter(user=user) result = SoftwareSecurePhotoVerification.verification_for_datetime(deadline, query) self.assertEqual(result, second_attempt)
result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIs(result, None)
attempt = SoftwareSecurePhotoVerification(user=user, photo_id_key="dummy_photo_id_key") attempt.status = 'approved' attempt.save()
first_result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIsNotNone(first_result)
attempt = SoftwareSecurePhotoVerification(user=user) attempt.status = 'submitted' attempt.save()
second_result = SoftwareSecurePhotoVerification.get_initial_verification(user=user) self.assertIsNotNone(second_result) self.assertEqual(second_result, first_result)
verification_checkpoint = VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=checkpoint_location ) self.assertEqual( VerificationCheckpoint.get_or_create_verification_checkpoint(self.course.id, checkpoint_location), verification_checkpoint )
location = u'i4x://edX/DemoX/edx-reverification-block/invalid_location' checkpoint = VerificationCheckpoint.get_or_create_verification_checkpoint(self.course.id, location)
VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=self.checkpoint_midterm, )
VerificationCheckpoint.objects.create(course_id=self.course.id, checkpoint_location=self.checkpoint_midterm)
with self.assertRaises(IntegrityError): VerificationCheckpoint.objects.create(course_id=self.course.id, checkpoint_location=self.checkpoint_midterm)
first_checkpoint = VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=self.checkpoint_midterm ) second_checkpoint = VerificationCheckpoint.objects.create( course_id=self.course.id, checkpoint_location=self.checkpoint_final )
first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user)) self.assertEqual(first_checkpoint.photo_verification.count(), 1)
first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user)) self.assertEqual(first_checkpoint.photo_verification.count(), 2)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) second_checkpoint.add_verification_attempt(attempt) self.assertEqual(second_checkpoint.photo_verification.count(), 1)
second_checkpoint.photo_verification.remove(attempt) self.assertEqual(second_checkpoint.photo_verification.count(), 0)
VerificationStatus.add_verification_status( checkpoint=self.first_checkpoint, user=self.user, status=status )
result = VerificationStatus.objects.filter(checkpoint=self.first_checkpoint)[0] self.assertEqual(result.status, status) self.assertEqual(result.user, self.user)
initial_status = "submitted" VerificationStatus.add_verification_status( checkpoint=self.first_checkpoint, user=self.user, status=initial_status ) VerificationStatus.add_verification_status( checkpoint=self.second_checkpoint, user=self.user, status=initial_status )
VerificationStatus.add_status_from_checkpoints( checkpoints=[self.first_checkpoint, self.second_checkpoint], user=self.user, status=status )
self.first_checkpoint.add_verification_attempt(SoftwareSecurePhotoVerification.objects.create(user=self.user))
VerificationStatus.add_verification_status( checkpoint=self.first_checkpoint, user=self.user, status='submitted', ) attempt = SoftwareSecurePhotoVerification.objects.filter(user=self.user)
SkippedReverification.add_skipped_reverification_attempt( checkpoint=self.checkpoint, user_id=self.user.id, course_id=unicode(self.course.id) )
user2 = UserFactory.create() SkippedReverification.add_skipped_reverification_attempt( checkpoint=self.checkpoint, user_id=user2.id, course_id=unicode(self.course.id) )
with self.assertNumQueries(1): all_deadlines = VerificationDeadline.deadlines_for_courses(course_keys) self.assertEqual(all_deadlines, {})
for course_key, deadline in deadlines.iteritems(): VerificationDeadline.objects.create( course_key=course_key, deadline=deadline, )
with self.assertNumQueries(1): VerificationDeadline.deadlines_for_courses(course_keys)
with self.assertNumQueries(0): all_deadlines = VerificationDeadline.deadlines_for_courses(course_keys) self.assertEqual(all_deadlines, deadlines)
VerificationDeadline.objects.all().delete()
with self.assertNumQueries(1): all_deadlines = VerificationDeadline.deadlines_for_courses(course_keys) self.assertEqual(all_deadlines, {})
self.enrollment = CourseEnrollment.enroll(self.user, self.course_id, mode=CourseMode.VERIFIED)
self.assertEqual( reverification_service.get_status(self.user.id, unicode(self.course_id), self.final_checkpoint_location), 'skipped' )
self.enrollment.update_enrollment(mode=CourseMode.HONOR)
service = ReverificationService() status = service.get_status(self.user.id, unicode(self.course_id), self.final_checkpoint_location) self.assertEqual(status, service.NON_VERIFIED_TRACK)
self.make_course(textbooks=[IMAGE_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('book', book_index='fooey')
self.make_course(textbooks=[IMAGE_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('book', book_index=0, page='xyzzy')
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index='fooey', chapter=1)
self.make_course() url = self.make_url('pdf_book', book_index=0, chapter=1) response = self.client.get(url) self.assertEqual(response.status_code, 404)
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, chapter='xyzzy')
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, page='xyzzy')
self.make_course(pdf_textbooks=[PDF_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('pdf_book', book_index=0, chapter='fooey', page='xyzzy')
self.make_course() url = self.make_url('html_book', book_index=0, chapter=1) response = self.client.get(url) self.assertEqual(response.status_code, 404)
self.make_course(pdf_textbooks=[HTML_BOOK]) with self.assertRaises(NoReverseMatch): self.make_url('html_book', book_index=0, chapter='xyzzy')
if 'chapters' in textbook: for entry in textbook['chapters']: entry['url'] = remap_static_url(entry['url'], course)
QUEUING = 'QUEUING' PROGRESS = 'PROGRESS'
task_id = str(uuid4())
instructor_task = cls( course_id=course_id, task_type=task_type, task_id=task_id, task_key=task_key, task_input=json_task_input, task_state=QUEUING, requester=requester ) instructor_task.save_now()
json_output = json.dumps(returned_result) if len(json_output) > 1023: raise ValueError("Length of task output is too long: {0}".format(json_output)) return json_output
task_progress['traceback'] = traceback_string
key.set_contents_from_string( data, headers={ "Content-Encoding": content_encoding, "Content-Length": len(data), "Content-Type": content_type, } )
for state in READY_STATES: running_tasks = running_tasks.exclude(task_state=state) return len(running_tasks) > 0
return InstructorTask.create(course_id, task_type, task_key, task_input, requester)
entry_needs_updating = True entry_needs_saving = False task_output = None
log.info("background task (%s), state %s: result: %s", task_id, result_state, returned_result) task_output = InstructorTask.create_output_for_success(returned_result)
entry_needs_saving = True log.warning("background task (%s) revoked.", task_id) task_output = InstructorTask.create_output_for_revoked()
if entry_needs_updating: instructor_task.task_state = result_state if task_output is not None: instructor_task.task_output = task_output
if instructor_task.task_state not in READY_STATES: result = AsyncResult(task_id) _update_instructor_task(instructor_task, result)
task_key = hashlib.md5(task_key_stub).hexdigest()
task_key = hashlib.md5(task_key_stub).hexdigest()
instructor_task = _reserve_task(course_key, task_type, task_key, task_input, request.user)
STATES_WITH_STATUS = [state for state in READY_STATES] + [PROGRESS]
msg_format = _("Progress: {action} {succeeded} of {attempted} so far")
msg_format = _("Problem {action} for {succeeded} of {attempted} students")
msg_format = _("Message {action} for {succeeded} of {attempted} recipients")
msg_format = _("Status: {action} {succeeded} of {attempted}")
msg_format += _(" (skipping {skipped})")
msg_format += _(" (out of {total})")
message = msg_format.format( action=action_name, succeeded=num_succeeded, attempted=num_attempted, total=num_total, skipped=num_skipped, student=student ) return (succeeded, message)
from __future__ import unicode_literals
for state in READY_STATES: instructor_tasks = instructor_tasks.exclude(task_state=state) return instructor_tasks.order_by('-id')
check_arguments_for_rescoring(usage_key)
check_arguments_for_rescoring(usage_key)
task_type = 'rescore_problem' task_class = rescore_problem task_input, task_key = encode_problem_and_student_input(usage_key) return submit_task(request, task_type, task_class, usage_key.course_key, task_input, task_key)
check_entrance_exam_problems_for_rescoring(usage_key)
task_type = 'rescore_problem' task_class = rescore_problem task_input, task_key = encode_entrance_exam_and_student_input(usage_key, student) return submit_task(request, task_type, task_class, usage_key.course_key, task_input, task_key)
modulestore().get_item(usage_key)
modulestore().get_item(usage_key)
modulestore().get_item(usage_key)
modulestore().get_item(usage_key)
milestones_helpers.remove_course_content_user_milestones( course_key=usage_key.course_key, content_key=usage_key, user=student, relationship='fulfills' )
email_obj = CourseEmail.objects.get(id=email_id) targets = [target.target_type for target in email_obj.targets.all()]
task_key = hashlib.md5(task_key_stub).hexdigest() return submit_task(request, task_type, task_class, course_key, task_input, task_key)
TASK_LOG = logging.getLogger('edx.celery.task')
UNKNOWN_TASK_ID = 'unknown-task_id' FILTERED_OUT_ROLES = ['staff', 'instructor', 'finance_admin', 'sales_admin'] UPDATE_STATUS_SUCCEEDED = 'succeeded' UPDATE_STATUS_FAILED = 'failed' UPDATE_STATUS_SKIPPED = 'skipped'
REPORT_REQUESTED_EVENT_NAME = u'edx.instructor.report.requested'
TASK_LOG.error(u"Task (%s) has no InstructorTask object for id %s", task_id, entry_id)
with outer_atomic(): entry = InstructorTask.objects.get(pk=entry_id) entry.task_state = PROGRESS entry.save_now()
task_id = entry.task_id course_id = entry.course_id task_input = json.loads(entry.task_input)
with dog_stats_api.timer('instructor_tasks.time.overall', tags=[u'action:{name}'.format(name=action_name)]): task_progress = task_fcn(entry_id, course_id, task_input, action_name)
reset_queries()
TASK_LOG.info(u'%s, Task type: %s, Finishing task: %s', task_info_string, action_name, task_progress) return task_progress
if problem_url: usage_key = course_id.make_usage_key_from_deprecated_string(problem_url) usage_keys.append(usage_key)
problem_descriptor = modulestore().get_item(usage_key) problems[unicode(usage_key)] = problem_descriptor
if entrance_exam_url: problems = get_problems_in_section(entrance_exam_url) usage_keys = [UsageKey.from_string(location) for location in problems.keys()]
modules_to_update = StudentModule.objects.filter(course_id=course_id, module_state_key__in=usage_keys)
request_info = xmodule_instance_args.get('request_info', {}) if xmodule_instance_args is not None else {} task_info = {'student': student.username, 'task_id': _get_task_id_from_xmodule_args(xmodule_instance_args)}
field_data_cache = FieldDataCache.cache_for_descriptor_descendents(course_id, student, module_descriptor) student_data = KvsFieldData(DjangoKeyValueStore(field_data_cache))
request_info = xmodule_instance_args.get('request_info', {}) if xmodule_instance_args is not None else {} task_info = {"student": student.username, "task_id": _get_task_id_from_xmodule_args(xmodule_instance_args)}
request_token=None, course=course
course_id = student_module.course_id student = student_module.student usage_key = student_module.module_state_key
msg = "No module {loc} for student {student}--access denied?".format( loc=usage_key, student=student ) TASK_LOG.debug(msg) raise UpdateProblemModuleStateError(msg)
msg = "Specified problem does not support rescoring." raise UpdateProblemModuleStateError(msg)
track_function = _get_track_function_for_task(student_module.student, xmodule_instance_args) track_function('problem_delete_state', {}) return UPDATE_STATUS_SUCCEEDED
output_buffer = StringIO(render_to_string("instructor/instructor_dashboard_2/executive_summary.html", data_dict))
header = None rows = [] err_rows = [["id", "username", "error_msg"]] current_step = {'step': 'Calculating Grades'}
if task_progress.attempted % status_interval == 0: task_progress.update_task_state(extra_meta=current_step) task_progress.attempted += 1
task_progress.failed += 1 err_rows.append([student.id, student.username, err_msg])
upload_csv_to_report_store(rows, 'grade_report', course_id, start_date)
if len(err_rows) > 1: upload_csv_to_report_store(err_rows, 'grade_report_err', course_id, start_date)
TASK_LOG.info(u'%s, Task type: %s, Finalizing grade task', task_info_string, action_name) return task_progress.update_task_state(extra_meta=current_step)
for block in blocks: if blocks[block]['block_type'] == 'sequential': block_format = blocks[block]['format'] if block_format not in assignments: assignments[block_format] = OrderedDict() assignments[block_format][block] = list()
problem_location = task_input.get('problem_location') student_data = list_problem_responses(course_id, problem_location) features = ['username', 'state'] header, rows = format_dictlist(student_data, features)
problem_location = re.sub(r'[:/]', '_', problem_location) csv_name = 'student_state_from_{}'.format(problem_location) upload_csv_to_report_store(rows, csv_name, course_id, start_date)
header_row = OrderedDict([('id', 'Student ID'), ('email', 'Email'), ('username', 'Username')])
if not err_msg: err_msg = u"Unknown error" error_rows.append(student_fields + [err_msg]) task_progress.failed += 1 continue
query_features = task_input.get('features') student_data = enrolled_students_features(course_id, query_features) header, rows = format_dictlist(student_data, query_features)
upload_csv_to_report_store(rows, 'student_profile_info', course_id, start_date)
if task_progress.attempted % status_interval == 0: task_progress.update_task_state(extra_meta=current_step) task_progress.attempted += 1
display_headers.append(enrollment_report_headers.get(header_element, header_element))
upload_csv_to_report_store(rows, 'enrollment_report', course_id, start_date, config_name='FINANCIAL_REPORTS')
TASK_LOG.info(u'%s, Task type: %s, Finalizing detailed enrollment task', task_info_string, action_name) return task_progress.update_task_state(extra_meta=current_step)
query_features = task_input.get('features') student_data = list_may_enroll(course_id, query_features) header, rows = format_dictlist(student_data, query_features)
upload_csv_to_report_store(rows, 'may_enroll_info', course_id, start_date)
data_dict = get_executive_report(course_id) data_dict.update( { 'total_enrollments': true_enrollment_count, 'report_generation_date': report_generation_date.strftime("%Y-%m-%d"), } )
upload_csv_to_report_store(csv_rows, 'course_survey_results', course_id, start_date)
query_features = _task_input.get('features') student_data = get_proctored_exam_results(course_id, query_features) header, rows = format_dictlist(student_data, query_features)
upload_csv_to_report_store(rows, 'proctored_exam_results_report', course_id, start_date)
students_to_generate_certs_for = students_to_generate_certs_for.filter( certificatewhitelist__course_id=course_id, certificatewhitelist__whitelist=True )
students_to_generate_certs_for = students_to_generate_certs_for.filter( certificatewhitelist__course_id=course_id, certificatewhitelist__whitelist=True ).exclude( generatedcertificate__course_id=course_id, generatedcertificate__status__in=CertificateStatuses.PASSED_STATUSES )
students_require_certs = students_to_generate_certs_for
invalidate_generated_certificates(course_id, students_to_generate_certs_for, statuses_to_regenerate)
for student in students_require_certs: task_progress.attempted += 1 status = generate_user_certificates( student, course_id, course=course )
with DefaultStorage().open(task_input['file_name']) as f: total_assignments = 0 for _line in unicodecsv.DictReader(UniversalNewlineIterator(f)): total_assignments += 1
cohorts_status = {}
username_or_email = row.get('email') or row.get('username') cohort_name = row.get('cohort') or '' task_progress.attempted += 1
task_progress.skipped += 1
students_require_certificates = enrolled_students.filter( generatedcertificate__course_id=course_id, generatedcertificate__status__in=statuses_to_regenerate ) return list(students_require_certificates)
students_already_have_certs = User.objects.filter( ~Q(generatedcertificate__status=CertificateStatuses.unavailable), generatedcertificate__course_id=course_id)
return list(set(enrolled_students) - set(students_already_have_certs))
certificates.update( status=CertificateStatuses.unavailable, verify_uuid='', download_uuid='', download_url='', grade='', )
status = InstructorTaskModuleTestCase.get_task_status(instructor_task.task_id) self.assertEqual(status['message'], expected_message)
self.setup_user()
problem_url_name = 'H1P1' self.define_option_problem(problem_url_name) location = InstructorTaskModuleTestCase.problem_location(problem_url_name) descriptor = self.module_store.get_item(location)
self.redefine_option_problem(problem_url_name) self.render_problem('u1', problem_url_name) self.check_state('u1', descriptor, 2, 2, 1)
self.define_randomized_custom_response_problem(problem_url_name, redefine=True) self.render_problem('u1', problem_url_name) self.check_state('u1', descriptor, 1, 1, 2)
self.submit_rescore_all_student_answers('instructor', problem_url_name)
for username in userlist: self.check_state(username, descriptor, 0, 1, 2)
self.submit_student_answer(self.student_a.username, problem_a_url, [OPTION_1, OPTION_1]) self.submit_student_answer(self.student_b.username, problem_b_url, [OPTION_1, OPTION_2])
instructor_task = InstructorTask.objects.get(id=instructor_task.id) instructor_task.task_state = PROGRESS instructor_task.save()
self.assertTrue(certificate_generation_history.exists())
self.assertTrue(certificate_generation_history.exists())
progress = {'message': TEST_FAILURE_MESSAGE, 'exception': TEST_FAILURE_EXCEPTION, } return self._create_entry(task_state=FAILURE, task_output=progress)
chapter = ItemFactory.create(parent_location=self.course.location, display_name=TEST_SECTION_NAME)
self.problem_section = ItemFactory.create(parent_location=chapter.location, category='sequential', metadata={'graded': True, 'format': 'Homework'}, display_name=TEST_SECTION_NAME)
csv_rows = [row for row in unicodecsv.DictReader(csv_file)]
instructor_task = self._create_entry() succeeded, message = get_task_completion_info(instructor_task) self.assertFalse(succeeded) self.assertEquals(message, "No status information available")
instructor_task = self._create_success_entry() instructor_task.task_output = None succeeded, message = get_task_completion_info(instructor_task) self.assertFalse(succeeded) self.assertEquals(message, "No status information available")
mock_create_subtask_fcn_args = mock_create_subtask_fcn.call_args_list self.assertEqual(len(mock_create_subtask_fcn_args[0][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[1][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[2][0][0]), 2)
mock_create_subtask_fcn_args = mock_create_subtask_fcn.call_args_list self.assertEqual(len(mock_create_subtask_fcn_args[0][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[1][0][0]), 3) self.assertEqual(len(mock_create_subtask_fcn_args[2][0][0]), 5)
self._assert_num_attempts(students, initial_attempts) self._test_run_with_task(reset_problem_attempts, 'reset', num_students) self._assert_num_attempts(students, 0)
self._assert_num_attempts(students, initial_attempts) self._test_run_with_task(reset_problem_attempts, 'reset', 0, expected_num_skipped=num_students) self._assert_num_attempts(students, 0)
self._verify_cell_data_for_user(user_1, course.id, 'Cohort Name', '') self._verify_cell_data_for_user(user_2, course.id, 'Cohort Name', '')
self.initialize_course( course_factory_kwargs={ 'user_partitions': [user_partition] } )
cohort_a = CohortFactory.create(course_id=course.id, name=u'Cohørt A', users=[user_a]) CourseUserGroupPartitionGroup( course_user_group=cohort_a, partition_id=cohort_scheme_partition.id, group_id=cohort_scheme_partition.groups[0].id ).save()
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertTrue('Activate Course Enrollment' in response.content)
for row in unicodecsv.DictReader(csv_file): if row.get('Username') == username: self.assertEqual(row[column_header], expected_cell_content)
problem_section = ItemFactory.create(parent_location=chapter.location, category='sequential', metadata={'graded': True, 'format': problem_section_format}, display_name=problem_section_name)
problem_vertical = ItemFactory.create( parent_location=problem_section.location, category='vertical', display_name=problem_vertical_name ) problem_vertical_list.append(problem_vertical)
expected_grades = [self._format_user_grade(header_row, **user_grade) for user_grade in user_grades] self.verify_rows_in_csv(expected_grades)
self.assertTrue(cohorts.is_course_cohorted(self.course.id))
for user in [self.alpha_user, self.beta_user, self.non_cohorted_user]: self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
expected_grades = [self._format_user_grade(header_row, **grade) for grade in user_grades] self.verify_rows_in_csv(expected_grades)
self.assertTrue('Activate Course Enrollment' in response.content)
resp = self.client.post(reverse('shoppingcart.views.use_code'), {'code': 'coupon1'}) self.assertEqual(resp.status_code, 200)
num_students = len(students) self.assertDictContainsSubset({'attempted': num_students, 'succeeded': num_students, 'failed': 0}, result)
num_enrollments = len(enrollments) self.assertDictContainsSubset({'attempted': num_enrollments, 'succeeded': num_enrollments, 'failed': 0}, result)
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
for student in students[2:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
for student in students[:3]: CertificateWhitelistFactory.create( user=student, course_id=self.course.id, whitelist=True )
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=status, )
for student in students[:3]: self.assertIn( GeneratedCertificate.certificate_for_student(student, self.course.id).status, CertificateStatuses.PASSED_STATUSES )
for student in students[3:]: self.assertIsNone( GeneratedCertificate.certificate_for_student(student, self.course.id) )
students = self._create_students(5)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=status, )
for student in students[:4]: CertificateWhitelistFactory.create( user=student, course_id=self.course.id, whitelist=True )
for student in students[:4]: self.assertIn( GeneratedCertificate.certificate_for_student(student, self.course.id).status, CertificateStatuses.PASSED_STATUSES )
self.assertIsNone( GeneratedCertificate.certificate_for_student(students[4], self.course.id) )
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor' )
for student in students[5:6]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.deleted, mode='honor' )
for student in students[:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = {'statuses_to_regenerate': [CertificateStatuses.downloadable, CertificateStatuses.error]}
default_grade = '-1'
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor', grade=default_grade )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor', grade=default_grade )
for student in students[5:6]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.deleted, mode='honor', grade=default_grade )
for student in students[:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = {'statuses_to_regenerate': [CertificateStatuses.deleted, CertificateStatuses.generating]}
self.assertEqual(certificate_grades.count('0.0'), 5) self.assertEqual(certificate_grades.count(default_grade), 5)
default_grade = '-1'
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor', grade=default_grade )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor', grade=default_grade )
for student in students[5:7]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.unavailable, mode='honor', grade=default_grade )
for student in students[7:]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.generating, mode='honor', grade=default_grade )
for student in students[:]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = { 'statuses_to_regenerate': [ CertificateStatuses.downloadable, CertificateStatuses.error, CertificateStatuses.generating ] }
self.assertEqual(certificate_statuses.count(CertificateStatuses.generating), 8) self.assertEqual(certificate_statuses.count(CertificateStatuses.unavailable), 2)
self.assertEqual(certificate_grades.count('0.0'), 8) self.assertEqual(certificate_grades.count(default_grade), 2)
unavailable_certificates = \ [cert for cert in generated_certificates if cert.status == CertificateStatuses.unavailable and cert.grade == default_grade]
students = self._create_students(10)
for student in students[:2]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
for student in students[2:5]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.error, mode='honor' )
for student in students[5:6]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.deleted, mode='honor' )
for student in students[6:7]: GeneratedCertificateFactory.create( user=student, course_id=self.course.id, status=CertificateStatuses.notpassing, mode='honor' )
for student in students[:7]: CertificateWhitelistFactory.create(user=student, course_id=self.course.id, whitelist=True)
task_input = {'student_set': "all_whitelisted"}
MAX_DATABASE_LOCK_RETRIES = 5
if items_for_task: yield items_for_task num_items_queued += len(items_for_task)
entry.save_now() return task_progress
total_num_subtasks = _get_number_of_subtasks(total_num_items, items_per_task) subtask_id_list = [str(uuid4()) for _ in range(total_num_subtasks)]
item_list_generator = _generate_items_for_subtask( item_querysets, item_fields, total_num_items, items_per_task, total_num_subtasks, entry.course_id, )
return progress
key = "subtask-{}".format(task_id) cache.delete(key)
subtask_status_info[current_task_id] = new_subtask_status.to_dict()
action_name = ugettext_noop('rescored') update_fcn = partial(rescore_problem_module_state, xmodule_instance_args)
action_name = ugettext_noop('reset') update_fcn = partial(reset_attempts_module_state, xmodule_instance_args) visit_fcn = partial(perform_module_state_update, update_fcn, None) return run_main_task(entry_id, visit_fcn, action_name)
action_name = ugettext_noop('deleted') update_fcn = partial(delete_problem_module_state, xmodule_instance_args) visit_fcn = partial(perform_module_state_update, update_fcn, None) return run_main_task(entry_id, visit_fcn, action_name)
action_name = ugettext_noop('emailed') visit_fcn = perform_delegate_email_batches return run_main_task(entry_id, visit_fcn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_problem_responses_csv, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_students_csv, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generating_enrollment_report') task_fn = partial(upload_enrollment_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = 'generating_exec_summary_report' task_fn = partial(upload_exec_summary_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_course_survey_report, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('generated') task_fn = partial(upload_may_enroll_csv, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
action_name = ugettext_noop('cohorted') task_fn = partial(cohort_students_and_upload, xmodule_instance_args) return run_main_task(entry_id, task_fn, action_name)
super(ViewsExceptionTestCase, self).setUp()
self.course = CourseFactory.create(org='MITx', course='999', display_name='Robot Super Course')
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org' password = 'test'
self.student = UserFactory(username=uname, password=password, email=email)
CourseEnrollmentFactory(user=self.student, course_id=self.course.id)
self.client = Client() assert_true(self.client.login(username=uname, password=password))
mock_threads.return_value = [], 1, 1
mock_from_django_user.return_value = Mock()
mock_threads.return_value = CommentClientPaginatedResult(collection=[], page=1, num_pages=1)
mock_from_django_user.return_value = Mock()
if kwargs.get('params', {}).get('course_id'): data.update({ "threads_count": 1, "comments_count": 2 })
self.assertEquals( response_data["content"], strip_none(make_mock_thread_data(course=self.course, text=text, thread_id=thread_id, num_children=1)) ) mock_request.assert_called_with( "get",
self.assertEquals( response_data["content"], strip_none(make_mock_thread_data(course=self.course, text=text, thread_id=thread_id, num_children=1)) ) mock_request.assert_called_with( "get",
cached_calls = [ [num_uncached_mongo_calls, num_uncached_sql_queries], [num_cached_mongo_calls, num_cached_sql_queries], ] for expected_mongo_calls, expected_sql_queries in cached_calls: with self.assertNumQueries(expected_sql_queries): with check_mongo_calls(expected_mongo_calls): call_single_thread()
self.assertRegexpMatches(html, r'&#34;group_name&#34;: &#34;student_cohort&#34;')
self.assert_can_access(self.beta_user, self.alpha_module.discussion_id, thread_id, False)
self.assert_can_access(self.beta_user, self.alpha_module.discussion_id, thread_id, True)
try: CourseUserGroup.objects.get(id=group_id) kwargs['group_id'] = group_id except CourseUserGroup.DoesNotExist: pass
params_without_course_id = get_params_from_user_info_call(False) self.assertNotIn("group_id", params_without_course_id)
verify_group_id_always_present(profiled_user=self.student, pass_group_id=True) verify_group_id_always_present(profiled_user=self.student, pass_group_id=False) verify_group_id_always_present(profiled_user=self.moderator, pass_group_id=True) verify_group_id_always_present(profiled_user=self.moderator, pass_group_id=False)
verify_group_id_present(profiled_user=self.student, pass_group_id=True) verify_group_id_present(profiled_user=self.moderator, pass_group_id=True) verify_group_id_present( profiled_user=self.student, pass_group_id=True, requested_cohort=self.student_cohort )
verify_group_id_not_present(profiled_user=self.student, pass_group_id=False) verify_group_id_not_present(profiled_user=self.moderator, pass_group_id=False)
with super(InlineDiscussionUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(ForumFormDiscussionUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(ForumDiscussionSearchUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(SingleThreadUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create(discussion_topics={'dummy_discussion_id': {'id': 'dummy_discussion_id'}})
with super(UserProfileUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(FollowedThreadsUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
if discussion_id is not None: default_query_params['commentable_id'] = discussion_id if get_team(discussion_id) is not None: default_query_params['context'] = ThreadContext.STANDALONE
cc_user = cc.User.from_django_user(request.user) cc_user.default_sort_key = request.GET.get('sort_key') cc_user.save()
if 'pinned' not in thread: thread['pinned'] = False
return render_to_response('discussion/index.html', context)
thread_context = getattr(thread, "context", "course") if thread_context == "course" and not utils.discussion_category_id_access(course, request.user, discussion_id): raise Http404
if "pinned" not in thread: thread["pinned"] = False
}
call_command('seed_permissions_roles', unicode(self.course_id))
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org'
CourseEnrollmentFactory(user=self.student, course_id=self.course_id)
CourseEnrollmentFactory(user=self.moderator, course_id=self.course.id) self.moderator.roles.add(Role.objects.get(name="Moderator", course_id=self.course.id))
call_command('seed_permissions_roles', unicode(cls.course_id))
super(ViewsTestCase, self).setUp()
with patch('student.models.cc.User.save'): uname = 'student' email = 'student@edx.org'
CourseEnrollmentFactory(user=self.student, course_id=self.course_id)
CourseEnrollmentFactory(user=self.moderator, course_id=self.course.id) self.moderator.roles.add(Role.objects.get(name="Moderator", course_id=self.course.id))
team.add_user(self.student)
self.create_thread_helper(mock_request, extra_response_data={'context': ThreadContext.STANDALONE})
with super(ViewPermissionsTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(CreateThreadUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(UpdateThreadUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(CreateCommentUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(UpdateCommentUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(CreateSubCommentUnicodeTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
cls.team_commentable_id = "team_discussion_id" cls.team = CourseTeamFactory.create( name=u'The Only Team', course_id=cls.course.id, topic_id='topic_id', discussion_topic_id=cls.team_commentable_id )
cls.course_commentable_id = "course_level_commentable"
thread_author = getattr(self, thread_author) self._setup_mock(
with super(ForumEventTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
with super(UsersEndpointTestCase, cls).setUpClassAndTestData(): cls.course = CourseFactory.create()
self.enrollment.delete()
} track_created_event(request, event_name, course, thread, event_data)
if get_team(commentable_id) is not None: params['context'] = ThreadContext.STANDALONE else: params['context'] = ThreadContext.COURSE
try: group_id = get_group_id_for_comments_service(request, course_key, commentable_id) except ValueError: return HttpResponseBadRequest("Invalid cohort id") if group_id is not None: thread.group_id = group_id
if 'pinned' not in thread.attributes: thread['pinned'] = False
return JsonError(status=404)
return JsonError(["username parameter is required"])
return check_question_author(user, Thread(id=content["thread_id"]).to_dict())
logging.warning("Did not find key commentable_id in content.") passes_condition = False
import datetime import json import ddt import mock from nose.plugins.attrib import attr from pytz import UTC from django.utils.timezone import UTC as django_utc
test_discussion = self.store.create_child(self.user.id, course.location, 'discussion', 'test_discussion')
self.assertNotIn(test_discussion.location, self.store.get_orphans(course.id))
self.assertEqual(len(utils.get_accessible_discussion_modules(course, self.user)), 1)
self.assertIn(orphan, self.store.get_orphans(course.id))
start=datetime.datetime(2012, 2, 3, tzinfo=UTC)
self.course.discussion_topics = {} self.course.save() self.discussion_num = 0 self.instructor = InstructorFactory(course_key=self.course.id)
set_course_cohort_settings( course_key=self.course.id, is_cohorted=False, cohorted_discussions=["Topic_A"], always_cohort_inline_discussions=False, ) check_cohorted_topics([])
check_cohorted(False)
set_course_cohort_settings(course_key=self.course.id, is_cohorted=False) check_cohorted(False)
set_course_cohort_settings(course_key=self.course.id, is_cohorted=True) check_cohorted(True)
self.assertFalse( utils.is_commentable_cohorted(course.id, to_id("General")), "Course doesn't even have a 'General' topic" )
config_course_cohorts(course, is_cohorted=False, discussion_topics=["General", "Feedback"])
config_course_cohorts(course, is_cohorted=True, discussion_topics=["General", "Feedback"])
config_course_cohorts( course, is_cohorted=True, discussion_topics=["General", "Feedback"], cohorted_discussions=["Feedback"] )
self.assertFalse(utils.is_commentable_cohorted(course.id, team.discussion_topic_id)) self.assertTrue(utils.is_commentable_cohorted(course.id, "random"))
raise SkipTest
server_port = 4567 self.server_url = 'http://127.0.0.1:%d' % server_port
server_thread = threading.Thread(target=self.server.serve_forever) server_thread.daemon = True server_thread.start()
response = urllib2.urlopen(req)
response_dict = json.loads(response.read())
self.assertEqual(response_dict, self.expected_response)
length = int(self.headers.getheader('content-length')) data_string = self.rfile.read(length) post_dict = json.loads(data_string)
logger.debug( "Comment Service received POST request {0} to path {1}" .format(json.dumps(post_dict), self.path) )
if 'X-Edx-Api-Key' in self.headers: response = self.server._response_str logger.debug("Comment Service: sending response %s", json.dumps(response))
self.send_response(200) self.send_header('Content-type', 'application/json') self.end_headers() self.wfile.write(response)
self.send_response(500, 'Bad Request: does not contain API key') self.send_header('Content-type', 'text/plain') self.end_headers() return False
length = int(self.headers.getheader('content-length')) data_string = self.rfile.read(length) post_dict = json.loads(data_string)
logger.debug( "Comment Service received PUT request {0} to path {1}" .format(json.dumps(post_dict), self.path) )
if 'X-Edx-Api-Key' in self.headers: response = self.server._response_str logger.debug("Comment Service: sending response %s", json.dumps(response))
self.send_response(200) self.send_header('Content-type', 'application/json') self.end_headers() self.wfile.write(response)
self.send_response(500, 'Bad Request: does not contain API key') self.send_header('Content-type', 'text/plain') self.end_headers() return False
HTTPServer.shutdown(self)
self.socket.close()
self.TA_role_2.inherit_permissions(self.TA_role)
roles = get_role_ids(course_id) for role in roles: if user.id in roles[role]: return True return False
category_start_date = None for entry in entries: if category_start_date is None or entry["start_date"] < category_start_date: category_start_date = entry["start_date"]
dupe_counters[title] += 1 title = u"{title} ({counter})".format(title=title, counter=dupe_counters[title])
query_time = query.get('duration', 0) / 1000
if content.get('group_id') is not None: content['group_name'] = get_cohort_by_id(course_key, content.get('group_id')).name
content.pop('group_id', None)
group_id = get_cohort_id(request.user, course_key)
return None
ans = False
ans = commentable_id in course_cohort_settings.cohorted_discussions
ans = True
multi_db = True
from __future__ import unicode_literals
from __future__ import unicode_literals
StudentModuleHistory( id=initial_id, course_key=None, usage_key=None, username="", version="", created=datetime.datetime.now(), ).save()
return "BIGSERIAL"
from course_modes.models import CourseMode
eligible_certificates = EligibleCertificateManager()
objects = models.Manager()
return _("regenerated") if self.is_regeneration else _("generated")
return _("All learners")
from course_modes.models import CourseMode
if 'honor' not in course_mode_slugs: cert_status['status'] = CertificateStatuses.auditing return cert_status
from course_modes.models import CourseMode cert_set = cls.objects.create(course_key=course_key)
STATUS_STARTED = 'started' STATUS_SUCCESS = 'success' STATUS_ERROR = 'error'
EXAMPLE_FULL_NAME = u'John Doë'
url( r'^user/(?P<user_id>[^/]*)/course/{course_id}'.format(course_id=settings.COURSE_ID_PATTERN), views.render_html_view, name='html_view' ),
url( r'^(?P<certificate_uuid>[0-9a-f]{32})$', views.render_cert_by_uuid, name='render_cert_by_uuid' ),
from . import signals
resp = self.client.get(self.get_url(self.student.username)) self.assertEqual(resp.status_code, status.HTTP_401_UNAUTHORIZED)
self.dot_access_token.expires = datetime.utcnow() - timedelta(weeks=1) self.dot_access_token.save() self.assert_oauth_status(self.dot_access_token, status.HTTP_401_UNAUTHORIZED)
course = modulestore().get_course(course_key, depth=2)
ret = generate_user_certificates( student, course_key, course=course, insecure=options['insecure'] )
ret = regenerate_user_certificates( student, course_id, course=course, forced_grade=options['grade_value'], template_file=options['template_file'], insecure=options['insecure'] )
status_headings = sorted( set([status for course in cert_data for status in cert_data[course]]) )
print "{:>26}".format("course ID"), print ' '.join(["{:>16}".format(heading) for heading in status_headings])
$ ./manage.py lms resubmit_error_certificates
$ ./manage.py lms resubmit_error_certificates -c edX/DemoX/Fall_2015 -c edX/DemoX/Spring_2016
queryset = (
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
"download_url": ( cert.download_url or get_certificate_url(cert.user.id, cert.course_id) if cert.status == CertificateStatuses.downloadable else None ),
if not settings.FEATURES.get('CERTIFICATES_HTML_VIEW', False): return False
return course.cert_html_view_enabled if course else False
course_organization = get_course_organizations(course_key) if course_organization: org_id = course_organization[0]['id']
terms_of_service_and_honor_code = branding_api.get_tos_and_honor_code_url() if terms_of_service_and_honor_code != branding_api.EMPTY_URL: data.update({'company_tos_url': terms_of_service_and_honor_code})
privacy_policy = branding_api.get_privacy_url() if privacy_policy != branding_api.EMPTY_URL: data.update({'company_privacy_url': privacy_policy})
about = branding_api.get_about_url() if about != branding_api.EMPTY_URL: data.update({'company_about_url': about})
if course is None: course = modulestore().get_course(course_id, depth=0)
self.request.user = student self.request.session = {}
if cert_mode == CourseMode.CREDIT_MODE: cert_mode = CourseMode.VERIFIED
template_pdf = "certificate-template-{id.org}-{id.course}.pdf".format(id=course_id)
if is_whitelisted: LOGGER.info( u"Student %s is whitelisted in '%s'", student.id, unicode(course_id) ) passing = True else: passing = False
if self.restricted.filter(user=student).exists(): cert.status = status.restricted cert.save()
return self._generate_cert(cert, course, student, grade_contents, template_pdf, generate_pdf)
'example_certificate': True,
callback_url_path = reverse('certificates.views.update_example_certificate')
params, response = _validate_post_params(request.POST) if response is not None: return response
try: api.regenerate_user_certificates(params["user"], params["course_key"], course=course)
params, response = _validate_post_params(request.POST) if response is not None: return response
CourseOverview.get_from_id(params["course_key"])
generate_certificates_for_students( request, params["course_key"], student_set="specific_student", specific_student_id=params["user"].id ) return HttpResponse(200)
from .xqueue import * from .support import * from .webview import *
certificate_type = context.get('certificate_type')
context['document_title'] = _("{partner_short_name} {course_number} Certificate | {platform_name}").format( partner_short_name=context['organization_short_name'], course_number=context['course_number'], platform_name=platform_name )
context.update(configuration.get('default', {}))
reserved = _("All rights reserved") context['copyright_text'] = u'&copy; {year} {platform_name}. {reserved}.'.format( year=settings.COPYRIGHT_YEAR, platform_name=platform_name, reserved=reserved )
context['document_title'] = _("Invalid Certificate")
context['company_tos_urltext'] = _("Terms of Service &amp; Honor Code")
context['company_privacy_urltext'] = _("Privacy Policy")
context['logo_subtitle'] = _("Certificate Validation")
context['accomplishment_copy_about'] = _('About {platform_name} Accomplishments').format( platform_name=platform_name )
context['certificate_date_issued_title'] = _("Issued On:")
context['certificate_id_number_title'] = _('Certificate ID Number')
context['company_about_description'] = _("{platform_name} offers interactive online classes and MOOCs.").format( platform_name=platform_name)
context['document_banner'] = _("{platform_name} acknowledges the following student accomplishment").format( platform_name=platform_name )
context['accomplishment_copy_course_description'] = _('a course of study offered by ' '{partner_short_name}.').format( partner_short_name=context['organization_short_name'], platform_name=platform_name)
context['accomplishment_banner_opening'] = _("{fullname}, you earned a certificate!").format( fullname=user_fullname )
context['accomplishment_copy_more_about'] = _("More about {fullname}'s accomplishment").format( fullname=user_fullname )
try: user_certificate = GeneratedCertificate.eligible_certificates.get( user=user, course_id=course_key, status=CertificateStatuses.downloadable ) except GeneratedCertificate.DoesNotExist: pass
course_key = course.location.course_key
context = {} _update_context_with_basic_info(context, course_id, platform_name, configuration) invalid_template_path = 'certificates/invalid.html'
if not has_html_certificates_enabled(course_id): log.info( "Invalid cert: HTML certificates disabled for %s. User id: %d", course_id, user_id, ) return render_to_response(invalid_template_path, context)
try: course_key = CourseKey.from_string(course_id) user = User.objects.get(id=user_id) course = modulestore().get_course(course_key)
context.update(configuration.get(user_certificate.mode, {}))
_update_organization_context(context, course)
_update_course_context(request, context, course, platform_name)
_update_context_with_user_info(context, user, user_certificate)
_update_social_context(request, context, course, user, user_certificate, platform_name)
_update_certificate_context(context, user_certificate, platform_name)
_update_badge_context(context, course, user)
_update_microsite_context(context, configuration)
context.update(get_certificate_header_context(is_secure=request.is_secure())) context.update(get_certificate_footer_context())
context.update(course.cert_html_view_overrides)
_track_certificate_events(request, context, course, user, user_certificate)
return _render_certificate_template(request, context, course, user_certificate)
if rate_limiter.is_rate_limit_exceeded(request): log.info(u"Bad request rate limit exceeded for update example certificate end-point.") return HttpResponseForbidden("Rate limit exceeded")
return JsonResponse({'return_code': 0})
cert = GeneratedCertificate.eligible_certificates.get(user=self.student, course_id=self.course.id) self.assertEqual(cert.status, CertificateStatuses.downloadable)
cache.clear()
CertificateGenerationConfiguration.objects.create(enabled=True)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, True) self._assert_enabled_for_course(self.COURSE_KEY, True)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, False) self._assert_enabled_for_course(self.COURSE_KEY, False)
CertificateGenerationConfiguration.objects.create(enabled=True)
certs_api.set_cert_generation_enabled(self.COURSE_KEY, True) self._assert_enabled_for_course(self.COURSE_KEY, True)
other_course = CourseLocator(org='other', course='other', run='other') self._assert_enabled_for_course(other_course, False)
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug=CourseMode.HONOR) with self._mock_xqueue() as mock_queue: certs_api.generate_example_certificates(self.COURSE_KEY)
self._assert_certs_in_queue(mock_queue, 1)
self._assert_cert_status({ 'description': 'honor', 'status': 'started' })
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug='honor') CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug='verified')
with self._mock_xqueue() as mock_queue: certs_api.generate_example_certificates(self.COURSE_KEY)
self._assert_certs_in_queue(mock_queue, 2)
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug=CourseMode.HONOR) data = certs_api.get_certificate_header_context(is_secure=True)
CourseModeFactory.create(course_id=self.COURSE_KEY, mode_slug=CourseMode.HONOR) data = certs_api.get_certificate_footer_context()
self.assertItemsEqual( data.keys(), ['company_about_url', 'company_privacy_url', 'company_tos_url'] )
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['ABOUT'], data['company_about_url'] )
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['PRIVACY'], data['company_privacy_url'] )
self.assertIn( settings.MICROSITE_CONFIGURATION['test_microsite']["urls"]['TOS_AND_HONOR'], data['company_tos_url'] )
set_prerequisite_courses(course.id, [unicode(pre_requisite_course.id)]) completed_milestones = milestones_achieved_by_user(student, unicode(pre_requisite_course.id)) self.assertEqual(len(completed_milestones), 0)
CourseEnrollmentFactory( user=self.user_2, course_id=self.course.id, is_active=True, mode='audit' ) CertificateWhitelistFactory(course_id=self.course.id, user=self.user_2)
self.assertTrue(mock_send.called) __, kwargs = mock_send.call_args_list[0]
self.assertFalse(mock_send.called)
self._assert_queue_task(mock_send, cert)
self.assertEqual(cert.status, ExampleCertificate.STATUS_STARTED)
self.assertEqual(cert.status, ExampleCertificate.STATUS_ERROR) self.assertIn(self.ERROR_MSG, cert.error_reason)
from uuid import uuid4
cache.clear()
for _ in range(100): response = self.client.post(self.url, data=payload) if response.status_code == 403: break
self.assertEqual(response.status_code, 403)
command = resubmit_error_certificates
CourseEnrollmentFactory.create( user=user, course_id=course_key, mode=mode )
GeneratedCertificate.eligible_certificates.create( user=user, course_id=course_key, status=status )
self._create_cert(self.courses[0].id, self.user, CertificateStatuses.error, mode)
with check_mongo_calls(1): self._run_command()
self._assert_cert_status(self.courses[0].id, self.user, CertificateStatuses.notpassing)
for idx in range(3): self._create_cert(self.courses[idx].id, self.user, CertificateStatuses.error)
self._run_command(course_key_list=[ unicode(self.courses[0].id), unicode(self.courses[1].id) ])
self._create_cert(self.courses[0].id, self.user, CertificateStatuses.error) self._create_cert(self.courses[1].id, self.user, other_status)
self._run_command()
self._assert_cert_status(self.courses[0].id, self.user, CertificateStatuses.notpassing) self._assert_cert_status(self.courses[1].id, self.user, other_status)
with check_mongo_calls(1): self._run_command()
self._assert_cert_status(phantom_course, self.user, CertificateStatuses.error)
self.support = UserFactory( username=self.SUPPORT_USERNAME, email=self.SUPPORT_EMAIL, password=self.SUPPORT_PASSWORD, ) SupportStaffRole().add_users(self.support)
self.student = UserFactory( username=self.STUDENT_USERNAME, email=self.STUDENT_EMAIL, password=self.STUDENT_PASSWORD, )
self.cert = GeneratedCertificate.eligible_certificates.create( user=self.student, course_id=self.CERT_COURSE_KEY, grade=self.CERT_GRADE, status=self.CERT_STATUS, mode=self.CERT_MODE, download_url=self.CERT_DOWNLOAD_URL, )
success = self.client.login(username=self.SUPPORT_USERNAME, password=self.SUPPORT_PASSWORD) self.assertTrue(success, msg="Couldn't log in as support staff")
if role is not None: role().add_users(user)
response = self._search("foo")
if role is not None: role().add_users(user)
response = self._regenerate()
cert = GeneratedCertificate.eligible_certificates.get(user=self.student) self.assertEqual(cert.status, CertificateStatuses.notpassing)
response = self._regenerate(course_key=self.CERT_COURSE_KEY) self.assertEqual(response.status_code, 400)
response = self._regenerate(username=self.STUDENT_USERNAME) self.assertEqual(response.status_code, 400)
CourseEnrollment.unenroll(self.student, self.CERT_COURSE_KEY)
response = self._regenerate( course_key=self.CERT_COURSE_KEY, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 400)
GeneratedCertificate.eligible_certificates.all().delete()
response = self._regenerate( course_key=self.CERT_COURSE_KEY, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 200)
num_certs = GeneratedCertificate.eligible_certificates.filter(user=self.student).count() self.assertEqual(num_certs, 1)
if role is not None: role().add_users(user)
response = self._generate()
response = self._generate(course_key=self.EXISTED_COURSE_KEY_2) self.assertEqual(response.status_code, 400)
response = self._generate(username=self.STUDENT_USERNAME) self.assertEqual(response.status_code, 400)
CourseEnrollment.unenroll(self.student, self.EXISTED_COURSE_KEY_2)
response = self._generate( course_key=self.EXISTED_COURSE_KEY_2, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 400)
GeneratedCertificate.eligible_certificates.all().delete()
response = self._generate( course_key=self.EXISTED_COURSE_KEY_2, username=self.STUDENT_USERNAME ) self.assertEqual(response.status_code, 200)
num_certs = GeneratedCertificate.eligible_certificates.filter(user=self.student).count() self.assertEqual(num_certs, 1)
TEST_DIR = path(__file__).dirname() TEST_DATA_DIR = 'common/test/data/' PLATFORM_ROOT = TEST_DIR.parent.parent.parent.parent TEST_DATA_ROOT = PLATFORM_ROOT / TEST_DATA_DIR
ExampleCertificateSet.objects.all().delete()
result = ExampleCertificateSet.latest_status(self.COURSE_KEY) self.assertIs(result, None)
certificate_template_asset.asset = SimpleUploadedFile('picture1.jpg', 'file contents') certificate_template_asset.save() self.assertEqual(certificate_template_asset.asset, 'certificate_template_assets/1/picture1.jpg')
certificate_template_asset.asset = SimpleUploadedFile('picture2.jpg', 'file contents') certificate_template_asset.save()
response = self.client.get(test_url) self.assertIn(str(self.cert.verify_uuid), response.content)
self.cert.mode = 'audit' self.cert.status = status self.cert.save()
response = self.client.get(test_url) self.assertIn(str(self.cert.verify_uuid), response.content)
self.assertContains(response, "<li class=\"wrapper-organization\">", 1)
self.assertIn('Cannot Find Certificate', response.content)
with self.assertRaises(Exception): self.client.get(test_url)
CertificateGenerationConfiguration.objects.create(enabled=True)
from datetime import datetime from mock import patch, Mock
default_store=ModuleStoreEnum.Type.mongo
return response
response = self.http_get_for_course(HTTP_AUTHORIZATION=None) self.assertEqual(response.status_code, 401)
response = self.http_get_for_course(HTTP_AUTHORIZATION=auth_header) self.assertEqual(response.status_code, 200)
response = self.http_get_for_course(course_id=unicode(self.empty_course.id), HTTP_AUTHORIZATION=auth_header) self.assertEqual(response.status_code, 404)
response = self.http_get(reverse(self.view), HTTP_AUTHORIZATION=auth_header) self.assertEqual(response.status_code, 200)
update_course_structure(unicode(self.course.id))
self.assertTrue(CourseStructure.objects.filter(course_id=self.course.id).exists()) response = self.http_get_for_course() self.assertEqual(response.status_code, 200)
results = (course for course in results if course.scope_ids.block_type == 'course')
results = (course for course in results if self.user_can_access_course(self.request.user, course))
return sorted(results, key=lambda course: unicode(course.id))
return Response(status=503, headers={'Retry-After': '120'})
redirect_to = get_next_url_for_login_page(request)
if request.user.is_authenticated(): return redirect(redirect_to)
form_descriptions = _get_form_descriptions(request)
ext_auth_response = _external_auth_intercept(request, initial_mode) if ext_auth_response is not None: return ext_auth_response
email = user.email if user.is_authenticated() else request.POST.get('email')
limiter.tick_bad_request_counter(request)
context["autoSubmitRegForm"] = True
for msg in messages.get_messages(request): if msg.extra_tags.split()[0] == "social-auth":
request = HttpRequest() request.method = "GET" request.session = session
view, args, kwargs = resolve(url) response = view(request, *args, **kwargs)
return response.content
context['duplicate_provider'] = pipeline.get_duplicate_provider(messages.get_messages(request))
u"{user}@example.com".format( user=(u'e' * (EMAIL_MAX_LENGTH - 11)) )
activation_key = create_account(self.USERNAME, self.OLD_PASSWORD, self.OLD_EMAIL) activate_account(activation_key)
result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD) self.assertTrue(result)
response = self._change_password() self.assertEqual(response.status_code, 200)
self.assertEqual(len(mail.outbox), 1)
response = self.client.get(activation_link) self.assertEqual(response.status_code, 200)
self.client.logout()
result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD) self.assertTrue(result)
result = self.client.login(username=self.USERNAME, password=self.OLD_PASSWORD) self.assertFalse(result)
result = self.client.login(username=self.USERNAME, password=self.NEW_PASSWORD) self.assertTrue(result)
self.client.logout()
self.client.logout()
create_account(self.ALTERNATE_USERNAME, self.OLD_PASSWORD, self.NEW_EMAIL)
response = self._change_password(email=self.NEW_EMAIL)
self.assertEqual(response.status_code, 200) self.assertEqual(len(mail.outbox), 1)
self.client.logout()
response = self._change_password(email=self.NEW_EMAIL) self.assertEqual(response.status_code, 400)
self.client.logout()
for attempt in xrange(self.INVALID_ATTEMPTS): self._change_password(email=self.NEW_EMAIL)
response = self.client.get(reverse(url_name)) self.assertRedirects(response, reverse("dashboard"))
with with_edx_domain_context(is_edx_domain): response = self.client.get(reverse(url_name), params)
with with_edx_domain_context(is_edx_domain): response = self.client.get(reverse(url_name), params)
else: response = self.client.get(reverse(url_name), params)
self.configure_google_provider(enabled=True) self.configure_facebook_provider(enabled=True)
self.settings_patcher = patch.dict('django.conf.settings.FEATURES', {'MILESTONES_APP': True}) self.settings_patcher.start()
self.chapter1 = ItemFactory.create( parent_location=self.course.location, category='chapter', display_name='untitled chapter 1' )
self.vert1 = ItemFactory.create( parent_location=self.seq1.location, category='vertical', display_name='untitled vertical 1' )
self.prob1 = ItemFactory.create( parent_location=self.vert1.location, category='problem', display_name='untitled problem 1' )
self.prob2 = ItemFactory.create( parent_location=self.course.location, category='problem', display_name='untitled problem 2' )
gating_api.add_prerequisite(self.course.id, self.seq1.location)
sync_cohort_with_mode.apply_async(kwargs=args, countdown=300)
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(course_key))
config = VerifiedTrackCohortedCourse.objects.create(course_key=course_key, enabled=True) config.save() self.assertTrue(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(course_key))
config.enabled = False config.save() self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(course_key))
celery_task_patcher = patch.object( sync_cohort_with_mode, 'apply_async', mock.Mock(wraps=sync_cohort_with_mode.apply_async) ) self.mocked_celery_task = celery_task_patcher.start() self.addCleanup(celery_task_patcher.stop)
self._enable_cohorting() self._create_verified_cohort() self.assertFalse(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(self.course.id)) self._verify_no_automatic_cohorting() self.assertFalse(error_logger.called)
self._enable_cohorting() self._create_verified_cohort() self._enable_verified_track_cohorting() self.assertTrue(VerifiedTrackCohortedCourse.is_verified_track_cohort_enabled(self.course.id)) self._enroll_in_course()
self._enable_cohorting() self._create_verified_cohort() self._create_named_random_cohort("Random 1") self._create_named_random_cohort("Random 2") self._enable_verified_track_cohorting()
self._unenroll() self.assertEqual(DEFAULT_VERIFIED_COHORT_NAME, get_cohort(self.user, self.course.id, assign=False).name)
modified_cohort_name = "renamed random cohort" default_cohort.name = modified_cohort_name default_cohort.save()
current_cohort = get_cohort(user, course_key) verified_cohort = get_cohort_by_name(course_key, verified_cohort_name)
structure_json = models.TextField(verbose_name='Structure JSON', blank=True, null=True)
from .overrides import get_override_for_ccx return get_override_for_ccx(self, self.course, 'start')
from .overrides import get_override_for_ccx return get_override_for_ccx(self, self.course, 'due')
from .overrides import get_override_for_ccx return get_override_for_ccx(self, self.course, 'max_student_enrollments_allowed')
return restore( self._modulestore._clean_locator_for_mapping(locator) )
return self._modulestore._get_modulestore_for_courselike(locator)
return view(request, course, ccx)
assign_coach_role_to_ccx(ccx_locator, request.user, course.id)
context = get_ccx_creation_dict(course) messages.error(request, context['use_ccx_con_error_message']) return render_to_response('ccx/coach_dashboard.html', context)
start = TODAY().replace(tzinfo=pytz.UTC) override_field_for_ccx(ccx, course, 'start', start) override_field_for_ccx(ccx, course, 'due', None)
override_field_for_ccx(ccx, course, 'max_student_enrollments_allowed', settings.CCX_MAX_STUDENTS_ALLOWED)
email_params = get_email_params(course, auto_enroll=True, course_key=ccx_id, display_name=ccx.display_name) enroll_email( course_id=ccx_id, student_email=request.user.email, auto_enroll=True, email_students=True, email_params=email_params, )
ccx_ids_to_delete.append(get_override_for_ccx(ccx, block, 'due_id')) clear_ccx_field_info_from_ccx_map(ccx, block, 'due')
if child.visible_to_staff_only: continue
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
continue
continue
error_code = 'course_id_not_provided' if not is_ccx: log.info('Master course ID not provided') error_code = 'master_course_id_not_provided'
if not ignore_missing: for field in mandatory_fields: if field not in request_data: field_errors[field] = {'error_code': 'missing_field_{0}'.format(field)} if field_errors: return valid_input, field_errors
valid_input['course_modules'] = None
valid_input, field_errors = get_valid_input(request.data) if field_errors: return Response( status=status.HTTP_400_BAD_REQUEST, data={ 'field_errors': field_errors } )
course_modules_json = json.dumps(valid_input.get('course_modules'))
start = TODAY().replace(tzinfo=pytz.UTC) override_field_for_ccx(ccx_course_object, master_course_object, 'start', start) override_field_for_ccx(ccx_course_object, master_course_object, 'due', None)
override_field_for_ccx( ccx_course_object, master_course_object, 'max_student_enrollments_allowed', valid_input['max_students_allowed'] )
make_user_coach(coach, master_course_key)
master_course_object, master_course_key, _, _ = get_valid_course(unicode(ccx_course_object.course_id))
response.data["current_page"] = self.page.number
response.data["start"] = (self.page.number - 1) * self.get_page_size(self.request)
instructor = UserFactory() allow_access(self.course, instructor, 'instructor')
staff_user = UserFactory(username='test_staff_user', email='test_staff_user@openedx.org', password='test') CourseStaffRole(self.master_course_key).add_users(staff_user)
instructor_user = UserFactory( username='test_instructor_user', email='test_instructor_user@openedx.org', password='test' ) CourseInstructorRole(self.master_course_key).add_users(instructor_user)
coach_user = UserFactory( username='test_coach_user', email='test_coach_user@openedx.org', password='test' ) CourseCcxCoachRole(self.master_course_key).add_users(coach_user)
resp = self.client.get(self.list_url_master_course, {}, HTTP_AUTHORIZATION=self.auth)
url = '{0}&order_by=display_name&sort_order=desc'.format(self.list_url_master_course) resp = self.client.get(url, {}, HTTP_AUTHORIZATION=self.auth)
for key, val in data.iteritems():
self.assertEqual(len(outbox), 1)
staff_user = User.objects.create_user('test_staff_user', 'test_staff_user@openedx.org', 'test') CourseStaffRole(self.master_course_key).add_users(staff_user)
instructor_user = User.objects.create_user('test_instructor_user', 'test_instructor_user@openedx.org', 'test') CourseInstructorRole(self.master_course_key).add_users(instructor_user)
coach_user = User.objects.create_user('test_coach_user', 'test_coach_user@openedx.org', 'test') CourseCcxCoachRole(self.master_course_key).add_users(coach_user)
return False
return True
cls.coach = AdminFactory.create()
with self.assertNumQueries(6): override_field_for_ccx(self.ccx, chapter, 'start', ccx_start)
with self.assertNumQueries(6): override_field_for_ccx(self.ccx, chapter, 'start', ccx_start)
response = render_to_response(path, context) response.mako_context = context response.mako_template = path return response
self.client.login(username=self.coach.username, password="test")
staff = UserFactory() allow_access(self.course, staff, 'staff') self.assertTrue(CourseStaffRole(self.course.id).has_user(staff))
instructor = UserFactory() allow_access(self.course, instructor, 'instructor') self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
path = urlparse.urlparse(url).path resolver = resolve(path) ccx_key = resolver.kwargs['course_id']
ccx = CustomCourseForEdX.objects.get() course_enrollments = get_override_for_ccx(ccx, self.course, 'max_student_enrollments_allowed') self.assertEqual(course_enrollments, settings.CCX_MAX_STUDENTS_ALLOWED)
role = CourseCcxCoachRole(course_key) self.assertTrue(role.has_user(self.coach))
list_staff_master_course = list_with_level(self.course, 'staff') list_instructor_master_course = list_with_level(self.course, 'instructor')
self.assertEqual(len(response.redirect_chain), 1) self.assertIn(302, response.redirect_chain[0]) self.assertEqual(len(outbox), outbox_count) if send_email:
self.assertTrue( CourseEnrollment.objects.filter(course_id=self.course.id, user=student).exists() )
ccx = self.make_ccx(max_students_allowed=2) ccx_course_key = CCXLocator.from_course_locator(self.course.id, ccx.id) staff = self.make_staff() instructor = self.make_instructor()
students = [instructor, staff, self.coach] + [ UserFactory.create(is_staff=False) for _ in range(3) ]
self.assertEqual(len(response.redirect_chain), 1) self.assertIn(302, response.redirect_chain[0]) self.assertEqual(len(outbox), outbox_count) if send_email:
self.assertFalse( CourseEnrollment.objects.filter(course_id=self.course.id, user=student).exists() )
self.assertEqual(len(response.redirect_chain), 1) self.assertIn(302, response.redirect_chain[0]) self.assertEqual(len(outbox), outbox_count)
if view_name == 'ccx_manage_student' and not is_email(identifier): self.assertContains(response, 'Could not find a user with name or email ', status_code=200)
with self.store.bulk_operations(course.id, emit_signals=False):
self.coach = UserFactory.create() self.mstore = modulestore()
self.client.login(username=self.coach.username, password="test")
staff = UserFactory() allow_access(self.course, staff, 'staff') self.assertTrue(CourseStaffRole(self.course.id).has_user(staff))
instructor = UserFactory() allow_access(self.course, instructor, 'instructor') self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
self.coach = coach = AdminFactory.create() self.client.login(username=coach.username, password="test")
role = CourseCcxCoachRole(self._course.id) role.add_users(coach) ccx = CcxFactory(course_id=self._course.id, coach=self.coach)
self.assertEqual( response['content-disposition'], 'attachment' ) rows = response.content.strip().split('\r') headers = rows[0]
self.coach = AdminFactory.create() role = CourseCcxCoachRole(self.split_course.id) role.add_users(self.coach)
self.ccx = CcxFactory(course_id=self.split_course.id, coach=self.coach) last_week = datetime.datetime.now(UTC()) - datetime.timedelta(days=7)
fake_course_key = CourseKey.from_string('course-v1:FakeOrg+CN1+CR-FALLNEVER1') self.assertEqual(utils.get_course_chapters(fake_course_key), None)
self.client.login(username=self.coach.username, password="test")
self.mstore = modulestore()
staff = self.make_staff() self.assertTrue(CourseStaffRole(self.course.id).has_user(staff))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
list_staff_master_course = list_with_level(self.course, 'staff') list_instructor_master_course = list_with_level(self.course, 'instructor')
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
instructor = self.make_instructor() self.assertTrue(CourseInstructorRole(self.course.id).has_user(instructor))
remove_master_course_staff_from_ccx( self.course, self.ccx_locator, self.ccx.display_name, send_email=True ) self.assertEqual(len(outbox), len(list_staff_master_course) + len(list_instructor_master_course))
remove_master_course_staff_from_ccx(self.course, self.ccx_locator, self.ccx.display_name) self.assertEqual(len(outbox), len(list_staff_master_course) + len(list_instructor_master_course))
multi_db = True
TEST_DATA = None
with self.settings(MODULESTORE_BRANCH='published-only'): for cache in settings.CACHES: caches[cache].clear()
modulestore().get_course(self.course.id, depth=None)
RequestCache.clear_request_cache()
OverrideFieldData.provider_classes = None
with cls.store.bulk_operations(course.id, emit_signals=False):
self.coach = UserFactory.create() self.mstore = modulestore()
date = date.strftime('%Y-%m-%d %H:%M')
date = master_date.strftime('%Y-%m-%d %H:%M')
date = get_date(ccx, node=parent_node, date_type=date_type)
if ccxs.exists(): return ccxs[0] return None
if staff not in list_staff_ccx: try: enroll_email( course_id=ccx_key, student_email=staff.email, auto_enroll=True, email_students=send_email, email_params=email_params, )
allow_access(course_ccx, staff, 'staff')
if instructor not in list_instructor_ccx: try: enroll_email( course_id=ccx_key, student_email=instructor.email, auto_enroll=True, email_students=send_email, email_params=email_params, )
allow_access(course_ccx, instructor, 'instructor')
revoke_access(course_ccx, staff, 'staff')
unenroll_email( course_id=ccx_key, student_email=staff.email, email_students=send_email, email_params=email_params, )
revoke_access(course_ccx, instructor, 'instructor')
unenroll_email( course_id=ccx_key, student_email=instructor.email, email_students=send_email, email_params=email_params, )
from __future__ import unicode_literals
exists_ce = is_active is not None and is_active full_name = user.profile.name
return UserPreference.get_value(user, LANGUAGE_KEY)
if CourseMode.is_white_label(course_id): course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG else: course_mode = None
send_mail_to_student(student_email, email_params, language=language)
problem_state = json.loads(studentmodule.state) problem_state["attempts"] = 0
studentmodule.state = json.dumps(problem_state) studentmodule.save()
if 'display_name' in param_dict: param_dict['course_name'] = param_dict['display_name']
message_type = param_dict['message']
message = message.strip()
subject = ''.join(subject.splitlines()) from_address = theming_helpers.get_value( 'email_from_address', settings.DEFAULT_FROM_EMAIL )
meta = {} if user_info.profile.meta: meta = json.loads(user_info.profile.meta)
registration_code_redemption = RegistrationCodeRedemption.registration_code_used_for_enrollment( course_enrollment) paid_course_reg_item = PaidCourseRegistration.get_course_item_for_user_enrollment( user=user, course_id=course_id, course_enrollment=course_enrollment )
list_price = 'N/A' payment_amount = 'N/A' coupon_codes_used = 'N/A' registration_code_used = 'N/A' payment_status = _('Data Integrity Error') transaction_reference_number = 'N/A'
if invoice_transaction.amount > 0: payment_status = 'Invoice Paid' else: payment_status = 'Refunded'
url(r'^get_proctored_exam_results$', 'instructor.views.api.get_proctored_exam_results', name="get_proctored_exam_results"),
url(r'^list_financial_report_downloads$', 'instructor.views.api.list_financial_report_downloads', name="list_financial_report_downloads"),
url(r'get_coupon_codes', 'instructor.views.api.get_coupon_codes', name="get_coupon_codes"),
url(r'^gradebook$', 'instructor.views.gradebook_api.spoc_gradebook', name='spoc_gradebook'),
url(r'add_users_to_cohorts$', 'instructor.views.api.add_users_to_cohorts', name="add_users_to_cohorts"),
url(r'^generate_example_certificates$', 'instructor.views.api.generate_example_certificates', name='generate_example_certificates'),
MAX_STUDENTS_PER_PAGE_GRADE_BOOK = 20
next_offset = offset + MAX_STUDENTS_PER_PAGE_GRADE_BOOK previous_offset = offset - MAX_STUDENTS_PER_PAGE_GRADE_BOOK
page_num = ((offset / MAX_STUDENTS_PER_PAGE_GRADE_BOOK) + 1)
total_pages = int(math.ceil(float(total_students) / MAX_STUDENTS_PER_PAGE_GRADE_BOOK)) or 1
previous_offset = None
next_offset = None
enrolled_students = enrolled_students[offset: offset + MAX_STUDENTS_PER_PAGE_GRADE_BOOK]
'staff_access': True, 'ordered_grades': sorted(course.grade_cutoffs.items(), key=lambda i: i[1], reverse=True),
email_feature_dict['email'] = email_info
success, task_message = get_task_completion_info(task) status = _("Complete") if success else _("Incomplete") task_feature_dict['status'] = status task_feature_dict['task_message'] = task_message
if CourseMode.is_white_label(course_id): course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG else: course_mode = None
email = student[EMAIL_INDEX] username = student[USERNAME_INDEX] name = student[NAME_INDEX] country = student[COUNTRY_INDEX][:2]
user = User.objects.get(email=email)
password = generate_unique_password(generated_passwords) errors = create_and_enroll_user( email, username, name, country, password, course_id, course_mode, request.user, email_params ) row_errors.extend(errors)
user = create_user_and_user_profile(email, username, name, country, password)
create_manual_course_enrollment( user=user, course_id=course_id, mode=course_mode, enrolled_by=enrolled_by, reason='Enrolling via csv upload', state_transition=UNENROLLED_TO_ENROLLED, )
user = None email = None language = None try: user = get_student_from_identifier(identifier) except User.DoesNotExist: email = identifier else: email = user.email language = get_user_email_language(user)
results.append({ 'identifier': identifier, 'invalidIdentifier': True, })
log.exception(u"Error while #{}ing student") log.exception(exc) results.append({ 'identifier': identifier, 'error': True, })
if email_students: send_beta_role_email(action, user, email_params) if auto_enroll: if not CourseEnrollment.is_enrolled(user, course_id): CourseEnrollment.enroll(user, course_id)
results.append({ 'identifier': identifier, 'error': error, 'userDoesNotExist': user_does_not_exist })
if not user.is_active: response_payload = { 'unique_student_identifier': user.username, 'inactiveUser': True, } return JsonResponse(response_payload)
run = problem_key.run if not run: problem_key = course_key.make_usage_key_from_deprecated_string(problem_location) if problem_key.course_key != course_key: raise InvalidKeyError(type(problem_key), problem_key)
query_features = microsite.get_value('student_profile_download_fields')
query_features.append('cohort') query_features_names['cohort'] = _('Cohort')
instructor_task.api.submit_cohort_students(request, course_key, filename)
matching_coupons = Coupon.objects.filter(code=code, is_active=True) if matching_coupons: return save_registration_code( user, course_id, mode_slug, invoice=invoice, order=order, invoice_item=invoice_item )
try: course_code_number = int(request.POST['total_registration_codes']) except ValueError: course_code_number = int(float(request.POST['total_registration_codes']))
subject = u'Confirmation and Invoice for {course_name}'.format(course_name=course.display_name) message = render_to_string('emails/registration_codes_sale_email.txt', context)
recipient_list.append(finance_email)
registration_codes_list = CourseRegistrationCode.objects.filter( course_id=course_id ).order_by('invoice_item__invoice__company_name')
if all_students and student: return HttpResponseBadRequest( "all_students and unique_student_identifier are mutually exclusive." ) if all_students and delete_module: return HttpResponseBadRequest( "all_students and delete_module are mutually exclusive." )
if all_students or delete_module: if not has_access(request.user, 'instructor', course): return HttpResponseForbidden("Requires instructor access.")
error_msg = _("An error occurred while deleting the score.") return HttpResponse(error_msg, status=500)
if all_students or delete_module: if not has_access(request.user, 'instructor', course): return HttpResponseForbidden(_("Requires instructor access."))
tasks = instructor_task.api.get_instructor_task_history(course_id, task_type=task_type)
emails = instructor_task.api.get_instructor_task_history(course_id, task_type=task_type)
tasks = instructor_task.api.get_instructor_task_history(course_id, module_state_key, student)
tasks = instructor_task.api.get_instructor_task_history(course_id, module_state_key)
tasks = instructor_task.api.get_running_instructor_tasks(course_id)
tasks = instructor_task.api.get_entrance_exam_instructor_task_history(course_id, entrance_exam_key, student)
tasks = instructor_task.api.get_entrance_exam_instructor_task_history(course_id, entrance_exam_key)
if not (has_forum_admin or has_instructor_access): return HttpResponseBadRequest( "Operation requires staff & forum admin or instructor access" )
if rolename == FORUM_ROLE_ADMINISTRATOR and not has_instructor_access: return HttpResponseBadRequest("Operation requires instructor access.")
if rolename not in [FORUM_ROLE_ADMINISTRATOR, FORUM_ROLE_MODERATOR, FORUM_ROLE_COMMUNITY_TA]: return HttpResponseBadRequest(strip_tags( "Unrecognized rolename '{}'.".format(rolename) ))
try: email = CourseEmail.create( course_id, request.user, targets, subject, message, template_name=template_name, from_addr=from_addr ) except ValueError as err: return HttpResponseBadRequest(repr(err))
instructor_task.api.submit_bulk_course_email(request, course_id, email.id)
if not (has_forum_admin or has_instructor_access): return HttpResponseBadRequest( "Operation requires staff & forum admin or instructor access" )
if rolename == FORUM_ROLE_ADMINISTRATOR and not has_instructor_access: return HttpResponseBadRequest("Operation requires instructor access.")
return JsonResponse( _("Successfully removed invalid due date extension (unit has no due date).") )
try: certificate_exception, student = parse_request_data_and_get_user(request, course_key) except ValueError as error: return JsonResponse({'success': False, 'message': error.message}, status=400)
pass
students = 'all_whitelisted'
return JsonResponse( { 'success': False, 'message': _('Invalid data, generate_for must be "new" or "all".'), }, status=400 )
if len(student) != 2: if len(student) > 0: build_row_errors('data_format_error', student[user_index], row_num)
try: certificate_invalidation_data = parse_request_data(request) certificate = validate_request_data_and_get_certificate(certificate_invalidation_data, course_key) except ValueError as error: return JsonResponse({'message': error.message}, status=400)
elif request.method == 'DELETE': try: re_validate_certificate(request, course_key, certificate) except ValueError as error: return JsonResponse({'message': error.message}, status=400)
certificate_invalidation = CertificateInvalidation.objects.get(generated_certificate=generated_certificate)
certificate_invalidation.deactivate()
student = certificate_invalidation.generated_certificate.user instructor_task.api.generate_certificates_for_students( request, course_key, student_set="specific_student", specific_student_id=student.id )
sections.append(_section_analytics(course, access))
if BulkEmailFlag.feature_enabled(course_key): sections.append(_section_send_email(course, access))
if settings.FEATURES['CLASS_DASHBOARD'] and access['staff']: sections.append(_section_metrics(course, access))
if course_mode_has_price and (access['finance_admin'] or access['sales_admin']): sections.append(_section_e_commerce(course, access, paid_modes[0], is_white_label, is_white_label))
certs_enabled = CertificateGenerationConfiguration.current().enabled if certs_enabled and access['admin']: sections.append(_section_certificates(course))
log.info('deleting redemption entry (%s) from the database.', code_redemption.id) code_redemption.delete()
with disable_overrides(): original_due_date = getattr(unit, 'due', None)
if not get_override_for_user(student, unit, 'due'): raise DashboardError(_("No due date extension is set for that student and unit."))
cls.audit_course = CourseFactory.create() CourseModeFactory.create(course_id=cls.audit_course.id, mode_slug=CourseMode.AUDIT)
self.white_label_course = CourseFactory.create() self.white_label_course_mode = CourseModeFactory.create( course_id=self.white_label_course.id, mode_slug=CourseMode.HONOR, min_price=10, suggested_prices='10', )
info_log.assert_called_with('email sent to new created user at %s', 'test_student@example.com')
info_log.assert_called_with('email sent to new created user at %s', 'test_student@example.com')
info_log.assert_called_with( u"user already exists with username '%s' and email '%s'", 'test_student_1', 'test_student@example.com' )
self.client.login(username=self.audit_course_instructor.username, password='test')
for enrollment in manual_enrollments: self.assertEqual(enrollment.enrollment.mode, CourseMode.AUDIT)
self.white_label_course_mode.min_price = 0 self.white_label_course_mode.suggested_prices = ''
self.client.login(username=self.white_label_course_instructor.username, password='test')
for enrollment in manual_enrollments: self.assertEqual(enrollment.enrollment.mode, CourseMode.HONOR)
self.client.login(username=self.white_label_course_instructor.username, password='test')
for enrollment in manual_enrollments: self.assertEqual(enrollment.enrollment.mode, CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG)
cea = CourseEnrollmentAllowed(email='robot-allowed@robot.org', course_id=self.course.id) cea.save() self.allowed_email = 'robot-allowed@robot.org'
user = User.objects.get(email=self.notenrolled_student.email) self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
self.assertEqual(len(mail.outbox), 0)
user = User.objects.get(email=self.notenrolled_student.email) self.assertTrue(CourseEnrollment.is_enrolled(user, self.course.id))
user = User.objects.get(email=self.enrolled_student.email) self.assertFalse(CourseEnrollment.is_enrolled(user, self.course.id))
self.assertEqual(len(mail.outbox), 0)
user = User.objects.get(email=self.enrolled_student.email) self.assertFalse(CourseEnrollment.is_enrolled(user, self.course.id))
mock_uses_shib.return_value = True
course_enrollment.mode = u'verified' course_enrollment.save() self.assertEqual(course_enrollment.mode, u'verified')
self._change_student_enrollment(self.enrolled_student, self.course, 'enroll')
course_enrollment.mode = u'verified' course_enrollment.save() self.assertEqual(course_enrollment.mode, u'verified')
expected = { "action": "add", "results": [ { "identifier": identifier, "error": False, "userDoesNotExist": False } ] }
self.assertEqual(len(mail.outbox), 0)
self.assertEqual(len(mail.outbox), 0)
if hasattr(self.beta_tester, '_roles'): del self.beta_tester._roles self.assertFalse(CourseBetaTesterRole(self.course.id).has_user(self.beta_tester))
self.assertEqual(len(mail.outbox), 0)
if hasattr(self.beta_tester, '_roles'): del self.beta_tester._roles self.assertFalse(CourseBetaTesterRole(self.course.id).has_user(self.beta_tester))
seed_permissions_roles(self.course.id)
self.assertEqual(response.status_code, 200)
self.assertTrue('Activate Course Enrollment' in response.content)
response = self.assert_request_status_code(400, url, method="POST", data=data) self.assertIn("The sale associated with this invoice has already been invalidated.", response.content)
data['event_type'] = "re_validate" self.assert_request_status_code(200, url, method="POST", data=data)
response = self.assert_request_status_code(400, url, method="POST", data=data) self.assertIn("This invoice is already active.", response.content)
coupon = Coupon( code='test_code', description='test_description', course_id=self.course.id, percentage_discount='10', created_by=self.instructor, is_active=True ) coupon.save()
CourseFinanceAdminRole(self.course.id).add_users(self.instructor)
resp = self.client.post(reverse('shoppingcart.views.use_code'), {'code': coupon.code}) self.assertEqual(resp.status_code, 200)
self.cart.purchase() resp = self.client.get(instructor_dashboard) self.assertEqual(resp.status_code, 200)
test_user = UserFactory() self.register_with_redemption_code(test_user, course_registration_code.code)
changed_module = StudentModule.objects.get(pk=self.module_to_reset.pk) self.assertEqual( json.loads(changed_module.state)['attempts'], 0 )
self.assertEqual( StudentModule.objects.filter( student=self.module_to_reset.student, course_id=self.module_to_reset.course_id, ).count(), 0 )
CourseInstructorRole(self.course_with_invalid_ee.id).add_users(self.instructor) self.client.login(username=self.instructor.username, password='test')
changed_modules = StudentModule.objects.filter(module_state_key__in=self.ee_modules) for changed_module in changed_modules: self.assertEqual( json.loads(changed_module.state)['attempts'], 0 )
changed_modules = StudentModule.objects.filter(module_state_key__in=self.ee_modules) self.assertEqual(changed_modules.count(), 0)
tasks = json.loads(response.content)['tasks'] self.assertEqual(len(tasks), 0)
response = self.client.post(url, { 'unique_student_identifier': self.student.email, })
message = _('This student (%s) is already allowed to skip the entrance exam.') % self.student.email self.assertContains(response, message)
self.duration_sec = 'unknown'
self.assertEqual(len(email_info), 1)
expected_message = self.emails[0].html_message returned_email_info = email_info[0] received_message = returned_email_info[u'email'][u'html_message'] self.assertEqual(expected_message, received_message)
self.assertEqual(len(email_info), 0)
certificate_count = 3 for __ in xrange(certificate_count): self.generate_certificate(course_id=self.course.id, mode='honor', status=CertificateStatuses.generating)
certificate_count = 3 for __ in xrange(certificate_count): self.generate_certificate(course_id=self.course.id, mode='honor', status=CertificateStatuses.downloadable)
for __ in xrange(certificate_count): self.generate_certificate( course_id=self.course.id, mode='verified', status=CertificateStatuses.downloadable )
self.assertEqual(len(res_json['certificates']), 2)
certificate = res_json['certificates'][1] self.assertEqual(certificate.get('total_issued_certificate'), 3) self.assertEqual(certificate.get('mode'), 'verified')
certificate_count = 3 for __ in xrange(certificate_count): self.generate_certificate(course_id=self.course.id, mode='honor', status=CertificateStatuses.downloadable)
for i in range(5): i += 1 registration_code_redemption = RegistrationCodeRedemption( registration_code_id=i, redeemed_by=self.instructor ) registration_code_redemption.save()
self.assertEqual(mail.outbox[-1].to[0], 'finance@example.com')
url_user_invoice_preference = reverse('get_user_invoice_preference', kwargs={'course_id': self.course.id.to_deprecated_string()})
url_user_invoice_preference = reverse('get_user_invoice_preference', kwargs={'course_id': self.course.id.to_deprecated_string()})
for i in range(9): i += 13 registration_code_redemption = RegistrationCodeRedemption( registration_code_id=i, redeemed_by=self.instructor ) registration_code_redemption.save()
reg_code = CourseRegistrationCode.objects.get(code=reg_code.code) self.assertEqual(reg_code.is_valid, False)
enrollment = CourseEnrollment.get_enrollment(student, self.course.id) self.assertEqual(enrollment.is_active, False)
enrollment = CourseEnrollment.get_enrollment(student, self.course.id) self.assertEqual(enrollment.is_active, False)
reg_code = CourseRegistrationCode.objects.get(code=reg_code.code) self.assertEqual(reg_code.is_valid, True)
self.update_enrollement("enroll", "newuser@hotmail.com") self.check_outbox("You have been")
self.assertTrue('Coupon Code List' in response.content)
CourseFinanceAdminRole(self.course.id).remove_users(self.instructor)
course_honor_mode = CourseMode.mode_for_course(self.course.id, 'honor')
CourseFinanceAdminRole(self.course.id).remove_users(self.instructor)
url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()}) response = self.client.get(url)
response = self.client.post(set_course_price_url, data) self.assertTrue("Please Enter the numeric value for the course price" in response.content)
data['course_price'] = 100 response = self.client.post(set_course_price_url, data) self.assertTrue("CourseMode price updated successfully" in response.content)
response = self.client.get(self.url) self.assertTrue(self.e_commerce_link in response.content) self.assertFalse('Coupons List' in response.content)
super(BadImplementationAbstractEnrollmentReportProvider, self)
instructor = AdminFactory.create() self.client.login(username=instructor.username, password="test")
def test_email_flag_false_mongo_true(self): BulkEmailFlag.objects.create(enabled=False) response = self.client.get(self.url) self.assertFalse(self.email_link in response.content)
cauth = CourseAuthorization(course_id=self.course.id, email_enabled=True) cauth.save()
self.assertTrue(BulkEmailFlag.feature_enabled(self.course.id)) response = self.client.get(self.url) self.assertTrue(self.email_link in response.content)
def test_course_authorized_feature_off(self): BulkEmailFlag.objects.create(enabled=False, require_course_email_auth=True) cauth = CourseAuthorization(course_id=self.course.id, email_enabled=True) cauth.save()
cls.url = reverse('instructor_dashboard', kwargs={'course_id': cls.course_key.to_deprecated_string()}) cls.email_link = '<a href="" data-section="send_email">Email</a>'
instructor = AdminFactory.create() self.client.login(username=instructor.username, password="test")
self.url = reverse('instructor_dashboard', kwargs={'course_id': self.course_key.to_deprecated_string()}) self.email_link = '<a href="" data-section="send_email">Email</a>'
eobjs = mes.create_user(self.course_key) ees = EmailEnrollmentState(self.course_key, eobjs.email) self.assertEqual(mes, ees)
print "checking initialization..." eobjs = before_ideal.create_user(self.course_key) before = EmailEnrollmentState(self.course_key, eobjs.email) self.assertEqual(before, before_ideal)
print "running action..." action(eobjs.email)
print "checking effects..." after = EmailEnrollmentState(self.course_key, eobjs.email) self.assertEqual(after, after_ideal)
StudentModule.objects.create( student=user, course_id=self.course_key, module_state_key=problem_location, state=json.dumps({}) )
reset_student_attempts( self.course_key, user, problem_location, requesting_user=user, delete_module=True, )
score = sub_api.get_score(student_item) self.assertIs(score, None)
result = get_email_params( self.course, True, course_key=self.course_key, display_name=self.ccx.display_name )
result = get_email_params(self.course, False)
with mock.patch.dict('django.conf.settings.FEATURES', {'ENABLE_MKTG_SITE': True}): result = get_email_params(self.course, True)
self.instructor = AdminFactory.create() self.client.login(username=self.instructor.username, password="test")
response = render_to_response(path, context) response.mako_context = context response.mako_template = path return response
self.instructor = AdminFactory.create() self.client.login(username=self.instructor.username, password="test")
self.url = reverse('instructor_dashboard', kwargs={'course_id': self.course.id.to_deprecated_string()})
self.assertFalse('<h2>Enrollment Information</h2>' in response.content)
self.assertFalse(self.get_dashboard_enrollment_message() in response.content)
self.assertContains(response, "<td>Professional</td><td>2</td>")
expected_message = self.get_dashboard_enrollment_message() self.assertTrue(expected_message in response.content)
expected_message = self.get_dashboard_analytics_message() self.assertTrue(expected_message in response.content)
self.instructor = AdminFactory.create() self.client.login(username=self.instructor.username, password="test")
course = ItemFactory.create( parent_location=self.course.location, category="course", display_name="Test course", )
kwargs = {} if cls.grading_policy is not None: kwargs['grading_policy'] = cls.grading_policy cls.course = CourseFactory.create(**kwargs)
self.assertEquals(7, self.response.content.count('grade_Pass'))
self.assertEquals(22, self.response.content.count('grade_F'))
self.assertEquals(293, self.response.content.count('grade_None'))
self.assertEquals(5, self.response.content.count('grade_A'))
self.assertEquals(3, self.response.content.count('grade_B'))
self.assertEquals(3, self.response.content.count('grade_C'))
self.assertEquals(3, self.response.content.count('grade_C'))
self.assertEquals(11, self.response.content.count('grade_F'))
self.assertEquals(3, self.response.content.count('grade_None'))
cache.clear()
CertificateGenerationConfiguration.objects.create(enabled=True)
self.client.login(username=self.instructor.username, password="test") self._assert_certificates_visible(False)
self.client.login(username=self.global_staff.username, password="test") self._assert_certificates_visible(True)
CertificateGenerationConfiguration.objects.create(enabled=False) cache.clear()
self.client.login(username=self.global_staff.username, password="test") self._assert_certificates_visible(False)
self._assert_enable_certs_button_is_disabled()
self._assert_enable_certs_button(True)
certs_api.set_cert_generation_enabled(self.course.id, True)
self._assert_enable_certs_button(False)
certs_api.set_cert_generation_enabled(self.course.id, False) self._assert_enable_certs_button_is_disabled()
certs_api.set_cert_generation_enabled(self.course.id, True) self._assert_enable_certs_button(False)
cache.clear() CertificateGenerationConfiguration.objects.create(enabled=True)
self.client.login(username=self.instructor.username, password='test') response = self.client.post(url) self.assertEqual(response.status_code, 403)
self.client.login(username=self.global_staff.username, password='test') response = self.client.post(url) self.assertEqual(response.status_code, 302)
self._assert_redirects_to_instructor_dash(response)
status = certs_api.example_certificates_status(self.course.id) self.assertIsNot(status, None)
self._assert_redirects_to_instructor_dash(response)
actual_enabled = certs_api.cert_generation_enabled(self.course.id) self.assertEqual(is_enabled, actual_enabled)
GeneratedCertificateFactory.create( user=self.user, course_id=self.course.id, status=CertificateStatuses.downloadable, mode='honor' )
self.assertEqual(response.status_code, 200) res_json = json.loads(response.content)
self.assertTrue(res_json['success'])
dummy_course = CourseFactory.create() GeneratedCertificateFactory.create( user=self.user, course_id=dummy_course.id, status=CertificateStatuses.generating, mode='honor' )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual( res_json['message'], u'Please select one or more certificate statuses that require certificate regeneration.' )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual(res_json['message'], u'Please select certificate statuses from the list only.')
cache.clear() CertificateGenerationConfiguration.objects.create(enabled=True) self.client.login(username=self.global_staff.username, password='test')
self.assertEqual(response.status_code, 200) certificate_exception = json.loads(response.content)
self.assertEqual(certificate_exception['user_email'], self.user.email) self.assertEqual(certificate_exception['user_name'], self.user.username)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual( res_json['message'], u"{user} does not exist in the LMS. Please check your spelling and retry.".format(user=invalid_user) )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual( res_json['message'], u"Student (username/email={user_name}) already in certificate exception list.".format(user_name=user) )
self.assertEqual(certificate_exception['user_email'], self.user.email) self.assertEqual(certificate_exception['user_name'], self.user.username)
self.client.post( url_course2, data=json.dumps(self.certificate_exception), content_type='application/json' )
self.assertEqual(certificate_exception['user_email'], self.user.email) self.assertEqual(certificate_exception['user_name'], self.user.username)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertFalse(res_json['success'])
self.assertEqual( res_json['message'], "{user} is not enrolled in this course. Please check your spelling and retry.".format( user=self.certificate_exception['user_name'] ) )
self.assertEqual(response.status_code, 204)
response = self.client.post( self.url, data='Test Invalid data', content_type='application/json', REQUEST_METHOD='DELETE' ) self.assertEqual(response.status_code, 400)
self.assertEqual(response.status_code, 400)
cache.clear() CertificateGenerationConfiguration.objects.create(enabled=True) self.client.login(username=self.global_staff.username, password='test')
self.assertEqual(response.status_code, 200)
self.assertTrue(res_json['success']) self.assertEqual( res_json['message'], u"Certificate generation started for white listed students." )
self.assertEqual(response.status_code, 200)
self.assertTrue(res_json['success']) self.assertEqual( res_json['message'], u"Certificate generation started for white listed students." )
self.assertEqual(response.status_code, 400)
self.assertFalse(res_json['success']) self.assertEqual( res_json['message'], u'Invalid data, generate_for must be "new" or "all".' )
self.client.login(username=self.global_staff.username, password="test")
self.client.login(username=self.global_staff.username, password="test")
self.assertEqual(response.status_code, 200) result = json.loads(response.content)
try: CertificateInvalidation.objects.get( generated_certificate=self.generated_certificate, invalidated_by=self.global_staff, notes=self.notes, active=True, ) except ObjectDoesNotExist: self.fail("The certificate is not invalidated.")
generated_certificate = GeneratedCertificate.eligible_certificates.get( user=self.enrolled_user_1, course_id=self.course.id, ) self.assertFalse(generated_certificate.is_valid())
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual( res_json['message'], u"{user} does not exist in the LMS. Please check your spelling and retry.".format(user=invalid_user), )
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 204)
with self.assertRaises(ObjectDoesNotExist): CertificateInvalidation.objects.get( generated_certificate=self.generated_certificate, invalidated_by=self.global_staff, active=True, )
self.generated_certificate.invalidate()
self.assertEqual(response.status_code, 400) res_json = json.loads(response.content)
self.assertEqual( res_json['message'], u"Certificate Invalidation does not exist, Please refresh the page and try again.", )
self.assertEqual( StudentModule.objects.filter( student=self.module_to_reset.student, course_id=self.course.id, module_state_key=self.module_to_reset.module_state_key, ).count(), 1 )
self.assertEqual( StudentModule.objects.filter( student=self.module_to_reset.student, course_id=self.course.id, module_state_key=self.module_to_reset.module_state_key, ).count(), 0 )
assert_in(role, ['instructor', 'staff'])
world.clear_courses()
course = world.CourseFactory.create( org='edx', number='999', display_name='Test Course' )
if role == 'instructor': world.instructor = InstructorFactory(course_key=world.course_key) world.enroll_user(world.instructor, world.course_key)
world.staff = StaffFactory(course_key=world.course_key) world.enroll_user(world.staff, world.course_key)
go_to_section("data_download")
world.css_click('input[name="calculate-grades-csv"]')
go_to_section("data_download")
go_to_section("data_download")
go_to_section("data_download")
world.wait_for_visible('#data-student-profiles-table')
world.wait_for(lambda _: world.css_text('#data-student-profiles-table') not in [u'', u'Loading'])
kwargs = {'course_id': self.course_id.to_deprecated_string(), 'note_id': str(self.pk)} return reverse('notes_api_note', kwargs=kwargs)
self.assertFalse(self.has_notes_tab(self.course, self.user))
self.assertFalse(self.has_notes_tab(self.course, self.user))
self.course.advanced_modules = ["notes"] self.assertFalse(self.has_notes_tab(self.course, self.user))
patcher = patch.object(api, 'api_enabled', Mock(return_value=True)) patcher.start() self.addCleanup(patcher.stop)
self.NOTE_ID_DOES_NOT_EXIST = 99999
from __future__ import unicode_literals
'MAX_NOTE_LIMIT': 1000,
ApiResponse = collections.namedtuple('ApiResponse', ['http_response', 'data'])
if not api_enabled(request, course_key): log.debug('Notes are disabled for course: {0}'.format(course_id)) raise Http404
resource_map = API_SETTINGS.get('RESOURCE_MAP', {}) resource_name = kwargs.pop('resource') resource_method = request.method resource = resource_map.get(resource_name)
if api_response.data is not None and api_response.data != '': content = json.dumps(api_response.data)
if offset.isdigit(): offset = int(offset) else: offset = 0
filters = {'course_id': course_key, 'user': request.user} if uri != '': filters['uri'] = uri
CourseEnrollmentFactory(user=other_user, course_id=self.courses[0].id)
with self.assertNumQueries(6): self._get_list()
queryset = User.objects.filter( preferences__key=NOTIFICATION_PREF_KEY ).select_related( "profile" ).prefetch_related( "preferences", "courseenrollment_set", "course_groups", "roles__permissions" )
self.assertEqual(str(user.username.encode('utf-8')), UsernameCipher().decrypt(str(pref.value)))
test_invalid_token("AAAAAAAAAAA=", "initialization_vector")
test_invalid_token(self.tokens[self.user][:-4], "aes")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAAMoazRI7ePLjEWXN1N7keLw=", "padding")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAAC6iLXGhjkFytJoJSBJZzJ4=", "padding")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAANRGw8HDEmlcLVFawgY9wI8=", "padding")
test_invalid_token("AAAAAAAAAAAAAAAAAAAAACpyUxTGIrUjnpuUsNi7mAY=", "username")
self.assertFalse(UserPreference.objects.filter(user=user, key=NOTIFICATION_PREF_KEY)) request = self.request_factory.get("dummy") request.user = AnonymousUser()
UserPreference.objects.get_or_create( user=user, key=NOTIFICATION_PREF_KEY, defaults={ "value": UsernameCipher.encrypt(user.username) } )
if self.instance and self.instance.get("pinned") is None: self.instance["pinned"] = False
for field_name in remove_fields: self.fields.pop(field_name)
if not ( self._is_anonymous(self.context["thread"]) and not self._is_user_privileged(endorser_id) ): return DjangoUser.objects.get(id=endorser_id).username
if 'parent_id' not in data: data["parent_id"] = None
raise ThreadNotFoundError("Thread not found.")
if paginated_results.page != page: raise PageNotFoundError("Page not found (No results on this page).")
if cc_thread['closed']: raise PermissionDenied
ret |= {"voted"} if _is_author_or_privileged(cc_content, context): ret |= {"raw_body"}
self.register_get_threads_response([], page=3, num_pages=3) with self.assertRaises(PageNotFoundError): get_thread_list(self.request, self.course.id, page=4, page_size=10)
thread = self.make_minimal_cs_thread({ "thread_type": thread_type, response_field: [make_minimal_cs_comment()], response_total_field: 5, })
assert_page_correct( page=1, page_size=10, expected_start=0, expected_stop=10, expected_next=None, expected_prev=None )
assert_page_correct( page=1, page_size=4, expected_start=0, expected_stop=4, expected_next=2, expected_prev=None )
assert_page_correct( page=2, page_size=4, expected_start=4, expected_stop=8, expected_next=3, expected_prev=1 )
assert_page_correct( page=3, page_size=4, expected_start=8, expected_stop=10, expected_next=None, expected_prev=2 )
with self.assertRaises(PageNotFoundError): self.get_comment_list(thread, endorsed=True, page=2, page_size=10)
url(r'^programs/(?P<program_id>\d+)/[\w\-]*/?$', views.program_details, name='program_details_view'),
self.mock_programs_api() self.mock_credentials_api(self.student, data=self.CREDENTIALS_API_RESPONSE, reset_url=False)
self.mock_programs_api() self.mock_credentials_api(self.student, data={"results": []}, reset_url=False)
('cart', 'cart'),
('paying', 'paying'),
('purchased', 'purchased'),
('refunded', 'refunded'),
('defunct-cart', 'defunct-cart'),
('defunct-paying', 'defunct-paying'),
ORDER_STATUS_MAP = { 'cart': 'defunct-cart', 'paying': 'defunct-paying', }
OrderItemSubclassPK = namedtuple('OrderItemSubclassPK', ['cls', 'pk'])
processor_reply_dump = models.TextField(blank=True)
return cart.has_items()
for item_type in item_types: if cart.has_items(item_type): return True
CouponRedemption.remove_code_redemption_from_item(item, user)
if is_order_type_business: email.content_subtype = "html"
self.save() orderitems = OrderItem.objects.filter(order=self).select_subclasses() site_name = microsite.get_value('SITE_NAME', settings.SITE_NAME)
csv_file, courses_info = self.generate_registration_codes_csv(orderitems, site_name)
log.exception('Error occurred while sending payment confirmation email')
log.exception( u'Unable to emit {event} event for user {user} and order {order}'.format( event=event_name, user=self.user.id, order=self.id) )
if self.status in ORDER_STATUS_MAP.values(): return
report_comments = models.TextField(default="")
total_amount = models.FloatField()
course_id = CourseKeyField(max_length=255, db_index=True)
('started', 'started'),
('completed', 'completed'),
('cancelled', 'cancelled')
snapshot = models.TextField(blank=True)
invoice = models.ForeignKey(Invoice, null=True) invoice_item = models.ForeignKey(CourseRegistrationCodeInvoiceItem, null=True)
reg_codes = cls.objects.filter(course_enrollment=course_enrollment).order_by('-redeemed_at') if reg_codes: return reg_codes[0]
course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE
self.course_enrollment = CourseEnrollment.enroll(user=self.user, course_key=self.course_id, mode=self.mode) self.save()
course_mode = CourseMode.DEFAULT_SHOPPINGCART_MODE
from instructor.views.api import save_registration_code
return u"{} : {}".format(self.course_id.to_deprecated_string(), self.annotation)
return u"{} : {}".format(self.course_id.to_deprecated_string(), self.annotation)
if (not course_enrollment.refundable()) or skip_refund: return
return u"{verification_reminder} {refund_reminder}".format( verification_reminder=verification_reminder, refund_reminder=refund_reminder )
DONATION_TYPES = ( ("general", "A general donation"), ("course", "A donation to a particular course") )
donation_type = models.CharField(max_length=32, default="general", choices=DONATION_TYPES)
course_id = CourseKeyField(max_length=255, db_index=True)
super(Donation, cls).add_to_order(order, currency=currency)
description = cls._line_item_description(course_id=course_id)
else: return _(u"Donation for {platform_name}").format(platform_name=settings.PLATFORM_NAME)
request.user.is_authenticated() and is_shopping_cart_enabled() and Order.does_user_have_cart(request.user) and Order.user_cart_has_items( request.user, [PaidCourseRegistration, CourseRegCodeItem] )
self.pdf.drawString(horizontal_padding_from_border, y_pos,
self.pdf.drawString( horizontal_padding_from_border, y_pos, _(u'Date: {date}').format(date=self.date) )
('ALIGN', (5, 0), (5, 0), 'RIGHT'),
('RIGHTPADDING', (5, 0), (5, -1), 7 * mm),
('ALIGN', (2, 0), (4, 0), 'CENTER'),
('ALIGN', (1, 0), (1, -1), 'LEFT'),
('ALIGN', (2, 1), (2, -1), 'CENTER'),
('INNERGRID', (1, 1), (-2, -1), 0.50, '#cccccc'),
split_table = split_tables[0] __, rendered_height = split_table.wrap(0, 0) split_table.drawOn(self.pdf, table_left_padding, y_pos - rendered_height)
course_items_table.drawOn(self.pdf, table_left_padding, y_pos - rendered_height)
totals_data.append( ['', '{tax_label}: {tax_id}'.format(tax_label=self.tax_label, tax_id=self.tax_id)] )
self.prepare_new_page() totals_table.drawOn(self.pdf, self.margin + left_padding, self.second_page_start_y_pos - rendered_height) return self.second_page_start_y_pos - rendered_height - self.min_clearance
('LEFTPADDING', (0, 1), (0, 1), 5 * mm),
('BACKGROUND', (1, 2), (1, 2), '#EEEEEE'),
('BACKGROUND', (1, 4), (1, 4), '#EEEEEE'),
footer_style.append(('BACKGROUND', (1, 6), (1, 6), '#EEEEEE'))
AUDIT_LOG.info("Redemption of a invalid RegistrationCode %s", registration_code) limiter.tick_bad_request_counter(request) raise Http404()
embargo_redirect = embargo_api.redirect_if_blocked( course.id, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect is not None: return redirect(embargo_redirect)
embargo_redirect = embargo_api.redirect_if_blocked( course.id, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect is not None: return redirect(embargo_redirect)
cart = Order.get_cart_for_user(request.user) try: cart_items = cart.find_item_by_course_id(course_registration.course_id)
if amount < decimal.Decimal('0.01'): return HttpResponseBadRequest("Amount must be greater than 0")
cart = Order.get_cart_for_user(request.user) cart.clear()
Donation.add_to_order(cart, amount, course_id=course_id)
cart.start_purchase()
callback_url = request.build_absolute_uri( reverse("shoppingcart.views.postpay_callback") )
extra_data = [ unicode(course_id) if course_id else "", "donation_course" if course_id else "donation_general" ]
"payment_url": get_purchase_endpoint(),
"payment_params": get_signed_purchase_params( cart, callback_url=callback_url, extra_data=extra_data ),
cert_items = CertificateItem.objects.filter(order=order)
url += '?payment-order-num={order_num}'.format(order_num=order.id) return HttpResponseRedirect(url)
return HttpResponseRedirect(reverse('shoppingcart.views.show_receipt', args=[result['order'].id]))
if order_items.count() == 1: receipt_template = order_items[0].single_item_receipt_template context.update(order_items[0].single_item_receipt_context)
return _render_report_form(start_date, end_date, start_letter, end_letter, report_type, date_fmt_error=True)
self.cart, __ = self._create_cart()
record_purchase(params, result['order']) return {'success': True, 'order': result['order'], 'error_html': ''}
charged_amt = Decimal(params['ccAuthReply_amount'])
payment_support_email = microsite.get_value('payment_support_email', settings.PAYMENT_SUPPORT_EMAIL)
return '<p class="error_msg">EXCEPTION!</p>'
PROCESSOR_MODULE = __import__( 'shoppingcart.processors.' + settings.CC_PROCESSOR_NAME, fromlist=[ 'render_purchase_form_html', 'process_postpay_callback', 'get_purchase_endpoint', 'get_signed_purchase_params', ] )
DEFAULT_REASON = ugettext_noop("UNKNOWN REASON")
if hasattr(error, 'order'): _record_payment_info(params, error.order) else: log.info(json.dumps(params)) return { 'success': False,
if params.get('decision') == u'CANCEL': raise CCProcessorUserCancelled()
if params.get('decision') == u'DECLINE': raise CCProcessorUserDeclined()
for num, item in enumerate(extra_data, start=1): key = u"merchant_defined_data{num}".format(num=num) params[key] = item
config = settings.CC_PROCESSOR.get( settings.CC_PROCESSOR_NAME, {} )
config_key = microsite.get_value('cybersource_config_key') if config_key: config = config['microsites'][config_key]
self.assertEqual(1, 1)
self.assertEqual(1, 1)
self.assertIn("EXCEPTION!", get_processor_exception_html(CCProcessorException()))
for key in baseline: params = baseline.copy() del params[key] with self.assertRaises(CCProcessorDataException): payment_accepted(params)
for key in wrong: params = baseline.copy() params[key] = wrong[key] with self.assertRaises(CCProcessorDataException): payment_accepted(params)
params_bad_ordernum = params.copy() params_bad_ordernum['orderNumber'] = str(order1.id + 10) with self.assertRaises(CCProcessorDataException): payment_accepted(params_bad_ordernum)
params_wrong_type_amt = params.copy() params_wrong_type_amt['ccAuthReply_amount'] = 'ab' with self.assertRaises(CCProcessorDataException): payment_accepted(params_wrong_type_amt)
params_wrong_amt = params.copy() params_wrong_amt['ccAuthReply_amount'] = '1.00' with self.assertRaises(CCProcessorWrongAmountException): payment_accepted(params_wrong_amt)
params_not_accepted = params.copy() params_not_accepted['decision'] = "REJECT" self.assertFalse(payment_accepted(params_not_accepted)['accepted'])
self.assertTrue(payment_accepted(params)['accepted'])
self.assertEqual(params['override_custom_receipt_page'], self.CALLBACK_URL)
self.assertEqual(params['access_key'], '0123456789012345678901') self.assertEqual(params['profile_id'], 'edx')
self.assertGreater(len(params['signed_date_time']), 0) self.assertGreater(len(params['transaction_uuid']), 0)
self.assertEqual(params['signature'], self._signature(params))
@patch.object(OrderItem, 'purchased_callback')
@patch.object(OrderItem, 'purchased_callback') @patch.object(OrderItem, 'pdf_receipt_display_name')
params = self._signed_callback_params(self.order.id, self.COST, self.COST) result = process_postpay_callback(params)
purchased_callback.assert_called_with()
self.assertEqual(result['order'].status, 'purchased') self.assert_dump_recorded(result['order'])
params = self._signed_callback_params(self.order.id, self.COST, self.COST, decision='REJECT') result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"did not accept your payment", result['error_html']) self.assert_dump_recorded(result['order'])
params = self._signed_callback_params(self.order.id, self.COST, self.COST, signature="invalid!") result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"corrupted message regarding your charge", result['error_html'])
params = self._signed_callback_params("98272", self.COST, self.COST) result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"inconsistent data", result['error_html'])
params = self._signed_callback_params(self.order.id, "145.00", "145.00") result = process_postpay_callback(params)
params = self._signed_callback_params(self.order.id, self.COST, "abcd") result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"badly-typed value", result['error_html'])
params = self._signed_callback_params(self.order.id, self.COST, "abcd") params['decision'] = u'CANCEL' result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"you have cancelled this transaction", result['error_html'])
params = self._signed_callback_params( self.order.id, self.COST, self.COST, card_number='nodigits' ) result = process_postpay_callback(params)
self.assertEqual(result['order'].bill_to_ccnum, '####')
params = self._signed_callback_params(self.order.id, self.COST, self.COST) del params[missing_param]
params['signed_field_names'] = 'reason_code,message' params['signature'] = self._signature(params)
self.assertFalse(result['success']) self.assertIn(u"did not return a required parameter", result['error_html'])
result = process_postpay_callback(params) self.assertTrue(result['success']) self.assert_dump_recorded(result['order'])
if decision in self.FAILED_DECISIONS: signed_field_names.remove("auth_amount")
"decision": decision, "req_reference_number": str(order_id), "req_amount": order_amount, "auth_amount": paid_amount, "req_card_number": card_number,
params['signature'] = signature if signature is not None else self._signature(params) return params
params = self._signed_callback_params(self.order.id, self.COST, self.COST, decision='DECLINE') result = process_postpay_callback(params)
self.assertFalse(result['success']) self.assertIn(u"payment was declined", result['error_html'])
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
course5 = CourseFactory.create(org='otherorg', number='999') course5_key = course5.id
Donation.add_to_order(cart, 10.0, None) cart.purchase(first='FirstNameTesting123', street1='StreetTesting123') self.orderid_courseless_donation = cart.id
self.cart1 = Order.get_cart_for_user(self.first_verified_user) CertificateItem.add_to_order(self.cart1, self.course_key, self.cost, 'verified') self.cart1.purchase()
CourseEnrollment.enroll(self.honor_user, self.course_key, "honor")
num_certs = 0 for cert in refunded_certs: num_certs += 1 self.assertEqual(num_certs, 2)
self.assertEqual(csv.replace('\r\n', '\n').strip(), self.CORRECT_REFUND_REPORT_CSV.strip())
num_purchases = 0 for item in purchases: num_purchases += 1 self.assertEqual(num_purchases, 2)
self.assertEqual(csv.replace('\r\n', '\n').strip(), self.CORRECT_CSV.strip())
self.annotation.delete() self.assertEqual(u"", self.reg.csv_report_comments)
self.testing_cost = 20 self.testing_course_mode = CourseMode( course_id=self.testing_course.id, mode_slug=CourseMode.HONOR, mode_display_name="testing honor cert", min_price=self.testing_cost ) self.testing_course_mode.save()
CourseMode( course_id=self.xss_course_key, mode_slug=CourseMode.HONOR, mode_display_name="honor cert", min_price=self.cost ).save()
self.verified_course_mode = CourseMode( course_id=self.verified_course_key, mode_slug=CourseMode.HONOR, mode_display_name="honor cert", min_price=self.cost ) self.verified_course_mode.save()
resp = self.client.get(billing_url) self.assertEqual(resp.status_code, 404)
self.assertEqual(context['currency'], 'usd') self.assertEqual(context['currency_symbol'], '$')
self.assertEqual(context['currency'], 'PKR') self.assertEqual(context['currency_symbol'], 'Rs')
item = self.cart.orderitem_set.all().select_subclasses()[0] self.assertEquals(item.unit_cost, self.get_discount(self.cost))
self.assertEqual(self.cart.total_cost, self.get_discount(self.cost))
item = self.cart.orderitem_set.all().select_subclasses()[0] self.assertEquals(item.unit_cost, self.get_discount(self.cost))
item = self.cart.orderitem_set.all().select_subclasses()[0] self.assertEquals(item.unit_cost, self.get_discount(self.cost))
self.assertTrue('Activate Course Enrollment' in response.content)
course_key = self.course_key.to_deprecated_string() self._add_course_mode(mode_slug='verified') self.add_reg_code(course_key, mode_slug='verified')
self.assertTrue('Activate Course Enrollment' in response.content)
current_enrollment, __ = CourseEnrollment.enrollment_mode_for_user(self.user, self.course_key) self.assertEquals('verified', current_enrollment)
resp = self.client.post(reverse('shoppingcart.views.remove_item', args=[]), {'id': reg_item.id})
resp = self.client.post(reverse('shoppingcart.views.remove_item', args=[]), {'id': cert_item.id})
self.assertEqual(context['currency'], 'usd') self.assertEqual(context['currency_symbol'], '$')
self.assertEqual(context['currency'], 'PKR') self.assertEqual(context['currency_symbol'], 'Rs')
self.assertEqual(resp.status_code, 200)
self.add_course_to_user_cart(self.xss_course_key) self.assertEquals(self.cart.orderitem_set.count(), 1)
self.assertEqual(resp.status_code, 200)
json_resp = json.loads(resp.content) self.assertEqual(json_resp.get('total_cost'), self.cart.total_cost)
total_amount = PaidCourseRegistration.get_total_amount_of_purchased_item(self.course_key) self.assertEqual(total_amount, 36)
self.add_course_to_user_cart(self.course_key) self.assertEquals(self.cart.orderitem_set.count(), 1)
self.assertTrue('Activate Course Enrollment' in response.content)
self.assertTrue('Activate Course Enrollment' in resp.content)
item2 = PaidCourseRegistration.objects.get(id=item2.id) self.assertIsNotNone(item2.course_enrollment) self.assertEqual(item2.course_enrollment.course_id, self.testing_course.id)
self.assertEqual(context['currency_symbol'], '$') self.assertEqual(context['currency'], 'usd')
self.assertEqual(context['currency_symbol'], 'Rs') self.assertEqual(context['currency'], 'PKR')
self.assertEquals(len(mail.outbox), 3)
course_registration_codes = CourseRegistrationCode.objects.filter(order=self.cart)
redeem_url = reverse('register_code_redemption', args=[context['reg_code_info_list'][0]['code']])
resp = self.client.get(reverse('shoppingcart.views.show_receipt', args=[self.cart.id])) self.assertEqual(resp.status_code, 200)
self.assertTrue(context['reg_code_info_list'][0]['is_redeemed']) self.assertFalse(context['reg_code_info_list'][1]['is_redeemed'])
CourseMode.objects.create( course_id=self.verified_course_key, mode_slug="verified", mode_display_name="verified cert", min_price=self.cost )
self.cart = Order.get_cart_for_user(self.user) CertificateItem.add_to_order(self.cart, self.verified_course_key, self.cost, 'verified') self.cart.start_purchase()
session = self.client.session session['attempting_upgrade'] = True session.save()
Order.get_cart_for_user(self.user).start_purchase() Order.get_cart_for_user(self.user).start_purchase() Order.get_cart_for_user(self.user).start_purchase()
self.cart = Order.get_cart_for_user(self.user) CertificateItem.add_to_order( self.cart, self.course_key, self.COST, 'verified' ) self.cart.start_purchase()
self.testing_course.enrollment_start = self.tomorrow self.testing_course.enrollment_end = self.nextday self.testing_course = self.update_course(self.testing_course, self.user.id)
self.testing_course.enrollment_start = self.tomorrow self.testing_course.enrollment_end = self.nextday self.testing_course = self.update_course(self.testing_course, self.user.id)
self.testing_course.enrollment_start = self.tomorrow self.testing_course.enrollment_end = self.nextday self.testing_course = self.update_course(self.testing_course, self.user.id)
response = self.client.post(url) self.assertEquals(response.status_code, 403)
reset_time = datetime.now(UTC) + timedelta(seconds=300) with freeze_time(reset_time): response = self.client.post(url) self.assertEquals(response.status_code, 404)
response = self.client.get(url) self.assertEquals(response.status_code, 403)
reset_time = datetime.now(UTC) + timedelta(seconds=300) with freeze_time(reset_time): response = self.client.get(url) self.assertEquals(response.status_code, 404)
CourseSalesAdminRole(self.course.id).add_users(instructor)
registration_code = CourseRegistrationCode.objects.all()[0].code redeem_url = reverse('register_code_redemption', args=[registration_code]) self.login_user()
self.assertIn('Activate Course Enrollment', response.content)
reg_code = CourseRegistrationCode.objects.create( code="abcd1234", course_id=self.course.id, created_by=self.user )
is_redeemed = RegistrationCodeRedemption.objects.filter( registration_code=reg_code ).exists() self.assertFalse(is_redeemed)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertFalse(is_enrolled)
config = DonationConfiguration.current() config.enabled = True config.save()
self._donate(self.DONATION_AMOUNT, course_id=self.course.id)
self._assert_receipt_contains("tax purposes") self._assert_receipt_contains(self.course.display_name)
response = self.client.post(reverse('donation')) self.assertEqual(response.status_code, 404)
self.client.logout() response = self.client.post(reverse('donation')) self.assertEqual(response.status_code, 404)
params = {'amount': donation_amount} if course_id is not None: params['course_id'] = course_id
payment_info = json.loads(response.content) self.assertEqual(payment_info["payment_url"], "/shoppingcart/payment_fake")
url = reverse('shoppingcart.views.postpay_callback') response = self.client.post(url, processor_response_params) self.assertRedirects(response, self._receipt_url)
self.assertFalse(any(settings.PDF_RECEIPT_TERMS_AND_CONDITIONS in s for s in pdf_content))
PaymentFakeView.PAYMENT_STATUS_RESPONSE = "success"
post_params = sign(self.CLIENT_POST_PARAMS)
resp = self.client.post( '/shoppingcart/payment_fake', dict(post_params) )
self.assertEqual(resp.status_code, 200)
self.assertIn("Payment Form", resp.content)
post_params = sign(self.CLIENT_POST_PARAMS)
post_params['signature'] = "invalid"
resp = self.client.post( '/shoppingcart/payment_fake', dict(post_params) )
self.assertIn("Error", resp.content)
post_params = sign(self.CLIENT_POST_PARAMS)
resp_params = PaymentFakeView.response_post_params(post_params)
try: verify_signatures(resp_params)
post_params = sign(self.CLIENT_POST_PARAMS)
resp = self.client.put( '/shoppingcart/payment_fake', data="decline", content_type='text/plain' ) self.assertEqual(resp.status_code, 200)
resp_params = PaymentFakeView.response_post_params(post_params) self.assertEqual(resp_params.get('decision'), 'DECLINE')
resp = self.client.put( '/shoppingcart/payment_fake', data="failure", content_type='text/plain' ) self.assertEqual(resp.status_code, 200)
resp_params = PaymentFakeView.response_post_params(post_params) self.assertEqual(resp_params.get('decision'), 'REJECT')
resp = self.client.put( '/shoppingcart/payment_fake', data="success", content_type='text/plain' ) self.assertEqual(resp.status_code, 200)
resp_params = PaymentFakeView.response_post_params(post_params) self.assertEqual(resp_params.get('decision'), 'ACCEPT')
patcher = patch('shoppingcart.models.analytics') self.mock_tracker = patcher.start() self.addCleanup(patcher.stop)
next_cart = Order.get_cart_for_user(user=self.user) self.assertNotEqual(cart, next_cart) self.assertEqual(next_cart.status, 'cart')
cart.purchase() cart.purchase() self.assertEquals(len(mail.outbox), 1)
with patch.object(mail.message.EmailMessage, 'send') as mock_send: mock_send.side_effect = Exception("Kaboom!") cart.purchase()
self.assertEqual(cart.status, 'purchased')
mode, is_active = CourseEnrollment.enrollment_mode_for_user(self.user, self.course_key) self.assertTrue(is_active) self.assertEqual(mode, 'verified')
registration_codes = CourseRegistrationCode.order_generated_registration_codes(self.course_key) self.assertEqual(registration_codes.count(), item.qty)
many_days = datetime.timedelta(days=60)
many_days = datetime.timedelta(days=60)
many_days = datetime.timedelta(days=60)
CourseEnrollment.enroll(self.user, self.course_key, 'verified') ret_val = CourseEnrollment.unenroll(self.user, self.course_key) self.assertFalse(ret_val)
donation = Donation.add_to_order(self.cart, self.COST) self._assert_donation( donation, donation_type="general", unit_cost=self.COST, line_desc="Donation for edX" )
course = CourseFactory.create(display_name="Test Course")
Donation.add_to_order(self.cart, self.COST) self.cart.start_purchase() self.cart.purchase()
self.assertTrue(self.cart.has_items(item_type=Donation)) self.assertEqual(self.cart.total_cost, unit_cost)
self.cart.start_purchase() self.cart.purchase()
donation = Donation.objects.get(pk=donation.id) self.assertEqual(donation.status, "purchased")
first_transaction.delete() second_transaction.delete() self._assert_history_transactions([])
from shoppingcart.processors.CyberSource2 import processor_hash
PAYMENT_STATUS_RESPONSE = "success"
PaymentFakeView.PAYMENT_STATUS_RESPONSE = new_status return HttpResponse()
signed_fields = post_params.get('signed_field_names').split(',')
hash_val = ",".join([ "{0}={1}".format(key, post_params[key]) for key in signed_fields ]) public_sig = processor_hash(hash_val)
"decision": decision,
resp_params['signed_field_names'] = ",".join(signed_fields)
hash_val = ",".join([ "{0}={1}".format(key, resp_params[key]) for key in signed_fields ]) resp_params['signature'] = processor_hash(hash_val)
"callback_url": callback_url,
'post_params_success': post_params_success,
'post_params_decline': post_params_decline
parser = PDFParser(pdf_buffer) document = PDFDocument(parser, password)
layout = device.get_result()
text_content.append(lt_object.get_text().encode('utf-8'))
if matched == ';': return ';;' elif matched == '/': return ';_' else: return matched
func = getattr(block.__class__, handler_name, None) if not func: raise ValueError("{!r} is not a function name".format(handler_name))
#if not getattr(func, "_is_xblock_handler", False):
if not suffix: url = url.rstrip('/')
if query: url += '?' + query
if thirdparty: scheme = "https" if settings.HTTPS == "on" else "http" url = '{scheme}://{host}{path}'.format( scheme=scheme, host=settings.SITE_NAME, path=url )
tag = self.runtime.service(self.mock_block, 'user_tags').get_tag(self.scope, self.key) self.assertIsNone(tag)
with self.assertRaises(ValueError): self.runtime.service(self.mock_block, 'user_tags').set_tag('fake_scope', self.key, set_value)
with self.assertRaises(ValueError): self.runtime.service(self.mock_block, 'user_tags').get_tag('fake_scope', self.key)
if isinstance(authored_data, LmsFieldData):
from __future__ import unicode_literals
_ = lambda text: text
help=_("What format this module is in (used for deciding which " "grader to apply, and what to show in the TOC)"), scope=Scope.settings,
continue
merged_access[partition_id] = group_ids
user_partitions = UserPartitionList( help=_("The list of group configurations for partitioning students in content experiments."), default=[], scope=Scope.settings )
if user_partition.active: for group_id in group_ids: try: user_partition.get_group(group_id) except NoSuchUserPartitionGroupError: has_invalid_groups = True
courses = CourseOverview.get_all_courses( org=org, filter_=filter_, ) if org == microsite_org else []
target_org = org or microsite_org courses = CourseOverview.get_all_courses(org=target_org, filter_=filter_)
if microsite_org: return courses
filtered_visible_ids = None
microsite_orgs = microsite.get_all_orgs() return [course for course in courses if course.location.org not in microsite_orgs]
if domain and 'edge.edx.org' in domain: return redirect(reverse("signin_user"))
return student.views.index(request, user=request.user)
return courseware.views.views.courses(request)
accepts = request.META.get('HTTP_ACCEPT', '*/*')
show_openedx_logo = bool(request.GET.get('show-openedx-logo', False))
include_dependencies = bool(request.GET.get('include-dependencies', False))
language = request.GET.get('language', translation.get_language())
from __future__ import unicode_literals
u"\u00A9 {org_name}. All rights reserved except where noted. " u"EdX, Open edX and the edX and Open EdX logos are registered trademarks " u"or trademarks of edX Inc."
title = _("Powered by Open edX") return { "url": settings.FOOTER_OPENEDX_URL, "title": title, "image": settings.FOOTER_OPENEDX_LOGO_IMAGE, }
if urlparse.urlparse(url_path).netloc: return url_path
return _absolute_url(is_secure, url_path)
microsite_url = get_microsite_url(name) if microsite_url != EMPTY_URL: return microsite_url
url = marketing_link(name)
image_url = microsite.get_value('logo_image_url') if image_url: return '{static_url}{image_url}'.format( static_url=settings.STATIC_URL, image_url=image_url )
university = microsite.get_value('university')
resp = self.client.get('/') self.assertEquals(resp['X-Frame-Options'], 'ALLOW')
resp = self.client.get('/') self.assertEquals(resp['X-Frame-Options'], 'DENY')
request.META["HTTP_HOST"] = "edge.edx.org" response = index(request)
self.assertIsInstance(response, HttpResponseRedirect)
self.assertIn('pre requisite course', resp.content) self.assertIn('course that has pre requisite', resp.content)
self.assertNotIn('Search for a course', response.content)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
self.assertNotIn('Search for a course', response.content) self.assertNotIn('<aside aria-label="Refine Your Search" class="search-facets phone-menu">', response.content)
self.assertIn('<div class="courses no-course-discovery"', response.content)
self.assertIn('Search for a course', response.content)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('branding.views.courses')) self.assertEqual(response.status_code, 200)
self.assertIn("logo_image", json_data)
self.assertIn("copyright", json_data)
resp = self._get_footer(params={'language': language}) self.assertEqual(resp.status_code, 200) json_data = json.loads(resp.content)
self.assertIn(expected_copyright, json_data['copyright'])
(False, "en", "lms-footer.css"), (False, "ar", "lms-footer-rtl.css"),
(True, "en", "lms-footer-edx.css"), (True, "ar", "lms-footer-edx-rtl.css"),
(False, True), (False, False),
(True, True), (True, False),
(False, False), (False, True),
(True, False), (True, True),
with patch_edxnotes_api_settings("http://example.com/"): self.assertEqual("http://example.com/", get_endpoint_function())
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/", get_endpoint_function())
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/some_path/", get_endpoint_function("/some_path"))
with patch_edxnotes_api_settings("http://example.com"): self.assertEqual("http://example.com/some_path/", get_endpoint_function("some_path/"))
with patch_edxnotes_api_settings(None): self.assertRaises(ImproperlyConfigured, get_endpoint_function)
if expected is None: self.assertEqual(expected, constructed) else: self.assertTrue(constructed.startswith(notes_url))
self.assertNotIn('user', constructed)
allowed_params = ('page', 'page_size', 'text')
parsed = urlparse.urlparse(constructed) params = urlparse.parse_qs(parsed.query)
for param, value in params.items(): self.assertIn(param, allowed_params) self.assertIn('{}={}'.format(param, value[0]), expected)
self.course.edxnotes = False self.assertFalse(has_notes_tab(self.user, self.course))
self.course.edxnotes = True self.assertTrue(has_notes_tab(self.user, self.course))
CLIENT_NAME = "edx-notes" DEFAULT_PAGE = 1 DEFAULT_PAGE_SIZE = 25
def default(self, obj): if isinstance(obj, datetime): return get_default_time_display(obj) return json.JSONEncoder.default(self, obj)
usage_key = usage_key.replace(course_key=store.fill_in_run(usage_key.course_key))
course = item.get_parent() item_dict['index'] = get_index(item_dict['location'], course.children)
section = get_current_child(chapter, min_depth=1) if section is None: log.debug("No section found when loading current position in course") return None
CONFIG_FILE = open(settings.REPO_ROOT / "docs" / "lms_config.ini") CONFIG = ConfigParser.ConfigParser() CONFIG.readfp(CONFIG_FILE)
course_id = serializers.CharField(source='id', read_only=True)
return CourseDetails.fetch_about_attribute(course_overview.id, 'overview')
if requesting_user.username == target_username: return True elif not target_username: raise TypeError("target_username must be specified") else: staff = GlobalStaff() return staff.has_user(requesting_user)
url( r'^v1/blocks/{}'.format(settings.USAGE_KEY_PATTERN), BlocksView.as_view(), name="blocks_in_block_tree" ),
url( r'^v1/blocks/', BlocksInCourseView.as_view(), name="blocks_in_course" ),
course_key_string = request.query_params.get('course_id', None) if not course_key_string: raise ValidationError('course_id is required.')
return (requested_fields or set()) | {'type', 'display_name'}
additional_requested_fields = [ 'student_view_data', 'block_counts', 'nav_depth', 'block_types_filter', ] for additional_field in additional_requested_fields: field_value = cleaned_data.get(additional_field)
if not cleaned_data.get('all_blocks', None): raise ValidationError({'username': ['This field is required unless all_blocks is requested.']})
return None
try: return User.objects.get(username=requested_username) except User.DoesNotExist: raise Http404( "Requested user '{requested_username}' does not exist.".format(requested_username=requested_username) )
transformers = BlockStructureTransformers() if user is not None: transformers += COURSE_BLOCK_ACCESS_TRANSFORMERS + [ProctoredExamTransformer()] transformers += [ BlocksAPITransformer( block_counts, student_view_data, depth, nav_depth ) ]
blocks = get_course_blocks(user, usage_key, transformers)
serializer_context = { 'request': request, 'block_structure': blocks, 'requested_fields': requested_fields or [], }
return serializer.data
block_structure.request_xblock_fields('category')
block_structure.request_xblock_fields('hide_from_toc')
for parent_desc_list in parents_descendants_list: if parent_desc_list is not None: parent_desc_list.items.append(unicode(block_key))
user_exam_summary = get_attempt_status_summary( usage_info.user.id, unicode(block_key.course_key), unicode(block_key), ) return user_exam_summary and user_exam_summary['status'] != ProctoredExamStudentAttemptStatus.declined
SupportedFieldType(StudentViewTransformer.STUDENT_VIEW_DATA, StudentViewTransformer), SupportedFieldType(StudentViewTransformer.STUDENT_VIEW_MULTI_DEVICE, StudentViewTransformer),
SupportedFieldType(None, BlockCountsTransformer, BlockCountsTransformer.BLOCK_COUNTS),
SupportedFieldType( 'merged_visible_to_staff_only', VisibilityTransformer, requested_field_name='visible_to_staff_only', )
block_structure.request_xblock_fields('category')
block_structure.request_xblock_fields('graded', 'format', 'display_name', 'category')
StudentViewTransformer.collect(block_structure) BlockCountsTransformer.collect(block_structure) BlockDepthTransformer.collect(block_structure) BlockNavigationTransformer.collect(block_structure)
BlockDepthTransformer.collect(block_structure) BlockNavigationTransformer.collect(block_structure) block_structure._collect_requested_xblock_fields()
BlockDepthTransformer().transform(usage_info=None, block_structure=block_structure) BlockNavigationTransformer(0).transform(usage_info=None, block_structure=block_structure) block_structure._prune_unreachable()
StudentViewTransformer.collect(self.block_structure) self.block_structure._collect_requested_xblock_fields()
StudentViewTransformer('video').transform(usage_info=None, block_structure=self.block_structure)
from openedx.core.lib.block_structure.factory import BlockStructureFactory from xmodule.modulestore.tests.django_utils import ModuleStoreTestCase from xmodule.modulestore.tests.factories import SampleCourseFactory
BlockCountsTransformer.collect(self.block_structure) self.block_structure._collect_requested_xblock_fields()
BlockCountsTransformer(['problem', 'chapter']).transform(usage_info=None, block_structure=self.block_structure)
self.assertEquals(block_counts_for_course['chapter'], 2)
self.assertEquals(block_counts_for_course['problem'], 6) self.assertEquals(block_counts_for_chapter_x['problem'], 3)
for block_type in ['course', 'html', 'video']: self.assertNotIn(block_type, block_counts_for_course) self.assertNotIn(block_type, block_counts_for_chapter_x)
self.course_hierarchy = self.get_course_hierarchy() self.blocks = self.build_course(self.course_hierarchy) self.course = self.blocks['course']
CourseEnrollmentFactory.create(user=self.user, course_id=self.course.id, is_active=True)
self.form_data.setlist('requested_fields', ['field1', 'field2'])
block_types_list = {'block_type1', 'block_type2'} for field_name in ['block_counts', 'student_view_data']: self.form_data.setlist(field_name, block_types_list) self.cleaned_data[field_name] = block_types_list
self.cleaned_data['requested_fields'] |= {'field1', 'field2', 'student_view_data', 'block_counts'} self.assert_equals_cleaned_data()
self.user = UserFactory.create() self.client.login(username=self.user.username, password='test') CourseEnrollmentFactory.create(user=self.user, course_id=self.course_key)
if serialized_block['type'] == 'video': self.assertIn('student_view_data', serialized_block)
if serialized_block['type'] == 'html': self.assertIn('student_view_multi_device', serialized_block) self.assertTrue(serialized_block['student_view_multi_device'])
staff_user = UserFactory.create() CourseStaffRole(self.course.location.course_key).add_users(staff_user)
self.assertEquals(serializer.data['root'], unicode(self.block_structure.root_block_usage_key))
for block_key_string, serialized_block in serializer.data['blocks'].iteritems(): self.assertEquals(serialized_block['id'], block_key_string) self.assert_basic_block(block_key_string, serialized_block)
alternate_course = self.create_course( org=md5(self.course.org).hexdigest() )
alternate_course = self.create_course(course='mobile', mobile_available=True)
alternate_course = self.create_course( org=md5(self.course.org).hexdigest() )
unfiltered_response = self.verify_response(params={'username': self.staff_user.username}) for org in [self.course.org, alternate_course.org]: self.assertTrue(
filtered_response = self.verify_response(params={'org': self.course.org, 'username': self.staff_user.username}) self.assertTrue(
alternate_course = self.create_course(course='mobile', mobile_available=True)
'course_id': u'edX/toy/2012_Fall',
expected_mongo_calls = 1 serializer_class = CourseDetailSerializer
about_descriptor = XBlock.load_class('about') overview_template = about_descriptor.get_template('overview.yaml') self.expected_data['overview'] = overview_template.get('data')
from safe_lxml import defuse_xml_libs defuse_xml_libs()
import contracts contracts.disable_all()
modulestore()
from django.core.wsgi import get_wsgi_application
dashboard = DashboardPage(self.browser) dashboard.wait_for_page() return dashboard
if self._course_id is not None: url += "?{params}".format( params=urlencode({ "course_id": self._course_id, "enrollment_action": "enroll" }) )
EmptyPromise( lambda: self.current_form != old_form, "Finish toggling to the other form" ).fulfill()
self.q(css=".register-button").click()
self.q(css=".login-button").click()
self.q(css="a.forgot-password").click()
EmptyPromise( lambda: self.current_form != login_form, "Finish toggling to the password reset form" ).fulfill()
self.wait_for_element_visibility('#password-reset-email', 'Email field is shown') self.q(css="#password-reset-email").fill(email)
self.q(css="button.js-reset").click()
active_script = "return " + title_selector + " === document.activeElement;" return self.browser.execute_script(active_script)
self.current_view = self.MAPPING["search"](self.browser) if text.strip(): self.current_view.wait_for_page()
chapter_index = self._chapter_index(chapter) if chapter_index is None: return None
return self._section_scores(chapter_index, section_index)
return chapter_titles.index(title.lower()) + 1
section_titles = [t.split('\n')[0] for t in section_titles]
section_titles = [t for t in section_titles if t]
return section_titles.index(title.lower()) + 1
score_css = "div.chapters>section:nth-of-type({0}) div.sections>div:nth-of-type({1}) div.scores>ol>li".format( chapter_index, section_index )
return [tuple(map(int, score.split('/'))) for score in text_scores]
course_listing = self.q(css=".course").filter(lambda el: course_name in el.text).results
course_listing = self.q(css=".course").filter(lambda el: course_name in el.text).results
el = course_listing[0]
el.find_element_by_css_selector('#upgrade-to-verified').click()
all_links = self.q(css='a.enter-course').map(lambda el: el.get_attribute('href')).results
link_index = None for index in range(len(all_links)): if course_id in all_links[index]: link_index = index break
tab_css = self._tab_css(tab_name)
return EmptyPromise( lambda: self._is_on_tab(tab_name), "{0} is the current tab".format(tab_name) )
BASE_URL = os.environ.get('test_url', 'http://localhost:8003')
AUTH_BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
Promise(_check_func, "The 'Next Step' button is enabled.").fulfill()
for idx, text in enumerate(text_options): if text == POLL_ANSWER: self.q(css=text_selector).nth(idx).click()
cohort_management_section.wait_for_ajax() cohort_management_section.wait_for_page() return cohort_management_section
folders_list_in_path = folders_list_in_path[:-4]
folders_list_in_path.extend(['data', 'uploads', file_name])
return os.sep.join(folders_list_in_path)
return self.q(css='.cohorts-state-section').visible or self.q(css='.new-cohort-form').visible
self.wait_for( lambda: "Add a New Cohort" in self.q(css=self._bounded_selector(".form-title")).text, "Create cohort form is visible" )
if assignment_type: self.set_assignment_type(assignment_type)
self.wait_for( lambda: "added to this cohort" in self.get_cohort_confirmation_messages(wait_for_messages=True)[0], "Student(s) added confirmation message." )
self.wait_for_element_visibility(email_selector, 'Email field is visible') self.q(css=email_selector).fill(email)
EmptyPromise( lambda: self.q(css=enrollment_button).present, "Enrollment button" ).fulfill() self.q(css=enrollment_button).click()
self.q(css="select#allowance_type").present or self.q(css="label#timed_exam_allowance_type").present
dashboard = DashboardPage(self.browser) dashboard.wait_for_page() return dashboard
self.q(css=".contribution-option > input").first.click() self.q(css="input[name='verified_mode']").click()
self.wait_for_element_visibility(ccx_name_selector, 'CCX name field is visible') self.q(css=ccx_name_selector).fill(ccx_name)
EmptyPromise( lambda: self.q(css=create_ccx_button).present, "Create a new Custom Course for edX" ).fulfill() self.q(css=create_ccx_button).click()
url_path = ""
self._discussion_page = InlineDiscussionPage(self.browser, self.discussion_id)
EmptyPromise(lambda: self.is_button_shown('transcript_button'), "transcript button is shown").fulfill()
if self.is_captions_visible() != captions_new_state: self.click_player_button('transcript_button')
EmptyPromise(lambda: self.is_captions_visible() == captions_new_state, "Transcripts are {state}".format(state=state)).fulfill()
EmptyPromise(lambda: self.is_closed_captions_visible() == closed_captions_new_state, "Closed captions are {state}".format(state=state)).fulfill()
self.wait_for_ajax()
if button == 'pause': self.wait_for(lambda: self.state != 'buffering', 'Player is Ready for Pause')
wrapper_width = 75 if is_transcript_visible else 100 initial = self.browser.get_window_size()
time.sleep(0.2)
time.sleep(0.2)
self.browser.set_window_size( initial['width'], initial['height'] )
if '.' + transcript_format not in self.q(css=transcript_selector).text[0]: return False
cc_button_selector = self.get_element_selector(VIDEO_BUTTONS["transcript"]) element_to_hover_over = self.q(css=cc_button_selector).results[0] ActionChains(self.browser).move_to_element(element_to_hover_over).perform()
if self.current_language() != code: self.select_language(code)
self.wait_for_ajax()
logging.debug("Current state of '{}' element is '{}'".format(state_selector, current_state))
all_times = self.q(css=selector).text[0]
nav_dict = dict()
for sec_index, sec_title in enumerate(section_titles):
nav_dict[sec_title] = self._subsection_titles(sec_index + 1)
self.browser.execute_script("jQuery.fx.off = true;")
try: sec_index = self._section_titles().index(section_title) except ValueError: self.warning("Could not find section '{0}'".format(section_title)) return
section_css = '.course-navigation .chapter:nth-of-type({0})'.format(sec_index + 1) self.q(css=section_css).first.click()
subsection_css = ( ".course-navigation .chapter-content-container:nth-of-type({0}) " ".menu-item:nth-of-type({1})" ).format(sec_index + 1, subsec_index + 1)
self.q(css=subsection_css).first.click() self._on_section_promise(section_title, subsection_title).fulfill()
all_items = self.sequence_items
seq_css = "ol#sequence-list>li:nth-of-type({0})>.nav-item".format(seq_index + 1) self.q(css=seq_css).first.click() self.wait_for_ajax()
subsection_css = ( ".course-navigation .chapter-content-container:nth-of-type({0}) " ".menu-item a p:nth-of-type(1)" ).format(section_index)
REMOVE_SPAN_TAG_RE = re.compile(r'</span>(.+)<span')
return self.q(css=".badges-modal").visible
self._user_info = None
self._params = {}
BASE_URL = os.environ.get('test_url', 'http://localhost:8003')
AUTH_BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
footer_el = footer_nav.find_element_by_xpath('..') return 'hidden' not in footer_el.get_attribute('class').split()
return element.is_displayed() and all(size > 0 for size in element.size.itervalues())
disable_animations(page) page.q(css=css).filter(_is_visible).nth(source_index).click()
page.wait_for_ajax()
return ( self.q(css='{} .acid-block'.format(self.context_selector)).present and wait_for_xblock_initialization(self, self.context_selector) and self._ajax_finished() )
is_done = page.browser.execute_script("return $({!r}).data('initialized')".format(xblock_css)) return (is_done, is_done)
type_in_codemirror(self, 0, content)
test_dir = path(__file__).abspath().dirname().dirname().dirname() file_path = test_dir + '/data/uploads/' + file_name
self.wait_for_ajax()
def is_browser_on_page(self): wait_for_ajax_or_reload(self.browser) return self.q(css='body.view-settings').visible
license_text = self.q(css='section.license span.license-text') if license_text.is_present(): return license_text.text[0] return None
folders_list_in_path = folders_list_in_path[:-4]
folders_list_in_path.extend(['data', 'uploads', file_name])
return os.sep.join(folders_list_in_path)
self.wait_for_element_visibility(upload_btn_selector, 'upload button is present')
self.wait_for_element_presence(self.upload_image_popup_window_selector, 'upload dialog is present')
filepath = SettingsPage.get_asset_path(file_to_upload) self.q(css=self.upload_image_browse_button_selector).results[0].send_keys(filepath) self.q(css=self.upload_image_upload_button_selector).results[0].click()
self.wait_for_element_absence(self.upload_image_popup_window_selector, 'upload dialog is hidden')
BASE_URL = os.environ.get('test_url', 'http://localhost:8031')
return None
return None
return os.sep.join(__file__.split(os.sep)[:-4]) + '/data/uploads/' + filename
self.q(css='button.signatory-panel-save').click() self.mode = 'details' self.wait_for_ajax() self.wait_for_signatory_detail_view()
return self.q(css='h1.page-header')[0].text.split('\n')[-1]
return "/".join([BASE_URL, self.url_path, unicode(self.locator)])
return os.sep.join(__file__.split(os.sep)[:-4]) + '/data/imports/' + filename
completed = True
try:
self.q(css=self._bounded_selector(self.NAME_INPUT_SELECTOR)).results[0].send_keys(Keys.ENTER) self.wait_for_ajax()
return "is-editing" in self.q( css=self._bounded_selector(self.NAME_FIELD_WRAPPER_SELECTOR) )[0].get_attribute("class")
return self.q(css=self._bounded_selector(child_class.BODY_SELECTOR)).map( lambda el: child_class(self.browser, el.get_attribute('data-locator'))).results
self.browser.execute_script("jQuery.fx.off = true;")
grandkids = [] for descendant in descendants: grandkids.extend(descendant.children)
if not self.q(css="input.no_special_exam").present: return False
if not self.q(css="input.timed_exam").present: return False
if not self.q(css="input.proctored_exam").present: return False
if not self.q(css="input.practice_exam").present: return False
return self.q(css="#is_prereq").visible
return self.q(css="#is_prereq:checked").present
return self.q(css="#prereq").visible
return self.q(css="#prereq_min_score").visible
self.wait_for_ajax()
elem.clear() elem.send_keys(value) elem.send_keys(Keys.TAB) self.save()
EmptyPromise( lambda: "login" not in self.browser.current_url, "redirected from the login page" ).fulfill()
key = self.q(css=KEY_CSS).nth(i).text[0] if key == expected_key: return i
self.browser.switch_to_window(browser_window_handles[-1])
click_css(self, 'a.delete-button', source_index, require_notification=False) confirm_prompt(self)
self.wait_for_page()
grandkids = [] for descendant in descendants: grandkids.extend(descendant.children)
url_path = ""
page.wait_for_component_menu() click_css(page, 'button>span.large-advanced-icon', menu_index, require_notification=False)
page.wait_for_element_visibility('.new-component-advanced', 'Advanced component menu is visible')
component_css = 'button[data-category={}]'.format(name) page.wait_for_element_visibility(component_css, 'Advanced component {} is visible'.format(name))
click_css(page, component_css, 0)
if is_advanced_problem: advanced_tab = page.q(css='.problem-type-tabs a').filter(text='Advanced').first advanced_tab.click()
page.wait_for_component_menu() click_css(page, 'button>span.large-html-icon', menu_index, require_notification=False)
page.wait_for_element_visibility('.new-component-html', 'HTML component menu is visible')
component_css = 'button[data-category=html]' if boilerplate: component_css += '[data-boilerplate={}]'.format(boilerplate) else: component_css += ':not([data-boilerplate])'
click_css(page, component_css, 0)
input_element.click() input_element.send_keys(Keys.CONTROL + 'a') input_element.send_keys(value) return input_element
LIBRARY_LABEL = "Library" COUNT_LABEL = "Count" SCORED_LABEL = "Scored" PROBLEM_TYPE_LABEL = "Problem Type"
self.wait_for_ajax() self.wait_for_element_absence(btn_selector, 'Wait for the XBlock to finish reloading')
[DISPLAY_NAME, 'Video', False], ['Default Video URL', 'https://www.youtube.com/watch?v=3_yD_cEKoCk, , ', False],
DELAY = 0.5
self.click_button('create_video', require_notification=True) self.wait_for_video_component_render()
self._params = {}
PAGES_PACKAGE_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'pages')
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
LogoutPage(self.browser).visit() self._make_har_file(login_page)
STUDIO_BASE_URL = os.environ.get('studio_url', 'http://localhost:8031')
LMS_BASE_URL = os.environ.get('lms_url', 'http://localhost:8003')
XQUEUE_STUB_URL = os.environ.get('xqueue_url', 'http://localhost:8040')
ORA_STUB_URL = os.environ.get('ora_url', 'http://localhost:8041')
COMMENTS_STUB_URL = os.environ.get('comments_url', 'http://localhost:4567')
EDXNOTES_STUB_URL = os.environ.get('edxnotes_url', 'http://localhost:8042')
PROGRAMS_STUB_URL = os.environ.get('programs_url', 'http://localhost:8090')
session = requests.Session() response = session.get(LMS_BASE_URL + "/auto_auth?superuser=true")
self.user = {}
session = requests.Session() response = session.get(STUDIO_BASE_URL + "/auto_auth?staff=true")
response = self.session.post( STUDIO_BASE_URL + '/xblock/', data=json.dumps(create_payload), headers=self.headers, )
response = self.session.post( STUDIO_BASE_URL + '/xblock/' + loc, data=xblock_desc.serialize(), headers=self.headers, )
response = self.session.put( "{}/xblock/{}".format(STUDIO_BASE_URL, locator), data=json.dumps(data), headers=self.headers, )
CourseUpdateDesc = namedtuple("CourseUpdateDesc", ['date', 'content'])
if start_date is None: start_date = datetime.datetime(1970, 1, 1)
response = self.session.post( STUDIO_BASE_URL + '/course/', data=self._encode_post_dict(self._course_dict), headers=self.headers )
if err is not None: raise FixtureError("Could not create course {0}. Error message: '{1}'".format(self, err))
response = self.session.get(url, headers=self.headers)
details.update(self._course_details)
response = self.session.post( url, data=self._encode_post_dict(details), headers=self.headers, )
payload = json.dumps({ 'children': None, 'data': handouts_html, 'id': self._handouts_loc, 'metadata': dict(), })
response = self.session.post( url, data=self._encode_post_dict(self._advanced_settings), headers=self.headers, )
payload = {self._pattern: json.dumps(self._response_dict)} response = requests.put(url, data=payload)
xblock_desc.publish = "not-applicable"
if 'user_id' in kwargs: kwargs['user_id'] = str(kwargs['user_id']) return kwargs
self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) self.instructor_dashboard_page.visit() self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management()
self.cohort_name = "OnlyCohort" self.setup_cohort_config(self.course_fixture) self.cohort_id = self.add_manual_cohort(self.course_fixture, self.cohort_name)
self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) self.instructor_dashboard_page.visit() self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management() self.cohort_management_page.wait_for_page()
self.cohort_management_page.save_discussion_topics(key)
confirmation_message = self.cohort_management_page.get_cohort_discussions_message(key=key) self.assertEqual("Your changes have been saved.", confirmation_message)
self.assertTrue(self.cohort_management_page.is_save_button_disabled(key))
self.cohort_management_page.select_always_inline_discussion()
self.cohort_management_page.select_cohort_some_inline_discussion() self.assertFalse(self.cohort_management_page.is_save_button_disabled(self.inline_key)) self.assertFalse(self.cohort_management_page.inline_discussion_topics_disabled())
self.cohort_management_page.select_cohort_some_inline_discussion()
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertFalse(self.cohort_management_page.is_save_button_disabled(self.inline_key))
self.save_and_verify_discussion_topics(key=self.inline_key)
self.cohort_management_page.select_cohort_some_inline_discussion()
self.assertFalse(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertTrue(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_cohort_some_inline_discussion()
self.assertFalse(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertTrue(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_discussion_topic(self.inline_key)
self.assertFalse(self.cohort_management_page.is_category_selected())
self.cohort_management_page.select_cohort_some_inline_discussion()
self.assertFalse(self.cohort_management_page.is_category_selected())
self.save_and_verify_discussion_topics(key=self.inline_key)
self.verify_discussion_topics_after_reload(self.inline_key, cohorted_topics_after)
self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id) self.instructor_dashboard_page.visit() self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management()
self.disable_cohorting(self.course_fixture) self.refresh_thread_page(self.thread_id) self.assertEquals(self.thread_page.get_group_visibility_label(), "This post is visible to everyone.")
def refresh_thread_page(self, thread_id): self.browser.refresh() self.thread_page.wait_for_page()
pass
pass
pass
pass
self.assertTrue( self.thread_page_1.check_threads_rendered_successfully(thread_count=self.thread_count) )
self.thread_page_1.click_and_open_thread(thread_id=self.thread_ids[1]) self.assertTrue(self.thread_page_2.is_browser_on_page())
self.thread_page_2.check_focus_is_set(selector=".discussion-article")
self.assertFalse(thread_page.check_if_selector_is_focused(selector='.thread-wrapper'))
for i in range(current_page, total_pages): _check_page() if current_page < total_pages: page.click_on_page(current_page + 1) current_page += 1
for i in range(current_page, 0, -1): _check_page() if current_page > 1: page.click_on_page(current_page - 1) current_page -= 1
CourseFixture(**self.course_info).install()
self.cohort_a_student_username = "cohort_a_student" self.cohort_a_student_email = "cohort_a_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_a_student_username, email=self.cohort_a_student_email, no_login=True ).visit()
self.cohort_b_student_username = "cohort_b_student" self.cohort_b_student_email = "cohort_b_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_b_student_username, email=self.cohort_b_student_email, no_login=True ).visit()
self.cohort_default_student_username = "cohort_default_student" self.cohort_default_student_email = "cohort_default_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_default_student_username, email=self.cohort_default_student_email, no_login=True ).visit()
StudioAutoAuthPage( self.browser, username=self.staff_user["username"], email=self.staff_user["email"] ).visit()
EmptyPromise( lambda: cohort_name == cohort_management_page.get_selected_cohort(), "Waiting for new cohort" ).fulfill() cohort_management_page.add_students_to_selected_cohort([student])
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file)
course_outline.visit() subsection = course_outline.section_at(0).subsection_at(0) subsection.expand_subsection() subsection.add_unit()
self._auto_auth(self.USERNAME, self.EMAIL, False) self.dashboard.visit()
super(BaseLmsIndexTest, self).setUp()
self.page = IndexPage(self.browser)
self.page.visit()
self.now = datetime.datetime.now()
self.assertFalse(self.page.intro_video_element.visible)
time.sleep(time_between_creation)
self.teams_page.verify_my_team_count(expected_number_of_teams)
url = self.browser.current_url fragment_index = url.find('#') if fragment_index >= 0: url = url[0:fragment_index]
self.assertEqual(self.team_page.team_name, self.team['name']) self.assertTrue(self.team_page.edit_team_button_present)
self.teams_page.click_specific_topic("Example Topic") self.teams_page.verify_topic_team_count(1)
self.verify_my_team_count(1)
self.teams_page.click_all_topics() self.verify_my_team_count(1)
self.teams_page.click_all_topics() self.verify_my_team_count(0)
self._auto_auth("STAFF_TESTER", "staff101@example.com", True)
self.course_outline.visit() self.course_outline.open_subsection_settings_dialog(0) self.course_outline.select_access_tab() self.course_outline.make_gating_prerequisite()
self._auto_auth("STAFF_TESTER", "staff101@example.com", True)
self.course_outline.visit() self.course_outline.open_subsection_settings_dialog(1) self.course_outline.select_access_tab() self.course_outline.add_prerequisite_to_subsection("80")
self.course_fixture = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
children_headers = self._set_library_content_settings(count=2, capa_type="Custom Evaluated Script") self.assertEqual(children_headers, set())
super(BaseLmsDashboardTest, self).setUp()
self.dashboard_page = DashboardPage(self.browser)
self.dashboard_page.visit()
self.now = datetime.datetime.now()
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
self.dashboard_page.visit()
self.assertEqual(course_date, expected_course_date)
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file) self.addCleanup(remove_file, self.TEST_INDEX_FILENAME)
course_fixture = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage( self.browser, username=self.username, email=self.email, password=self.password, course_id=self.course_id, staff=False ).visit()
problem_page.click_hint() self.assertIn("Hint (1 of 2): mathjax should work1", problem_page.extract_hint_text_from_html) problem_page.verify_mathjax_rendered_in_hint()
problem_page.click_hint()
self.assertTrue(self.coach_dashboard_page.is_browser_on_enrollment_page())
super(XBlockAcidBase, self).setUp()
self.oauth_page.confirm() self.oauth_page.wait_for_element_absence( 'input[name=authorize]', 'Authorization button is not present' )
self.submission = "a=1" + self.unique_id[0:5]
if self.xqueue_grade_response is not None: XQueueResponseFixture(self.submission, self.xqueue_grade_response).install()
time.sleep(5)
self.course_fixture = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=True).visit()
EmptyPromise( lambda: cohort_name == cohort_management_page.get_selected_cohort(), "Waiting for new cohort" ).fulfill() cohort_management_page.add_students_to_selected_cohort([student])
course_page = self._goto_staff_page() course_page.set_staff_view_mode_specific_student(student_a_username) verify_expected_problem_visibility(self, course_page, [self.alpha_text, self.everyone_text])
course_page.set_staff_view_mode_specific_student(student_b_username) verify_expected_problem_visibility(self, course_page, [self.beta_text, self.everyone_text])
self.account_settings_page = AccountSettingsPage(self.browser) self.account_settings_page.visit() self.account_settings_page.wait_for_ajax()
self.expected_settings_change_initiated_event( 'email', email, 'you@there.com', username=username, user_id=user_id),
self.assert_no_setting_changed_event()
self.assert_no_setting_changed_event()
self.assertEqual(self.account_settings_page.value_for_dropdown_field('year_of_birth', ''), '')
self.reset_password_page.visit()
self.assertTrue(self.reset_password_page.is_form_visible())
self.reset_password_page.visit()
self.reset_password_page.fill_password_reset_form(self.user_info['email'])
self.assertIn("Password Reset Email Sent", self.reset_password_page.get_success_message())
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
email, password = self._create_unique_user()
self.login_page.visit().login(email=email, password=password)
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
self.login_page.visit()
self.login_page.login(email="nobody@nowhere.com", password="password")
self.assertIn("Email or password is incorrect.", self.login_page.wait_for_errors())
self.login_page.visit().password_reset(email=email)
self.assertIn("Password Reset Email Sent", self.login_page.wait_for_success())
self.login_page.visit()
self.login_page.password_reset(email="nobody@nowhere.com")
self.assertIn( "No user with the provided email address exists.", self.login_page.wait_for_errors() )
email, password = self._create_unique_user()
self.login_page.visit() self.assertScreenshot('#login .login-providers', 'login-providers-{}'.format(self.browser.name))
self.login_page.click_third_party_dummy_provider()
self.login_page.login(email=email, password=password)
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
LogoutPage(self.browser).visit()
AutoAuthPage(self.browser, course_id=self.course_id).visit() self._link_dummy_account() LogoutPage(self.browser).visit()
course_page = CoursewarePage(self.browser, self.course_id) self.browser.get(course_page.url + '?tpa_hint=oa2-dummy')
course_page.wait_for_page()
account_settings.switch_account_settings_tabs('accounts-tab')
account_settings.switch_account_settings_tabs('accounts-tab') account_settings.wait_for_link_title_for_link_field(field_id, "Unlink This Account")
account_settings = AccountSettingsPage(self.browser).visit() account_settings.switch_account_settings_tabs('accounts-tab')
AutoAuthPage( self.browser, username=username, email=email, password=password ).visit()
LogoutPage(self.browser).visit()
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
self.register_page.visit()
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
self.register_page.visit()
self.register_page.visit() self.assertScreenshot('#register .login-providers', 'register-providers-{}'.format(self.browser.name))
self.register_page.click_third_party_dummy_provider()
self.register_page.register(country="US", favorite_movie="Battlestar Galactica", terms_of_service=True)
course_names = self.dashboard_page.wait_for_page().available_courses self.assertIn(self.course_info["display_name"], course_names)
LogoutPage(self.browser).visit()
account_settings = AccountSettingsPage(self.browser).visit() account_settings.switch_account_settings_tabs('accounts-tab')
CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] ).install()
ModeCreationPage(self.browser, self.course_id).visit()
ModeCreationPage(self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20').visit()
student_id = AutoAuthPage(self.browser).visit().get_user_id()
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
self.payment_and_verification_flow.immediate_verification()
self.payment_and_verification_flow.webcam_capture() self.payment_and_verification_flow.next_verification_step(self.immediate_verification_page)
self.payment_and_verification_flow.webcam_capture() self.payment_and_verification_flow.next_verification_step(self.immediate_verification_page)
self.payment_and_verification_flow.next_verification_step(self.immediate_verification_page)
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'verified')
student_id = AutoAuthPage(self.browser).visit().get_user_id()
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'verified')
student_id = AutoAuthPage(self.browser, course_id=self.course_id).visit().get_user_id()
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'honor')
self.dashboard_page.upgrade_enrollment(self.course_info["display_name"], self.upgrade_page)
self.upgrade_page.indicate_contribution()
self.upgrade_page.proceed_to_payment()
self.fake_payment_page.submit_payment()
self.dashboard_page.visit()
enrollment_mode = self.dashboard_page.get_enrollment_mode(self.course_info["display_name"]) self.assertEqual(enrollment_mode, 'verified')
self.course_info['number'] = self.unique_id[0:6]
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.course_info_page.visit() self.tab_nav.go_to_tab('Wiki')
self.course_info['number'] = self.unique_id[0:6]
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.progress_page.visit() self.tab_nav.go_to_tab('Home')
self.assertEqual(self.course_info_page.num_updates, 1)
handout_links = self.course_info_page.handout_links self.assertEqual(len(handout_links), 1) self.assertIn('demoPDF.pdf', handout_links[0])
self.course_info_page.visit() self.tab_nav.go_to_tab('Progress')
CHAPTER = 'Test Section' SECTION = 'Test Subsection' EXPECTED_SCORES = [(0, 3), (0, 1)]
self.course_info_page.visit() self.tab_nav.go_to_tab('Test Static Tab') self.assertTrue(self.tab_nav.is_on_tab('Test Static Tab'))
self.course_info_page.visit() self.tab_nav.go_to_tab('Test Static Tab') self.assertTrue(self.tab_nav.is_on_tab('Test Static Tab'))
self.tab_nav.mathjax_has_rendered()
self.course_info_page.visit() self.tab_nav.go_to_tab('Wiki') self.assertTrue(self.tab_nav.is_on_tab('Wiki'))
self.course_info_page.visit() self.tab_nav.go_to_tab('Course')
EXPECTED_SECTIONS = { 'Test Section': ['Test Subsection'], 'Test Section 2': ['Test Subsection 2', 'Test Subsection 3'] }
self.course_nav.go_to_section('Test Section', 'Test Subsection')
EXPECTED_ITEMS = ['Test Problem 1', 'Test Problem 2', 'Test HTML']
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, course_id=self.course_id).visit()
for i in range(1, 3): self.tab_nav.go_to_tab("PDF Book {}".format(i))
AutoAuthPage(self.browser, course_id=self.course_id).visit()
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.dashboard_page.visit() self.assertFalse(self.dashboard_page.pre_requisite_message_displayed())
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=True).visit()
self.settings_page.visit() self._set_pre_requisite_course()
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=False).visit()
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.course_info_page.visit() self.tab_nav.go_to_tab('Course') self.course_nav.go_to_section('Test Section', 'Test Subsection')
self.assertIn("What is the sum of 17 and 3?", problem_page.problem_text)
problem_page.fill_answer("20") problem_page.click_check() self.assertTrue(problem_page.is_correct())
problem_page.fill_answer("4") problem_page.click_check() self.assertFalse(problem_page.is_correct())
AutoAuthPage(self.browser, course_id=self.course_id).visit()
self.courseware_page.visit() self.courseware_page.wait_for_page() self.assertFalse(element_has_text( page=self.courseware_page, css_selector=entrance_exam_link_selector, text='Entrance Exam' ))
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=True).visit()
LogoutPage(self.browser).visit() AutoAuthPage(self.browser, course_id=self.course_id, staff=False).visit()
self.courseware_page.visit() self.courseware_page.wait_for_page() self.assertTrue(element_has_text( page=self.courseware_page, css_selector=entrance_exam_link_selector, text='Entrance Exam' ))
ModeCreationPage(self.browser, self.course_id).visit()
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
select_option_by_text(language_selector, 'English') self.account_settings.wait_for_ajax() self.assertEqual(self.account_settings.value_for_dropdown_field('pref-lang'), u'English')
self.register_page = CombinedLoginAndRegisterPage(self.browser, start_page="register") self.dashboard_page = DashboardPage(self.browser)
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
self._auto_auth(self.USERNAME, self.EMAIL, False)
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True) self.course_outline.visit()
self.course_outline.open_subsection_settings_dialog()
self.course_outline.select_advanced_tab()
LogoutPage(self.browser).visit() self._login_as_a_verified_user() self.courseware_page.visit()
self.courseware_page.start_proctored_exam()
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True) self.course_outline.visit()
self.course_outline.open_subsection_settings_dialog()
self.course_outline.select_advanced_tab()
LogoutPage(self.browser).visit() self._login_as_a_verified_user() self.courseware_page.visit()
self.courseware_page.start_timed_exam()
self.courseware_page.stop_timed_exam()
self._create_a_timed_exam_and_attempt()
__, __ = self.log_in_as_instructor()
instructor_dashboard_page = self.visit_instructor_dashboard() allowance_section = instructor_dashboard_page.select_special_exams().select_allowance_section()
self.assertTrue(allowance_section.is_add_allowance_button_visible)
allowance_section.click_add_allowance_button()
self.assertTrue(allowance_section.is_add_allowance_popup_visible)
allowance_section.submit_allowance_form('10', self.USERNAME)
self.assertTrue(allowance_section.is_allowance_record_visible)
self._create_a_timed_exam_and_attempt()
__, __ = self.log_in_as_instructor()
instructor_dashboard_page = self.visit_instructor_dashboard() exam_attempts_section = instructor_dashboard_page.select_special_exams().select_exam_attempts_section()
self.assertTrue(exam_attempts_section.is_search_text_field_visible)
self.assertTrue(exam_attempts_section.is_student_attempt_visible)
exam_attempts_section.remove_student_attempt() self.assertFalse(exam_attempts_section.is_student_attempt_visible)
AutoAuthPage( self.browser, username="johndoe_saee", email=self.student_identifier, course_id=self.course_id, staff=False ).visit()
self.log_in_as_instructor() self.student_admin_section = self.visit_instructor_dashboard().select_student_admin()
alert = get_modal_alert(self.student_admin_section.browser) alert.dismiss()
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_exceptions_section()
self.assertIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertIn(notes, self.certificates_section.last_certificate_exception.text)
self.certificates_section.remove_first_certificate_exception() self.assertNotIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertNotIn(notes, self.certificates_section.last_certificate_exception.text)
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_exceptions_section()
self.assertNotIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertNotIn(notes, self.certificates_section.last_certificate_exception.text)
self.certificates_section.add_certificate_exception(self.user_name, '')
self.certificates_section.add_certificate_exception(self.user_name, '')
self.certificates_section.wait_for_certificate_exceptions_section() self.certificates_section.click_add_exception_button()
self.certificates_section.wait_for_certificate_exceptions_section()
self.certificates_section.wait_for_certificate_exceptions_section()
self.certificates_section.add_certificate_exception(self.user_name, '')
self.certificates_section.click_generate_certificate_exceptions_button() self.certificates_section.wait_for_ajax()
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_exceptions_section()
self.assertIn(self.user_name, self.certificates_section.last_certificate_exception.text) self.assertIn(expected_notes, self.certificates_section.last_certificate_exception.text)
CourseFixture( org='test_org', number='335535897951379478207964576572017930000', run='test_run', display_name='Test Course 335535897951379478207964576572017930000', ).install()
self.course_info['number'] = "335535897951379478207964576572017930000"
self.student_id = "99" self.student_name = "testcert" self.student_email = "cert@example.com"
AutoAuthPage( self.browser, username=self.student_name, email=self.student_email, course_id=self.course_id, ).visit()
self.assertIn( "Certificate has been successfully invalidated for {user}.".format(user=self.student_name), self.certificates_section.certificate_invalidation_message.text )
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_invalidations_section()
self.assertIn(self.student_name, self.certificates_section.last_certificate_invalidation.text) self.assertIn(notes, self.certificates_section.last_certificate_invalidation.text)
self.certificates_section.refresh()
self.certificates_section.wait_for_certificate_invalidations_section()
self.certificates_section.remove_first_certificate_invalidation()
self.assertNotIn(self.student_name, self.certificates_section.last_certificate_invalidation.text) self.assertNotIn(notes, self.certificates_section.last_certificate_invalidation.text)
self.certificates_section.fill_certificate_invalidation_user_name_field("") self.certificates_section.click_invalidate_certificate_button() self.certificates_section.wait_for_ajax()
self.certificates_section.fill_certificate_invalidation_user_name_field(invalid_user) self.certificates_section.click_invalidate_certificate_button() self.certificates_section.wait_for_ajax()
self.certificates_section.wait_for_certificate_invalidations_section()
self.certificate_page = CertificatePage(self.browser, self.user_id, self.course_id)
self.course_info['number'] = "3355358979513794782079645765720179311111"
self.assertEqual(actual_padding, expected_padding)
self.course_nav.go_to_section('Test Section', 'Test Subsection')
self.course_nav.go_to_vertical('Test Problem 1')
self.course_nav.q(css='select option[value="{}"]'.format('blue')).first.click()
self.course_nav.q(css='fieldset label:nth-child(3) input').nth(0).click()
self.course_nav.q(css='button.check.Check').click() self.course_nav.wait_for_ajax()
self.course_nav.go_to_section('Test Section 2', 'Test Subsection 2')
self.course_nav.go_to_vertical('Test Problem 2')
self.course_nav.q(css='input[id^=input_][id$=_2_1]').fill('A*x^2 + sqrt(y)')
self.course_nav.q(css='button.check.Check').click() self.course_nav.wait_for_ajax()
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file) self.addCleanup(remove_file, self.TEST_INDEX_FILENAME)
self.cohort_default_student_username = "cohort_default_student" self.cohort_default_student_email = "cohort_default_student@example.com" StudioAutoAuthPage( self.browser, username=self.cohort_default_student_username, email=self.cohort_default_student_email, no_login=True ).visit()
EmptyPromise( lambda: cohort_name == cohort_management_page.get_selected_cohort(), "Waiting for new cohort" ).fulfill() cohort_management_page.add_students_to_selected_cohort([student])
self.course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
self._auto_auth(self.USERNAME, self.EMAIL, False)
self._goto_problem_page()
LogoutPage(self.browser).visit() self._auto_auth("STAFF_TESTER", "staff101@example.com", True)
self.course_outline.visit()
self.course_outline.change_problem_release_date()
LogoutPage(self.browser).visit() self._auto_auth(self.USERNAME, self.EMAIL, False)
self.courseware_page.visit() self.assertEqual(self.problem_page.problem_name, 'Test Problem 2')
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
self._auto_auth(self.USERNAME, self.EMAIL, False)
self.track_selection_page.visit()
self.track_selection_page.enroll('verified')
self.payment_and_verification_flow.proceed_to_payment()
self.fake_payment_page.submit_payment()
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 0, next_enabled=True, prev_enabled=False)
self.courseware_page.click_next_button_on_top() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 1, next_enabled=True, prev_enabled=True)
self.courseware_page.go_to_sequential_position(4) self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 3, next_enabled=True, prev_enabled=True)
self.courseware_page.click_next_button_on_bottom() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,2', 0, next_enabled=True, prev_enabled=True)
self.courseware_page.click_next_button_on_top() self.assert_navigation_state('Test Section 2', 'Test Subsection 2,1', 0, next_enabled=False, prev_enabled=True)
self.courseware_page.click_previous_button_on_top() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,2', 0, next_enabled=True, prev_enabled=True)
self.courseware_page.click_previous_button_on_bottom() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 3, next_enabled=True, prev_enabled=True)
self.courseware_page.click_previous_button_on_bottom() self.assert_navigation_state('Test Section 1', 'Test Subsection 1,1', 2, next_enabled=True, prev_enabled=True)
filter_sequence_ui_event = lambda event: event.get('name', '').startswith('edx.ui.lms.sequence.')
filter_selected_events = lambda event: event.get('name', '') == 'edx.ui.lms.outline.selected' selected_events = self.wait_for_events(event_filter=filter_selected_events, timeout=2)
self.courseware_page.a11y_audit.config.set_scope( include=['div.sequence-nav']) self.courseware_page.a11y_audit.check_for_accessibility_errors()
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage( self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False ).visit()
self.go_to_tab_and_assert_problem(1, self.problem1_name)
self.problem_page.click_choice('choice_choice_1') self.problem_page.click_check() self.problem_page.wait_for_expected_status('label.choicegroup_incorrect', 'incorrect')
problem1_content_before_switch = self.problem_page.problem_content
self.go_to_tab_and_assert_problem(2, self.problem2_name)
self.go_to_tab_and_assert_problem(1, self.problem1_name) problem1_content_after_coming_back = self.problem_page.problem_content self.assertEqual(problem1_content_before_switch, problem1_content_after_coming_back)
self.go_to_tab_and_assert_problem(1, self.problem1_name)
self.problem_page.click_choice('choice_choice_1') self.problem_page.click_save() self.problem_page.wait_for_expected_status('div.capa_alert', 'saved')
problem1_content_before_switch = self.problem_page.problem_content
self.go_to_tab_and_assert_problem(2, self.problem2_name)
self.go_to_tab_and_assert_problem(1, self.problem1_name) problem1_content_after_coming_back = self.problem_page.problem_content self.assertIn(problem1_content_after_coming_back, problem1_content_before_switch)
self.go_to_tab_and_assert_problem(1, self.problem1_name)
problem1_content_before_switch = self.problem_page.problem_content
self.go_to_tab_and_assert_problem(2, self.problem2_name)
self.go_to_tab_and_assert_problem(1, self.problem1_name) problem1_content_after_coming_back = self.problem_page.problem_content self.assertEqual(problem1_content_before_switch, problem1_content_after_coming_back)
if privacy is not None: profile_page.visit()
profile_page.privacy = privacy
if privacy == self.PRIVACY_PUBLIC: self.assertEqual(profile_page.privacy, 'all_users') else: self.assertEqual(profile_page.privacy, 'private')
profile_page.visit()
if privacy is None: privacy = self.PRIVACY_PUBLIC self.visit_profile_page(username, privacy=privacy)
if birth_year: self.set_birth_year(birth_year)
LogoutPage(self.browser).visit()
self.browser.refresh() profile_page.wait_for_page() self.verify_profile_page_is_public(profile_page)
self.browser.refresh() profile_page.wait_for_page() self.verify_profile_page_is_private(profile_page)
self.notes_page.search("note") self.assertFalse(self.notes_page.is_error_visible) self.assertIn(u"Search Results", self.notes_page.tabs)
self.assertGroupContent( groups[0], title=u"cool (2)", notes=[u"Third note", None] )
self.assertGroupContent( groups[1], title=u"review (2)", notes=[u"Fourth note", None] )
self.assertGroupContent( groups[3], title=u"[no tags] (2)", notes=["Fifth note", "First note"] )
self.notes_page.wait_for_ajax() note = self.notes_page.notes[0] assert_page(note, self.raw_note_list[4]['usage_id'], "Recent Activity")
self.notes_page.wait_for_ajax() note = self.notes_page.notes[1] assert_page(note, self.raw_note_list[2]['usage_id'], "Location in Course")
self.notes_page.wait_for_ajax() note = self.notes_page.notes[0] assert_page(note, self.raw_note_list[2]['usage_id'], "Tags")
self.notes_page.wait_for_ajax()
pear_group = self.notes_page.tag_groups[group_index] self.assertEqual(tag_name + " (3)", pear_group.title) self.assertTrue(pear_group.scrolled_to_top(group_index))
self.notes_page.go_to_page(2) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
self.notes_page.go_to_page(3) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
self.notes_page.go_to_page(2) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
self.notes_page.go_to_page(3) self._verify_pagination_info( notes_count_on_current_page=1, header_text='Showing 26-26 out of 26 total', previous_button_enabled=True, next_button_enabled=False, current_page_number=2, total_pages=2 )
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file) self.addCleanup(remove_file, self.TEST_INDEX_FILENAME)
self.course_outline.visit() subsection = self.course_outline.section_at(section_index).subsection_at(0) subsection.expand_subsection() subsection.add_unit()
self._studio_add_content(0)
self.assertFalse(self._search_for_content(self.SEARCH_STRING))
self._studio_publish_content(0)
self.assertTrue(self._search_for_content(self.SEARCH_STRING))
self._studio_add_content(1)
self.assertFalse(self._search_for_content(self.EDITED_SEARCH_STRING))
self._studio_publish_content(1)
self._studio_reindex()
self.assertFalse(self._search_for_content(self.EDITED_SEARCH_STRING))
self._studio_reindex()
self.assertTrue(self._search_for_content(self.EDITED_SEARCH_STRING))
return XBlockFixtureDesc( 'problem', self.problem_name, data=self.factory.build_xml(**self.factory_kwargs), metadata={'rerandomize': 'always'} )
self.assertEqual(self.problem_page.problem_name, self.problem_name)
self.answer_problem(correct=True) self.problem_page.click_check() self.wait_for_status('correct')
self.answer_problem(correct=False) self.problem_page.click_check() self.wait_for_status('incorrect')
self.problem_page.a11y_audit.config.set_scope( include=['div#seq_content'])
self.problem_page.a11y_audit.check_for_accessibility_errors()
first_addend = random.randint(-100, 100) second_addend = 10 - first_addend
if not correct: second_addend += random.randint(1, 10)
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False).visit()
with open(self.TEST_INDEX_FILENAME, "w+") as index_file: json.dump({}, index_file)
conditional_page = ConditionalPage(self.browser) conditional_page.fill_in_poll() self.courseware_page.visit() self.assertTrue(conditional_page.is_content_visible())
LmsAutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id).visit()
LogoutPage(self.browser).visit() StudioAutoAuthPage( self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=True ).visit()
self.course_outline_page.visit() self.course_outline_page.wait_for_page()
LogoutPage(self.browser).visit() LmsAutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id).visit()
self.courseware_page.visit() self.courseware_page.wait_for_page()
breadcrumbs = self._breadcrumb(num_units=num_units, modified_name=modified_name) breadcrumbs.reverse() self.assertEqual(bookmarked_breadcrumbs, breadcrumbs)
course_fix = CourseFixture( self.course_info['org'], self.course_info['number'], self.course_info['run'], self.course_info['display_name'] )
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False).visit()
disable_animations(annotation_component_page)
'transcript': 'http://video.google.com/timedtext?lang=en&v=3_yD_cEKoCk',
if all_options_selected and not has_option: all_options_selected = False return all_options_selected
EmptyPromise(options_selected, "Option is selected").fulfill()
continue
AutoAuthPage(self.browser, username=self.USERNAME, email=self.EMAIL, course_id=self.course_id, staff=False).visit()
self.assert_payload_contains_ids(load_video_event)
self.assert_field_type(load_video_event, 'time', datetime.datetime) del load_video_event['time']
sources, duration = self.video.sources[0], self.video.duration self.assert_bumper_payload_contains_ids(load_video_event, sources, duration)
self.assert_field_type(load_video_event, 'time', datetime.datetime) del load_video_event['time']
self.navigate_to_video()
self.navigate_to_video()
self.video.wait_for_state('pause')
self.navigate_to_video()
self.video.wait_for_state('pause')
self.navigate_to_video()
self.video.wait_for_state('pause')
if lang_code == 'zh_HANT': self.video.select_language(lang_code) unicode_text = lang_text.decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
self.addCleanup(YouTubeStubConfig.reset)
if not _contents_of_verticals: _contents_of_verticals = [[{'display_name': 'Video', 'metadata': self.metadata}]]
self.assertTrue(self.video.is_video_rendered('youtube'))
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
self.video.hide_closed_captions() self.video.wait_for_closed_captions_to_be_hidden() self.video.reload_page() self.video.wait_for_closed_captions_to_be_hidden()
self.assertIn('Welcome to edX.', self.video.captions_text)
self.video.click_player_button('fullscreen')
self.assertTrue(self.video.is_aligned(False))
self.navigate_to_video()
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.assertIn('Welcome to edX.', self.video.captions_text)
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', 'Welcome to edX.'))
self.assertTrue(self.video.select_language('zh'))
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.video.show_captions()
self.video.click_player_button('fullscreen')
self.assertTrue(self.video.is_aligned(True))
self.video.click_player_button('transcript_button')
self.assertTrue(self.video.is_aligned(False))
self.youtube_configuration['time_to_response'] = 0.4 self.metadata = self.metadata_for_mode('youtube_html5')
self.youtube_configuration['time_to_response'] = 2.0 self.metadata = self.metadata_for_mode('youtube_html5')
self.youtube_configuration.update({ 'youtube_api_blocked': True, })
self.assertEqual(len(self.video.q(css='video')), 1)
self.youtube_configuration.update({ 'time_to_response': 2.0, 'youtube_api_blocked': True, })
self.assertEqual(len(self.video.q(css='video')), 1)
self.youtube_configuration.update({ 'time_to_response': 2.0, 'youtube_api_blocked': True, })
self.assertTrue(self.video.is_button_shown('transcript_button')) self._verify_caption_text('Welcome to edX.')
self.navigate_to_video()
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', '00:00:00,260'))
self.assertTrue(self.video.select_transcript_format('txt'))
self.assertTrue(self.video.downloaded_transcript_contains_text('txt', 'Welcome to edX.'))
self.course_nav.go_to_vertical('Test Vertical-1')
self.assertTrue(self.video.downloaded_transcript_contains_text('txt', 'Equal transcripts'))
self.course_nav.go_to_vertical('Test Vertical-2')
self.assertFalse(self.video.is_menu_present('download_transcript'))
self.navigate_to_video()
self.navigate_to_video() self.video.show_closed_captions()
self.video.click_player_button('play') self.video.wait_for_position('0:03') self.video.click_player_button('pause')
self.navigate_to_video() execute_video_steps(tab1_video_names)
self.go_to_sequential_position(2) execute_video_steps(tab2_video_names)
self.go_to_sequential_position(1) execute_video_steps(tab1_video_names)
self.course_nav.go_to_vertical('Test Vertical-0') self.video.wait_for_video_player_render() self.video.speed = '2.0'
self.course_nav.go_to_vertical('Test Vertical-1') self.video.wait_for_video_player_render() self.video.speed = '0.50'
self.course_nav.go_to_vertical('Test Vertical-2') self.video.wait_for_video_player_render()
self.course_nav.go_to_vertical('Test Vertical-0')
self.assertEqual(self.video.speed, '2.0x')
self.video.reload_page()
self.course_nav.go_to_vertical('Test Vertical-0')
self.assertEqual(self.video.speed, '2.0x')
self.video.speed = '1.0'
self.course_nav.go_to_vertical('Test Vertical-1')
self.assertEqual(self.video.speed, '0.50x')
self.course_nav.go_to_vertical('Test Vertical-2')
self.video.verify_speed_changed('1.0x')
self.navigate_to_video()
self.video.wait_for_video_player_render() self.assertIn(self.video.state, ['playing', 'buffering', 'finished'])
self.courseware.go_to_sequential_position(2)
self.courseware.go_to_sequential_position(1) execute_video_steps(tab1_video_names)
self.assertTrue(self.video.is_video_rendered('youtube'))
self.assertFalse(self.video.is_autoplay_enabled)
self.assertTrue(self.video.is_error_message_shown)
correct_error_message_text = 'No playable video sources found.' self.assertIn(correct_error_message_text, self.video.error_message_text)
self.assertFalse(self.video.is_spinner_shown)
self.navigate_to_video()
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.assertIn('Welcome to edX.', self.video.captions_text)
self.assertTrue(self.video.downloaded_transcript_contains_text('srt', 'Welcome to edX.'))
self.assertTrue(self.video.select_language('zh'))
unicode_text = "好 各位同学".decode('utf-8')
unicode_text = "好 各位同学".decode('utf-8') self.assertTrue(self.video.downloaded_transcript_contains_text('srt', unicode_text))
self.navigate_to_video()
self.video.show_captions()
self.video.click_player_button('fullscreen')
self.assertTrue(self.video.is_aligned(True))
self.navigate_to_video()
self.video.show_captions()
self.assertIn("Welcome to edX.", self.video.captions_text)
self.navigate_to_video()
self.video.show_captions()
unicode_text = "好 各位同学".decode('utf-8') self.assertIn(unicode_text, self.video.captions_text)
self.navigate_to_video() self.video.show_captions()
self.video.a11y_audit.config.set_scope( include=["div.video"] ) self.video.a11y_audit.check_for_accessibility_errors()
self.unit_page = None
self.assertTrue(video_xblocks == 2)
self.outline.visit()
self.unit_page = self.outline.section('Test Section').subsection('Test Subsection').expand_subsection().unit( 'Test Unit').go_to()
self.unit_page.xblocks[1].open_advanced_tab()
self.unit_page.xblocks[1].open_basic_tab()
self.unit_page.xblocks[1].save_settings()
self._create_video()
self.edit_component(1) self.open_advanced_tab() self.video.set_field_value('YouTube ID', 'sampleid123') self.save_unit_settings()
self._navigate_to_course_unit_page() self.assertTrue(self.video.is_controls_visible())
self.assertFalse(self.video.is_error_message_shown)
self._create_course_unit(subtitles=True) self.edit_component() self.video.upload_transcript('english_single_transcript.srt')
self.outline.a11y_audit.config.set_scope( include=["div.video"] ) self.outline.a11y_audit.check_for_accessibility_errors()
for page in self.pages: page.visit()
ModeCreationPage( self.browser, self.course_id, mode_slug=u'verified', mode_display_name=u'Verified Certificate', min_price=10, suggested_prices='10,20' ).visit()
certificate.course_title = course_title_override
self.assertEqual(certificate.get_text('.action-primary'), "Create") certificate.click_create_certificate_button() self.assertIn(course_title_override, certificate.course_title) return certificate
certificate.click_edit_certificate_button() certificate.course_title = "Updated Course Title Override 2" self.assertEqual(certificate.get_text('.action-primary'), "Save") certificate.click_save_certificate_button()
certificate.click_delete_certificate_button() self.certificates_page.click_confirmation_prompt_primary_button()
self.certificates_page.visit() self.assertEqual(len(self.certificates_page.certificates), 0)
signatory = certificate.signatories[0] signatory.edit()
self.assertEqual(len(self.certificates_page.certificates), 1)
self.assertEqual(len(self.certificates_page.certificates), 1) course_number = self.certificates_page.get_course_number() self.assertEqual(self.course_info['number'], course_number)
self.assertEqual(len(self.certificates_page.certificates), 1)
self.advanced_settings_page.visit() self.advanced_settings_page.set_values(self.course_advanced_settings) self.advanced_settings_page.wait_for_ajax()
add_discussion(container, group_a_menu) container.duplicate(self.group_a_item_1_action_index)
drag(container, first_handle + 3, first_handle, 40) drag(container, first_handle + 2, first_handle, 40)
group_a_item_1_delete_index = 1 self.delete_and_verify(group_a_item_1_delete_index, expected_ordering)
if expected_labels != [self.VISIBILITY_LABEL_ALL]: expected_labels.append(self.VISIBILITY_LABEL_SPECIFIC) self.assertItemsEqual(expected_labels, [option.text for option in visibility_editor.selected_options])
visibility_editor = self.edit_component_visibility(component) for label in labels: visibility_editor.select_option(label, save=False) visibility_editor.save()
visibility_editor = self.edit_component_visibility(component) self.verify_selected_labels(visibility_editor, expected_labels) visibility_editor.save()
self._verify_components_visible(['problem']) self._verify_student_view_locked()
self._verify_components_visible(['discussion']) self._verify_student_view_visible(['discussion'])
unit = self.go_to_unit_page() test_block = unit.xblocks[1] title_on_unit_page = test_block.name container = test_block.go_to_container() self.assertEqual(container.name, title_on_unit_page)
lib_page = LibraryEditPage(self.browser, LibraryLocator(org, number)) lib_page.wait_for_page()
self.dashboard_page.visit() self.assertTrue(self.dashboard_page.has_library(name=name, org=org, number=number))
self.assertFalse(library_container.has_validation_not_configured_warning)
expected_text = "This component is out of date. The library has new content." library_block = self._get_library_xblock_wrapper(self.unit_page.xblocks[1])
#self.assertIn("3 matching components", library_block.author_content)
#self.assertIn("4 matching components", library_block.author_content)
self.assertFalse(library_container.has_validation_error) self.assertFalse(library_container.has_validation_warning)
self.assertFalse(library_container.has_validation_error) self.assertFalse(library_container.has_validation_warning)
self.assertFalse(library_container.has_validation_error) self.assertFalse(library_container.has_validation_warning)
self.library_fixture.create_xblock(self.library_fixture.library_location, XBlockFixtureDesc("html", "Html4"))
block.edit() block.reset_field_val("Display Name") block.save_settings() self.assertEqual(block.name, name_default)
upload_start_time = datetime.utcnow().replace(microsecond=0, second=0) self.import_page.upload_tarball(self.tarball_name) self.import_page.wait_for_upload()
upload_finish_time = datetime.utcnow().replace(microsecond=0, second=0)
course_outline_page = CourseOutlinePage( self.browser, self.course_org, self.course_number, self.course_run ) course_outline_page.visit() course_outline_page.wait_for_page()
self.dashboard_page.visit() self.assertTrue(self.dashboard_page.has_course( org=self.course_org, number=self.course_number, run=self.course_run ))
course_outline_page = CourseOutlinePage( self.browser, new_org, self.course_number, self.course_run ) course_outline_page.visit() course_outline_page.wait_for_page()
self.dashboard_page.visit() self.assertTrue(self.dashboard_page.has_course( org=new_org, number=self.course_number, run=self.course_run ))
outline_page = self.course_outline_page.visit() outline_page.q(css='.outline-item.outline-subsection.is-collapsed .ui-toggle-expansion').click() verify_ordering(self, outline_page, expected_ordering)
self.chap_1_handle = 0 self.chap_1_seq_1_handle = 1
self.seq_1_vert_1_handle = 2 self.seq_1_vert_2_handle = 3 self.chap_1_seq_2_handle = 4
course_outline_page.q(css='.outline-item.outline-subsection.is-collapsed .ui-toggle-expansion').first.click()
features = [
course_fixture.add_children(*[ self._build_fixture(self.UnitState(*state)) for state in itertools.product(*features) ])
self.assertTrue(subsection.release_date) self.assertFalse(subsection.due_date) self.assertFalse(subsection.policy)
modal.release_date = '3/12/1972' modal.release_time = '04:01' modal.due_date = '7/21/2014' modal.due_time = '23:39' modal.policy = 'Lab'
self.assertTrue(section.release_date) self.assertFalse(section.due_date) self.assertFalse(section.policy)
self.assertTrue(modal.has_release_date()) self.assertFalse(modal.has_due_date()) self.assertFalse(modal.has_policy())
self.assertEqual(modal.release_date, u'1/1/1970')
modal.release_date = '5/14/1969'
self.assertFalse(section.due_date) self.assertFalse(section.policy)
modal.policy = 'Lab' modal.save()
self.course_fixture.create_xblock( parent_vertical.locator, XBlockFixtureDesc(category='poll', display_name="", data=load_data_str('poll_markdown.xml')) ) self.course_outline_page.visit()
second_config.edit() second_config.name = "Updated Second Content Group" self.assertEqual(second_config.get_text('.action-primary'), "Save") second_config.save()
config.delete() self.assertEqual(len(self.group_configurations_page.content_groups), 0)
EmptyPromise( lambda: self.outline_page.is_browser_on_page(), "loaded page {!r}".format(self.outline_page), timeout=30 ).fulfill()
self.advanced_settings.visit() self.assertTrue(self.advanced_settings.is_browser_on_page())
course_display_name = self.advanced_settings.get('Course Display Name') self.advanced_settings.set('Course Display Name', 1) self.advanced_settings.wait_for_modal_load()
self.check_modal_shows_correct_contents(['Course Display Name']) self.advanced_settings.refresh_and_wait_for_load()
original_values_map = self.get_settings_fields_of_each_type() self.set_wrong_inputs_to_fields() self.advanced_settings.wait_for_modal_load()
self.check_modal_shows_correct_contents(self.type_fields) self.advanced_settings.refresh_and_wait_for_load()
original_values_map = self.get_settings_fields_of_each_type() self.set_wrong_inputs_to_fields()
self.advanced_settings.wait_for_modal_load()
self.advanced_settings.undo_changes_via_modal()
for key, val in original_values_map.iteritems(): self.assertEquals( self.advanced_settings.get(key), val, 'Undoing Should revert back to original value' )
self.assertFalse(self.advanced_settings.is_validation_modal_present())
self.assertTrue(self.advanced_settings.is_validation_modal_present())
error_item_names = self.advanced_settings.get_error_item_names() self.assertEqual(set(wrong_settings_list), set(error_item_names))
self.assertIn("Some Rights Reserved", self.lms_courseware.course_license)
self.settings_page.a11y_audit.config.set_rules({ "ignore": [
self.course_outline.a11y_audit.config.set_scope( include=['section.edit-settings-timed-examination'] ) self.course_outline.a11y_audit.check_for_accessibility_errors()
self.settings_page.visit()
self.settings_page.wait_for_ajax() self.settings_page.wait_for_jquery_value('input#course-name:text', 'test_run')
file_to_upload = 'image.jpg' self.settings_page.upload_image('#upload-course-image', file_to_upload) self.assertIn(file_to_upload, self.settings_page.get_uploaded_image_path('#course-image'))
file_to_upload = 'image.jpg' self.settings_page.upload_image('#upload-banner-image', file_to_upload) self.assertIn(file_to_upload, self.settings_page.get_uploaded_image_path('#banner-image'))
file_to_upload = 'image.jpg' self.settings_page.upload_image('#upload-video-thumbnail-image', file_to_upload) self.assertIn(file_to_upload, self.settings_page.get_uploaded_image_path('#video-thumbnail-image'))
return (len(active_groups) + len(inactive_groups) == len(container.xblocks) - 1, len(active_groups))
check_xblock_names(active_groups + inactive_groups, container.xblocks[1:]) if verify_missing_groups_not_present: self.verify_add_missing_groups_button_not_present(container)
wait_for_xblock_initialization(self, '.xblock[data-block-type="split_test"]')
container.add_missing_groups() self.verify_groups(container, ['alpha', 'gamma'], ['beta'])
container = self.go_to_nested_container_page() self.verify_groups(container, ['alpha', 'gamma'], ['beta'])
container.delete(0) self.verify_groups(container, ['alpha'], [], verify_missing_groups_not_present=False)
self.assertTrue(config.id)
config.toggle()
config.toggle()
self.page.visit() config = self.page.experiment_group_configurations[0]
if publish: unit.publish_action.click() unit.view_published_version() self.assertEqual(len(self.browser.window_handles), 2) courseware_page.wait_for_page()
self.assertEqual(config.get_text('.action-primary'), "Create") self.assertFalse(config.delete_button_is_present) config.save()
config.groups[1].remove() config.groups[0].name = "First Group" config.save()
split_test = self._add_split_test_to_vertical(number=0, group_configuration_metadata={'user_partition_id': 0})
config.groups[2].name = "Second Group"
config.groups[0].remove() config.save()
container.add_missing_groups() self.verify_groups(container, ['Group B', 'Second Group', 'Group D'], ['Group ID 0'])
self.page.create_experiment_group_configuration()
config.cancel()
config.cancel()
config.save() self.assertEqual(config.mode, 'edit') self.assertEqual(message, config.validation_message)
self.page.create_experiment_group_configuration() config = self.page.experiment_group_configurations[0] config.description = "Description of the group configuration."
config.save()
self.page.visit() config = self.page.experiment_group_configurations[0] config.toggle() config.click_outline_anchor()
EmptyPromise( lambda: self.outline_page.is_browser_on_page(), "loaded page {!r}".format(self.outline_page), timeout=30 ).fulfill()
self.page.visit() config = self.page.experiment_group_configurations[0] config.toggle() usage = config.usages[0] config.click_unit_anchor()
EmptyPromise( lambda: unit.is_browser_on_page(), "loaded page {!r}".format(unit), timeout=30 ).fulfill()
config.delete() self.assertEqual(len(self.page.experiment_group_configurations), 1)
config.delete() self.assertEqual(len(self.page.experiment_group_configurations), 0)
self.assertFalse(self.page.experiment_group_configurations[0].is_expanded) self.assertTrue(self.page.experiment_group_configurations[1].is_expanded)
config, _ = self.create_group_configuration_experiment([Group("0", "Group A"), Group("1", "Group B")], True)
config.toggle() self.assertFalse(config.details_error_icon_is_present) self.assertFalse(config.details_message_is_present)
config.toggle() config.edit() config.add_group() config.save()
config.toggle() self.assertFalse(config.details_warning_icon_is_present) self.assertFalse(config.details_message_is_present)
config.toggle() config.edit() config.groups[2].remove() config.save()
courseware_page = CoursewarePage(self.browser, self.course_id) self.publish_unit_and_verify_groups_in_lms(courseware_page, [u'Group A', u'Group B', u'Group C'])
self.publish_unit_and_verify_groups_in_lms( courseware_page, [u'Group A', u'Group B', u'Group ID 2 (inactive)'], publish=False )
container.visit() container.delete(0)
self.publish_unit_and_verify_groups_in_lms(courseware_page, [u'Group A', u'Group B'])
add_component(self.lib_page, "html", "Text") self.assertEqual(len(self.lib_page.xblocks), 1) first_block_id = self.lib_page.xblocks[0].locator
self.lib_page.click_delete_button(first_block_id, confirm=True) self.assertEqual(len(self.lib_page.xblocks), 1) self.assertEqual(self.lib_page.xblocks[0].locator, second_block_id)
self.assertEqual(len(self.lib_page.xblocks), 1) problem_block = self.lib_page.xblocks[0] self.assertIn("Laura Roslin", problem_block.author_content)
AutoAuthPage(self.browser, username="second", email="second@example.com", no_login=True).visit()
lib_page.a11y_audit.config.set_rules({ "ignore": [
super(ContainerBase, self).setUp(is_staff=is_staff)
container = unit.xblocks[1].go_to_container() return container
container = self.go_to_nested_container_page() verify_ordering(self, container, expected_ordering)
self.settings_detail.visit() self.assertTrue(self.settings_detail.is_browser_on_page())
self.settings_detail.refresh_page() self.settings_detail.wait_for_prerequisite_course_options() self.assertTrue(is_option_value_selected( browser_query=self.settings_detail.pre_requisite_course_options, value=pre_requisite_course_id ))
self.settings_detail.refresh_page() self.settings_detail.wait_for_prerequisite_course_options() self.assertTrue(is_option_value_selected( browser_query=self.settings_detail.pre_requisite_course_options, value='' ))
select_option_by_value( browser_query=self.settings_detail.pre_requisite_course_options, value=pre_requisite_course_id ) self.settings_detail.save_changes() self.assertEqual( 'Your changes have been saved.', self.settings_detail.alert_confirmation_title.text )
self.settings_detail.refresh_page() self.settings_detail.wait_for_prerequisite_course_options() dropdown_status = is_option_value_selected( browser_query=self.settings_detail.pre_requisite_course_options, value=pre_requisite_course_id ) self.assertTrue(dropdown_status)
course_outline_page = CourseOutlinePage( self.browser, self.course_info['org'], self.course_info['number'], self.course_info['run'] ) course_outline_page.visit()
self.assertTrue(element_has_text( page=course_outline_page, css_selector='span.section-title', text='Entrance Exam' ))
self.settings_detail.visit() self.settings_detail.require_entrance_exam(required=False) self.settings_detail.save_changes()
self.assertTrue(element_has_text( page=course_outline_page, css_selector='.add-item a.button-new', text='New Unit' ))
self.assertFalse(element_has_text( page=course_outline_page, css_selector='.add-item a.button-new', text='New Subsection' ))
self.course_fixture.add_course_details({'start_date': datetime.now() + timedelta(days=1)})
super(XBlockAcidBase, self).setUp()
_ = lambda text: text
if (('python2.7/site-packages/gunicorn/workers/sync.py' in exc_str) and ('[Errno 11] Resource temporarily unavailable' in exc_str)): exc_str = ''
from crum import get_current_request
Score = namedtuple("Score", "earned possible graded section module_id")
NOT_CONFIGURED = "not-configured"
class_priority = ['video', 'problem']
DEPRECATION_VSCOMPAT_EVENT = 'deprecation.vscompat'
STUDENT_VIEW = 'student_view'
AUTHOR_VIEW = 'author_view'
STUDIO_VIEW = 'studio_view'
PREVIEW_VIEWS = [STUDENT_VIEW, AUTHOR_VIEW]
coffee = cls.js.setdefault('coffee', []) js = cls.js.setdefault('js', [])
cls.js.setdefault('xmodule_js', resource_string(__name__, 'js/src/xmodule.js'))
default=None
has_score = False
show_in_read_only_mode = False
self.save()
if usage_id_filter is None and usage_key_filter is not None: usage_id_filter = usage_key_filter
if self.scope_ids.user_id is not None and user_id == self.scope_ids.user_id: if getattr(xmodule_runtime, 'position', None):
self.save()
self.scope_ids = self.scope_ids._replace(user_id=user_id)
self.clear_child_cache()
for field in self.fields.values(): if field.scope in (Scope.parent, Scope.children): continue
if field in self._dirty_fields: del self._dirty_fields[field]
self.xmodule_runtime = xmodule_runtime
return [XBlock.tags, XBlock.name]
fields = getattr(self, 'unmixed_class', self.__class__).fields
self.descriptor = descriptor self._runtime = None super(XModule, self).__init__(*args, **kwargs) self.runtime.xmodule_instance = self
child_descriptor = self.descriptor.get_child(usage_id) child_block = None if child_descriptor is not None: child_block = self.system.get_module(child_descriptor)
metadata_translations = { 'slug': 'url_name', 'name': 'display_name', }
self.previous_version = self.update_version = self.definition_locator = None self.xmodule_runtime = None
xml = etree.tostring(node) block = cls.from_xml(xml, runtime, id_generator) return block
self.xmodule_runtime.xmodule_instance = None
self.system.error_tracker(msg) return 'Oops, couldn't load grommet'
self.export_fs = None
return descriptor_global_local_resource_url(block, uri)
pass
service = super(DescriptorSystem, self).service(block=block, service_name=service_name) if callable(service): return service(block) return service
node.attrib.pop('xblock-family', None)
xblock_family = child.attrib.pop('xblock-family', None) if xblock_family: xblock_family = self._family_id_to_superclass(xblock_family) if issubclass(xblock_family, XBlockAside): aside_children.append(child)
kwargs.pop('_view_name')
service = super(ModuleSystem, self).service(block=block, service_name=service_name) if callable(service): return service(block) return service
try: return getattr(self._module_system, name) except AttributeError: return getattr(self._descriptor_system, name)
return u''.join(filter(None, parts))
(n, d) = a.frac() (n2, d2) = b.frac() return Progress(n + n2, d + d2)
return course.display_name_with_default.replace('<', '&lt;').replace('>', '&gt;')
return _('TBD')
announcement, start, now = sorting_dates(start, advertised_start, announcement)
if self.transcript_language == 'en': return Transcript.asset(self.location, youtube_id).data
if self.transcript_language == 'en':
if not self.transcript_language == 'en': return response
if not isinstance(self.course_id, CourseLocator): return response
return self.get_static_transcript(request, transcripts)
_ = lambda text: text
module = __name__.replace('.video_module', '', 2)
sorted_languages = sorted(languages.items(), key=itemgetter(1))
cdn_url = getattr(settings, 'VIDEO_CDN_URL', {}).get(self.system.user_location)
if self.edx_video_id and edxval_api: try: val_profiles = ["youtube", "desktop_webm", "desktop_mp4"] val_video_urls = edxval_api.get_urls_for_profiles(self.edx_video_id, val_profiles)
if val_video_urls["youtube"]: youtube_streams = "1.00:{}".format(val_video_urls["youtube"])
if getattr(self, 'video_speed_optimizations', True) and cdn_url: branding_info = BrandingInfoConfig.get_config().get(self.system.user_location)
if not download_video_link and self.download_video: if self.source: download_video_link = self.source elif self.html5_sources: download_video_link = self.html5_sources[0]
'captionDataDir': getattr(self, 'data_dir', None),
'recordedYoutubeIsAvailable': self.youtube_is_available,
if self.data: field_data = self._parse_video_xml(etree.fromstring(self.data)) self._field_data.set_many(self, field_data) del self.data
if self.source in self.html5_sources:
if not self.fields['download_video'].is_set_on(self): self.download_video = self.download_video self.force_save_fields(['download_video'])
if not self.fields['download_track'].is_set_on(self) and self.track: self.download_track = True
ScopeIds(None, block_type, definition_id, usage_id), field_data,
self.add_license_to_xml(xml)
if self.edx_video_id and edxval_api: val_youtube_id = edxval_api.get_url_for_profile(self.edx_video_id, "youtube") if val_youtube_id: video_id = val_youtube_id
youtube_id = deserialize_field(cls.youtube_id_1_0, pieces[1]) ret[speed] = youtube_id
conversions = { }
field_data[attr] = deserialize_field(cls.fields[attr], value)
if 'download_video' not in field_data and sources: field_data['source'] = field_data['html5_sources'][0]
if 'download_track' not in field_data and track is not None: field_data['download_track'] = True
edxval_api.import_from_xml( video_asset_elem, field_data['edx_video_id'], course_id=course_id )
field_data = LicenseMixin.parse_license_from_xml(field_data, xml)
if self.transcripts: for language in self.transcripts.keys(): _update_transcript_for_index(language)
if self.only_on_web: return {"only_on_web": True}
if self.edx_video_id: video_profile_names = context.get("profiles", ["mobile_low"])
val_course_data = self.get_cached_val_data_for_course(video_profile_names, self.location.course_key) val_video_data = val_course_data.get(self.edx_video_id, {})
if val_video_data: encoded_videos = val_video_data.get('profiles', {})
if not encoded_videos: video_url = self.html5_sources[0] if self.html5_sources else self.source if video_url: encoded_videos["fallback"] = { "url": video_url,
if self.youtube_id_1_0: encoded_videos["youtube"] = { "url": self.create_youtube_url(self.youtube_id_1_0),
_ = lambda text: text
source = String( help=_("The external URL to download the video."), display_name=_("Download Video"), scope=Scope.settings, default="" ) download_video = Boolean(
transcripts = Dict(
(it is done to allow user to enter both /static/filename.srt and filename.srt)
if generate_translation: for lang, filename in item.transcripts.items(): item.transcripts[lang] = os.path.split(filename)[-1]
if generate_translation: old_langs = set(old_metadata.get('transcripts', {})) if old_metadata else set() new_langs = set(item.transcripts)
item.transcripts.pop(lang) reraised_message += ' ' + ex.message
generate_subs_from_source( result_subs_dict, os.path.splitext(user_filename)[1][1:], srt_transcripts.data.decode('utf-8-sig'), item, lang )
if not verify_assets: if other_langs: translations = list(other_langs) if not translations or sub: translations += ['en'] return translations
for lang, transcript_url in bumper_settings.get('transcripts', {}).items(): bumper_settings['transcripts'][lang] = transcript_url.replace("/static/", "")
log.warning( "Could not retrieve information from VAL for Bumper edx Video ID: %s.", video.bumper['edx_video_id'] ) return []
rewritten_url = cdn_base_url.rstrip("/") + "/" + parsed.path.lstrip("/") validator = URLValidator()
return None
if user is not None and password is not None: mongo_conn.authenticate(user, password)
_ = lambda text: text
type = ''
title = None
is_hideable = False
is_hidden = False
priority = None
is_movable = True
is_collection = False
is_dynamic = False
is_default = True
allow_multiple = False
view_name = None
return False
name_is_eq = (other.get('name') is None or self.name == other['name'])
return self.type == other.get('type') and name_is_eq
if hasattr(course, 'syllabus_present') and course.syllabus_present: course.tabs.append(CourseTab.load('syllabus'))
if course.discussion_link: discussion_tab = CourseTab.load( 'external_discussion', name=_('External Discussion'), link=course.discussion_link ) else: discussion_tab = CourseTab.load('discussion')
if course.discussion_link: return CourseTab.load( 'external_discussion', name=_('External Discussion'), link=course.discussion_link )
for tab in course.tabs: if tab.type == 'discussion' or tab.type == 'external_discussion': return tab return None
if inline_collections: for item in tab.items(course): yield item elif len(list(tab.items(course))) > 0: yield tab
_ = lambda text: text
default=_("Text")
if self.system.anonymous_student_id: return self.data.replace("%%USER_ID%%", self.system.anonymous_student_id) return self.data
new_candidates = [] for candidate in candidates: if candidate.endswith('.xml'): new_candidates.append(candidate[:-4] + '.html') return candidates + new_candidates
_context.update({ 'base_asset_url': StaticContent.get_base_url_path_for_course_assets(self.location.course_key), 'enable_latex_compiler': self.use_latex_compiler, 'editor': self.editor }) return _context
for candidate in candidates: if system.resources_fs.exists(candidate): filepath = candidate break
raise Exception(msg), None, sys.exc_info()[2]
pathname = name_to_pathname(self.url_name) filepath = u'{category}/{pathname}.html'.format( category=self.category, pathname=pathname )
relname = path(pathname).basename()
STATUS_VISIBLE = 'visible' STATUS_DELETED = 'deleted' TEMPLATE_DIR = 'courseware'
dog_stats_api.increment( DEPRECATION_VSCOMPAT_EVENT, tags=["location:customtag_descriptor_render_template"] )
template_loc = self.location.replace(category='custom_tag_template', name=template_name)
VERSION = 1
USER_PARTITION_SCHEME_NAMESPACE = 'openedx.user_partition_scheme'
scheme_extensions = None
VERSION_1_SCHEME = "random"
scheme_id = UserPartition.VERSION_1_SCHEME
elif value["version"] >= 2: if "scheme" not in value: raise TypeError("UserPartition dict {0} missing value key 'scheme'".format(value))
self.addCleanup(self.cleanup_scheme_extensions)
self.user_partition = UserPartition( self.TEST_ID, self.TEST_NAME, self.TEST_DESCRIPTION, self.TEST_GROUPS, extensions[0].plugin, self.TEST_PARAMETERS, )
self.user_partition.get_scheme(self.non_random_scheme.name) self.user_partition.get_scheme(self.random_scheme.name)
user_id = abs(hash(username))
user_partition_id = self.user_partition.id groups = self.user_partition.groups self.user_partition.scheme.current_group = groups[0]
group1_id = self.partition_service.get_user_group_id_for_partition(user_partition_id) self.assertEqual(group1_id, groups[0].id)
self.user_partition.scheme.current_group = groups[1] group2_id = self.partition_service.get_user_group_id_for_partition(user_partition_id) self.assertEqual(group2_id, groups[1].id)
ps_shared_cache_1 = self._create_service(username, shared_cache) ps_shared_cache_2 = self._create_service(username, shared_cache)
ps_diff_cache = self._create_service(username, {})
ps_uncached = self._create_service(username)
first_group = self.user_partition.groups[0] self.user_partition.scheme.current_group = first_group
for part_svc in [ps_shared_cache_1, ps_diff_cache, ps_uncached]: self.assertEqual( first_group.id, part_svc.get_user_group_id_for_partition(user_partition_id) )
second_group = self.user_partition.groups[1] self.user_partition.scheme.current_group = second_group
for part_svc in [ps_shared_cache_1, ps_shared_cache_2, ps_diff_cache]: self.assertEqual( first_group.id, part_svc.get_user_group_id_for_partition(user_partition_id) )
self.assertEqual( second_group.id, ps_uncached.get_user_group_id_for_partition(user_partition_id) )
ps_new_cache = self._create_service(username, {}) self.assertEqual( second_group.id, ps_new_cache.get_user_group_id_for_partition(user_partition_id) )
self.user_partition.scheme.current_group = groups[0] group1 = self.partition_service.get_group(self.user_partition) self.assertEqual(group1, groups[0])
self.user_partition.scheme.current_group = groups[1] group2 = self.partition_service.get_group(self.user_partition) self.assertEqual(group2, groups[1])
_ = lambda text: text
user_partition_values = [] no_partition_selected = {'display_name': _("Not Selected"), 'value': -1}
user_partitions = UserPartitionList( help=_("The list of group configurations for partitioning students in content experiments."), default=[], scope=Scope.settings )
group_id_to_child = ReferenceValueDict( help=_("Which child module students in a particular group_id should see"), scope=Scope.content )
log.debug("configuration error in split test module: no such child") return []
sorted_active_contents = sorted(active_contents, key=itemgetter('group_name')) sorted_inactive_contents = sorted(inactive_contents, key=itemgetter('group_name'))
return Fragment(content=u"<div>Nothing here. Move along.</div>")
module_class = SplitTestModule
if 'user_partition_id' not in old_content or old_content['user_partition_id'] != self.user_partition_id: selected_partition = self.get_selected_partition() if selected_partition is not None:
else: for str_group_id, usage_key in self.group_id_to_child.items():
SplitTestFields.build_partition_values(self.user_partitions, self.get_selected_partition())
editable_fields[SplitTestFields.user_partition_id.name] = self._create_metadata_editor_info( SplitTestFields.user_partition_id )
inactive_children = [child for child in children if child not in active_children]
self.system.modulestore.update_item(self, None)
_ = lambda text: text
last_el = self.table_of_contents[-1] while last_el.getchildren(): last_el = last_el[-1]
log.exception("Couldn't load textbook ({0}, {1})".format(title, book_url)) continue
default="images_course_image.jpg"
default="images_course_image.jpg"
default="images_course_image.jpg"
display_name=_("Certificate Web/HTML View Overrides"), help=_("Enter course-specific overrides for the Web/HTML template parameters here (JSON format)"), scope=Scope.settings,
certificates = Dict( display_name=_("Certificate Configuration"), help=_("Enter course-specific configuration information here (JSON format)"), scope=Scope.settings, )
if self.system.resources_fs is None: self.syllabus_present = False else: self.syllabus_present = self.system.resources_fs.exists(path('syllabus'))
grading_policy.update(course_policy)
policy_str = '{}'
break
course_file = StringIO(xml_data.encode('ascii', 'ignore')) xml_obj = etree.parse(course_file, parser=edx_xml_parser).getroot()
paths = ['grading_policy.json'] if policy_dir: paths = [policy_dir + '/grading_policy.json'] + paths
instance.set_grading_policy(policy)
wiki_slug = None wiki_tag = xml_object.find("wiki") if wiki_tag is not None: wiki_slug = wiki_tag.attrib.get("slug", default=None) xml_object.remove(wiki_tag)
definition = LicenseMixin.parse_license_from_xml(definition, xml_object)
self.add_license_to_xml(xml_object, default="all-rights-reserved")
self.grading_policy['GRADER'] return self._grading_policy['RAW_GRADER']
self._grading_policy['RAW_GRADER'] = value self.grading_policy['GRADER'] = value
policy = self.grading_policy policy['GRADE_CUTOFFS'] = value self.grading_policy = policy
try: module = getattr(self, '_xmodule', None) if not module: module = self except UndefinedContext: module = self
section_description = { 'section_descriptor': section, 'xmoduledescriptors': [child for child in xmoduledescriptors if child.has_score] }
CLASS_PRIORITY = ['video', 'problem']
for child in self.get_display_items(): rendered_child = child.render(STUDENT_VIEW, child_context) fragment.add_frag_resources(rendered_child)
_ = lambda text: text
_ = lambda text: text
valid_block_keys = set([(c.block_type, c.block_id) for c in children]) invalid_block_keys = (selected - valid_block_keys) if invalid_block_keys: selected -= invalid_block_keys
overlimit_block_keys = set() while len(selected) > max_count: overlimit_block_keys.add(selected.pop())
num_to_add = max_count - len(selected)
publish_event( "removed", result=format_block_keys(block_keys['selected']), removed=format_block_keys(block_keys['invalid']), reason="invalid" )
selected = block_keys['selected']
fragment.add_javascript_url(self.runtime.local_resource_url(self, 'public/js/library_content_edit.js')) fragment.initialize_js('LibraryContentAuthorView') return fragment
non_editable_fields.extend([LibraryContentFields.mode, LibraryContentFields.source_library_version]) return non_editable_fields
user_id = user_service.get_current_user().opt_attrs.get('edx-platform.user_id', None)
return True
_ = lambda text: text
display_name = String(help="Display name for this module", scope=Scope.settings)
answers = List(help="Poll answers from xml", scope=Scope.content, default=[])
temp_poll_answers = self.poll_answers temp_poll_answers[dispatch] += 1 self.poll_answers = temp_poll_answers
temp_poll_answers = self.poll_answers temp_poll_answers[self.poll_answer] -= 1 self.poll_answers = temp_poll_answers
if self.poll_answers is None: self.poll_answers = {}
temp_poll_answers = self.poll_answers
if len(xml_object.xpath(cls._child_tag_name)) == 0: raise ValueError("Poll_question definition must include \ at least one 'answer' tag")
_ = lambda text: text
metadata_translations = dict(RawDescriptor.metadata_translations) metadata_translations['id'] = 'discussion_id' metadata_translations['for'] = 'discussion_target'
non_editable_fields.extend([DiscussionDescriptor.discussion_id, DiscussionDescriptor.sort_key]) return non_editable_fields
_ = lambda text: text
for field in InheritanceMixin.fields.values(): if field.is_set_on(descriptor): parent_metadata[field.name] = field.read_json(descriptor)
self._fields[key.field_name] = value
EXCLUDE_ALL = '*'
self.modules = defaultdict(dict) self.definitions = {} self.definitions_in_db = set() self.course_key = None
if self.index is None: return []
if self.initial_index is None: return self.index.get('versions', {}).keys()
if course_key is None: return self._bulk_ops_record_type()
if course_key.org is None or course_key.course is None or course_key.run is None: return self._active_bulk_ops.records[ course_key.replace(org=None, course=None, run=None, branch=None) ]
return super(SplitBulkWriteMixin, self)._get_bulk_ops_record( course_key.replace(branch=None, version_guid=None), ignore_case )
bulk_write_record.index = copy.deepcopy(bulk_write_record.initial_index) bulk_write_record.course_key = course_key
for _id in bulk_write_record.structures.viewkeys() - bulk_write_record.structures_in_db: dirty = True
if structure is None: structure = self.db_connection.get_structure(version_guid, course_key) bulk_write_record.structures[version_guid] = structure if structure is not None: bulk_write_record.structures_in_db.add(version_guid)
version_guid = course_key.as_object_id(version_guid) return self.db_connection.get_structure(version_guid, course_key)
if definition is None: definition = self.db_connection.get_definition(definition_guid, course_key) bulk_write_record.definitions[definition_guid] = definition if definition is not None: bulk_write_record.definitions_in_db.add(definition_guid)
definition_guid = course_key.as_object_id(definition_guid) return self.db_connection.get_definition(definition_guid, course_key)
for definition in bulk_write_record.definitions.values(): definition_id = definition.get('_id') if definition_id in ids: ids.remove(definition_id) definitions.append(definition)
if bulk_write_record.active and course_key.branch in bulk_write_record.dirty_branches: return bulk_write_record.structure_for_branch(course_key.branch)
if bulk_write_record.active: bulk_write_record.set_structure_for_branch(course_key.branch, new_structure)
for _, record in self._active_records: if branch and branch not in record.index.get('versions', {}): continue
if org_target: if record.index['org'] != org_target: continue
super(SplitMongoModuleStore, self)._drop_database(database, collections, connections)
if not lazy: descendent_definitions = self.get_definitions( course_key, [ block.definition for block in new_module_data.itervalues() ] ) definitions = {definition['_id']: definition for definition in descendent_definitions}
block.fields.update(definition.get('fields')) block.definition_loaded = True
index = self.get_course_index(course_key)
raise VersionConflictError(course_key, version_guid)
version_guids = [] id_version_map = defaultdict(list) for course_index in matching_indexes: version_guid = course_index['versions'][branch] version_guids.append(version_guid) id_version_map[version_guid].append(course_index) return version_guids, id_version_map
return self._get_structures_for_branch_and_locator(branch, self._create_course_locator, **kwargs)
raise ItemNotFoundError(course_id)
raise ItemNotFoundError(library_id)
return False
return False
return False
raise ItemNotFoundError(usage_key)
return []
if 'children' in qualifiers: settings['children'] = qualifiers.pop('children')
path_cache = None parents_cache = None
if path_cache is not None: path_cache[block_key] = True
raise ItemNotFoundError(locator)
parent_ids = [ valid_parent for valid_parent in all_parent_ids if self.has_path_to_root(valid_parent, course) ]
parent_ids.sort(key=lambda parent: (parent.type, parent.id)) return BlockUsageLocator.make_relative( locator, block_type=parent_ids[0].type, block_id=parent_ids[0].id, )
raise ItemNotFoundError(course_key)
raise ItemNotFoundError(course_key)
raise ItemNotFoundError(course_key)
raise ItemNotFoundError(definition_locator)
raise ItemNotFoundError(course_locator)
old_definition = self.get_definition(course_key, definition_locator.definition_id) if old_definition is None: raise ItemNotFoundError(definition_locator)
index_entry = self._get_index_if_valid(course_key, force) structure = self._lookup_course(course_key).structure
new_structure = self.version_structure(course_key, structure, user_id)
return self.get_item(item_loc)
new_structure = self._lookup_course(xblock.location.course_key).structure
block_id = BlockKey.from_usage_key(parent_usage_key) if block_id not in new_structure['blocks']: raise ItemNotFoundError(parent_usage_key)
self.version_block(parent, user_id, new_structure['_id'])
self.update_structure(parent_usage_key.course_key, new_structure)
return xblock
super(SplitMongoModuleStore, self).clone_course(source_course_id, dest_course_id, user_id, fields, **kwargs) return new_course
if versions_dict is None or master_branch not in versions_dict: definition_id = self.create_definition_from_data(locator, definition_fields, root_category, user_id).definition_id
settings = partitioned_fields[Scope.settings] settings = self._serialize_fields(block_key.type, settings) if not is_updated: is_updated = self._compare_settings(settings, original_entry.fields)
if is_updated or asides_updated: new_structure = self.version_structure(course_key, original_structure, user_id) block_data = self._get_block_from_structure(new_structure, block_key)
block_data.edit_info.source_version = None
new_locator = course_key.make_usage_key(block_key.type, block_key.id) return self.get_item(new_locator, **kwargs)
inherited_settings = {}
parent_xblock.save()
if index_entry is not None: self._update_head(course_key, index_entry, xblock.location.branch, new_id)
return self.get_item(xblock.location.for_version(new_id))
with self.bulk_operations(source_course): source_structure = self._lookup_course(source_course).structure
raise ItemNotFoundError(destination_course)
self.update_structure(destination_course, destination_structure) self._update_head(destination_course, index_entry, destination_course.branch, destination_structure['_id'])
dest_info = dest_structure['blocks'][block_key]
dest_info.edit_info.previous_version = dest_info.edit_info.update_version dest_info.edit_info.update_version = old_dest_structure_version dest_info.edit_info.edited_by = user_id dest_info.edit_info.edited_on = datetime.datetime.now(UTC)
return [ destination_course.make_usage_key(*k) for k in dest_structure['blocks'][block_key].fields['children'] ]
new_block_info = copy.deepcopy(source_block_info) existing_block_info = dest_structure['blocks'].get(new_block_key, BlockData()) new_block_info.defaults = new_block_info.fields
new_children.append(new_block_key)
dest_structure['blocks'][new_parent_block_key].fields['children'] = new_children
raise ItemNotFoundError(usage_locator)
parent_block.edit_info.source_version = None self.decache_block(usage_locator.course_key, new_id, parent_block_key)
self.update_structure(usage_locator.course_key, new_structure)
self._update_head(usage_locator.course_key, index_entry, usage_locator.branch, new_id) result = usage_locator.course_key.for_version(new_id)
if parents.issubset(to_delete): next_tier.add(child_block_key)
log.info(u"deleting course from split-mongo: %s", course_key) self.delete_course_index(course_key)
inherited_settings_map.setdefault(block_key, {}).update(inheriting_settings)
inheriting_settings = inherited_settings_map[block_key].copy() block_fields = block_data.fields for field_name in inheritance.InheritanceMixin.fields: if field_name in block_fields: inheriting_settings[field_name] = block_fields[field_name]
pass
all_assets.extend(course_assets.setdefault(asset_type, [])) asset_idx = all_assets.find(asset_key)
self.update_structure(asset_key.course_key, new_structure)
self._update_head(asset_key.course_key, index_entry, asset_key.branch, new_structure['_id'])
asset_key = asset_metadata_list[0].asset_id course_key = asset_key.course_key
self.update_structure(course_key, new_structure)
self._update_head(course_key, index_entry, asset_key.branch, new_structure['_id'])
mdata = AssetMetadata(asset_key, asset_key.path) mdata.from_storable(all_assets[asset_idx]) mdata.update(attr_dict)
all_assets[asset_idx] = mdata.to_storable() return all_assets
self.update_structure(dest_course_key, new_structure)
self._update_head(dest_course_key, index_entry, dest_course_key.branch, new_structure['_id'])
self._update_head(course_locator, index_entry, course_locator.branch, new_structure['_id'])
if isinstance(block_key, BlockUsageLocator): return block_key.map_into_course(course_key) elif not isinstance(block_key, BlockKey): block_key = BlockKey(*block_key)
result = defaultdict(dict) for field in xblock.fields.itervalues(): if field.is_set_on(xblock): result[field.scope][field.name] = field.read_from(xblock) return result
if isinstance(reference, basestring): reference = BlockUsageLocator.from_string(reference) elif isinstance(reference, BlockKey): return reference return BlockKey.from_usage_key(reference)
for key, val in new_block.edit_info.to_storable().iteritems(): if getattr(destination_block.edit_info, key) is None: setattr(destination_block.edit_info, key, val)
destination_block.edit_info.source_version = ( new_block.edit_info.source_version or new_block.edit_info.update_version )
self._auto_publish_no_children(item.location, item.location.category, user_id, **kwargs)
self.publish(location.version_agnostic(), user_id, blacklist=EXCLUDE_ALL, **kwargs)
self._auto_publish_no_children(item.location, item.location.category, user_id, **kwargs) self._auto_publish_no_children(parent_usage_key, item.location.category, user_id, **kwargs) return item
draft_branch = ModuleStoreEnum.BranchName.library published_branch = ModuleStoreEnum.BranchName.library
if self._get_version(draft_block) != self._get_version(published_block): return True
if 'children' in draft_block.fields: return any( [has_changes_subtree(child_block_id) for child_block_id in draft_block.fields['children']] )
draft_course_structure = self._lookup_course(draft_course_key).structure new_structure = self.version_structure(draft_course_key, draft_course_structure, user_id)
self._remove_subtree(BlockKey.from_usage_key(location), new_structure['blocks'])
pass
return None
if block_type == 'course': block_id = self.DEFAULT_ROOT_COURSE_BLOCK_ID elif block_type == 'library': block_id = self.DEFAULT_ROOT_LIBRARY_BLOCK_ID new_usage_key = course_key.make_usage_key(block_type, block_id)
partitioned_fields = self.partition_fields_by_scope(block_type, fields)
xblock._published_by = published_block.edit_info.edited_by xblock._published_on = published_block.edit_info.edited_on
tagger.sample_rate = 1 return None
compressed_pickled_data = zlib.compress(pickled_data, 1) tagger.measure('compressed_size', len(compressed_pickled_data))
self.cache.set(key, compressed_pickled_data, None)
kwargs['w'] = 1
tagger_get_structure.sample_rate = 1
if 'last_update' in from_index: query['last_update'] = from_index['last_update']
self.course_id = course_entry.course_key self.lazy = lazy self.module_data = module_data self.default_class = default_class self.local_modules = {} self._services['library_tools'] = LibraryToolsService(modulestore)
if isinstance(usage_key, BlockUsageLocator):
course_key = usage_key.course_key
cached_module = self.modulestore.get_cached_block(course_key, version_guid, block_key) if cached_module: return cached_module
self.modulestore.cache_items(self, [block_key], course_key, lazy=self.lazy) json_data = self.module_data.get(block_key) if json_data is None: raise ItemNotFoundError(block_key)
self.course_entry = CourseEnvelope(course_entry_override.course_key, self.course_entry.structure)
if block_key is None: block_key = BlockKey(block_data.block_type, LocalId())
if definition_id is None: definition_id = LocalId()
block_locator = course_key.make_usage_key( block_type=block_key.type, block_id=block_key.id, )
try: if block_data.asides: aside_fields = {block_key.type: {}} for aside in block_data.asides: aside_fields[block_key.type].update(aside['fields']) except AttributeError: pass
module.save()
if isinstance(block_locator.block_id, LocalId): self.local_modules[block_locator] = module
max_date = block_data.edit_info.edited_on max_date_by = block_data.edit_info.edited_by
return self._cds.local_modules[usage_id].scope_ids.def_id
SplitMongoKVSid = namedtuple('SplitMongoKVSid', 'id, def_id') new_contract('BlockUsageLocator', BlockUsageLocator)
super(SplitMongoKVS, self).__init__(copy.deepcopy(initial_values))
if field_decorator is None: self.field_decorator = lambda x: x else: self.field_decorator = field_decorator
self._load_definition() if key.block_scope_id.block_type not in self.aside_fields: raise KeyError()
if key.field_name not in aside_fields: self._load_definition()
return self.field_decorator(field_value)
if key.scope not in self.VALID_SCOPES: raise InvalidScopeError(key, self.VALID_SCOPES) if key.scope == Scope.content: self._load_definition()
self._fields[key.field_name] = value
if key.scope not in self.VALID_SCOPES: raise InvalidScopeError(key, self.VALID_SCOPES) if key.scope == Scope.content: self._load_definition()
if key.field_name in self._fields: del self._fields[key.field_name]
if key.scope not in self.VALID_SCOPES: return False
return key.field_name in self._fields
return super(SplitMongoKVS, self).default(key)
DIRECT_ONLY_CATEGORIES = ['course', 'chapter', 'sequential', 'about', 'static_tab', 'course_info']
self.thread_cache = threading.local()
thread_local_branch_setting = getattr(self.thread_cache, 'branch_setting', None) if thread_local_branch_setting: return thread_local_branch_setting else: return self.default_branch_setting_func()
self.signal_handler.send("course_published", course_key=course_key.for_branch(None))
original_course = self.source_modulestore.get_course(source_course_key, **kwargs) if original_course is None: raise ItemNotFoundError(unicode(source_course_key))
self._add_draft_modules_to_course(new_course.location, source_course_key, user_id, **kwargs)
self.split_modulestore.fix_not_found(course_version_locator, user_id)
if any(new_locator.block_id == child.block_id for child in new_parent.children): continue new_parent_cursor = 0 for old_child_loc in old_parent.children: if old_child_loc.block_id == draft_location.block_id:
for idx in range(new_parent_cursor, len(new_parent.children)): if new_parent.children[idx].block_id == old_child_loc.block_id: new_parent_cursor = idx + 1
draft_node_list = []
parent_url = None if parent_loc is not None: parent_url = parent_loc.to_deprecated_string()
if not hasattr(draft_node.module, 'xml_attributes'): draft_node.module.xml_attributes = {}
if draft_node.parent_location is None: continue
xml_centric_courselike_key = self.get_key() adapt_references(courselike, xml_centric_courselike_key, export_fs) courselike.add_xml_to_node(root)
self.process_root(root, export_fs)
root_courselike_dir = self.root_dir + '/' + self.target_dir self.process_extra(root, courselike, root_courselike_dir, xml_centric_courselike_key, export_fs)
self.post_process(root, export_fs)
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'static_tab', 'tabs', '.html' )
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'custom_tag_template', 'custom_tags' )
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'course_info', 'info', '.html' )
export_extra_content( export_fs, self.modulestore, self.courselike_key, xml_centric_courselike_key, 'about', 'about', '.html' )
course_policy_dir_name = courselike.url_name
with course_run_policy_dir.open('grading_policy.json', 'w') as grading_policy: grading_policy.write(dumps(courselike.grading_policy, cls=EdxJSONEncoder, sort_keys=True, indent=4))
export_fs.makeopendir('policies')
xml_file = export_fs.open(LIBRARY_ROOT, 'w') xml_file.write(lxml.etree.tostring(root, pretty_print=True, encoding='utf-8')) xml_file.close()
[adapt_references(child, destination_course_key, export_fs) for child in subtree.get_children()]
_export_field_content(item, item_dir)
phase_data = self.run_data.setdefault(test_phase, {}) amount_data = phase_data.setdefault(amount_md, {}) __ = amount_data.setdefault(modulestores, time_taken)
for phase in self.run_data.keys(): if phase in ('fake_assets',): continue per_phase = self.run_data[phase] html.add_header(1, phase)
html.add_header(2, title_map[table_type]) html.add_to_body(phase_table.table)
ASSET_XSD_FILE = 'assets.xsd'
NAME_CHARS = u'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_-' NAME_CHARS_W_UNICODE = NAME_CHARS + u'àĚŘǅΦШΩΣӔ'
validate_xml(input_xsd, output_xml)
try: from code_block_timer import CodeBlockTimer except ImportError: CodeBlockTimer = None
ASSET_AMOUNT_PER_TEST = (0, 1, 10, 100, 1000, 10000)
COURSE_NAME = 'manual-testing-complete'
TEST_COURSE = (COURSE_NAME, )
TEST_DIR = path(__file__).dirname() PLATFORM_ROOT = TEST_DIR.parent.parent.parent.parent.parent.parent TEST_DATA_ROOT = PLATFORM_ROOT / TEST_DATA_DIR COURSE_DATA_DIR = TEST_DATA_ROOT / COURSE_NAME
ASSET_XML_PATH = COURSE_DATA_DIR / AssetMetadata.EXPORTED_ASSET_DIR / AssetMetadata.EXPORTED_ASSET_FILENAME
ASSET_XSD_PATH = PLATFORM_ROOT / "common" / "lib" / "xmodule" / "xmodule" / "assetstore" / "tests" / ASSET_XSD_FILE
perf_test = True
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
perf_test = True
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
__ = source_store.find_asset_metadata(asset_key)
perf_test = True
make_asset_xml(num_assets, ASSET_XML_PATH) validate_xml(ASSET_XSD_PATH, ASSET_XML_PATH)
if asset_collection.name in asset_collection.database.collection_names():
XMODULE_FIELDS_WITH_USAGE_KEYS = ['location', 'parent']
draft_preferred = 'rev-opt-draft-preferred'
draft_only = 'rev-opt-draft-only'
published_only = 'rev-opt-published-only'
all = 'rev-opt-all'
mgmt_command = -1
primitive_command = -2
test = -3
system = -4
_bulk_ops_record_type = BulkOpsRecord
bulk_ops_record.nest()
if bulk_ops_record.is_root: self._start_outermost_bulk_operation(bulk_ops_record, course_key)
bulk_ops_record = self._get_bulk_ops_record(structure_key) if not bulk_ops_record.active: return
if emit_signals and bulk_ops_record.is_root: self.send_pre_publish_signal(bulk_ops_record, structure_key)
if bulk_ops_record.active: return
bulk_ops_record.nest()
bulk_ops_record.unnest()
self.signal_handler.send("course_published", course_key=course_id.for_branch(None)) bulk_ops_record.has_publish_item = False
self._subtree_edited_on = kwargs.get('_subtree_edited_on', None) self._subtree_edited_by = kwargs.get('_subtree_edited_by', None)
self.previous_version = edit_info.get('previous_version', None)
self.update_version = edit_info.get('update_version', None)
self.edited_on = edit_info.get('edited_on', None) self.edited_by = edit_info.get('edited_by', None)
self.original_usage = edit_info.get('original_usage', None) self.original_usage_version = edit_info.get('original_usage_version', None)
self.definition_loaded = False self.from_storable(kwargs)
self.fields = block_data.get('fields', {})
self.block_type = block_data.get('block_type', None)
self.definition = block_data.get('definition', None)
self.defaults = block_data.get('defaults', {})
self.asides = block_data.get('asides', {})
self.edit_info = EditInfo(**block_data.get('edit_info', {}))
self.add(metadata_to_insert)
self[asset_idx] = metadata_to_insert
all_assets.extend(course_assets.setdefault(asset_key.block_type, [])) idx = all_assets.find(asset_key)
all_assets = SortedAssetList(iterable=[], key=key_func) for asset_type, val in course_assets.iteritems(): all_assets.update(val)
all_assets = SortedAssetList(iterable=course_assets.get(asset_type, []), key=key_func)
end_idx = num_assets
step_incr = -1 start_idx = (num_assets - 1) - start_idx end_idx = (num_assets - 1) - end_idx
assets_by_type = defaultdict(lambda: SortedAssetList(iterable=course_assets.get(asset_type, [])))
log.warning("Asset's course {} does not match other assets for course {} - not saved.".format( asset_md.asset_id.course_key, course_key )) continue
xblock, fields = (block, block.fields)
xblock, fields = (None, block.__dict__)
return any(self._value_matches(target, test_val) for test_val in criteria['$in'])
return not any(self._value_matches(target, test_val) for test_val in criteria['$nin'])
def __init__( self, contentstore=None,
db=None, collection=None, host=None, port=None, tz_aware=True, user=None, password=None, ** kwargs
return {'default_impl': True}
about_location = self.make_course_key(org, course, run).make_usage_key('about', 'overview')
if self.contentstore: self.contentstore.copy_all_course_assets(source_course_id, dest_course_id) return dest_course_id
if self.contentstore: self.contentstore.delete_all_course_assets(course_key) super(ModuleStoreWriteBase, self).delete_course(course_key, user_id)
from xmodule.modulestore.mongo.draft import DraftModuleStore as DraftMongoModuleStore
if revision == ModuleStoreEnum.RevisionOption.published_only: return get_published()
elif usage_key.category in DIRECT_ONLY_CATEGORIES: return get_published()
elif revision == ModuleStoreEnum.RevisionOption.draft_only: return get_draft()
try: return get_draft() except ItemNotFoundError: return get_published()
super(DraftModuleStore, self).delete_course(course_key, user_id)
course_query = self._course_key_to_son(course_key) self.collection.remove(course_query, multi=True) self.delete_all_asset_metadata(course_key, user_id)
if not self.has_course(source_course_id): raise ItemNotFoundError("Cannot find a course at {0}. Aborting".format(source_course_id))
super(DraftModuleStore, self).clone_course(source_course_id, dest_course_id, user_id, fields)
if module.has_children: new_children = [] for child_loc in module.children: child_loc = child_loc.map_into_course(dest_course_id) new_children.append(child_loc)
query = self._course_key_to_son(location.course_key) query['definition.children'] = location.to_deprecated_string()
parents = self.collection.find(query, {'_id': True}, sort=[SORT_REVISION_FAVOR_DRAFT])
draft_items_locations = {item.location for item in draft_items} return [ item for item in base_get_items(MongoRevisionKey.published) if item.location not in draft_items_locations ]
return self.get_item(location)
self._verify_branch_setting(ModuleStoreEnum.Branch.draft_preferred) _verify_revision_is_published(location)
if location.category in DIRECT_ONLY_CATEGORIES: raise InvalidVersionError(location)
if delete_published: item['_id']['revision'] = MongoRevisionKey.published to_be_deleted.append(item['_id'])
self._breadth_first(convert_item, [location])
self._convert_to_draft(xblock.location, user_id, ignore_if_draft=True)
if not (allow_not_found and exception.args[0] == xblock.location): raise
if draft_only: revision = MongoRevisionKey.draft else: revision = ModuleStoreEnum.RevisionOption.all
self.refresh_cached_metadata_inheritance_tree(location.course_key)
item = self.get_item(item_location)
if item.has_children: for child_loc in item.children: _internal_depth_first(child_loc, False)
return
try: original_published = super(DraftModuleStore, self).get_item(item_location) except ItemNotFoundError: original_published = None
super(DraftModuleStore, self).update_item( item, user_id, isPublish=True, is_publish_root=is_root, allow_not_found=True ) to_be_deleted.append(as_draft(item_location).to_deprecated_son())
self._verify_branch_setting(ModuleStoreEnum.Branch.draft_preferred) _verify_revision_is_published(location)
if location.category in DIRECT_ONLY_CATEGORIES: raise InvalidVersionError(location)
to_process_non_drafts = super(DraftModuleStore, self)._query_children_for_cache_children(course_key, items)
for draft in to_process_drafts: draft_loc = Location._from_deprecated_son(draft["_id"], course_key.run) draft_as_non_draft_loc = as_published(draft_loc)
if draft_as_non_draft_loc in to_process_dict: to_process_dict[draft_as_non_draft_loc] = draft
queried_children = to_process_dict.values()
SORT_REVISION_FAVOR_DRAFT = ('_id.revision', pymongo.DESCENDING)
SORT_REVISION_FAVOR_PUBLISHED = ('_id.revision', pymongo.ASCENDING)
_OSFS_INSTANCE = {}
self.course_id = course_key self.cached_metadata = cached_metadata
try: category = json_data['location']['category'] class_ = self.load_block_type(category)
parent = self.modulestore.get_parent_location( as_published(location), ModuleStoreEnum.RevisionOption.published_only if location.revision is None else ModuleStoreEnum.RevisionOption.draft_preferred )
non_draft_loc = as_published(location)
metadata_to_inherit = self.cached_metadata.get(unicode(non_draft_loc), {}) inherit_metadata(module, metadata_to_inherit)
module.save() return module
return []
if value is None and key != '_id.revision': del query[key]
bulk_ops_record.dirty = False
DEFAULT_ASSET_COLLECTION_NAME = 'assetstore'
kwargs['w'] = 1
if asset_collection is None: asset_collection = self.DEFAULT_ASSET_COLLECTION_NAME self.asset_collection = self.database[asset_collection]
super(MongoModuleStore, self)._drop_database(database, collections, connections)
for field_name in InheritanceMixin.fields: record_filter['metadata.{0}'.format(field_name)] = 1
resultset = self.collection.find(query, record_filter)
results_by_url = {} root = None
for result in resultset: location = as_published(Location._from_deprecated_son(result['_id'], course_id.run))
metadata_to_inherit = {}
tree = self._compute_metadata_inheritance_tree(course_id)
if self.metadata_inheritance_cache_subsystem is not None: self.metadata_inheritance_cache_subsystem.set(unicode(course_id), tree)
cached_metadata = self._get_cached_metadata_inheritance_tree(course_id, force_refresh=True) if runtime: runtime.cached_metadata = cached_metadata
to_process = [] if children: to_process = self._query_children_for_cache_children(course_key, children)
if depth is not None: depth -= 1
return [ self._load_item( course_key, item, data_cache, using_descriptor_system=using_descriptor_system, apply_cached_metadata=self._should_apply_cached_metadata(item, depth), for_parent=for_parent, ) for item in items ]
super(MongoModuleStore, self).create_course( org, course, run, user_id, runtime=xblock.runtime, **kwargs )
if block_id is None: if block_type == 'course': block_id = course_key.run else: block_id = u'{}_{}'.format(block_type, uuid4().hex[:5])
ScopeIds(None, block_type, location, location), dbmodel, for_parent=kwargs.get('for_parent'),
xmodule.save() return xmodule
parent = None
result = self.collection.update( {'_id': location.to_deprecated_son()}, {'$set': update}, multi=False, upsert=allow_not_found,
xblock._edit_info = payload['edit_info']
self.refresh_cached_metadata_inheritance_tree(xblock.scope_ids.usage_id.course_key, xblock.runtime)
bulk_record = self._get_bulk_ops_record(location.course_key)
query = self._course_key_to_son(location.course_key) query['definition.children'] = unicode(location)
if revision == ModuleStoreEnum.RevisionOption.published_only: query['_id.revision'] = MongoRevisionKey.published
parents = list( self.collection.find(query, {'_id': True}, sort=[SORT_REVISION_FAVOR_DRAFT]) ) if len(parents) == 0: return cache_and_return(None)
return cache_and_return(None)
raise ReferentialIntegrityError( u"{} parents claim {}".format(len(parents), location) )
return cache_and_return(Location._from_deprecated_son(parents[0]['_id'], location.course_key.run))
all_parents = [] published_parents = 0 for parent in parents: if parent['_id']['revision'] is None: published_parents += 1 all_parents.append(parent)
if published_parents > 1: non_orphan_parents = self._get_non_orphan_parents(location, all_parents, revision) return cache_and_return(non_orphan_parents[0].replace(run=location.course_key.run))
return cache_and_return(Location._from_deprecated_son(found_id, location.course_key.run))
item_locs.add( unicode(as_published(Location._from_deprecated_son(item['_id'], course_key.run))) )
return [ Location._from_deprecated_son(course['_id'], course['_id']['name']).course_key for course in courses ]
assert len(course_assets['assets']) == 0 self.asset_collection.update( {'_id': doc_id}, {'$set': {'assets': {}}} )
return CourseAssetsFromStorage(course_key, doc_id, course_assets['assets'])
updates_by_type = {} for asset_type, assets in assets_by_type.iteritems(): updates_by_type[self._make_mongo_asset_key(asset_type)] = assets.as_list()
self.asset_collection.update( {'_id': course_assets.doc_id}, {'$set': updates_by_type} ) return True
self.asset_collection.insert(dest_assets)
all_assets = course_assets[asset_key.asset_type] md = AssetMetadata(asset_key, asset_key.path) md.from_storable(all_assets[asset_idx]) md.update(attr_dict)
all_assets[asset_idx] = md.to_storable()
self.asset_collection.update( {'_id': course_assets.doc_id}, {'$set': {self._make_mongo_asset_key(asset_key.asset_type): all_asset_info}} ) return 1
try: course_assets = self._find_course_assets(course_key) self.asset_collection.remove(course_assets.doc_id) except ItemNotFoundError: pass
create_collection_index(self.collection, '_id.category', background=True)
create_collection_index(self.collection, 'definition.children', sparse=True, background=True)
create_collection_index(self.collection, '_id.revision', background=True)
def convert_to_draft(self, location, user_id): raise NotImplementedError()
from xmodule.modulestore.mongo.draft import DraftModuleStore
continue
fullname_with_subpath = content_path.replace(static_dir, '') if fullname_with_subpath.startswith('/'): fullname_with_subpath = fullname_with_subpath[1:] asset_key = StaticContent.compute_location(target_id, fullname_with_subpath)
if not mime_type or mime_type not in mimetypes_list:
thumbnail_content, thumbnail_location = static_content_store.generate_thumbnail(content)
try: static_content_store.save(content) except Exception as err: log.exception(u'Error importing {0}, error={1}'.format( fullname_with_subpath, err ))
remap_dict[fullname_with_subpath] = asset_key
if self.target_id: assert len(self.xml_module_store.modules) == 1
import_static_content( data_path, self.static_content_store, dest_id, subpath='static', verbose=self.verbose )
asset_key = make_asset_id(course_id, asset) asset_md = AssetMetadata(asset_key) asset_md.from_xml(asset) all_assets.append(asset_md)
if len(all_assets) > 0: self.store.save_asset_metadata_list(all_assets, all_assets[0].edited_by, import_only=True)
course_data_path = path(self.data_dir) / source_courselike.data_dir
source_courselike.static_asset_path = source_courselike.data_dir source_courselike.save() log.debug('course static_asset_path=%s', source_courselike.static_asset_path)
pass
with self.store.bulk_operations(dest_id): source_courselike, courselike, data_path = self.get_courselike(courselike_key, runtime, dest_id)
self.import_static(data_path, dest_id)
self.import_asset_metadata(data_path, dest_id)
self.import_children(source_courselike, courselike, courselike_key, dest_id)
course, course_data_path = self.import_courselike( runtime, courselike_key, dest_id, source_course, ) return source_course, course, course_data_path
dest_id = self.store.make_course_key(courselike_key.org, courselike_key.course, courselike_key.run)
if existing_id: dest_id = existing_id
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, dest_id): self.recursive_build(source_courselike, courselike, courselike_key, dest_id)
return self.store.get_course(courselike.id.replace(branch=None, version_guid=None))
if 'parent_url' in value: del value['parent_url'] if 'parent_sequential_url' in value: del value['parent_sequential_url']
module.data = rewrite_nonportable_content_links( source_course_id, dest_course_id, module.data )
errorlog = make_error_tracker()
module_location = module.location.map_into_course(target_id) _update_module_location(module, module_location.replace(revision=MongoRevisionKey.draft))
if parent_url is not None and index is not None: course_key = descriptor.location.course_key parent_location = course_key.make_usage_key_from_deprecated_string(parent_url)
parent_location = parent_location.map_into_course(target_id)
drafts.sort(key=lambda x: x.index)
return 0
'parent_url', module.xml_attributes.get('parent_sequential_url')
if attr == 'parent_sequential_url': attr = 'parent_url' xml_attrs[attr] = val
module.xml_attributes = xml_attrs
for module in module_store.modules[course_id].itervalues(): if module.location.category == parent_category: parents.append(module)
for course_dir in source_dirs: _err_cnt, _warn_cnt = validate_data_source_paths(path(data_dir), course_dir) err_cnt += _err_cnt warn_cnt += _warn_cnt
self.course_id = course_id self.load_error_modules = load_error_modules self.modulestore = xmlstore
need_uniq_names = ('problem', 'sequential', 'video', 'course', 'chapter', 'videosequence', 'poll_question', 'vertical')
orig_name = orig_name[len(tag) + 1:-12]
if url_name is None or url_name == "": url_name = fallback_name()
)
if child.parent is None or child.parent > descriptor.scope_ids.usage_id: child.parent = descriptor.location child.save()
descriptor.save() return descriptor
def add_node_as_child(self, block, node, id_generator): child_block = self.process_xml(etree.tostring(node)) block.children.append(child_block.scope_ids.usage_id)
self.field_data = inheriting_field_data(kvs=DictKeyValueStore())
errorlog = make_error_tracker() course_descriptor = None try: course_descriptor = self.load_course(course_dir, course_ids, errorlog.tracker, target_course_id)
self.errored_courses[course_dir] = errorlog
courselike_label = self.parent_xml.split('.')[0]
if policy == {}:
if course_data.get('name'):
if isinstance(course_descriptor, ErrorDescriptor): return course_descriptor
compute_inherited_metadata(course_descriptor)
self.load_extra_content( system, course_descriptor, 'course_info', self.data_dir / course_dir / 'info', course_dir, url_name )
self.load_extra_content( system, course_descriptor, 'static_tab', self.data_dir / course_dir / 'tabs', course_dir, url_name )
return SlashSeparatedCourseKey(org, course, url_name)
if os.path.isdir(base_dir / url_name): self._load_extra_content(system, course_descriptor, category, base_dir / url_name, course_dir)
data_content = None
ScopeIds(None, category, loc, loc), DictFieldData(data_content),
if category == "static_tab": dog_stats_api.increment( DEPRECATION_VSCOMPAT_EVENT, tags=( "location:xml_load_extra_content_static_tab", u"course_dir:{}".format(course_dir), ) )
if mod_loc.name not in name: return False
raise NotImplementedError
return None
lazy.invalidate(library_descriptor, '_unwrapped_field_data') library_descriptor._field_data = inheriting_field_data(InheritanceKeyValueStore(init_dict))
raise NotImplementedError
if not settings.configured: settings.configure()
from request_cache.middleware import RequestCache
try: from xblock_django.user_service import DjangoXBlockUserService from crum import get_current_user
_MIXED_MODULESTORE = None
pass
mappings = getattr(settings, 'HOSTNAME_MODULESTORE_DEFAULT_MAPPINGS', None)
if mappings: for key in mappings.iterkeys(): if re.match(key, hostname): return mappings[key]
return get_branch_setting()
queue = [(usage_key, ())] while len(queue) > 0:
parent = modulestore.get_parent_location(next_usage)
if next_usage.block_type == "course": path = (next_usage, path) return flatten(path) elif parent is None: return None
newpath = (next_usage, path) queue.append((parent, newpath))
chapter = path[1].name if n > 1 else None section = path[2].name if n > 2 else None vertical = path[3].name if n > 3 else None position = None
modulestore = XMLModuleStore( DATA_DIR, source_dirs=['toy'], xblock_mixins=(XModuleMixin,), load_error_modules=False)
errors = modulestore.get_course_errors(SlashSeparatedCourseKey("edX", "toy", "2012_Fall")) assert errors == []
toy_course = store.get_course(SlashSeparatedCourseKey('edX', 'toy', '2012_Fall')) toy_course.wiki_slug = 'simple'
with store.branch_setting(ModuleStoreEnum.Branch.published_only, course.id): store.get_item(course.location)
with self.assertRaises(ValueError): with store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course.id):
other_parent_loc = course_key.make_usage_key('vertical', 'zeta') other_parent = store.get_item(other_parent_loc) self.assertIn(shared_item_loc, other_parent.children)
TESTABLE_BLOCK_TYPES = set(DIRECT_ONLY_CATEGORIES) TESTABLE_BLOCK_TYPES.discard('course')
self.assertEqual(len(course_summaries), 1)
self.assertCourseSummaryFields(course_summaries)
self.assertTrue(all(isinstance(course, CourseSummary) for course in course_summaries))
self.assertBlockHasContent(child_usage_key, 'data', child_data)
new_mixed_setting = convert_module_store_setting_if_needed(copy.deepcopy(old_setting))
self.assertEqual(new_mixed_setting["default"]["ENGINE"], "xmodule.modulestore.mixed.MixedModuleStore")
new_stores = get_mixed_stores(new_mixed_setting) self.assertIsInstance(new_stores, list)
new_stores = [store for store in get_mixed_stores(new_mixed_setting) if store['NAME'] != 'split'] old_stores = get_mixed_stores(self.OLD_MIXED_CONFIG_WITH_DICT)
self.assertEqual(len(new_stores), len(old_stores)) for new_store in new_stores: self.assertStoreValuesEqual(new_store, old_stores[new_store['NAME']])
old_mixed_setting = self.ALREADY_UPDATED_MIXED_CONFIG new_mixed_setting, new_default_store_setting = self.assertMigrated(old_mixed_setting) self.assertTrue(self.is_split_configured(new_mixed_setting)) self.assertEquals(old_mixed_setting, new_mixed_setting)
self.writable_chapter_location = self.store = self.fake_location = None self.course_locations = {}
chapter = self.store.create_child(self.user_id, self.course.location, 'chapter', block_id='Overview', asides=asides) self.writable_chapter_location = chapter.location
self.assertEqual(self.store.get_modulestore_type( SlashSeparatedCourseKey('foo', 'bar', '2012_Fall')), default_ms )
self.store.mappings = {} course_key = self.course_locations[self.MONGO_COURSEID].course_key with check_exact_number_of_calls(self.store.default_modulestore, 'has_course', 1):
with check_mongo_calls(max_find.pop(0), max_send): self.assertFalse(self.store.has_item(self.fake_location))
with self.assertRaises(UnsupportedRevisionError): self.store.has_item(self.fake_location, revision=ModuleStoreEnum.RevisionOption.draft_preferred)
with check_mongo_calls(max_find.pop(0), max_send): with self.assertRaises(ItemNotFoundError): self.store.get_item(self.fake_location)
with self.assertRaises(UnsupportedRevisionError): self.store.get_item(self.fake_location, revision=ModuleStoreEnum.RevisionOption.draft_preferred)
with self.assertRaises(UnsupportedRevisionError): self.store.get_items( self.course_locations[self.MONGO_COURSEID].course_key, revision=ModuleStoreEnum.RevisionOption.draft_preferred )
orphans = self.store.get_orphans(course_key) self.assertEqual(len(orphans), 0)
orphan = course_key.make_usage_key('chapter', 'OrphanChapter') self.store.create_item(self.user_id, orphan.course_key, orphan.block_type, block_id=orphan.block_id)
orphans = self.store.get_orphans(course_key) self.assertIn(orphan, orphans) self.assertEqual(len(orphans), 1)
items = self.store.get_items(course_key) self.assertIn(orphan, [item.location for item in items]) self.assertEqual(len(items), 3)
items_in_tree = self.store.get_items(course_key, include_orphans=False)
chapter = self.store.create_item( self.user_id, test_course.id, 'chapter', block_id='vertical_container' )
self.assertFalse(self.store.has_changes(test_course)) self.assertFalse(self.store.has_changes(chapter))
xblock = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' )
self.assertTrue(self.store.has_changes(xblock))
newXBlock = self.store.publish(xblock.location, self.user_id) self.assertFalse(self.store.has_changes(newXBlock))
component = self.store.get_item(xblock.location) component.display_name = 'Changed Display Name'
component = self.store.publish(component.location, self.user_id) self.assertFalse(self.store.has_changes(component))
xblock = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' )
self.assertTrue(self.store.has_changes(xblock))
component = self.store.publish(xblock.location, self.user_id) self.assertFalse(self.store.has_changes(component))
component = self.store.publish(component.location, self.user_id) self.assertFalse(self.store.has_changes(component))
xblock = self.store.create_item( self.user_id, test_course.id, 'vertical', block_id='test_vertical' )
self.assertTrue(self.store.has_changes(xblock))
component = self.store.publish(xblock.location, self.user_id) self.assertFalse(self.store.has_changes(component))
self.store.revert_to_published(component.location, self.user_id) component = self.store.get_item(component.location) self.assertFalse(self.store.has_changes(component))
component = self.store.get_item(component.location) component.display_name = 'Changed Display Name' self.store.update_item(component, self.user_id)
self.assertTrue(self.store.has_changes(component))
self.store.publish(vertical.location, self.user_id) self.assertFalse(self._has_changes(vertical.location))
self.store.revert_to_published(vertical.location, self.user_id) self.assertFalse(self._has_changes(vertical.location))
self.store.delete_item(component.location, self.user_id) vertical = self.store.get_item(vertical.location) self.assertTrue(self._has_changes(vertical.location))
self.store.publish(sequential.location, self.user_id) self.assertFalse(self._has_changes(sequential.location))
self.store.delete_item(vertical.location, self.user_id) self.assertFalse(self._has_changes(sequential.location))
self.store.publish(locations['parent_sibling'], self.user_id) self.store.publish(locations['parent'], self.user_id)
for key in locations: self.assertFalse(self._has_changes(locations[key]))
child = self.store.get_item(locations['child']) child.display_name = 'Changed Display Name' self.store.update_item(child, self.user_id)
self.store.publish(locations['parent'], self.user_id)
for key in locations: self.assertFalse(self._has_changes(locations[key]))
for key in locations: self.assertFalse(self._has_changes(locations[key]))
self.assertTrue(self._has_changes(locations['grandparent'])) self.assertTrue(self._has_changes(locations['parent']))
self.store.publish(locations['child_sibling'], self.user_id)
self.assertTrue(self._has_changes(locations['grandparent'])) self.assertTrue(self._has_changes(locations['parent']))
self.store.publish(locations['child'], self.user_id)
self.assertFalse(self._has_changes(locations['grandparent'])) self.assertFalse(self._has_changes(locations['parent']))
self.assertFalse(self._has_changes(locations['grandparent'])) self.assertFalse(self._has_changes(locations['parent']))
self.store.create_child( self.user_id, locations['parent'], 'vertical', block_id='new_child', )
self.assertTrue(self._has_changes(locations['grandparent'])) self.assertTrue(self._has_changes(locations['parent']))
self.assertFalse(self._has_changes(locations['grandparent'])) self.assertFalse(self._has_changes(locations['parent']))
self.assertFalse(self._has_changes(parent.location)) self.assertFalse(self._has_changes(child.location))
child.display_name = 'Changed Display Name' self.store.update_item(child, user_id=self.user_id)
self.assertTrue(self._has_changes(parent.location)) self.assertTrue(self._has_changes(child.location))
self.assertTrue(self.store.has_changes(parent))
with self.assertRaises(ItemNotFoundError): self.store.get_item(self.writable_chapter_location)
with self.assertRaises(UnsupportedRevisionError): self.store.delete_item( private_leaf.location, self.user_id, revision=ModuleStoreEnum.RevisionOption.draft_preferred )
self.store.create_child( self.user_id, self.course.location, 'static_tab' )
mongo_course = self.store.get_course(self.course_locations[self.MONGO_COURSEID].course_key) self.assertEqual(len(mongo_course.children), 1)
library = self.store.get_library(library_key) self.assertEqual(library.location.library_key, library_key)
self.store.mappings.clear() library = self.store.get_library(library_key) self.assertEqual(library.location.library_key, library_key)
self.course = self.store.publish(self.course.location, self.user_id)
self.store.convert_to_draft(self.vertical_x1a, self.user_id) self.store.convert_to_draft(self.vertical_y1a, self.user_id)
child_to_move_location = self.problem_x1a_1 new_parent_location = self.vertical_y1a old_parent_location = self.vertical_x1a
self.store.publish(self.course.location, self.user_id)
self.store.convert_to_draft(self.vertical_y1a, self.user_id)
child_to_delete_location = self.problem_y1a_1 old_parent_location = self.vertical_y1a self.store.delete_item(child_to_delete_location, self.user_id)
(child_to_delete_location, old_parent_location, ModuleStoreEnum.RevisionOption.draft_preferred), (child_to_delete_location, old_parent_location, ModuleStoreEnum.RevisionOption.published_only),
self._create_block_hierarchy() self.store.publish(self.course.location, self.user_id)
mongo_store.collection.update( self.vertical_x1b.to_deprecated_son('_id.'), {'$push': {'definition.children': unicode(self.problem_x1a_1)}} )
self.store.convert_to_draft(self.vertical_x1a, self.user_id) item = self.store.get_item(self.vertical_x1a) self.assertTrue(self.store.has_published_version(item))
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_id): parent = mongo_store.get_parent_location(self.problem_x1a_1) self.assertEqual(parent, self.vertical_x1a)
with check_mongo_calls(num_finds.pop(0), num_sends): path = path_to_location(self.store, location) self.assertEqual(path, expected)
orphan = course_key.make_usage_key('chapter', 'OrphanChapter') self.store.create_item( self.user_id, orphan.course_key, orphan.block_type, block_id=orphan.block_id )
self.store.delete_item(self.problem_x1a_1, self.user_id) self.assertTrue(self._has_changes(self.vertical_x1a))
problem.display_name = "updated before calling revert" self.store.update_item(problem, self.user_id) self.store.revert_to_published(self.vertical_x1a, self.user_id)
self.assertEqual(num_children, len(reverted_parent.children))
self._create_block_hierarchy()
detached_locations = [ course_id.make_usage_key('static_tab', 'StaticTab'), course_id.make_usage_key('course_info', 'updates'), ]
self._create_block_hierarchy() self.store.publish(self.course.location, self.user_id)
with check_mongo_calls(max_find, max_send): wiki_courses = self.store.get_courses_for_wiki('999') self.assertEqual(len(wiki_courses), 1) self.assertIn(
with check_mongo_calls(max_find, max_send): self.store.unpublish(self.vertical_x1a, self.user_id)
draft_xblock = self.store.get_item( self.vertical_x1a, revision=ModuleStoreEnum.RevisionOption.draft_only ) self.assertIsNotNone(draft_xblock)
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
self.store.unpublish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertFalse(self.store.has_published_version(item))
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
self.store.convert_to_draft(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item))
for block in [component, child, sibling]: check_node(block.location, None, after_create, self.user_id, None, after_create, self.user_id)
component.display_name = 'Changed Display Name'
check_node(child.location, None, after_create, self.user_id, None, after_create, self.user_id)
child = self.store.get_item(child.location) child.display_name = 'Changed Display Name' self.store.update_item(child, user_id=editing_user)
check_node(child.location, after_create, after_edit, editing_user, after_create, after_edit, editing_user)
check_node(test_course.location, None, after_create, self.user_id, after_create, after_edit, editing_user)
check_node(sibling.location, None, after_create, self.user_id, None, after_create, self.user_id)
component = self.store.create_child( self.user_id, test_course.location, 'vertical', )
self.assertEqual(component.edited_by, self.user_id) old_edited_on = component.edited_on
component.display_name = 'Changed' self.store.update_item(component, edit_user) updated_component = self.store.get_item(component.location)
self.assertLess(old_edited_on, updated_component.edited_on) self.assertEqual(updated_component.edited_by, edit_user)
component = self.store.create_child( self.user_id, test_course.location, 'vertical', )
old_time = datetime.datetime.now(UTC) self.store.publish(component.location, publish_user) updated_component = self.store.get_item(component.location)
self.assertLessEqual(old_time, updated_component.published_on) self.assertEqual(updated_component.published_by, publish_user)
test_course = self.store.create_course('testx', 'GreekHero', 'test_run', self.user_id) self.assertTrue(self.store.has_published_version(test_course))
sequential.display_name = 'sequential1' sequential = self.store.update_item(sequential, self.user_id) self.assertTrue(self.store.has_published_version(sequential))
problem_child = self.store.create_child(self.user_id, chapter_location, 'problem', 'Problem_Child') self.assertFalse(self.store.has_published_version(problem_child))
problem_item = self.store.create_item(self.user_id, test_course_key, 'problem', 'Problem_Item') self.assertFalse(self.store.has_published_version(problem_item))
problem_item.display_name = 'Problem_Item1' problem_item = self.store.update_item(problem_item, self.user_id) self.assertFalse(self.store.has_published_version(problem_item))
wiki_courses = self.store.get_courses_for_wiki('999') self.assertIn(
mongo_course = self.store.get_course(self.course_locations[self.MONGO_COURSEID].course_key) mongo_course.wiki_slug = 'simple' self.store.update_item(mongo_course, self.user_id)
wiki_courses = self.store.get_courses_for_wiki('999') self.assertEqual(len(wiki_courses), 0)
problem = self.store.get_item(problem_location) self.assertEquals(problem.display_name, expected_display_name)
assertNumProblems(expected_display_name, 1)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_key): self.assertTrue(self.store.has_item(problem_location)) assertProblemNameEquals(problem_original_name)
self.store.publish(self.vertical_x1a, self.user_id) self.store.publish(problem_location, self.user_id)
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, course_key): self.assertTrue(self.store.has_item(problem_location)) assertProblemNameEquals(problem_original_name)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_key): assertProblemNameEquals(problem_original_name)
problem = self.store.get_item(problem_location) problem.display_name = problem_new_name self.store.update_item(problem, self.user_id)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course_key): assertProblemNameEquals(problem_new_name)
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, course_key): assertProblemNameEquals(problem_original_name) assertNumProblems(problem_new_name, 0)
self.store.publish(problem_location, self.user_id)
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, course_key): assertProblemNameEquals(problem_new_name) assertNumProblems(problem_original_name, 0)
self._initialize_mixed(mappings={})
self._initialize_mixed(mappings={})
self._initialize_mixed(mappings={})
self._initialize_mixed(contentstore=contentstore, mappings={})
source_store = self.store._get_modulestore_by_type(source_modulestore) dest_store = self.store._get_modulestore_by_type(destination_modulestore) self.assertCoursesEqual(source_store, source_course_key, dest_store, dest_course_id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
signal_handler.reset_mock() section = self.store.create_item(self.user_id, course.id, 'chapter') signal_handler.send.assert_called_with('course_published', course_key=course.id)
signal_handler.reset_mock() unit = self.store.create_child(self.user_id, subsection.location, 'vertical') signal_handler.send.assert_not_called()
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) signal_handler.send.assert_called_with('course_published', course_key=course.id)
unit = self.store.create_child(self.user_id, subsection.location, 'vertical') signal_handler.send.assert_not_called()
course = self.store.create_course('org_x', 'course_y', 'run_z', self.user_id) course_key = course.id
course = self.store.delete_course(course_key, self.user_id)
signal_handler.send.assert_called_with('course_deleted', course_key=course_key)
course_orphans = self.store.get_orphans(course_locator) self.assertEqual(len(course_orphans), 0) self.store.delete_item(vertical.location, self.user_id)
course_publish_orphans = self.store.get_orphans(course_locator_publish)
course_orphans = self.store.get_orphans(course_locator) self.assertEqual(len(course_orphans), 0)
course_publish_orphans = self.store.get_orphans(course_locator_publish)
self.assertTrue(self._has_changes(draft_xblock.location))
self.assertFalse(self._has_changes(published_xblock.location))
self.assertFalse(self._has_changes(published_xblock.location))
self.assertFalse(self._has_changes(published_xblock.location))
self.assertTrue(self._has_changes(published_xblock.location))
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, source_course_key): component = self.store.get_item(published_xblock.location) self.assertEqual(component.display_name, updated_display_name)
sequential = self.store.create_child( self.user_id, chapter.location, 'sequential', block_id='subsection_one' ) self.store.publish(sequential.location, self.user_id)
vertical = self.store.create_child( self.user_id, sequential.location, 'vertical', block_id='moon_unit' )
self.assertTrue(self._has_changes(chapter.location)) self.assertTrue(self._has_changes(sequential.location)) self.assertTrue(self._has_changes(vertical.location))
sequential = self.store.create_child( self.user_id, chapter.location, 'sequential', block_id='subsection_one' ) self.store.publish(sequential.location, self.user_id)
self._export_import_course_round_trip( self.store, contentstore, source_course_key, self.export_dir )
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, source_course_key): component = self.store.get_item(unit.location) self.assertEqual(component.display_name, updated_display_name)
sequential = self.store.create_child( self.user_id, chapter.location, 'sequential', block_id='subsection_one' ) self.store.publish(sequential.location, self.user_id)
self._export_import_course_round_trip( self.store, contentstore, source_course_key, self.export_dir )
with self.store.branch_setting(ModuleStoreEnum.Branch.published_only, source_course_key): component = self.store.get_item(unit.location) self.assertEqual(component.display_name, updated_display_name)
new_chapter = self.store.create_child(self.user_id, courses[0].location, 'chapter', 'new_chapter') asides = new_chapter.runtime.get_asides(new_chapter)
chapter_aside.data_field = 'new value' self.store.update_item(new_chapter, self.user_id, asides=[chapter_aside])
chapter_aside.data_field = 'another one value' self.store.update_item(new_chapter, self.user_id, asides=[chapter_aside])
top_level_export_dir = 'exported_source_course_with_asides' export_course_to_xml( self.store, contentstore, dest_course_key, self.export_dir, top_level_export_dir, )
courses2 = import_course_from_xml( self.store, self.user_id, self.export_dir, source_dirs=[top_level_export_dir], static_content_store=contentstore, target_id=dest_course_key2, create_if_not_present=True, raise_on_failure=True, )
top_level_export_dir = 'exported_source_course_with_asides' export_course_to_xml( self.store, contentstore, dest_course_key, self.export_dir, top_level_export_dir, )
courses2 = import_course_from_xml( self.store, self.user_id, self.export_dir, source_dirs=[top_level_export_dir], static_content_store=contentstore, target_id=dest_course_key2, create_if_not_present=True, raise_on_failure=True, )
chapters = courses2[0].get_children() self.assertEquals(2, len(chapters)) self.assertIn(new_chapter_display_name, [item.display_name for item in chapters])
aside1 = AsideFoo(scope_ids=ScopeIds('user', block_type1, def_id, usage_id), runtime=self.runtime) aside1.field11 = 'new_value11' aside1.field12 = 'new_value12'
aside2 = AsideBar(scope_ids=ScopeIds('user', block_type2, def_id, usage_id), runtime=self.runtime) aside2.field21 = 'new_value21'
published_xblock = self.store.create_item( self.user_id, self.course.id, 'vertical', block_id='test_vertical', asides=[aside1, aside2] )
self._initialize_mixed(contentstore=contentstore, mappings={})
actual_items = source_store.get_items(dest_course_id, revision=ModuleStoreEnum.RevisionOption.published_only) chapter_is_found = False
self.store.delete_item(published_xblock.location, self.user_id)
published_xblock2 = self.store.create_item( self.user_id, self.course.id, 'vertical', block_id='test_vertical' )
asides2 = published_xblock2.runtime.get_asides(published_xblock2) self.assertEquals(asides2[0].field11, 'aside1_default_value1') self.assertEquals(asides2[0].field12, 'aside1_default_value2')
self.store.publish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertTrue(self.store.has_published_version(item)) _check_asides(item)
self.store.unpublish(item_location, self.user_id) item = self.store.get_item(item_location) self.assertFalse(self.store.has_published_version(item)) _check_asides(item)
class Meta(object): model = Dummy
kwargs.update(kwargs.pop('metadata', {})) default_store_override = kwargs.pop('default_store', None)
except CyclicDefinitionError: return default_location
parent = kwargs.pop('parent', None) or store.get_item(parent_location)
if display_name is not None: metadata['display_name'] = display_name
@functools.wraps(func) def capture(*args, **kwargs): stacks.capture_stack(args, kwargs) return func(*args, **kwargs)
assert_greater_equal(call_count, minimum_calls)
assert_less_equal(call_count, maximum_calls)
db_config = { 'host': MONGO_HOST, 'port': MONGO_PORT_NUM, 'db': 'test_xmodule', }
self.split_mongo.create_course( self.split_course_key.org, self.split_course_key.course, self.split_course_key.run, self.user_id, fields=fields, root_block_id='runid' )
source_keys = [source_container.children[0]] new_blocks = self.store.copy_from_template(source_keys, dest_key=course.location, user_id=self.user_id) self.assertEqual(len(new_blocks), 1)
self.assertIsNotNone(problem_block.markdown) self.assertIsNone(problem_block_course.markdown)
new_display_name = "The Trouble with Tribbles" new_weight = 20 problem_block_course.display_name = new_display_name problem_block_course.weight = new_weight self.store.update_item(problem_block_course, self.user_id)
extra_block = self.make_block("html", vertical_block_course)
source_course = self.store.get_course( source_course.location.course_key, remove_version=False, remove_branch=False )
self.assertFalse(self.store.has_changes(new_blocks["about"])) self.assertTrue(published_version_exists(new_blocks["chapter"]))
self.create_random_units(False, conditional_loc)
if store is not None and i not in (4, 5): store.save_asset_metadata(asset_md, asset[4])
asset_md = store.find_asset_metadata(new_asset_loc) self.assertIsNone(asset_md)
asset_md = store.get_all_asset_metadata(course.id, 'asset') self.assertEquals(asset_md, [])
with self.assertRaises(ItemNotFoundError): store.find_asset_metadata(new_asset_loc) with self.assertRaises(ItemNotFoundError): store.get_all_asset_metadata(fake_course_id, 'asset')
with storebuilder.build() as (__, store): course = CourseFactory.create(modulestore=store)
with storebuilder.build() as (__, store): course = CourseFactory.create(modulestore=store)
store.save_asset_metadata_list(md_list, ModuleStoreEnum.UserID.test)
with storebuilder.build() as (__, store): course1 = CourseFactory.create(modulestore=store) course2 = CourseFactory.create(modulestore=store)
store.save_asset_metadata_list(md_list, ModuleStoreEnum.UserID.test)
match = re.search(r'(.*?/common)(?:$|/)', path(__file__)) COMMON_ROOT = match.group(1)
SplitModuleTest.modulestore = None
self.assertEqual(len(courses), 3)
self.assertEqual(len(courses), 1)
self.assertEqual(len(courses), 2)
courses = modulestore().get_courses(branch=BRANCH_NAME_DRAFT) self.assertEqual(len(courses), 3)
self.assertEqual(course.edited_by, "testassist@edx.org") self.assertDictEqual(course.grade_cutoffs, {"Pass": 0.55})
self.assertEqual(course.edited_by, "testassist@edx.org") self.assertDictEqual(course.grade_cutoffs, {"Pass": 0.45})
version_history = modulestore().get_block_generations(updated_problem.location) self.assertEqual(version_history.locator.version_guid, first_problem.location.version_guid)
self.cache = caches['default']
self.cache.clear() self.addCleanup(self.cache.clear)
self.user = random.getrandbits(32) self.new_course = modulestore().create_course( 'org', 'course', 'test_run', self.user, BRANCH_NAME_DRAFT, )
mock_get_cache.return_value = self.cache
with check_mongo_calls(0): cached_structure = self._get_structure(self.new_course)
self.assertEqual(cached_structure, not_cached_structure)
with check_mongo_calls(1): cached_structure = self._get_structure(self.new_course)
self.assertEqual(cached_structure, not_cached_structure)
with check_mongo_calls(1): cached_structure = self._get_structure(self.new_course)
self.assertEqual(cached_structure, not_cached_structure)
locator = course.location.map_into_course(CourseLocator(version_guid=previous_version)) self.assertTrue( modulestore().has_item(locator), "couldn't find in %s" % previous_version )
locator = BlockUsageLocator(course_locator, block_type='chapter', block_id='chapter1') self.assertTrue( modulestore().has_item(locator), "couldn't find chapter1" )
self.assertEqual(block.edited_by, "testassist@edx.org") self.assertDictEqual( block.grade_cutoffs, {"Pass": 0.45}, )
with self.assertRaises(ItemNotFoundError): modulestore().get_item(course.location.for_branch(BRANCH_NAME_PUBLISHED))
new_module = modulestore().get_item(chapter_locator)
with self.assertRaises(VersionConflictError): _fail = modulestore().create_child( user, new_course.location, 'chapter', fields={'display_name': 'chapter 3'}, )
self.assertGreater(len(block.children), 0, "meaningless test") moved_child = block.children.pop()
block = modulestore().get_item(locator) pre_def_id = block.definition_locator.definition_id pre_version_guid = block.location.version_guid
nodes = modulestore().get_items(reusable_location, qualifiers={'category': 'chapter'}) new_course_loc = modulestore().delete_item(nodes[0].location, self.user_id)
store.delete_course(refetch_course.id, user)
split_store = modulestore()
courses = split_store.get_courses(BRANCH_NAME_DRAFT)
chapter = modulestore().get_item(chapter.location.version_agnostic()) del chapter.visible_to_staff_only modulestore().update_item(chapter, self.user_id)
SplitModuleTest.modulestore = class_(
def render_to_template_mock(*args): pass
from collections import namedtuple import datetime BlockInfo = namedtuple('BlockInfo', 'block_id, category, fields, sub_tree')
ModuleStoreNoSettings.modulestore = class_(
def render_to_template_mock(*args): pass
self.xblock.location = Location("org", "import", "run", "category", "stubxblock")
self.xblock.test_content_field = "Explicitly set" self.xblock.test_settings_field = "Explicitly set" self.xblock.save()
self.assertEqual(new_version.location.course_key, target_location_namespace)
self.assertEqual(new_version.test_content_field, 'Explicitly set') self.assertEqual(new_version.test_settings_field, 'Explicitly set')
self.assertIn( 'test_content_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.content) ) self.assertIn( 'test_settings_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) )
self.xblock.location = Location("org", "import", "run", "category", "stubxblock")
self.xblock.save()
self.assertEqual(new_version.test_content_field, 'default value') self.assertEqual(new_version.test_settings_field, 'default value')
self.assertNotIn( 'test_content_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.content) ) self.assertNotIn( 'test_settings_field', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) )
self.xblock.location = Location("org", "import", "run", "category", "stubxblock") self.xblock.save()
self.assertNotIn( 'start', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) ) self.assertNotIn( 'graded', new_version.get_explicitly_set_fields_by_scope(scope=Scope.settings) )
self.xblock.location = Location("org", "import", "run", "category", "stubxblock")
target_location = self.xblock.location.replace(revision='draft') _update_module_location(self.xblock, target_location)
self.assertEqual(new_version.location, target_location)
for field in self.CONTENT_FIELDS + self.SETTINGS_FIELDS + self.CHILDREN_FIELDS: self.assertTrue(new_version.fields[field].is_set_on(new_version))
courses = ['toy', 'simple', 'simple_with_draft', 'test_unicode']
doc_store_config = { 'host': HOST, 'port': PORT, 'db': DB, 'collection': COLLECTION, } cls.add_asset_collection(doc_store_config)
import_course_from_xml( draft_store, 999, DATA_DIR, ['test_import_course'], static_content_store=content_store, do_import_static=False, verbose=True )
import_course_from_xml( draft_store, 999, DATA_DIR, ['test_import_course'], static_content_store=content_store, do_import_static=False, verbose=True, target_id=SlashSeparatedCourseKey('guestx', 'foo', 'bar') )
connection.drop_database(DB)
course_locations = self.draft_store.get_courses_for_wiki('toy') assert_equals(len(course_locations), 0)
self.content_store.find(location)
component = self.draft_store.get_item(location) self.assertEqual(component.published_on, published_date) self.assertEqual(component.published_by, published_by)
self.draft_store.delete_course(course.id, self.dummy_user)
self.assertEquals(self.draft_store.get_all_asset_metadata(course.id, 'asset'), [])
self.assertRaises(ItemNotFoundError, lambda: self.draft_store.get_all_asset_metadata(course_key, 'asset')[:1])
with patch('mongodb_proxy.MongoProxy') as mock_proxy: mock_proxy.return_value.alive.return_value = False useless_conn = MongoConnection('useless', 'useless', 'useless')
asset_deprecated = None ssck_deprecated = None
self.contentstore = MongoContentStore(HOST, DB, port=PORT)
self.contentstore.delete(asset_key)
__, count = self.contentstore.get_all_content_for_course(self.course2_key) self.assertEqual(count, len(self.course2_files))
self.assertEqual(set(subtree_roots_urls), set(expected_roots_urls))
self.bulk.update_definition(self.course_key, self.definition) self.assertConnCalls(call.insert_definition(self.definition, self.course_key))
self.bulk.insert_course_index(self.course_key, self.index_entry) self.assertConnCalls(call.insert_course_index(self.index_entry, self.course_key)) self.assertCacheNotCleared()
self.bulk._end_bulk_operation(self.course_key)
self.assertEquals(self.conn.get_definitions.call_count, 0)
with check_mongo_calls(import_reads, first_import_writes): import_course_from_xml( source_store, 'test_user', TEST_DATA_DIR, source_dirs=['manual-testing-complete'], static_content_store=source_content, target_id=source_course_key, create_if_not_present=True, raise_on_failure=True, )
import_course_from_xml( source_store, 'test_user', TEST_DATA_DIR, source_dirs=['manual-testing-complete'], static_content_store=source_content, target_id=source_course_key, create_if_not_present=True, raise_on_failure=True, )
for xblock in all_blocks: for __, field in xblock.fields.iteritems(): if field.is_set_on(xblock): __ = field.read_from(xblock)
assert_true(modulestore.has_course(locator, ignore_case))
{key_field: 'fake'}, {key_field: getattr(locator, key_field) + 'X'}, {key_field: 'X' + getattr(locator, key_field)},
fs_root = mkdtemp()
rmtree(fs_root, ignore_errors=True)
fs_root = mkdtemp()
rmtree(fs_root, ignore_errors=True)
store_iterator = iter(modulestores) next_modulestore = lambda *args, **kwargs: store_iterator.next()
stores = [{'NAME': name, 'ENGINE': 'This space deliberately left blank'} for name in names]
return store.asset_collection
return store.db_connection.structures
) DIRECT_MS_SETUPS_SHORT = ( 'mongo', #'split', ) MODULESTORE_SETUPS = DIRECT_MODULESTORE_SETUPS + MIXED_MODULESTORE_SETUPS MODULESTORE_SHORTNAMES = DIRECT_MS_SETUPS_SHORT + MIXED_MS_SETUPS_SHORT SHORT_NAME_MAP = dict(zip(MODULESTORE_SETUPS, MODULESTORE_SHORTNAMES))
with check_mongo_calls(4, 2):
item = self.draft_mongo.get_item(vert_location, 0) self.assertFalse(getattr(item, 'is_draft', False), "Item was published. Draft should not exist")
location = self.old_course_key.make_usage_key('discussion', block_id='Discussion1') self.draft_mongo.delete_item(location, self.user_id)
block_type = 'vertical' for idx in xrange(0, 8): block_id = _make_block_id(block_type, idx) self.all_verticals.append((block_type, block_id))
block_type = 'html' for idx in xrange(0, 16): block_id = _make_block_id(block_type, idx) self.all_units.append((block_type, block_id))
self.all_verticals = [] self.all_units = []
self.course_db = {}
if block_type == 'html': self.assertElementAttrsSubset(element, {'filename': filename})
self.assertParentReferences( element, course_key, **kwargs )
child_id_regex = None child_type = None if child_types_ids: child_type = child_types_ids[0][0] child_id_regex = '|'.join([child[1] for child in child_types_ids])
with MongoContentstoreBuilder().build() as self.contentstore: with modulestore_builder.build(contentstore=self.contentstore) as self.store: self._create_course(self.store) yield
export_course_to_xml( self.store, self.contentstore, self.course.id, self.root_export_dir, self.export_dir, )
self.publish((('html', 'html00'),))
with self.assertRaises(ItemNotFoundError): self.publish((('html', 'html00'),))
self.assertOLXIsDraftOnly(block_list_publish) self.assertOLXIsDraftOnly(block_list_untouched)
self.publish(block_list_parents_to_publish)
self.assertOLXIsPublishedOnly(block_list_publish) self.assertOLXIsDraftOnly(block_list_untouched)
self.assertOLXIsDraftOnly(block_list_to_unpublish) with self.assertRaises(ItemNotFoundError): self.unpublish(block_list_to_unpublish)
self.assertOLXIsDraftOnly(block_list_to_unpublish) with self.assertRaises(ItemNotFoundError): self.unpublish(block_list_to_unpublish)
(ModuleStoreEnum.RevisionOption.published_only, 'assertOLXIsDeleted'), (ModuleStoreEnum.RevisionOption.all, 'assertOLXIsDeleted'), (None, 'assertOLXIsDeleted'),
(ModuleStoreEnum.RevisionOption.published_only, 'assertOLXIsDraftOnly'), (ModuleStoreEnum.RevisionOption.all, 'assertOLXIsDeleted'), (None, 'assertOLXIsDeleted'),
self.assertOLXIsPublishedOnly(block_list_to_delete) self.delete_item(block_list_to_delete, revision=revision) self._check_for_item_deletion(block_list_to_delete, result) self.assertOLXIsDeleted(block_list_children)
self.assertOLXIsPublishedOnly(block_list_to_delete) self.delete_item(block_list_to_delete, revision=revision) self._check_for_item_deletion(block_list_to_delete, result) self.assertOLXIsDeleted(autopublished_children) self.assertOLXIsDeleted(block_list_draft_children)
self.assertOLXIsDraftOnly(block_list_to_revert) with self.assertRaises(InvalidVersionError): self.revert_to_published(block_list_to_revert)
self.assertOLXIsDraftOnly(block_list_to_revert) self.publish(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert) self.revert_to_published(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
self.assertOLXIsDraftOnly(block_list_to_revert) self.publish(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
self.assertOLXIsDraftAndPublished(block_list_to_revert) self.revert_to_published(block_list_to_revert) self.assertOLXIsPublishedOnly(block_list_to_revert)
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
mock_create.return_value = None
TEST_DATA_MONGO_MODULESTORE = functools.partial(mixed_store_config, mkdtemp_clean(), {})
TEST_DATA_SPLIT_MODULESTORE = functools.partial( mixed_store_config, mkdtemp_clean(), {}, store_order=[StoreConstructors.split, StoreConstructors.draft] )
multi_db = True
yield super(SharedModuleStoreTestCase, cls).setUpClass()
OverrideFieldData.provider_classes = None super(SharedModuleStoreTestCase, self).setUp()
multi_db = True
OverrideFieldData.provider_classes = None
self.user = User.objects.create_user(uname, email, self.user_password)
self.user.is_staff = True self.user.save()
new_module_store_setting['default']['OPTIONS']['stores'] = convert_old_stores_into_list( module_store_setting ) module_store_setting = new_module_store_setting
elif isinstance(get_mixed_stores(module_store_setting), dict): warnings.warn( "Using a dict for the Stores option in the MixedModuleStore is deprecated. Please use a list instead.", DeprecationWarning )
module_store_setting['default']['OPTIONS']['stores'] = convert_old_stores_into_list( get_mixed_stores(module_store_setting) ) assert isinstance(get_mixed_stores(module_store_setting), list)
mixed_stores.remove(store) mixed_stores.insert(0, store) return
super(CourseKeyField, self).__init__(**kwargs)
return course_key.to_deprecated_string()
rem_vers = kwargs.pop('remove_version', True) rem_branch = kwargs.pop('remove_branch', True)
retval = func(field_decorator=strip_key_collection, *args, **kwargs)
return strip_key_collection(retval)
for course_key, store_name in self.mappings.iteritems(): if store_name == key: self.mappings[course_key] = store self.modulestores.append(store)
return self.default_modulestore
if course_id in course_summaries: log.warning( u"Modulestore %s have duplicate courses %s; skipping from result.", store, course_id ) else: course_summaries[course_id] = course_summary
for course in store.get_courses(**kwargs): course_id = self._clean_locator_for_mapping(course.id) if course_id not in courses: courses[course_id] = course
for library in store.get_libraries(**kwargs): library_id = self._clean_locator_for_mapping(library.location) if library_id not in libraries: libraries[library_id] = library
for course_id, store in self.mappings.iteritems(): candidate_key = store.make_course_key(org, course, run) if candidate_key == course_id: return candidate_key
return self.default_modulestore.make_course_key(org, course, run)
source_store.copy_all_asset_metadata(source_course_key, dest_course_key, user_id)
course_key = self.make_course_key(org, course, run) if course_key in self.mappings and self.mappings[course_key].has_course(course_key): raise DuplicateCourseError(course_key, course_key)
store = self._verify_modulestore_support(None, 'create_course') course = store.create_course(org, course, run, user_id, **kwargs)
self.mappings[course_key] = store
lib_key = LibraryLocator(org=org, library=library) if lib_key in self.mappings: raise DuplicateCourseError(lib_key, lib_key)
store = self._verify_modulestore_support(None, 'create_library') library = store.create_library(org, library, user_id, fields, **kwargs)
self.mappings[lib_key] = store
dest_modulestore = self._get_modulestore_for_courselike(dest_course_id) if source_modulestore == dest_modulestore: return source_modulestore.clone_course(source_course_id, dest_course_id, user_id, fields, **kwargs)
super(MixedModuleStore, self).clone_course(source_course_id, dest_course_id, user_id, fields, **kwargs)
if hasattr(modulestore, '_drop_database'):
return dict( itertools.chain.from_iterable( store.heartbeat().iteritems() for store in self.modulestores ) )
return thread_local_default_store
return self.modulestores[0]
incxml = etree.XML(ifp.read())
parent.insert(parent.index(next_include), incxml)
msg = "Error in problem xml include: %s" % ( etree.tostring(next_include, pretty_print=True)) system.error_tracker(msg)
filename = '_' + fragment_name contents[filename] = fragment
EDX_XML_PARSER = XMLParser(dtd_validation=False, load_dtd=False, remove_comments=True, remove_blank_text=True, encoding='utf-8')
return value
filename_extension = 'xml'
metadata_translations = { 'slug': 'url_name', 'name': 'display_name', }
'course', 'org', 'url_name', 'filename', 'xml_attributes')
msg = 'Unable to load file contents at path %s for item %s: %s ' % ( filepath, def_id, err) raise Exception, msg, sys.exc_info()[2]
definition_xml.attrib.update(xml_object.attrib)
attr = cls._translate(attr)
continue
metadata['xml_attributes'][attr] = value
definition, children = cls.load_definition(definition_xml, runtime, def_id, id_generator)
if is_pointer_tag(node): definition['filename'] = [filepath, filepath]
cls.apply_policy(metadata, runtime.get_policy(usage_id))
ScopeIds(None, node.tag, def_id, usage_id), field_data,
xml_object.tag = self.category node.tag = self.category
if self.category == 'course': node.set('org', self.location.org) node.set('course', self.location.course)
return super(XmlDescriptor, cls).parse_xml( etree.fromstring(xml_data), system,
node = Element(self.category) super(XmlDescriptor, self).add_xml_to_node(node) return etree.tostring(node)
GENERAL_ASSET_TYPE = 'asset'
ALL_ASSETS_XML_TAG = 'assets'
ASSET_XML_TAG = 'asset'
EXPORTED_ASSET_DIR = 'assets'
EXPORTED_ASSET_FILENAME = 'assets.xml'
self.created_by = created_by self.created_by_email = created_by_email self.created_on = created_on or now self.fields = fields or {}
continue
value = True if value == "true" else False
value = None
value = dateutil.parser.parse(value)
value = int(value)
value = json.loads(value)
if attr == self.ASSET_TYPE_ATTR: value = self.asset_id.asset_type elif attr == self.ASSET_BASENAME_ATTR: value = self.asset_id.path else: value = getattr(self, attr)
etree.fromstring(etree.tostring(root), self.xmlparser)
conditions_map = {
'correct': 'is_correct',
log.warn('Error in conditional module: \ required module {module} has no {module_attr}'.format(module=module, module_attr=attr_name)) return False
self.required_html_ids = [descriptor.location.html_id() for descriptor in self.descriptor.get_required_module_descriptors()]
class_priority = ['video', 'problem']
stringified_sources_list = map(lambda loc: loc.to_deprecated_string(), self.sources_list) self.xml_attributes['sources'] = ';'.join(stringified_sources_list) return xml_object
try: import dogstats_wrapper as dog_stats_api except ImportError: dog_stats_api = None
_ = lambda text: text
NUM_RANDOMIZATION_BINS = 20 MAX_RANDOMIZATION_BINS = 1000
return int(r_hash.hexdigest()[:7], 16) % NUM_RANDOMIZATION_BINS
default=_("Blank Advanced Problem")
self.runtime.set('location', self.location.to_deprecated_string())
self.seed = randomization_bin(self.runtime.seed, unicode(self.location).encode('utf-8'))
self.seed %= MAX_RANDOMIZATION_BINS
if self.weight == 0: return None
score = score * self.weight / total total = self.weight
_ = self.runtime.service(self, "i18n").ugettext check = _('Check') final_check = _('Final Check')
if 'custom_check' in self.text_customization:
if 'custom_checking' in self.text_customization: return self.text_customization.get('custom_checking')
if self.closed() or submitted_without_reset: return False else: return True
if self.closed() and not is_survey_question: return False
if self.rerandomize in [RANDOMIZATION.ALWAYS, RANDOMIZATION.ONRESET] and self.is_submitted(): return True else: if self.is_correct(): return False else: return self.show_reset_button
if self.force_save_button: return not self.closed() else: is_survey_question = (self.max_attempts == 0) needs_reset = self.is_submitted() and self.rerandomize == RANDOMIZATION.ALWAYS
elif (self.closed() and not is_survey_question) or needs_reset: return False else: return True
student_answers = self.lcp.student_answers answer_ids = student_answers.keys()
self.lcp = self.new_lcp(None) self.set_state_from_lcp()
warning_msg = _("The problem's state was corrupted by an invalid submission. The submission consisted of:") warning += warning_msg + '<ul>'
log.exception("Unable to generate html from LoncapaProblem") raise
prefix = _('Hint ({hint_num} of {hints_count}): ').format(hint_num=hint_index + 1, hints_count=len(demand_hints))
return { 'success': True, 'contents': prefix + hint_text, 'hint_index': hint_index }
if self.should_show_check_button(): check_button = self.check_button_name() check_button_checking = self.check_button_checking_name() else: check_button = False check_button_checking = False
demand_hints = self.lcp.tree.xpath("//problem/demandhint/hint") demand_hint_possible = len(demand_hints) > 0
return html
return self.lcp.done
return True
return self.lcp.done
self.lcp.ungraded_response(score_msg, queuekey) self.set_state_from_lcp() return dict()
self.set_state_from_lcp() return response
if not name: raise ValueError(u"{key} must contain at least one underscore".format(key=key))
except(KeyError, ValueError): raise ValueError( u"Invalid submission: {val} for {key}".format(val=data[key], key=key) )
if name in answers: raise ValueError(u"Key {name} already exists in answers dict".format(name=name)) else: answers[name] = val
current_time = datetime.datetime.now(UTC()) if override_time is not False: current_time = override_time
if self.lcp.is_queued(): prev_submit_time = self.lcp.get_recentmost_queuetime()
self.set_state_from_lcp()
if self.runtime.user_is_staff: msg = u"Staff debug info: {tb}".format(tb=cgi.escape(traceback.format_exc()))
else: msg = _(u"Error: {msg}").format(msg=inst.message)
self.set_state_from_lcp()
success = 'correct' for answer_id in correct_map: if not correct_map.is_correct(answer_id): success = 'incorrect'
html = self.get_problem_html(encapsulate=False)
event_unmasked = copy.deepcopy(event_info) self.unmask_event(event_unmasked) self.runtime.publish(self, title, event_unmasked)
answer = event_info.get('answers', {}).get(response.answer_id) if answer is not None: event_info['answers'][response.answer_id] = response.unmask_name(answer)
permutation_option = None if response.has_shuffle(): permutation_option = 'shuffle' elif response.has_answerpool(): permutation_option = 'answerpool'
if 'permutation' not in event_info: event_info['permutation'] = {} event_info['permutation'][response.answer_id] = (permutation_option, response.unmask_order())
log.exception('Unable to gather submission metadata, it will not be included in the event.')
raise NotImplementedError(_("Problem's definition does not support rescoring."))
orig_score = self.lcp.get_score() event_info['orig_score'] = orig_score['score'] event_info['orig_total'] = orig_score['total']
self.set_state_from_lcp()
success = 'correct' for answer_id in correct_map: if not correct_map.is_correct(answer_id): success = 'incorrect'
event_info['correct_map'] = correct_map.get_dict() event_info['success'] = success event_info['attempts'] = self.attempts self.track_function_unmask('problem_rescore', event_info)
'error': _("Problem is closed."),
'error': _("Refresh the page and make an attempt before resetting."),
self.choose_new_seed()
self.lcp = self.new_lcp(None)
self.set_state_from_lcp()
_ = lambda text: text
custom_parameters = {}
if param_name not in PARAMETERS: param_name = 'custom_' + param_name
u'resource_link_id': self.get_resource_link_id(), u'lis_result_sourcedid': self.get_lis_result_sourcedid(),
body.update(custom_parameters)
'Content-Type': 'application/x-www-form-urlencoded',
params = dict([param.strip().replace('"', '').split('=') for param in params.split(',')])
params[u'oauth_signature'] = urllib.unquote(params[u'oauth_signature']).decode('utf8')
params.update(body) return params
score = float(score) if not 0 <= score <= 1: raise LTIError('score value outside the permitted range of 0-1.')
num_choices = len(self.descriptor.get_children())
self.choice = None
log.debug("children of randomize module (should be only 1): %s", self.child)
return Fragment(content=u"<div>Nothing to randomize between</div>")
module_class = RandomizeModule resources_dir = None
return self.system.render_template('module-error.html', { 'staff_access': True, 'data': self.contents, 'error': self.error_msg, })
return self.system.render_template('module-error.html', { 'staff_access': False, 'data': "", 'error': "", })
error_msg = 'Error not available'
error_msg = exc_info_to_str(sys.exc_info())
assert library.location.library_key.version_guid is not None return library.location.library_key.version_guid
source_blocks.extend(self._problem_type_filter(library, dest_block.capa_type))
import logging
metadata_translations = dict(RawDescriptor.metadata_translations) metadata_translations['attempts'] = 'max_attempts'
fragment = Fragment( self.system.render_template(self.mako_template, self.get_context()) ) shim_xmodule_js(self, fragment) return fragment
request.url = 'http://testurl/' self.xmodule.verify_oauth_body_sign(request)
"Test for Annotation Xmodule functional logic."
def test_real_user(useless): useless_user = Mock(email='fake@fake.com', id=useless) return useless_user
def test_user_role(): return 'staff'
response = self.ajax_request('bad_answer', {}) self.assertDictEqual(response, {'error': 'Unknown Command!'})
response = self.ajax_request('No', {})
overflow_grader = graders.AssignmentFormatGrader("Lab", 3, 2) lab_grader = graders.AssignmentFormatGrader("Lab", 7, 3)
homework_grader = graders.AssignmentFormatGrader("Homework", 12, 2) homework_grader2 = graders.grader_from_conf(homework_grader)
course = xml.CourseFactory.build() sequence = xml.SequenceFactory.build(parent=course) vertical = xml.VerticalFactory.build(parent=sequence)
self.assertTrue(self.lc_block.has_dynamic_children())
self.assertEqual(len(self.lc_block.get_child_descriptors()), 1) self.assertEqual(len(self.lc_block.get_content_titles()), 1)
self.lc_block.source_library_id = "" result = self.lc_block.validate()
self.lc_block.source_library_id = "library-v1:BAD+WOLF" result = self.lc_block.validate()
self.lc_block.source_library_id = unicode(self.library.location.library_key) result = self.lc_block.validate()
self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
self.lc_block.max_count = 50 self.lc_block.refresh_children() result = self.lc_block.validate()
self.lc_block.max_count = 1 self._create_capa_problems() self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
self.lc_block.max_count = 1 self.lc_block.capa_type = 'multiplechoiceresponse' self.lc_block.refresh_children() self.assertTrue(self.lc_block.validate())
self.lc_block.max_count = 10 self.lc_block.capa_type = 'multiplechoiceresponse' self.lc_block.refresh_children() result = self.lc_block.validate()
self.lc_block.max_count = 1 self.lc_block.capa_type = 'customresponse' self.lc_block.refresh_children() result = self.lc_block.validate()
self.lc_block = self.store.get_item(self.lc_block.location) self._bind_course_module(self.lc_block) self.lc_block.xmodule_runtime.publish = self.publisher
course_usage_main_vertical = self.lc_block.children[0] course_usage_inner_vertical = self.store.get_item(course_usage_main_vertical).children[0] inner_vertical_in_course = self.store.get_item(course_usage_inner_vertical) course_usage_html = inner_vertical_in_course.children[0] course_usage_problem = inner_vertical_in_course.children[1]
self.lc_block.get_child_descriptors() event_data = self._assert_event_was_published("assigned")
del self.lc_block._xmodule._selected_set
del self.lc_block._xmodule._selected_set initial_blocks_assigned = self.lc_block.get_child_descriptors() self.assertEqual(len(initial_blocks_assigned), 2)
"original_usage_version": None, "descendants": [],
system = self.get_system() descriptor = system.process_xml(xml_str_in)
node = etree.Element('unknown') descriptor.add_xml_to_node(node)
self.assertEqual(node.tag, 'sequential')
print(descriptor, descriptor._field_data) self.assertEqual(descriptor.due, ImportTestCase.date.from_json(from_date_string))
descriptor.runtime.export_fs = MemoryFS() node = etree.Element('unknown') descriptor.add_xml_to_node(node)
with descriptor.runtime.export_fs.open('course/{url_name}.xml'.format(url_name=url_name)) as f: course_xml = etree.fromstring(f.read())
self.assertNotIn('course', course_xml.attrib) self.assertNotIn('org', course_xml.attrib)
self.assertNotIn('url_name', course_xml.attrib)
descriptor = descriptor.get_children()[0] self.course_descriptor_no_inheritance_check(descriptor)
child = descriptor.get_children()[0] self.assertEqual(child.due, None)
self.assertLessEqual( datetime.datetime.now(UTC()), child.start )
self.assertEqual( ImportTestCase.date.to_json(ImportTestCase.date.from_json(course_due)), child.xblock_kvs.inherited_settings['due'] )
child._field_data.set(child, 'due', child_due) compute_inherited_metadata(descriptor) self.override_metadata_check(descriptor, child, course_due, child_due)
self.assertEqual(two_toys.grade_cutoffs['C'], 0.5999)
self.assertEqual(toy.graded, True)
modulestore = XMLModuleStore(DATA_DIR, source_dirs=['toy']) courses = modulestore.get_courses() self.assertEquals(len(courses), 1) course = courses[0]
print "video {0} url_name: {1}".format(i, video.url_name)
self.assertFalse(course.is_cohorted)
course.cohort_config = {} self.assertFalse(course.is_cohorted)
course.cohort_config = {'cohorted': False} self.assertFalse(course.is_cohorted)
course.cohort_config = {'cohorted': True} self.assertTrue(course.is_cohorted)
self.xmodule.is_condition_satisfied = lambda: True self.xmodule.descriptor.get_children = lambda: []
assert_equals(out.count("But it is "), 1)
TestScenario( (self.demo_course.id, '='), "course_MVSFQL2EMVWW6WBOGEXUMYLMNRPTEMBRGQ======" ), TestScenario( (self.html_course.id, '~'), "course_MNXXK4TTMUWXMMJ2KVXGS5TFOJZWS5DZLAVUGUZNGIYDGK2ZGIYDSNQ~" ),
TestScenario((self.demo_course,), "Empty"), TestScenario((self.html_course,), "Intro to &lt;html&gt;"),
TestScenario((self.demo_course,), "Empty"), TestScenario((self.html_course,), "Intro to <html>"),
with self.assertRaises(ValueError): mock_strftime_localized(test_datetime, 'BAD_FORMAT_SPECIFIER')
DATA_DIR = MODULE_DIR.parent.parent.parent.parent / "test" / "data"
def get_asides(self, block): return []
module_system = get_test_system()
self.__manager = None
manager = self.__manager self.__manager = None yield
exc_type, exc_value, exc_tb = sys.exc_info()
relevant_frames = 0 for frame_record in inspect.stack(): frame = frame_record[0] if '__unittest' in frame.f_globals: break relevant_frames += 1
with self._capture_assertion_errors(): context = assertion(*args, **kwargs)
if context is not None: return nested(self._capture_assertion_errors(), context)
self.assertIn(map_key(actual_item_location), actual_item_map.keys())
self.assertEqual(expected_item.fields, actual_item.fields)
if field_name == 'children': continue
continue
with assert_raises(ValueError): course = self.process_xml(CourseFactory.build(policy={'days_early_for_beta': 'null'}))
XML_IMPORT_ARGS = inspect.getargspec(XmlImportData.__init__).args
inline_xml = kwargs.pop('inline_xml')
root = CourseFactory.build(days_early_for_beta="null") sequence = SequenceFactory.build(parent=root) ProblemFactory.build(parent=sequence)
return getattr(super(BulkAssertionTest, self), 'assert' + assertion_name)(*args, **kwargs)
course = xml.CourseFactory.build() sequence = xml.SequenceFactory.build(parent=course) split_test = SplitTestModuleFactory( parent=sequence, attribs={ 'user_partition_id': '0',
self.assertIn(self.split_test_module.child_descriptor.url_name, ['split_test_cond0', 'split_test_cond1'])
@patch('xmodule.html_module.HtmlDescriptor.definition_to_xml') def test_export_import_round_trip(self, def_to_xml): def_to_xml.return_value = lxml.etree.Element('html')
self.module_system.process_xml = Mock()
xml_obj = self.split_test_module.definition_to_xml(MemoryFS())
self.assertIn(SplitTestDescriptor.user_partition_id.name, editable_metadata_fields)
self.split_test_module.user_partition_id = SplitTestFields.no_partition_selected['value']
self.split_test_module.user_partition_id = 0
self.split_test_module.user_partition_id = 999
split_test_module.user_partition_id = -1 [active_children, inactive_children] = split_test_module.active_and_inactive_children() self.assertEqual(active_children, []) self.assertEqual(inactive_children, children)
self.split_test_module.user_partition_id = 2 [active_children, inactive_children] = split_test_module.active_and_inactive_children() self.assertEqual(active_children, []) self.assertEqual(inactive_children, children)
field_data['attempts'] = int(attempts)
self.two_day_delta_str = "2 days"
problem = CapaFactory.create() self.assertFalse(problem.answer_available())
used_all_attempts = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertTrue(used_all_attempts.answer_available())
after_due_date = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="0", due=self.yesterday_str)
attempts_left_open = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="0", due=self.tomorrow_str) self.assertFalse(attempts_left_open.answer_available())
still_in_grace = CapaFactory.create(showanswer='closed', max_attempts="1", attempts="0", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertFalse(still_in_grace.answer_available())
answer_correct = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="0", due=self.tomorrow_str, correct=True) self.assertTrue(answer_correct.answer_available())
past_due_date = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(past_due_date.answer_available())
past_due_date_correct = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="0", due=self.yesterday_str, correct=True) self.assertTrue(past_due_date_correct.answer_available())
still_in_grace = CapaFactory.create(showanswer='correct_or_past_due', max_attempts="1", attempts="1", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertFalse(still_in_grace.answer_available())
used_all_attempts = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertFalse(used_all_attempts.answer_available())
past_due_date = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(past_due_date.answer_available())
attempts_left_open = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="0", due=self.tomorrow_str) self.assertFalse(attempts_left_open.answer_available())
still_in_grace = CapaFactory.create(showanswer='past_due', max_attempts="1", attempts="1", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertFalse(still_in_grace.answer_available())
used_all_attempts = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="1", due=self.tomorrow_str) self.assertTrue(used_all_attempts.answer_available())
past_due_date = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(past_due_date.answer_available())
attempts_left_open = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="0", due=self.tomorrow_str) self.assertFalse(attempts_left_open.answer_available())
correct_ans = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="0", due=self.tomorrow_str, correct=True) self.assertTrue(correct_ans.answer_available())
still_in_grace = CapaFactory.create(showanswer='finished', max_attempts="1", attempts="1", due=self.yesterday_str, graceperiod=self.two_day_delta_str) self.assertTrue(still_in_grace.answer_available())
module = CapaFactory.create(max_attempts="1", attempts="0") self.assertFalse(module.closed())
module = CapaFactory.create(max_attempts="2", attempts="1") self.assertFalse(module.closed())
module = CapaFactory.create(max_attempts="1", attempts="1") self.assertTrue(module.closed())
module = CapaFactory.create(max_attempts="1", attempts="2") self.assertTrue(module.closed())
module = CapaFactory.create(max_attempts="0", attempts="2") self.assertTrue(module.closed())
module = CapaFactory.create(max_attempts="1", attempts="0", due=self.yesterday_str) self.assertTrue(module.closed())
valid_get_dict = MultiDict({'input_1[]': 'test'}) result = CapaModule.make_dict_of_responses(valid_get_dict) self.assertEqual(result['1'], ['test'])
invalid_get_dict = MultiDict({'input': 'test'}) with self.assertRaises(ValueError): result = CapaModule.make_dict_of_responses(invalid_get_dict)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.check_problem(get_request_dict)
self.assertEqual(result['success'], 'correct')
self.assertEqual(result['contents'], 'Test HTML')
self.assertEqual(module.attempts, 2)
with patch('capa.correctmap.CorrectMap.is_correct') as mock_is_correct: mock_is_correct.return_value = False
get_request_dict = {CapaFactory.input_key(): '0'} result = module.check_problem(get_request_dict)
self.assertEqual(result['success'], 'incorrect')
self.assertEqual(module.attempts, 1)
self.assertEqual(module.attempts, 3)
module = CapaFactory.create(rerandomize=rerandomize, attempts=0)
module.done = True
with self.assertRaises(xmodule.exceptions.NotFoundError): get_request_dict = {CapaFactory.input_key(): '3.14'} module.check_problem(get_request_dict)
self.assertEqual(module.attempts, 0)
module = CapaFactory.create(rerandomize=rerandomize, attempts=0, done=True)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.check_problem(get_request_dict)
self.assertEqual(module.attempts, 1)
self.assertIn('You must wait', result['success'])
self.assertEqual(module.attempts, 1)
get_request_dict = { CapaFactoryWithFiles.input_key(response_num=2): fileobjs, CapaFactoryWithFiles.input_key(response_num=3): 'None', }
exception_classes = [StudentInputError, LoncapaProblemError, ResponseError] for exception_class in exception_classes:
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = False
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: mock_grade.side_effect = exception_class('test error')
expected_msg = 'Error: test error' self.assertEqual(expected_msg, result['success'])
self.assertEqual(module.attempts, 1)
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = False
module.system.DEBUG = True
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: error_msg = u"Superterrible error happened: ☠" mock_grade.side_effect = Exception(error_msg)
self.assertIn(error_msg, result['success'])
module = CapaFactory.create(attempts=1)
module.lcp.get_score = lambda: {'score': 0, 'total': 0}
get_request_dict = {CapaFactory.input_key(): '3.14'} module.check_problem(get_request_dict)
exception_classes = [StudentInputError, LoncapaProblemError, ResponseError] for exception_class in exception_classes:
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = False
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: mock_grade.side_effect = exception_class(u"ȧƈƈḗƞŧḗḓ ŧḗẋŧ ƒǿř ŧḗşŧīƞɠ")
expected_msg = u'Error: ȧƈƈḗƞŧḗḓ ŧḗẋŧ ƒǿř ŧḗşŧīƞɠ' self.assertEqual(expected_msg, result['success'])
self.assertEqual(module.attempts, 1)
for exception_class in [StudentInputError, LoncapaProblemError, ResponseError]:
module = CapaFactory.create(attempts=1)
module.system.user_is_staff = True
with patch('capa.capa_problem.LoncapaProblem.grade_answers') as mock_grade: mock_grade.side_effect = exception_class('test error')
self.assertIn('test error', result['success'])
self.assertIn('Traceback', result['success'])
self.assertEqual(module.attempts, 1)
with patch('xmodule.capa_module.CapaModule.get_problem_html') as mock_html: mock_html.return_value = "<div>Test HTML</div>"
get_request_dict = {} result = module.reset_problem(get_request_dict)
self.assertTrue('success' in result and result['success'])
self.assertIn('html', result) self.assertEqual(result['html'], "<div>Test HTML</div>")
module.new_lcp.assert_called_once_with(None)
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS)
with patch('xmodule.capa_module.CapaModule.closed') as mock_closed: mock_closed.return_value = True
get_request_dict = {} result = module.reset_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
module = CapaFactory.create(done=False)
get_request_dict = {} result = module.reset_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
with patch('capa.responsetypes.LoncapaResponse.evaluate_answers') as mock_evaluate_answers: mock_evaluate_answers.return_value = CorrectMap(CapaFactory.answer_key(), 'correct') result = module.rescore_problem()
self.assertEqual(result['success'], 'correct')
self.assertNotIn('contents', result)
self.assertEqual(module.attempts, 1)
module = CapaFactory.create(attempts=0, done=True)
with patch('capa.responsetypes.LoncapaResponse.evaluate_answers') as mock_evaluate_answers: mock_evaluate_answers.return_value = CorrectMap(CapaFactory.answer_key(), 'incorrect') result = module.rescore_problem()
self.assertEqual(result['success'], 'incorrect')
self.assertEqual(module.attempts, 0)
module = CapaFactory.create(done=False)
with self.assertRaises(xmodule.exceptions.NotFoundError): module.rescore_problem()
with patch('capa.capa_problem.LoncapaProblem.supports_rescoring') as mock_supports_rescoring: mock_supports_rescoring.return_value = False with self.assertRaises(NotImplementedError): module.rescore_problem()
module = CapaFactory.create(attempts=1, done=True)
with patch('capa.capa_problem.LoncapaProblem.rescore_existing_answers') as mock_rescore: mock_rescore.side_effect = exception_class(u'test error \u03a9') result = module.rescore_problem()
expected_msg = u'Error: test error \u03a9' self.assertEqual(result['success'], expected_msg)
self.assertEqual(module.attempts, 1)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
expected_answers = {CapaFactory.answer_key(): '3.14'} self.assertEqual(module.lcp.student_answers, expected_answers)
self.assertTrue('success' in result and result['success'])
with patch('xmodule.capa_module.CapaModule.closed') as mock_closed: mock_closed.return_value = True
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
module = CapaFactory.create(rerandomize=rerandomize, done=True)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
self.assertTrue('success' in result and not result['success'])
module = CapaFactory.create(rerandomize=rerandomize, done=True)
get_request_dict = {CapaFactory.input_key(): '3.14'} result = module.save_problem(get_request_dict)
self.assertTrue('success' in result and result['success'])
attempts = random.randint(1, 10) module = CapaFactory.create(attempts=attempts - 1, max_attempts=attempts) self.assertEqual(module.check_button_name(), "Final Check")
module = CapaFactory.create(attempts=attempts - 2, max_attempts=attempts) self.assertEqual(module.check_button_name(), "Check")
module = CapaFactory.create(attempts=attempts - 3) self.assertEqual(module.check_button_name(), "Check")
module = CapaFactory.create(due=self.yesterday_str) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create(attempts=attempts, max_attempts=attempts) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create(max_attempts=0) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertFalse(module.should_show_check_button())
module = CapaFactory.create() self.assertTrue(module.should_show_check_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.NEVER, done=True) self.assertTrue(module.should_show_check_button())
module = CapaFactory.create(due=self.yesterday_str, done=True) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(attempts=attempts, max_attempts=attempts, done=True) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, max_attempts=0, done=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, max_attempts=0, done=True, correct=False) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.NEVER, max_attempts=0, done=True, correct=True) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, max_attempts=0, done=True, correct=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, show_reset_button=False, done=False) self.assertFalse(module.should_show_reset_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, show_reset_button=False, done=True) self.assertTrue(module.should_show_reset_button())
module = CapaFactory.create(due=self.yesterday_str, done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(attempts=attempts, max_attempts=attempts, done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(max_attempts=None, rerandomize=RANDOMIZATION.NEVER, done=False) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.ALWAYS, done=False) self.assertTrue(module.should_show_save_button())
module = CapaFactory.create(rerandomize=RANDOMIZATION.NEVER, max_attempts=2, done=True) self.assertTrue(module.should_show_save_button())
module = CapaFactory.create(max_attempts=0, done=False) self.assertTrue(module.should_show_save_button())
module = CapaFactory.create(due=self.yesterday_str, force_save_button="true", done=True) self.assertFalse(module.should_show_save_button())
attempts = random.randint(1, 10) module = CapaFactory.create(attempts=attempts, max_attempts=attempts, force_save_button="true", done=True) self.assertFalse(module.should_show_save_button())
module = CapaFactory.create(force_save_button="true", rerandomize=RANDOMIZATION.ALWAYS, done=True) self.assertTrue(module.should_show_save_button())
module.system.render_template = Mock(return_value="<div>Test Template HTML</div>")
with patch('capa.capa_problem.LoncapaProblem.get_html') as mock_html: mock_html.return_value = "<div>Test Problem HTML</div>"
html = module.get_problem_html(encapsulate=False)
html_encapsulated = module.get_problem_html(encapsulate=True)
self.assertEqual(html, "<div>Test Template HTML</div>")
render_args, _ = module.system.render_template.call_args self.assertEqual(len(render_args), 2)
self.assertIn(html, html_encapsulated)
module = CapaFactory.create(xml=self.demand_xml)
module.location = Mock(module.location) module.location.to_deprecated_string.return_value = 'i4x://edX/capa_test/problem/meh'
module1.set_state_from_lcp() self.assertEqual(module1.lcp.inputs.keys(), module1.input_state.keys())
original_problem = module.lcp
module.lcp.get_html = Mock(side_effect=Exception("Test"))
module.system.render_template = Mock(return_value="<div>Test Template HTML</div>")
module.system.DEBUG = False
html = module.get_problem_html()
render_args, _ = module.system.render_template.call_args context = render_args[1] self.assertIn("error", context['problem']['html'])
self.assertNotEqual(original_problem, module.lcp)
error_msg = u"Superterrible error happened: ☠" module.lcp.get_html = Mock(side_effect=Exception(error_msg))
module.system.render_template = Mock(return_value="<div>Test Template HTML</div>")
module.system.DEBUG = True
html = module.get_problem_html()
render_args, _ = module.system.render_template.call_args context = render_args[1] self.assertIn(error_msg, context['problem']['html'])
seed = module.seed self.assertTrue(seed is not None)
if rerandomize == RANDOMIZATION.NEVER: self.assertEqual(seed, 1, msg="Seed should always be 1 when rerandomize='%s'" % rerandomize)
get_request_dict = {CapaFactory.input_key(): '3.14'} module.check_problem(get_request_dict)
self.assertEqual(seed, module.seed)
module.save_problem(get_request_dict)
self.assertEqual(seed, module.seed)
module.reset_problem({})
return module.seed
seed = module.seed self.assertTrue(seed is not None)
if rerandomize in [RANDOMIZATION.NEVER, 'false', RANDOMIZATION.PER_STUDENT]: self.assertEqual(seed, _reset_and_get_seed(module))
else:
success = _retry_and_check(5, lambda: _reset_and_get_seed(module) != seed)
module.reset_problem({})
return module.seed
seed = module.seed self.assertTrue(seed is not None)
i = 200 while i > 0: module = CapaFactory.create(rerandomize=rerandomize) assert 0 <= module.seed < 1000 i -= 1
xml = ''.join(line.strip() for line in xml.split('\n')) factory = self.capa_factory_for_problem_xml(xml) module = factory.create()
mock_call = mock_track_function.mock_calls[-1] event = mock_call[1][2]
xqueue_interface = XQueueInterface("http://example.com/xqueue", Mock())
field_data['attempts'] = int(attempts)
module.get_score = lambda: {'score': 1, 'total': 1}
self.assertEqual(result['success'], 'correct') self.assertEqual(module.attempts, num_attempts + 1)
try: info_module.get_html() except ValueError: self.fail("CourseInfoModule could not parse an invalid date!")
self.assertRaises(ValueError, Progress, 0, 0) self.assertRaises(ValueError, Progress, 2, 0) self.assertRaises(ValueError, Progress, 1, -2)
self.assertRaises(TypeError, Progress, 2j, 3)
self.assertFalse(self.done.inprogress()) self.assertFalse(self.not_started.inprogress())
self.assertEqual(f(None), "0")
self.assertNotEqual(prg1, prg2) self.assertEqual(prg1, prg3)
'html5_sources': ['http://www.example.com/source.mp4'], 'data': '',
'html5_sources': ['http://www.example.com/source.mp4'], 'data': ''
'html5_sources': ['http://www.example.com/source.mp4'], 'data': ''
mock_val_api.ValVideoNotFoundError = _MockValVideoNotFoundError mock_val_api.export_to_xml = Mock(side_effect=mock_val_api.ValVideoNotFoundError) self.descriptor.edx_video_id = 'test_edx_video_id'
expected = '<video url_name="SampleProblem" download_video="false"/>\n' self.assertEquals(expected, etree.tostring(xml, pretty_print=True))
'API': 'www.youtube.com/iframe_api',
'METADATA_URL': 'www.googleapis.com/youtube/v3/videos/',
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
d = get_dummy_course('2012-12-02T12:00') self.assertEqual('', d.end_datetime_text())
course = get_dummy_course('2012-12-02T12:00') self.assertEqual('', course.end_datetime_text("DATE_TIME"))
self.assertFalse(self.course.teams_enabled)
self.add_team_configuration(max_team_size=4, topics=[self.make_topic()]) self.assertTrue(self.course.teams_enabled)
self.add_team_configuration(max_team_size=4, topics=[]) self.assertFalse(self.course.teams_enabled)
ItemFactory.create( category="html", parent_location=library.location, user_id=self.user_id, publish_item=False, modulestore=self.store, data=message ) library = self.store.get_library(library.location.library_key)
randomize_module = RandomizeModule( randomize_descriptor, self.system, scope_ids=ScopeIds(None, None, self.course.id, self.course.id) )
print "Starting export" file_system = OSFS(root_dir) initial_course.runtime.export_fs = file_system.makeopendir(course_dir) root = lxml.etree.Element('root')
strip_filenames(initial_course) strip_filenames(exported_course)
NOT_STUDIO_EDITABLE = ( PollDescriptor, )
if depth == 0: self.get_module.side_effect = lambda x: LeafModuleFactory(descriptor_cls=HtmlDescriptor) else: self.get_module.side_effect = lambda x: ContainerModuleFactory( descriptor_cls=VerticalBlock, depth=depth - 1 )
if depth == 0: self.load_item.side_effect = lambda x: LeafModuleFactory(descriptor_cls=HtmlDescriptor) else: self.load_item.side_effect = lambda x: ContainerModuleFactory( descriptor_cls=VerticalBlock, depth=depth - 1 )
@ddt.data(*flatten(CONTAINER_XMODULES))
@ddt.data(*flatten(CONTAINER_XMODULES))
"Test for Annotation Xmodule functional logic."
def test_real_user(useless): useless_user = Mock(email='fake@fake.com', id=useless) return useless_user
def test_user_role(): return 'staff'
self.assertTrue(self.xmodule.verify_oauth_body_sign.called)
([
u'{"@type": "Result", "resultScore": 0.1}',
def test_real_user(useless): useless_user = Mock(email='fake@fake.com', id=useless) return useless_user
def test_user_role(): return 'staff'
block = self.get_a_block() self.assertEqual(block.inherited, "the default") self.assertEqual(block.not_inherited, "nothing")
parent = self.get_a_block(usage_id="parent") parent.inherited = "Changed!" self.assertEqual(parent.inherited, "Changed!")
parent = self.get_a_block(usage_id="parent") parent.not_inherited = "Changed!" self.assertEqual(parent.not_inherited, "Changed!")
self.assertEqual(1, len(editable_fields), editable_fields) self.assert_field_values( editable_fields, 'display_name', XModuleMixin.display_name, explicitly_set=False, value=None, default_value=None )
def get_xml_editable_fields(self, field_data): runtime = get_test_descriptor_system() return runtime.construct_xblock_from_class( XmlDescriptor, scope_ids=Mock(), field_data=field_data, ).editable_metadata_fields
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true') self.assertDeserializeEqual(-2.78, '-2.78')
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true')
self.assertDeserializeEqual('"false"', '"false"') self.assertDeserializeNonString()
self.assertDeserializeEqual(False, 'false') self.assertDeserializeEqual(True, 'true')
self.assertDeserializeEqual('False', 'False') self.assertDeserializeEqual('True', 'True')
self.assertDeserializeEqual(-2.78, '-2.78')
self.assertDeserializeEqual('10:20:30', '"10:20:30"')
assert_false(hasattr(SequenceDescriptor, 'rerandomize'))
assert_equals('never', seq.rerandomize)
assert_not_in('rerandomize', seq.xml_attributes)
assert_false(hasattr(SequenceDescriptor, 'attempts'))
assert_false(hasattr(seq, 'attempts'))
assert_in('attempts', seq.xml_attributes)
assert_false(hasattr(SequenceDescriptor, attribute))
assert_true(hasattr(InheritanceMixin, attribute))
assert_in(InheritanceMixin, root.xblock_mixins)
assert_equals(value, getattr(seq, attribute))
assert_not_in(attribute, seq.xml_attributes)
content = StaticContent('loc', 'name', 'content_type', 'data', None, None, None) self.assertIsNone(content.thumbnail_location)
return {'cond_module': cond_descriptor, 'source_module': source_descriptor, 'child_module': child_descriptor}
location = Location("HarvardX", "ER22x", "2013_Spring", "conditional", "condone")
'ajax_url': '{}/xmodule_handler'.format(location.to_deprecated_string()), 'element_id': u'i4x-HarvardX-ER22x-conditional-condone', 'depends': u'i4x-HarvardX-ER22x-problem-choiceprob'
inner_module = inner_get_module(location.replace(category="problem", name='choiceprob')) inner_module.attempts = 1 inner_module.save()
import collections import json import logging from pkg_resources import resource_string
class_priority = ['video', 'problem']
_ = lambda text: text
position = getattr(self.system, 'position', None) if position is not None: assert isinstance(position, int) self.position = self.system.position
self._capture_basic_metrics()
if self.is_time_limited: view_html = self._time_limited_student_view(context)
if view_html: fragment.add_content(view_html) return fragment
return fragment
newrelic.agent.add_custom_parameter('seq.num_units', len(display_items))
all_item_keys = self._locations_in_subtree(self) newrelic.agent.add_custom_parameter('seq.num_items', len(all_item_keys))
view_html = None
if credit_service: credit_state = credit_service.get_credit_state(user_id, course_id) if credit_state: context.update({ 'credit_state': credit_state })
view_html = proctoring_service.get_student_view( user_id=user_id, course_id=course_id, content_id=content_id, context=context, user_role=user_role_in_course )
_ = lambda text: text
msg = "No valid user id found in endpoint URL" log.info("[LTI]: {}".format(msg)) raise LTIError(msg)
base_json_obj['resultScore'] = round(self.module_score, 2) base_json_obj['comment'] = self.score_comment return Response(json.dumps(base_json_obj), content_type=LTI_2_0_JSON_CONTENT_TYPE)
if score is None: self.clear_user_module_score(real_user) return Response(status=200)
self.set_user_module_score(real_user, score, self.max_score(), comment) return Response(status=200)
try:
return time.strftime('%Y-%m-%dT%H:%M:%SZ', value)
return value.isoformat()
MUTABLE = False
MUTABLE = False
if isinstance(value, float): return datetime.timedelta(seconds=value)
_ = lambda text: text
raw_student_words = data.getall('student_words[]') student_words = filter(None, map(self.good_word, raw_student_words))
temp_all_words = self.all_words
for word in self.student_words: temp_all_words[word] = temp_all_words.get(word, 0) + 1
self.top_words = self.top_dict( temp_all_words, self.num_top_words )
self.all_words = temp_all_words
self.import_path = import_path self.locked = locked
url_path = StaticContent.serialize_asset_key_with_slash( course_key.make_asset_key('asset', placeholder_id).for_branch(None) ) return url_path.replace(placeholder_id, '')
if path.startswith('/static/'): path = path[len('/static/'):]
return StaticContent.compute_location(course_key, path)
_, _, relative_path, params, query_string, fragment = urlparse(path)
asset_key = StaticContent.get_asset_key_from_path(course_key, relative_path)
if any(relative_path.lower().endswith(excluded_ext.lower()) for excluded_ext in excluded_exts): serve_from_cdn = False
thumbnail_name = StaticContent.generate_thumbnail_name( content.location.name, dimensions=dimensions ) thumbnail_file_location = StaticContent.compute_location( content.location.course_key, thumbnail_name, is_thumbnail=True )
im = im.convert('RGB')
thumbnail_content = StaticContent(thumbnail_file_location, thumbnail_name, 'image/jpeg', thumbnail_file)
logging.exception(u"Failed to generate thumbnail for {0}. Exception: {1}".format(content.location, str(e)))
proxy = False mongo_db = connect_to_mongodb( db, host, port=port, tz_aware=tz_aware, user=user, password=password, proxy=proxy, **kwargs )
locked=getattr(content, 'locked', False)) as fp:
self.fs.delete(location_or_id)
export_name = escape_invalid_characters(name=filename, invalid_char_list=['/', '\\'])
thumbnail_location=asset['thumbnail_location'], import_path=asset['import_path'], locked=asset.get('locked', False)
ordered_key_fields = ['category', 'name', 'course', 'tag', 'org', 'revision']
dbkey['run'] = location.run content_id = unicode(location.for_branch(None))
dbkey['run'] = _id_field['run']
thumbs = store.get_all_content_thumbnails_for_course(course_loc) for thumb in thumbs: print "Deleting {0}...".format(thumb) store.delete(thumb['_id'])
assets, __ = store.get_all_content_for_course(course_loc) for asset in assets: print "Deleting {0}...".format(asset) store.delete(asset['_id'])
store.save(content)
if content.thumbnail_location is not None: try: thumbnail_content = trash.find(content.thumbnail_location) store.save(thumbnail_content) except Exception:
_ = lambda text: text
_ = lambda text: text
return {'ok': False, 'msg': msg}
msg = '<div class="capa_alert">%s</div>' % msg return msg
if options is None: options = '' do_matrix = 'matrix' in options do_qubit = 'qubit' in options do_numerical = 'numerical' in options
try: fans = my_sympify(str(ans), matrix=do_matrix, do_qubit=do_qubit) except Exception, err: fans = None
if fexpect == fsym: return {'ok': True, 'msg': msg}
return {'ok': False, 'msg': msg}
expr_s = re.sub( r'script([a-zA-Z0-9]+)', '\\mathcal{\\1}', expr_s )
if symtab: varset = symtab else: varset = { 'p': sympy.Symbol('p'), 'g': sympy.Symbol('g'),
if ( tag == 'msup' and len(k) == 2 and gettag(k[1]) == 'mrow' and
if ( tag == 'msubsup' and len(k) == 3 and gettag(k[2]) == 'mrow' and
try: xml = self.preprocess_pmathml(self.expr)
self.the_cmathml = self.GetContentMathML(self.asciimath, pmathml) return self.the_cmathml
tag = gettag(xml)
result = symmath_check(expected_str, expected_str, dynamath=[dynamath]) self.assertTrue('ok' in result and result['ok'])
result = symmath_check(expected_str, input_str, dynamath=[dynamath]) self.assertTrue('ok' in result and result['ok'])
mathml_start = '<math xmlns="http://www.w3.org/1998/Math/MathML"><mstyle displaystyle="true">' mathml_end = '</mstyle></math>'
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
expr = stripXML(self.mathml_start + expr + self.mathml_end) expected = stripXML(self.mathml_start + expected + self.mathml_end)
xml = etree.fromstring(expr) xml = self.formulaInstance.preprocess_pmathml(xml) test = etree.tostring(xml)
self.assertEqual(test, expected)
self.start_time = datetime.now(UTC) - timedelta(seconds=1)
result = next(k for k in parse_result if isinstance(k, numbers.Number)) return result
parse_result = reversed( [k for k in parse_result
power = reduce(lambda a, b: b ** a, parse_result) return power
if math_expr.strip() == "": return float('nan')
math_interpreter = ParseAugmenter(math_expr, case_sensitive) math_interpreter.parse_algebra()
all_variables, all_functions = add_defaults(variables, functions, case_sensitive)
math_interpreter.check_variables(all_variables, all_functions)
if case_sensitive: casify = lambda x: x else:
number_part = Word(nums) inner_number = (number_part + Optional("." + Optional(number_part))) | ("." + number_part) inner_number = Combine(inner_number)
number_suffix = MatchFirst(Literal(k) for k in SUFFIXES.keys())
expr = Forward()
inner_varname = Word(alphas + "_", alphanums + "_") varname = Group(inner_varname)("variable") varname.setParseAction(self.variable_parse_action)
function = Group(inner_varname + Suppress("(") + expr + Suppress(")"))("function") function.setParseAction(self.function_parse_action)
pow_term = atom + ZeroOrMore("^" + atom) pow_term = Group(pow_term)("power")
if terminal_converter is None: return node else: return terminal_converter(node)
return handle_node(self.tree)
if parens is not None: left_parens = parens if left_parens == '{': left_parens = r'\{'
greek += [x.capitalize() for x in greek]
greek.append('hbar')
greek.append('infty')
varname = ur"{a}_{{{b}}}".format( a=enrich_varname(first), b=enrich_varname(second) )
latex = fname + inner return LatexRendered(latex, tall=children[1].tall)
fraction_mode_ever = True position = "denominator"
latex += render_frac(numerator, denominator) + r"\cdot "
position = "numerator" numerator = [] denominator = []
if position == "denominator": latex += render_frac(numerator, denominator) else: num_latex = r"\cdot ".join(k.latex for k in numerator) latex += num_latex
if math_expr.strip() == "": return ""
latex_interpreter = ParseAugmenter(math_expr, case_sensitive) latex_interpreter.parse_algebra()
variables, functions = add_defaults(variables, functions, case_sensitive)
if case_sensitive: casify = lambda x: x else:
inputs = inputs[1:] neg_inputs = neg_inputs[1:]
self.assert_function_values( 'sqrt',
self.assert_function_values('abs', [-1, 0, 1, 'j'], [1, 0, 1, 1])
self.assertAlmostEqual( calc.evaluator(variables, {}, '3*x-y'),
self.assertAlmostEqual( calc.evaluator(variables, {}, "T", case_sensitive=True), 298, delta=0.2 )
self.assertEquals( preview.latex_preview('-x+2-3+4', variables=['x']), '-x+2-3+4' )
bad_exceptions[math] = None
complex_value_list = [] v_value = value while isinstance(v_value, dict): v_key = v_value.keys()[0] v_value = v_value.values()[0] complex_value_list.append(v_key)
if not self or not other: return False
return False
return False
user_answer = json.loads(user_answer)
user_answer = flat_user_answer(user_answer)
if not isinstance(r, numbers.Number) or \ r < 0 or \ math.isnan(r) or \ math.isinf(r): return False
if r == 0: return True
while r < 100: r = r * 10 while r >= 1000: r = r / 10
if abs(r - round(r)) > 0.01: return False r = int(round(r))
for type_list in valid_types: if r in type_list: return True if int(r / 10.) in type_list and (r % 10) == 0: return True
print not iseia(2200, (E48, E96, E192)) print iseia(5490e2, (E48, E96, E192)) print iseia(2200) print not iseia(5490e2)
if len(cls.tags) == 0: raise ValueError("No tags specified for class {0}".format(cls.__name__))
continue
for t in cls.tags: self._mapping[t] = cls
return cls
solution_tags = ['solution']
response_properties = ["codeparam", "responseparam", "answer", "openendedparam"]
html_problem_semantics = [ "codeparam", "responseparam", "answer", "script", "hintgroup", "openendedparam", "openendedrubric", ]
self.seed = state.get('seed', seed) assert self.seed is not None, "Seed must be provided for LoncapaProblem."
problem_text = re.sub(r"startouttext\s*/", "text", problem_text) problem_text = re.sub(r"endouttext\s*/", "/text", problem_text) self.problem_text = problem_text
self.tree = etree.XML(problem_text)
self._process_includes()
self.context = self._extract_context(self.tree)
self.inputs = {}
responder.update_score(score_msg, cmap, queuekey)
for the_input in self.inputs.values(): if hasattr(the_input, 'ungraded_response'): the_input.ungraded_response(xqueue_msg, queuekey)
self.student_answers = convert_files_to_filenames(answers) return self._grade_answers(answers)
oldcmap = self.correct_map
answer_map = dict() for response in self.responders.keys(): results = self.responder_answers[response] answer_map.update(results)
if hasattr(self, 'has_targeted'): return
choicegroup = mult_choice_response.xpath('./choicegroup[@type="MultipleChoice"]')[0] choices_list = list(choicegroup.iter('choice'))
student_answer = self.student_answers.get(choicegroup.get('id')) expl_id_for_student_answer = None
if not show_explanation or not self.done: continue
if solution_element is None: continue
parent_element.remove(solution_element)
solution_element.tag = 'targetedfeedback' targetedfeedbackset.append(solution_element)
ifp = self.capa_system.filestore.open(filename)
incxml = etree.XML(ifp.read())
raw_path = script.get('system_path', '').split(":") + DEFAULT_PATH
path = []
zip_lib = self.capa_system.get_python_lib_zip() if zip_lib is not None: extra_files.append(("python_lib.zip", zip_lib)) python_path.append("python_lib.zip")
context['script_code'] = all_code context['python_path'] = python_path context['extra_files'] = extra_files or None return context
return
return deepcopy(problemtree)
self.inputs[input_id] = input_type_cls(self.capa_system, problemtree, state) return self.inputs[input_id].get_html()
if problemtree in self.responders: overall_msg = self.correct_map.get_overall_message() return self.responders[problemtree].render_html( self._extract_html, response_msg=overall_msg )
tree = etree.Element(problemtree.tag) for item in problemtree: item_xhtml = self._extract_html(item) if item_xhtml is not None: tree.append(item_xhtml)
for (key, value) in problemtree.items(): tree.set(key, value)
response.set('id', response_id_str) response_id += 1
responsetype_cls = responsetypes.registry.get_class_for_tag(response.tag) responder = responsetype_cls(response, inputfields, self.context, self.capa_system, self.capa_module) self.responders[response] = responder
old_stdout, old_stderr = sys.stdout, sys.stderr try: sys.stdout = StringIO() sys.stderr = StringIO()
real_answers = problem.get_question_answers()
all_answer_ids = problem.get_answer_ids() all_answers = dict((answer_id, real_answers.get(answer_id, "")) for answer_id in all_answer_ids)
_ = lambda text: text
multi_device_support = False
self.answer_ids = [x.get('id') for x in self.inputfields] if self.max_inputfields == 1: self.answer_id = self.answer_ids[0]
partial_credit = xml.xpath('.')[0].get('partial_credit', default=False)
tree = etree.Element('span')
if self.xml.get('inline', ''): tree.set('class', 'inline')
if response_msg: tree.append(self._render_response_msg_html(response_msg))
event_info = dict() event_info['module_id'] = self.capa_module.location.to_deprecated_string() event_info['problem_part_id'] = self.id
if correct: style = QUESTION_HINT_CORRECT_STYLE else: style = QUESTION_HINT_INCORRECT_STYLE
return u'<div class="{0}">{1}{2}</div>'.format(style, label_wrap, hints_wrap)
CORRECTMAP_PY = inspect.getsource(correctmap)
aid = self.answer_ids[-1] new_cmap.set_hint_and_mode(aid, hint_text, hintmode)
self.get_extended_hints(student_answers, new_cmap)
try: response_msg_div = etree.XML('<div>%s</div>' % str(response_msg))
response_msg_div.set("class", "response_message")
self.parse_xml()
if not choice.get('id'): choice.set("id", chr(ord("A") + index))
if not self.has_partial_credit: return self.grade_without_partial_credit(student_answer=student_answer)
graders = { 'edc': self.grade_via_every_decision_counts, 'halves': self.grade_via_halves, 'false': self.grade_without_partial_credit }
if len(self.credit_type) > 1: raise LoncapaProblemError('Only one type of partial credit is allowed for Checkbox problems.')
if self.credit_type[0] not in graders: raise LoncapaProblemError('partial_credit attribute should be one of: ' + ','.join(graders))
return graders[self.credit_type[0]]( all_choices=all_choices, student_answer=student_answer, student_non_answers=student_non_answers )
if self.get_compound_hints(new_cmap, student_answers): return
selectors = compound_hint.get('value').upper().split() selector_set = set(selectors)
self.mc_setup_response()
xml = self.xml cxml = xml.xpath('//*[@id=$id]//choice', id=xml.get('id'))
if isinstance(student_answer, list): student_answer = student_answer[0]
if not self.has_partial_credit: return self.grade_without_partial_credit(student_answers=student_answers)
graders = { 'points': self.grade_via_points, 'false': self.grade_without_partial_credit }
if len(self.credit_type) > 1: raise LoncapaProblemError('Only one type of partial credit is allowed for Multiple Choice problems.')
if self.credit_type[0] not in graders: raise LoncapaProblemError('partial_credit attribute should be one of: ' + ','.join(graders))
return graders[self.credit_type[0]]( student_answers=student_answers )
choices = self.xml.xpath('choicegroup/choice') return [choice.get("name") for choice in choices]
msg = _("answer-pool value should be an integer") raise LoncapaProblemError(msg)
if self.has_answerpool(): return
for choice in choices_list: choicegroup.remove(choice)
(solution_id, subset_choices) = self.sample_from_answer_pool(choices_list, rng, num_choices)
for choice in subset_choices: choicegroup.append(choice)
num_incorrect = num_pool - 1 num_incorrect = min(num_incorrect, len(incorrect_choices))
index = rng.randint(0, len(correct_choices) - 1) correct_choice = correct_choices[index] solution_id = correct_choice.get('explanation-id')
subset_choices = [correct_choice] rng.shuffle(incorrect_choices) subset_choices += incorrect_choices[:num_incorrect] rng.shuffle(subset_choices)
if unicode(val) == student_answers[aid]: return '$' + key
tolerance_xml = xml.xpath( '//*[@id=$id]//responseparam[@type="tolerance"]/@default', id=xml.get('id') )
has_partial_range = tree.xpath('responseparam[@partial_range]') if has_partial_range: partial_range = float(has_partial_range[0].get('partial_range', default='2')) else: partial_range = 2
_("There was a problem with the staff answer to this problem: complex boundary.")
_("There was a problem with the staff answer to this problem: empty boundary.")
if self.backward: self.setup_response_backward() return
responses = self.xml.xpath('//stringresponse[@id=$id]', id=self.id) if responses: response = responses[0]
regex = re.compile('^' + answer + '$', flags=flags | re.UNICODE) return re.search(regex, given)
if not given: return False
if self.backward: return self.check_string_backward(expected, given)
separator = u' <b>{}</b> '.format(_('or')) return {self.answer_id: separator.join(self.correct_answer)}
default_pc = 0.5
self.expect = contextualize_text(xml.get('expect') or xml.get('answer'), self.context)
self.code = None answer = None try: answer = xml.xpath('//*[@id=$id]//answer', id=xml.get('id'))[0] except IndexError:
cfn = xml.get('cfn') if cfn: log.debug("cfn = %s", cfn)
dynamath = [student_answers.get(k + '_dynamath', None) for k in idset]
correct = ['unknown'] * len(idset) messages = [''] * len(idset) overall_message = ""
self.context.update({ 'response_id': self.id,
'expect': self.expect,
'submission': submission,
'idset': idset,
'dynamath': dynamath,
'answers': student_answers,
'correct': correct,
'messages': messages,
'overall_message': overall_message,
'options': self.xml.get('options'), 'testdat': 'hello world',
self.context['debug'] = self.capa_system.DEBUG
self.execute_check_function(idset, submission)
if len(idset) > 1: self.context['overall_message'] = msg else: self.context['messages'][0] = msg
else: log.error(traceback.format_exc()) _ = self.capa_system.i18n.ugettext raise ResponseError( _("CustomResponse: check function returned an invalid dictionary!") )
if msg:
msg = '<html>' + msg + '</html>'
msg = msg.replace('&#60;', '&lt;')
msg = etree.tostring(fromstring_bs(msg, convertEntities=None), pretty_print=True)
msg = re.sub('(?ms)<html>(.*)</html>', '\\1', msg)
return msg.strip()
else: return ""
msg = 'Error occurred while evaluating CustomResponse' log.warning(msg, exc_info=True)
_, _, traceback_obj = sys.exc_info() raise ResponseError(err.message, traceback_obj)
self.xml.set('cfn', 'symmath_check')
super(SymbolicResponse, self).setup_response()
answer_given = submission[0]
msg = _(u"An error occurred with SymbolicResponse. The error was: {error_msg}").format( error_msg=err, ) raise Exception(msg)
codeparam = self.xml.find('codeparam') assert codeparam is not None, "Unsupported old format! <coderesponse> without <codeparam>" self._parse_coderesponse_xml(codeparam)
submission = student_answers[self.answer_id]
#
queuestate = {'key': queuekey, 'time': qtime, }
cmap.set(self.answer_id, queuestate=queuestate, correctness='incomplete', msg=msg)
error_msg = _('Invalid grader reply. Please contact the course staff.') oldcmap.set(self.answer_id, msg=error_msg) return oldcmap
self.url = xml.get('url') or "http://qisx.mit.edu:8889/pyloncapa"
rxml = etree.fromstring(req.text)
tolerance_xml = xml.xpath( '//*[@id=$id]//responseparam[@type="tolerance"]/@default', id=xml.get('id') )
self.case_sensitive = False
self.case_sensitive = True
self.case_sensitive = False
try: correctness = self.check_formula( correct_answer, given, samples ) except Exception: correctness = 'incorrect' if correctness == 'correct': hints_to_show.append(name)
self.code = self.capa_system.filestore.open('src/' + answer_src).read()
msg = _('Error in evaluating SchematicResponse. The error was: {error_msg}').format(error_msg=err) raise ResponseError(msg)
return self.default_answer_map
if (llx <= ans_x <= urx) and (lly <= ans_y <= ury): correct_map.set(aid, 'correct') break
choices_correct = self._check_student_choices(binary_choices) inputs_correct = self._check_student_inputs(numtolerance_inputs) correct = choices_correct and inputs_correct
numtolerance_choices = {} binary_choices = {}
selected_choices = [key for key in a_dict if key.endswith("bc")] for key in selected_choices: binary_choices[key] = a_dict[key]
selected_numtolerance_inputs = [ key for key in a_dict if key.partition("_numtolerance_input_")[0] + "bc" in selected_choices ]
params = self.correct_inputs.get(answer_name, {'answer': 0})
if answer_name in self.correct_inputs and not partial_correct: inputs_correct = False
__all__ = [ CodeResponse, NumericalResponse, FormulaResponse, CustomResponse, SchematicResponse, ExternalResponse, ImageResponse, OptionResponse, SymbolicResponse, StringResponse, ChoiceResponse, MultipleChoiceResponse, TrueFalseResponse, JavascriptResponse, AnnotationResponse, ChoiceTextResponse, ]
import hashlib import json import logging import requests import dogstats_wrapper as dog_stats_api
(error, msg) = self._send_to_queue(header, body, files_to_upload)
default_tolerance = '0.001%'
return student_complex == instructor_complex
student_complex = complex(student_complex) instructor_complex = complex(instructor_complex)
return abs(student_complex - instructor_complex) <= tolerance
if is_list_of_files(answer): new_answers[answer_id] = [f.name for f in answer] else: new_answers[answer_id] = answers[answer_id]
self.cmap = dict() self.items = self.cmap.items self.keys = self.cmap.keys self.overall_message = "" self.set(*args, **kwargs)
def set( self, answer_id=None, correctness=None, npoints=None, msg='', hint='', hintmode=None, queuestate=None,
self.__init__()
return 0
TEMPLATE_NAME = None
context_dict.setdefault("STATIC_URL", "/dummy-static/") try: xml_str = self.template.render_unicode(**context_dict) except: raise TemplateError(exceptions.text_error_template().render())
xml = self.render_to_xml(self.context) xpath = "//div[@class='indicator-container']/span[@class='status correct']" self.assert_has_xpath(xml, xpath, self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
xpath = "//div[@class='indicator-container']/span[@class='status correct']" self.assert_no_xpath(xml, xpath, self.context)
self.assert_no_xpath(xml, "//label[@class='choicegroup_incorrect']", self.context)
self.assert_has_text(xml, "//div[@class='capa_alert']", self.context['submitted_message'])
self.assert_no_xpath(xml, "//div[@class='capa_alert']", self.context)
xpath = "//div[@class='%s ']" % div_class self.assert_has_xpath(xml, xpath, self.context)
self.assert_has_text(xml, "//span[@class='status']/span[@class='sr']", status_mark, exact=False)
xpath = "//div[@class='%s inline']" % div_class self.assert_has_xpath(xml, xpath, self.context)
self.context['return_to_annotation'] = True xml = self.render_to_xml(self.context) self.assert_has_xpath(xml, xpath, self.context)
self.context['return_to_annotation'] = False xml = self.render_to_xml(self.context) self.assert_no_xpath(xml, xpath, self.context)
xpath = "//span[contains(@class,'selected')]/p/b" self.assert_has_text(xml, xpath, 'HTML 2', exact=False)
test_cases = [('unsubmitted', 'unanswered'), ('incomplete', 'incorrect'), ('incorrect', 'incorrect')]
xpath = "//div[@class='block']/p/b" self.assert_has_text(xml, xpath, 'prompt HTML')
xpath = "//div[@class='block']/p/b" self.assert_has_text(xml, xpath, 'HTML')
xpath = "//section[@class='math-string']/span[2]/p/b" self.assert_has_text(xml, xpath, 'tail')
self.context['options'] = [(id_num, '<b>Option {0}</b>'.format(id_num)) for id_num in range(5)] self.context['value'] = 2
xpath = "//option[@value='option_2_dummy_default']" self.assert_has_xpath(xml, xpath, self.context)
xpath = "//option[@selected='true']/b" self.assert_has_text(xml, xpath, 'Option 2')
xpath = "//div[@class='{0}']".format(expected_css_class) self.assert_has_xpath(xml, xpath, self.context)
xpath = "//p[@class='status']" self.assert_has_text(xml, xpath, expected_text, exact=False)
xpath = "//div[@class='drag_and_drop_problem_json']/p/b" self.assert_has_text(xml, xpath, 'HTML')
xml = self.render_to_xml(self.context) xpath = "//div[@class='indicator-container']/span[@class='status correct']" self.assert_has_xpath(xml, xpath, self.context)
self.assert_no_xpath(xml, "//label[@class='choicetextgroup_incorrect']", self.context)
grouping_tag = grouping_tags[test_conditions['input_type']] self.assert_no_xpath(xml, "//{0}[@class='choicetextgroup_incorrect']".format(grouping_tag), self.context)
grouping_tag = grouping_tags[test_conditions['input_type']] self.assert_no_xpath(xml, "//{0}[@class='choicetextgroup_incorrect']".format(grouping_tag), self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
xpath = "//div[@class='indicator-container']/span" self.assert_no_xpath(xml, xpath, self.context)
return capa_module
lookup_tag = inputtypes.registry.get_class_for_tag
check(u"('hasnt','hasn't')", [u'hasnt', u'hasn\'t'])
'msg': '', 'value': '3', 'params': params, 'display_file': display_file, 'display_class': display_class, 'problem_state': problem_state,
self.the_input.capa_system.render_template = lambda *args: "<aaa" with self.assertRaises(etree.XMLSyntaxError): self.the_input.get_html()
self.check('[50,40]', 35, 25)
'msg': '', 'width': width, 'height': height,
'msg': '', 'drag_and_drop_json': json.dumps(user_input)
statobj = inputtypes.Status('queued', func) self.assertEqual(statobj.display_name, u'PROCESSING')
the_html2 = problem.get_html() self.assertEquals(the_html, the_html2)
problem = new_loncapa_problem(xml_str) problem.done = True
the_html2 = problem.get_html() self.assertEquals(the_html, the_html2)
problem = new_loncapa_problem(xml_str) problem.done = True problem.student_answers = {'1_2_1': 'choice_1'}
problem = new_loncapa_problem(xml_str) problem.done = True problem.student_answers = {'1_2_1': 'choice_1'}
self.assertRegexpMatches( without_new_lines, r'<targetedfeedbackset.*?>.*?explanation-id="feedback1".*?</targetedfeedbackset>.*' + r'<targetedfeedbackset.*?>\s*</targetedfeedbackset>' )
maxDiff = None
correct_map = problem.grade_answers({'1_2_1': 'choice_0'}) self.assertAlmostEqual(correct_map.get_npoints('1_2_1'), 1)
self.assert_grade(problem, 'choice_3', 'incorrect') self.assert_grade(problem, 'not_a_choice', 'incorrect')
self.assert_grade(problem, 'choice_foil_4', 'incorrect') self.assert_grade(problem, 'not_a_choice', 'incorrect')
problem = self.build_problem(rectangle="(10,10)-(20,20)")
rectangle_str = "(10,10)-(20,20);(100,100)-(200,200)"
region_str = "[ [1,1], [5,10], [0,10] ]"
region_str = "[[[10,10], [20,10], [20, 30]], [[100,100], [120,100], [120,150]]]"
with self.assertRaises(Exception): self.build_problem(math_display=True, expect="2*x+3*y", num_inputs=3)
with mock.patch.object(requests, 'post') as mock_post: mock_post.return_value.text = snuggletex_resp
self.assert_grade(problem, "invalid_option", "incorrect")
problem = self.build_problem(options=["hasnot", "hasn't", "has'nt"], correct_option="hasn't")
sample_dict = {'x': (-10, 10), 'y': (-10, 10)}
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="x+2*y")
input_formula = "2*x - x + y + y" self.assert_grade(problem, input_formula, "correct")
input_formula = "x + y" self.assert_grade(problem, input_formula, "incorrect")
sample_dict = {'x': (-10, 10), 'y': (-10, 10)}
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="x+2*y", hints=hints)
script = "calculated_ans = 'x+x'"
sample_dict = {'x': (-10, 10)}
problem = self.build_problem(sample_dict=sample_dict, num_samples=10, tolerance=0.01, answer="$calculated_ans", script=script)
self.assert_grade(problem, '2*x', 'correct') self.assert_grade(problem, '3*x', 'incorrect')
self.assert_grade(problem, answer, "correct")
self.assert_grade(problem, answer, "correct") self.assert_grade(problem, answer.lower(), "correct")
problem = self.build_problem(answer=".*tre+", regexp=True) self.assert_grade(problem, "There is a tree", "correct")
problem_specified = self.build_problem(answer="Second", case_sensitive=True)
problem_not_specified = self.build_problem(answer="Second") problems = [problem_specified, problem_not_specified]
self.assert_grade(problem, "Second", "correct")
self.assert_grade(problem, "Other String", "incorrect") self.assert_grade(problem, "second", "incorrect")
answers = ["Second", "Third", "Fourth"]
self.assert_grade(problem, "Other String", "incorrect") self.assert_grade(problem, "second", "incorrect")
problem = self.build_problem(answer=u"\\\\", case_sensitive=False, regexp=True) self.assert_grade(problem, u"\\", "correct")
problem = self.build_problem(answer="Second", case_sensitive=False)
self.assert_grade(problem, "Second", "correct") self.assert_grade(problem, "second", "correct")
self.assert_grade(problem, "Other String", "incorrect")
answers = ["Second", "Third", "Fourth"] problem = self.build_problem(answer="sample_answer", case_sensitive=False, additional_answers=answers)
self.assert_grade(problem, answer, "correct") self.assert_grade(problem, answer.lower(), "correct")
self.assert_grade(problem, "Other String", "incorrect")
input_dict = {'1_2_1': 'Michigan'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
input_dict = {'1_2_1': 'California'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
input_dict = {'1_2_1': 'Michigan'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
input_dict = {'1_2_1': 'California'} correct_map = problem.grade_answers(input_dict) self.assertEquals(correct_map.get_hint('1_2_1'), "")
cmap = CorrectMap() for answer_id in answer_ids: cmap.update(CorrectMap(answer_id=answer_id, queuestate=None)) self.problem.correct_map.update(cmap)
for correctness in ['correct', 'incorrect']: self.problem.correct_map = CorrectMap()
cmap = CorrectMap() for answer_id in answer_ids: cmap.update(CorrectMap(answer_id=answer_id, queuestate=None)) self.problem.correct_map.update(cmap)
latest_timestamp = datetime.strptime( datetime.strftime(latest_timestamp, dateformat), dateformat ).replace(tzinfo=UTC)
self.assert_grade(problem, 'choice_3', 'incorrect')
self.assert_grade(problem, 'choice_3', 'incorrect')
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='edc' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='halves' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True, False], credit_type='halves' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='edc' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True], credit_type='halves' )
problem = self.build_problem( choice_type='checkbox', choices=[False, False, True, True, False], credit_type='halves' )
coffee_file_path = os.path.dirname(__file__) + "/test_files/js/*.coffee" os.system("node_modules/.bin/coffee -c %s" % (coffee_file_path))
self.assert_grade(problem, json.dumps({0: 4}), "correct") self.assert_grade(problem, json.dumps({0: 5}), "incorrect")
capa_system = test_capa_system() capa_system.can_execute_unsafe_code = lambda: False
problem = self.build_problem(answer='[1j, 5]') input_dict = {'1_2_1': '3'} with self.assertRaises(StudentInputError): problem.grade_answers(input_dict)
with self.assertRaises(StudentInputError): problem = self.build_problem(answer='(1 5)')
problem = self.build_problem(answer='(1, ]') input_dict = {'1_2_1': '3'} with self.assertRaises(StudentInputError): problem.grade_answers(input_dict)
self.assert_grade(problem, '42', 'correct') self.assert_grade(problem, '0', 'incorrect')
input_msg = correctmap.get_msg('1_2_1') self.assertEqual(input_msg, "Test Message")
overall_msg = correctmap.get_overall_message() self.assertEqual(overall_msg, "Overall message")
inline_script = "messages[0] = {code}".format(code=self._get_random_number_code()) problem = self.build_problem(answer=inline_script)
input_dict = {'1_2_1': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '21'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0'} correct_map = problem.grade_answers(input_dict)
self.assertEqual(problem.context['expect'], '42')
correctness = correctmap.get_correctness('1_2_1') self.assertEqual(correctness, 'correct')
input_dict = {'1_2_1': '42', '1_2_2': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0', '1_2_2': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0', '1_2_2': '0'} correct_map = problem.grade_answers(input_dict)
self.assertEqual(correct_map.get_overall_message(), "Overall message")
input_dict = {'1_2_1': '42'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '21'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '0'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '-999', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '-1', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
input_dict = {'1_2_1': '1', '1_2_2': '2', '1_2_3': '3'} correct_map = problem.grade_answers(input_dict)
self.assertEqual(correct_map.get_overall_message(), 'Message text')
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
script = 'raise Exception("Test")' problem = self.build_problem(answer=script)
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
with self.assertRaises(ResponseError): problem.grade_answers({'1_2_1': '42'})
problem = self.build_problem(answer=script)
try: problem.grade_answers({'1_2_1': '42'})
problem = self.build_problem(script=script, cfn="check_func")
try: problem.grade_answers({'1_2_1': '42'})
self.assertListEqual(problem.responders.values()[0].context['idset'], correct_order)
script = "correct = ['correct' if 'test' in submission[0] else 'incorrect']" problem = self.build_problem(answer=script)
submission_dict = {'test': 'the_answer'} input_dict = {'1_2_1': json.dumps(submission_dict)} correct_map = problem.grade_answers(input_dict)
self.assertEqual(correct_map.get_correctness('1_2_1'), 'correct')
script = "raise Exception('test')" problem = self.build_problem(answer=script)
with self.assertRaises(ResponseError): submission_dict = {'test': 'test'} input_dict = {'1_2_1': json.dumps(submission_dict)} problem.grade_answers(input_dict)
choice, answers = choice_answers_pair
choice_id = "1_2_1_choiceinput_{index}bc".format(index=index) choice_value = "choiceinput_{index}".format(index=index) answer_dict[choice_id] = choice_value
answer_id = "1_2_1_choiceinput_{index}_numtolerance_input_{ind}".format( index=index, ind=ind ) answer_dict[answer_id] = answer
self.assert_grade( two_choice_two_input, self._make_answer_dict([(True, ["Platypus"])]), "correct" )
self.assert_grade( two_choice_two_input, self._make_answer_dict([(True, ["1"]), (True, ["Platypus"])]), "correct" )
scenarios = { "2_choices_correct": ("checkbox_two_choices", "correct"), "2_choices_incorrect": ("checkbox_two_choices", "incorrect"),
problems = { "checkbox_two_choices": checkbox_two_choices, "checkbox_2_choices_2_inputs": checkbox_two_choices_two_inputs }
problem_name, correctness = scenarios[name] problem = problems[problem_name]
self.assert_grade( problem, submission, correctness, msg="{0} should be {1}".format(name, correctness) )
response = problem.responders.values()[0] self.assertFalse(response.has_mask()) self.assertFalse(response.has_answerpool())
response = problem.responders.values()[0] self.assertFalse(response.has_mask()) self.assertFalse(response.has_answerpool())
from lxml import etree import unittest import xml.sax.saxutils as saxutils
lookup_tag = customrender.registry.get_class_for_tag
xml = renderer.get_html() context = extract_context(xml) self.assertEqual(context, {'id': 'solution_12'})
root = etree.Element("problem")
if script: script_element = etree.SubElement(root, "script") script_element.set("type", "loncapa/python") script_element.text = str(script)
question = etree.SubElement(root, "p") question.text = question_text
for __ in range(int(num_responses)): response_element = self.create_response_element(**kwargs)
if credit_type is not None: response_element.set('partial_credit', str(credit_type))
for __ in range(int(num_inputs)): input_element = self.create_input_element(**kwargs) if not None == input_element: response_element.append(input_element)
group_element_names = { 'checkbox': 'checkboxgroup', 'radio': 'radiogroup', 'multiple': 'choicegroup' }
assert choice_type in group_element_names group_element = etree.Element(group_element_names[choice_type])
if name: choice_element.text = str(name) choice_element.set("name", str(name))
if pointval: choice_element.set("point_value", str(pointval))
responseparam_element = etree.SubElement(response_element, 'responseparam') responseparam_element.set('partial_answers', partial_answers)
response_element = etree.Element("customresponse")
response_element = etree.Element("schematicresponse")
if answer_script: answer_element = etree.SubElement(response_element, "answer") answer_element.set("type", "loncapa/python") answer_element.text = str(answer_script)
kwargs['explanation_text'] = None return super(CodeResponseXMLFactory, self).build_xml(**kwargs)
response_element = etree.Element("coderesponse")
codeparam_element = etree.SubElement(response_element, "codeparam")
initial_element = etree.SubElement(codeparam_element, "initial_display") initial_element.text = str(initial_display)
answer_element = etree.SubElement(codeparam_element, "answer_display") answer_element.text = str(answer_display)
grader_element = etree.SubElement(codeparam_element, "grader_payload") grader_element.text = str(grader_payload)
if not has_files: input_element = etree.SubElement(response_element, "textbox") input_element.set("mode", "python")
return None
response_element = etree.Element("formularesponse")
sample_str = self._sample_str(sample_dict, num_samples, tolerance) response_element.set("samples", sample_str)
responseparam_element = etree.SubElement(response_element, "responseparam") responseparam_element.set("type", "tolerance") responseparam_element.set("default", str(tolerance))
response_element.set("answer", str(answer))
if hint_list: hintgroup_element = etree.SubElement(response_element, "hintgroup")
formulahint_element = etree.SubElement(hintgroup_element, "formulahint")
formulahint_element.set("samples", sample_str)
assert((display_src and display_class) or (not display_src and not display_class))
response_element = etree.Element("javascriptresponse")
optioninput_element = etree.Element("optioninput")
optioninput_element.set('correct', str(correct_option))
response_element = etree.Element("stringresponse")
response_element.set("answer", unicode(answer))
expect = kwargs.get('expect', '') options = kwargs.get('options', [])
options_str = ",".join(options)
response_element = etree.Element('symbolicresponse')
if not isinstance(choices[0], (list, tuple)): choices = [choices]
if answers:
if not isinstance(answers, (list, tuple)): answers = [answers]
input_type = kwargs.get('type', 'radiotextgroup') input_element = etree.Element(input_type)
choice.text = "choice_{0}".format(ind) input_element.append(choice)
choice_element.append(inp)
problem = new_loncapa_problem(xml_str)
self._create_test_file( 'test_include.xml', '<test>Test include</test>' )
problem = new_loncapa_problem(xml_str, capa_system=self.capa_system)
rendered_html = etree.XML(problem.get_html())
test_element = rendered_html.find("test") self.assertEqual(test_element.tag, "test") self.assertEqual(test_element.text, "Test include")
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
span_element = rendered_html.find('span') self.assertEqual(span_element.text, 'Test text')
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
span_element = rendered_html.find('span') self.assertEqual(span_element.text, 'Welcome student')
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
script_element = rendered_html.find('script') self.assertEqual(None, script_element)
problem = new_loncapa_problem(xml_str)
rendered_html = etree.XML(problem.get_html())
self.assertIn( "<script type=\"text/javascript\">function(){}</script>", etree.tostring(rendered_html) )
the_system = test_capa_system() the_system.render_template = mock.Mock() the_system.render_template.return_value = "<div>Input Template Render</div>"
problem = new_loncapa_problem(xml_str, capa_system=the_system) rendered_html = etree.XML(problem.get_html())
self.assertEqual(rendered_html.tag, "div")
question_element = rendered_html.find("p") self.assertEqual(question_element.text, "Test question")
response_element = rendered_html.find("span") self.assertEqual(response_element.tag, "span")
textline_element = response_element.find("div") self.assertEqual(textline_element.text, 'Input Template Render')
solution_element = rendered_html.find("div") self.assertEqual(solution_element.text, 'Input Template Render')
kwargs = {'script': script, 'cfn': 'check_func'} xml_str = CustomResponseXMLFactory().build_xml(**kwargs)
problem = new_loncapa_problem(xml_str)
problem.grade_answers({'1_2_1': 'test'})
rendered_html = etree.XML(problem.get_html())
msg_p_elements = msg_div_element.findall('p') self.assertEqual(msg_p_elements[0].tag, "p") self.assertEqual(msg_p_elements[0].text, "Test message 1")
problem = new_loncapa_problem(xml_str) rendered_html = etree.XML(problem.get_html())
span_element = rendered_html.find('span') self.assertEqual(span_element.get('attr'), "TEST")
problem = new_loncapa_problem(xml_str)
the_html = problem.get_html() self.assertRegexpMatches(the_html, r"<div>\s+</div>")
self.assertFalse(self.cmap.is_partially_correct('9_2_1'))
self.assertEqual(self.cmap.get_overall_message(), "")
self.cmap.set_overall_message("Test message")
self.assertEqual(self.cmap.get_overall_message(), "Test message")
self.cmap.set_overall_message(None) self.assertEqual(self.cmap.get_overall_message(), "")
other_cmap = CorrectMap() other_cmap.update(self.cmap)
self.assertEqual( other_cmap.get_overall_message(), self.cmap.get_overall_message() )
invalid_list = [None, "string", 5, datetime.datetime.today()]
lazymod_py_file = lazymod.__file__ if lazymod_py_file.endswith("c"): lazymod_py_file = lazymod_py_file[:-1]
code_prolog = CODE_PROLOG % random_seed
if unsafely: exec_fn = codejail_not_safe_exec else: exec_fn = codejail_safe_exec
try: exec_fn( code_prolog + LAZY_IMPORTS + code, globals_dict, python_path=python_path, extra_files=extra_files, slug=slug, ) except SafeExecException as e: emsg = e.message else: emsg = None
if cache: cleaned_results = json_safe(globals_dict) cache.set(key, (emsg, cleaned_results))
if emsg: raise e
safe_exec("a = 1/2", g) self.assertEqual(g['a'], 0.5)
safe_exec("a = int(math.pi)", g) self.assertEqual(g['a'], 3)
safe_exec("rnums = [random.randint(0, 999) for _ in xrange(100)]", g) self.assertNotEqual(g['rnums'], rnums)
safe_exec("rnums = [random.randint(0, 999) for _ in xrange(100)]", g, random_seed=17) self.assertEqual(g['rnums'], rnums)
if not is_configured("python"): raise SkipTest
assert len(key) <= 250 return self.cache.get(key)
assert len(key) <= 250 self.cache[key] = value
cache[cache.keys()[0]] = (None, {'a': 17})
code = "a = 0\n" + ("a += 1\n" * 12345)
code = "1/0" g = {} cache = {} with self.assertRaises(SafeExecException): safe_exec(code, g, cache=DictCache(cache))
self.assertEqual(len(cache), 1) cache_exc_msg, cache_globals = cache.values()[0] self.assertIn("ZeroDivisionError", cache_exc_msg)
cache[cache.keys()[0]] = ("Hey there!", {})
for code in [129, 500, 2 ** 8 - 1, 2 ** 16 - 1]:
self.assertEqual(d1, d2) self.assertNotEqual(d1.keys(), d2.keys())
self.mods = set(sys.modules)
new_mods = [m for m in sys.modules if m not in self.mods] for m in new_mods: del sys.modules[m]
self.addCleanup(ModuleIsolation().clean_up)
self.assertNotIn("wsgiref.util", sys.modules) wsgiref_util = LazyModule("wsgiref.util") self.assertEqual(wsgiref_util.guess_scheme({}), "http")
xml.tail = self.tail return xml
'unsubmitted': 'unanswered', 'incomplete': 'incorrect', 'queued': 'processing',
_sentinel = object()
return self.default
if self.hintmode == 'always': self.msg = self.hint + ('<br/>' if self.msg else '') + self.msg
self.process_requirements()
self.setup()
msg = u"Error in xml '{x}': {err} ".format( x=etree.tostring(xml), err=err.message) raise Exception, msg, sys.exc_info()[2]
try: output = html5lib.parseFragment(html, treebuilder='lxml', namespaceHTMLElements=False)[0] except IndexError: raise ex
options = re.sub(r"([a-zA-Z])('|\\')([a-zA-Z])", r"\1&#39;\3", options)
lexer = shlex.shlex(options[1:-1].encode('utf8')) lexer.quotes = "'" lexer.whitespace = ", "
tokens = [x[1:-1].decode('utf8').replace("&#39;", "'") for x in lexer]
return [(t, t) for t in tokens]
error_message=_('Expected a <choice> or <compoundhint> tag; got {given_tag} instead').format( given_tag=choice.tag )
if self.value == "": self.value = 'null'
]
self.queue_len = 0 if self.status == 'incomplete': self.status = 'queued' self.queue_len = self.msg self.msg = self.submitted_msg
]
Attribute('mode', 'python'), Attribute('linenumbers', 'true'), Attribute('tabsize', 4, transform=int),
if not self.value and self.xml.text: self.value = self.xml.text.strip()
queue_msg = u"<span>{0}</span>".format(_("Error running code."))
if self.capa_system.xqueue is None: return {'success': False, 'message': _('Cannot connect to the queue')}
response = data['submission']
if error == 0: self.input_state['queuekey'] = queuekey self.input_state['queuestate'] = 'queued' self.input_state['queuetime'] = time.time()
(self.gx, self.gy) = [int(x) - 15 for x in m.groups()]
log.warning( "Error while previewing chemical formula", exc_info=True) result['error'] = _("Error while rendering preview")
log.warning( "Error while previewing formula", exc_info=True ) result['error'] = _("Error while rendering preview")
self.no_labels = Attribute('no_labels', default="False").parse_from_xml(self.xml)
to_js['base_image'] = Attribute('img').parse_from_xml(self.xml)
label_bg_color = Attribute('label_bg_color', default=None).parse_from_xml(self.xml) if label_bg_color: to_js['label_bg_color'] = label_bg_color
if self.value == '': self.value = 'null'
self.value = {}
_("Expected a {expected_tag} tag; got {given_tag} instead").format( expected_tag=u"<choice>", given_tag=choice.tag, )
adder = { 'type': 'text', 'contents': choice_text, 'tail_text': '', 'value': '' } components.append(adder)
adder['tail_text'] = elt.tail if elt.tail else '' components.append(adder)
choices.append((choice.get("name"), components))
import codecs from fractions import Fraction import unittest
self.assertFalse(chemical_equations_equal('H2 + O2 -> H2O2', 'O2 + H2 -> 2H2O2'))
self.assertTrue(chemical_equations_equal('H2 + O2 -> H2O2', 'O2 + H2 -> H2O2', exact=True))
def test_compare_phases_ignored(self): self.assertTrue(compare_chemical_expression( "H2O(s) + CO2", "H2O+CO2", ignore_state=True))
points = [round0_25(point) for point in points]
return nltk.tree.Tree(n.node, n[2:])
children = [] for child in tree: children.append(_merge_children(child, tags))
if len(tree) == 1: return tree[0][0] if len(tree) == 3: return " <sup>{num}</sup>&frasl;<sub>{den}</sub> ".format(num=tree[0][0], den=tree[2][0]) return "Error"
return arrow
return spanify(render_expression(left))
list1.sort() list2.sort() return list1 == list2
treedic = {} treedic['1'] = _get_final_tree(s1) treedic['2'] = _get_final_tree(s2)
if not _check_equality(treedic['1 cleaned_mm_list'], treedic['2 cleaned_mm_list']): return False
return False
return Fraction(treedic['1 factors'][0] / treedic['2 factors'][0])
for arrow in ARROWS: left, a, right = eq.partition(arrow) if a != '': return left, a, right
return False
return False
return False
return False
return False
from defusedxml.lxml import parse, fromstring, XML
if user.is_anonymous(): return None
pass
PROFILE_COUNTRY_CACHE_KEY = u"user.{user_id}.profile.country"
language = models.CharField(blank=True, max_length=255, db_index=True) location = models.CharField(blank=True, max_length=255, db_index=True)
return year - year_of_birth - 1
if user_profile.requires_parental_consent() and user_profile.has_profile_image: user_profile.profile_image_uploaded_at = None
user_profile._changed_fields = get_changed_fields_dict(user_profile, sender)
emit_field_changed_events( user_profile, user_profile.user, sender._meta.db_table, excluded_fields=['meta'] )
emit_field_changed_events( user, user, sender._meta.db_table, excluded_fields=['last_login', 'first_name', 'last_name'], hidden_fields=['password'] )
return anonymous_id_for_user(user, None, save=save)
time_last_reset = history[0].time_set
time_last_reset = user.date_joined
history = PasswordHistory.objects.filter(user=user).order_by('-time_set')[:min_diff_passwords_required]
if record.failure_count >= max_failures_allowed: lockout_period_secs = settings.MAX_FAILED_LOGIN_ATTEMPTS_LOCKOUT_PERIOD_SECS record.lockout_until = datetime.now(UTC) + timedelta(seconds=lockout_period_secs)
from student.roles import CourseCcxCoachRole, CourseInstructorRole, CourseStaffRole course_locator = course_id
is_active = models.BooleanField(default=True)
mode = models.CharField(default=CourseMode.DEFAULT_MODE_SLUG, max_length=100)
history = HistoricalRecords()
COURSE_ENROLLMENT_CACHE_KEY = u"enrollment.{}.{}.mode"
self._course_overview = None
if created: enrollment.mode = CourseMode.DEFAULT_MODE_SLUG enrollment.is_active = False enrollment.save()
if self.is_active != is_active and is_active is not None: self.is_active = is_active activation_changed = True
if self.mode != mode and mode is not None: self.mode = mode mode_changed = True
self.emit_event(EVENT_NAME_ENROLLMENT_MODE_CHANGED)
enrollment = cls.get_or_create_enrollment(user, course_key) enrollment.update_enrollment(is_active=True, mode=mode) if badges_enabled(): from lms.djangoapps.badges.events.course_meta import award_enrollment_badge award_enrollment_badge(user)
if GeneratedCertificate.certificate_for_student(self.user, self.course_id) is not None: return False
refund_cutoff_date = self.refund_cutoff_date() if refund_cutoff_date and datetime.now(UTC) > refund_cutoff_date: return False
return self.course_overview
org = models.CharField(max_length=64, db_index=True, blank=True) course_id = CourseKeyField(max_length=255, db_index=True, blank=True) role = models.CharField(max_length=64, db_index=True)
return
dashboard_tracking_code = models.TextField(default="", blank=True)
skip_entrance_exam = models.BooleanField(default=True)
unique_together = (('user', 'name',), )
STUDIO_EDIT_ROLES = 8 STUDIO_VIEW_USERS = 4 STUDIO_EDIT_CONTENT = 2 STUDIO_VIEW_CONTENT = 1
if (isinstance(role, (CourseStaffRole, CourseBetaTesterRole)) and CourseInstructorRole(role.course_key).has_user(user)): return True return False
if not(len(users) == 1 and caller == users[0]): _check_caller_authority(caller, role) role.remove_users(*users)
if GlobalStaff().has_user(caller): return
from notification_prefs.views import enable_notifications
context['show_partners'] = microsite.get_value('show_partners', True)
context['show_homepage_promo_video'] = microsite.get_value('show_homepage_promo_video', False)
youtube_video_id = microsite.get_value('homepage_promo_video_youtube_id', "your-youtube-id") context['homepage_promo_video_youtube_id'] = youtube_video_id
context['courses_list'] = microsite.get_template_path('courses_list.html')
context.update(extra_context)
for status in statuses: if reverifications[status]: reverifications[status].sort(key=lambda x: x.date) return reverifications
course_overview = enrollment.course_overview if not course_overview: log.error( "User %s enrolled in broken or non-existent course %s", user.username, enrollment.course_id ) continue
if org_to_include and course_overview.location.org != org_to_include: continue
elif course_overview.location.org in orgs_to_exclude: continue
else: yield enrollment
linkedin_config = LinkedInAddToProfileConfiguration.current()
redirect_to = get_next_url_for_login_page(request) if request.user.is_authenticated(): return redirect(redirect_to)
redirect_to = get_next_url_for_login_page(request) if request.user.is_authenticated(): return redirect(redirect_to)
course_org_filter = microsite.get_value('course_org_filter')
org_filter_out_set = microsite.get_all_orgs()
if course_org_filter: org_filter_out_set.remove(course_org_filter)
course_enrollments = list(get_course_enrollments(user, course_org_filter, org_filter_out_set))
course_enrollments.sort(key=lambda x: x.created, reverse=True)
enrollment_message = _create_recent_enrollment_message( course_enrollments, course_modes_by_course )
staff_access = False errored_courses = {} if has_access(user, 'staff', 'global'): staff_access = True errored_courses = modulestore().get_errored_courses()
course_programs = _get_course_programs(user, [enrollment.course_id for enrollment in course_enrollments])
course_mode_info = { enrollment.course_id: complete_course_mode_info( enrollment.course_id, enrollment, modes=course_modes_by_course[enrollment.course_id] ) for enrollment in course_enrollments }
show_email_settings_for = frozenset( enrollment.course_id for enrollment in course_enrollments if ( BulkEmailFlag.feature_enabled(enrollment.course_id) ) )
verification_status, verification_msg = SoftwareSecurePhotoVerification.user_status(user)
statuses = ["approved", "denied", "pending", "must_reverify"] reverifications = reverification_info(statuses)
denied_banner = any(item.display for item in reverifications["denied"])
order_history_list = order_history(user, course_org_filter=course_org_filter, org_filter_out_set=org_filter_out_set)
courses_having_prerequisites = frozenset( enrollment.course_id for enrollment in course_enrollments if enrollment.course_overview.pre_requisite_courses ) courses_requirements_not_met = get_pre_requisite_courses_not_completed(user, courses_having_prerequisites)
if enrollment.is_active and enrollment.created > time_delta
if not settings.FEATURES.get("ENABLE_CREDIT_ELIGIBILITY"): return {}
user = request.user
if not user.is_authenticated(): return HttpResponseForbidden()
action = request.POST.get("enrollment_action") if 'course_id' not in request.POST: return HttpResponseBadRequest(_("Course id not specified"))
if settings.FEATURES.get('ENABLE_MKTG_EMAIL_OPT_IN'): _update_email_opt_in(request, course_id.org)
redirect_url = embargo_api.redirect_if_blocked( course_id, user=user, ip_address=get_ip(request), url=request.path ) if redirect_url: return HttpResponse(redirect_url)
return HttpResponse()
@ensure_csrf_cookie
AUDIT_LOG.info(u"User %s w/o external auth attempting login", user)
username = user.username if user else ""
except RateLimitException: return JsonResponse({ "success": False, "value": _('Too many failed login attempts. Try again later.'),
if user_found_by_email_lookup and LoginFailures.is_feature_enabled(): LoginFailures.increment_lockout_counter(user_found_by_email_lookup)
if LoginFailures.is_feature_enabled(): LoginFailures.clear_lockout_counter(user)
return set_logged_in_cookies(request, response, user)
password_history_entry = PasswordHistory() password_history_entry.create(user)
params = dict(params.items())
extra_fields = microsite.get_value( 'REGISTRATION_EXTRA_FIELDS', getattr(settings, 'REGISTRATION_EXTRA_FIELDS', {}) )
with transaction.atomic(): (user, profile, registration) = _do_create_account(form, custom_form)
preferences_api.set_user_preference(user, LANGUAGE_KEY, get_language())
third_party_provider = None running_pipeline = None if third_party_auth.is_enabled() and pipeline.running(request): running_pipeline = pipeline.get(request) third_party_provider = provider.Registry.get_from_pipeline(running_pipeline)
if hasattr(settings, 'LMS_SEGMENT_KEY') and settings.LMS_SEGMENT_KEY: tracking_context = tracker.get_tracker().resolve_context() identity_args = [
subject = render_to_string('emails/activation_email_subject.txt', context) subject = ''.join(subject.splitlines()) message = render_to_string('emails/activation_email.txt', context)
new_user = authenticate(username=user.username, password=params['password']) login(request, new_user) request.session.set_expiry(0)
ManualEnrollmentAudit.create_manual_enrollment_audit( manual_enrollment_audit.enrolled_by, student.email, ALLOWEDTOENROLL_TO_ENROLLED, manual_enrollment_audit.reason, enrollment )
if third_party_auth.is_enabled() and pipeline.running(request): running_pipeline = pipeline.get(request) redirect_url = pipeline.get_complete_url(running_pipeline['backend'])
unique_name = uuid.uuid4().hex[0:30]
enrollment_mode = request.GET.get('enrollment_mode', 'honor')
if is_staff is not None: user.is_staff = (is_staff == "true") user.save()
reg.activate() reg.save()
year = datetime.date.today().year age_limit = settings.PARENTAL_CONSENT_AGE_LIMIT profile.year_of_birth = (year - age_limit) - 1 profile.save()
if course_key is not None: CourseEnrollment.enroll(user, course_key, mode=enrollment_mode)
for role_name in role_names: role = Role.objects.get(name=role_name, course_id=course_key) user.roles.add(role)
if login_when_done: user = authenticate(username=username, password=password) login(request, user)
_enroll_user_in_pending_courses(regs[0].user)
limiter = BadRequestRateLimiter() if limiter.is_rate_limit_exceeded(request): AUDIT_LOG.warning("Rate limit exceeded in password_reset") return HttpResponseForbidden()
tracker.emit( SETTING_CHANGE_INITIATED, { "setting": "password", "old": None, "new": None, "user_id": request.user.id, } )
AUDIT_LOG.info("Bad password_reset user passed in.") limiter.tick_bad_request_counter(request)
try: uid_int = base36_to_int(uidb36) user = User.objects.get(id=uid_int) user.is_active = True user.save() except (ValueError, User.DoesNotExist): pass
err_msg = None
extra_context = {"platform_name": microsite.get_value('platform_name', settings.PLATFORM_NAME)}
try: uidb64 = force_text(urlsafe_base64_encode(force_bytes(base36_to_int(uidb36)))) except ValueError:
old_password_hash = user.password
updated_user = User.objects.get(id=uid_int)
if updated_user.password != old_password_hash: entry = PasswordHistory() entry.create(updated_user)
if not activation_key: activation_key = uuid.uuid4().hex
try: user.email_user( subject, message, theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL) )
try: user.email_user( subject, message, theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL) )
exclude = ('dashboard_tracking_code',)
admin.site.register(User, UserAdmin)
REGISTERED_ACCESS_ROLES = {}
user.is_staff = False user.save()
if not hasattr(user, '_roles'): user._roles = RoleCache(user)
if self.course_key is None: self.course_key = CourseKeyField.Empty entries = User.objects.filter( courseaccessrole__role=self._role_name, courseaccessrole__org=self.org, courseaccessrole__course_id=self.course_key ) return entries
if not hasattr(self.user, '_roles'): self.user._roles = RoleCache(self.user)
from django.core.management.base import BaseCommand from django.contrib.auth.models import User
manage.py ... transfer_students -f edX/Open_DemoX/edx_demo_course -t edX/Open_DemoX/new_demoX
manage.py ... transfer_students -f edX/Open_DemoX/edx_demo_course -t edX/Open_DemoX/new_demoX -c true
manage.py ... transfer_students -f edX/Open_DemoX/edx_demo_course -t edX/Open_DemoX/new_demoX,edX/Open_DemoX/edX_Insider
enrollment = CourseEnrollment.objects.get( user=user, course_id=source_key )
msg = u"Skipping {}, already enrolled in destination course {}" print msg.format(user.username, unicode(dest_key))
if not old_is_active: new_enrollment.update_enrollment(is_active=False, skip_refund=True)
if options['course']: try: course_key = CourseKey.from_string(options['course']) except InvalidKeyError: course_key = SlashSeparatedCourseKey.from_deprecated_string(options['course'])
output_filename = course_key.to_deprecated_string().replace('/', '-') + ".csv"
students = User.objects.filter(courseenrollment__course_id=course_key) if len(students) == 0: self.stdout.write("No students enrolled in %s" % course_key.to_deprecated_string()) return
if options['course']: try: course = CourseKey.from_string(options['course']) except InvalidKeyError: course = SlashSeparatedCourseKey.from_deprecated_string(options['course'])
raise CommandError( _( 'Skipping user "{}" because the specified and existing email ' 'addresses do not match.' ).format(user.username) )
for group_name in groups or set():
self.stderr.write(_('Could not find a group named "{}" - skipping.').format(group_name))
group.full_clean()
raise CommandError( _( 'Invalid group name: "{group_name}". {messages}' ).format( group_name=group_name, messages=exc.messages[0] ) )
raise CommandError(_( 'Invalid permission option: "{}". Please specify permissions ' 'using the format: app_label:model_name:permission_codename.' ).format(permission))
call_command('manage_group', TEST_GROUP) self.check_groups([TEST_GROUP])
call_command('manage_group', TEST_GROUP, '--remove') self.check_groups([])
call_command('manage_group', TEST_GROUP, '--permissions', 'auth:Group:add_group') self.check_groups([TEST_GROUP]) self.check_permissions(TEST_GROUP, ['add_group'])
call_command('manage_group', TEST_GROUP, '--permissions', 'auth:Group:change_group') self.check_groups([TEST_GROUP]) self.check_permissions(TEST_GROUP, ['change_group'])
call_command('manage_group', TEST_GROUP) self.check_groups([TEST_GROUP]) self.check_permissions(TEST_GROUP, [])
call_command('manage_user', TEST_USERNAME, TEST_EMAIL, '--remove') self.assertEqual([], list(User.objects.all()))
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=user_ids)), 0 )
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=user_ids)), expected_conversions )
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=real_user_ids)), 0 )
self.assertEqual( len(CourseEnrollment.objects.filter(mode='honor', user_id__in=real_user_ids)), expected_success )
self._create_and_purchase_verified(student, course.id)
course_location_one = locator.CourseLocator('Org1', 'Course1', 'Run1') new_course_one = self._create_course(course_location_one)
transfer_students.Command().handle( source_course=original_key, dest_course_list=new_key_one + "," + new_key_two ) self.assertTrue(self.signal_fired)
cart = Order.get_cart_for_user(user=student) CertificateItem.add_to_order(cart, course_id, 50, 'verified') cart.purchase()
VERIFY_STATUS_NEED_TO_VERIFY = "verify_need_to_verify" VERIFY_STATUS_SUBMITTED = "verify_submitted" VERIFY_STATUS_APPROVED = "verify_approved" VERIFY_STATUS_MISSED_DEADLINE = "verify_missed_deadline" VERIFY_STATUS_NEED_TO_REVERIFY = "verify_need_to_reverify"
verifications = SoftwareSecurePhotoVerification.objects.filter(user=user)
has_active_or_pending = SoftwareSecurePhotoVerification.user_has_valid_or_pending( user, queryset=verifications )
enrolled_course_keys = [enrollment.course_id for enrollment in course_enrollments] course_deadlines = VerificationDeadline.deadlines_for_courses(enrolled_course_keys)
if enrollment.mode in CourseMode.VERIFIED_MODES:
deadline = course_deadlines.get(enrollment.course_id)
if relevant_verification is not None and relevant_verification.status == "approved": recent_verification_datetime = max( recent_verification_datetime if recent_verification_datetime is not None else relevant_verification.expiration_datetime, relevant_verification.expiration_datetime )
status = None
if relevant_verification is not None: if relevant_verification.status == "approved": status = VERIFY_STATUS_APPROVED elif relevant_verification.status == "submitted": status = VERIFY_STATUS_SUBMITTED
else: status = VERIFY_STATUS_MISSED_DEADLINE
if status is not None: days_until_deadline = None
POST_AUTH_PARAMS = ('course_id', 'enrollment_action', 'course_mode', 'email_opt_in')
params = [(param, request.GET[param]) for param in POST_AUTH_PARAMS if param in request.GET]
return redirect_to
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertTrue(PasswordHistory.is_allowable_password_reuse(user, "test")) self.assertTrue(PasswordHistory.is_allowable_password_reuse(staff, "test"))
grandfathered_student = UserFactory() grandfathered_student.date_joined = timezone.now()
self.assertEqual(response.status_code, 400)
self.assertEqual(response.status_code, 400)
from config_models.models import cache
course2.certificates_display_behavior = 'early_no_info' cert_status = {'status': 'unavailable'} self.assertEqual(_cert_info(user, course2, cert_status, course_mode), {})
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self.assertEqual(pq(response.content)(".sts-enrollment").length, 0)
self.assertTrue('Activate Course Enrollment' in response.content)
invoice = shoppingcart.models.Invoice.objects.get(id=sale_invoice_1.id) invoice.is_valid = True invoice.save()
self.client.login(username="jack", password="test")
self.client.login(username="jack", password="test") LinkedInAddToProfileConfiguration( company_identifier='0_mC_o2MizqdtZEmkVXjH4eYwMj4DnkCWrZP_D9', enabled=True ).save()
test_course = CourseFactory.create(default_store=modulestore_type, emit_signals=True) self.client.login(username="jack", password="test")
self.assertContains(response, "Explore courses")
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Schools & Partners")
CourseEnrollment.enroll(user, course_id) self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assertTrue(CourseEnrollment.is_enrolled_by_partial(user, course_id_partial)) self.assert_no_events_were_emitted()
CourseEnrollment.unenroll(user, course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assertFalse(CourseEnrollment.is_enrolled_by_partial(user, course_id_partial)) self.assert_no_events_were_emitted()
enrollment_record = CourseEnrollment.objects.get( user=user, course_id=course_id ) self.assertFalse(enrollment_record.is_active)
user = User(username="rusty", email="rusty@fake.edx.org") course_id = SlashSeparatedCourseKey("edX", "Test101", "2013")
CourseEnrollment.unenroll(user, course_id) self.assert_no_events_were_emitted()
CourseEnrollment.enroll(user, course_id) self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_enrollment_event_was_emitted(user, course_id)
self.assertIsNone( CourseEnrollment.enroll_by_email("not_jack@fake.edx.org", course_id) ) self.assert_no_events_were_emitted()
CourseEnrollment.unenroll_by_email("jack@fake.edx.org", course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_unenrollment_event_was_emitted(user, course_id)
CourseEnrollment.unenroll_by_email("jack@fake.edx.org", course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
CourseEnrollment.unenroll_by_email("not_jack@fake.edx.org", course_id) self.assert_no_events_were_emitted()
enrollment = CourseEnrollment.get_or_create_enrollment(user, course_id) self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
enrollment.activate() self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_enrollment_event_was_emitted(user, course_id)
enrollment.activate() self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
enrollment.deactivate() self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_unenrollment_event_was_emitted(user, course_id)
enrollment.deactivate() self.assertFalse(CourseEnrollment.is_enrolled(user, course_id)) self.assert_no_events_were_emitted()
CourseEnrollment.enroll(user, course_id) self.assertTrue(CourseEnrollment.is_enrolled(user, course_id)) self.assert_enrollment_event_was_emitted(user, course_id)
CourseEnrollment.enroll(user, course_id, "honor") self.assert_no_events_were_emitted()
response = self._enroll_through_view(self.course) self.assertEqual(response.status_code, 400)
self.assertContains(response, 'course-container', 2) self._assert_responses(response, 1)
if course_mode == 'verified': self.assertIn('xseries-base-btn', response.content) else: self.assertIn('xseries-border-btn', response.content)
__, course_ids = mock_get_programs.call_args[0] self.assertEqual(list(course_ids), [self.course_1.id]) self._assert_responses(response, 1)
self.assertContains(response, 'course-container', 3) self._assert_responses(response, program_count)
self.assertContains(response, 'course-container', 1) self.assertIn('Pursue a Certificate of Achievement to highlight', response.content)
role.add_users(self.student) role.remove_users(self.student) self.assertFalse(role.has_user(self.student))
courses_list = list(get_course_enrollments(self.student, None, [])) self.assertEqual(len(courses_list), 1) self.assertEqual(courses_list[0].course_id, course_location)
courses_list = list(get_course_enrollments(self.student, None, [])) self.assertEqual(len(courses_list), 0)
CourseOverview.objects.filter(id=course_key).delete()
self.user.is_staff = True self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
user_not_added = User.objects.create_user('testuser2', 'test+courses2@edx.org', 'foo2') self.assertFalse(user_has_role(user_not_added, CourseCreatorRole()))
remove_users(self.admin, CourseCreatorRole(), self.user) self.assertFalse(user_has_role(self.user, CourseCreatorRole()))
add_users(self.admin, CourseCreatorRole(), self.user)
self.assertFalse(user_has_role(self.user, CourseCreatorRole()))
self.user.is_staff = True self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
remove_users(self.admin, CourseCreatorRole(), self.user) self.assertTrue(user_has_role(self.user, CourseCreatorRole()))
from config_models.models import cache
self.enrollment.can_refund = True self.assertTrue(self.enrollment.refundable())
self.enrollment.can_refund = True self.assertTrue(self.enrollment.refundable())
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
response = self.client.post(reverse('admin:student_courseaccessrole_add'), data=data) self.assertRedirects(response, reverse('admin:student_courseaccessrole_changelist'))
age = years_ago - 1 self.assertEqual(self.profile.age, age)
course_location = locator.CourseLocator('Org1', 'Course1', 'Run1') self.course, self.enrollment = self._create_course_and_enrollment(course_location)
courses_list = list(get_course_enrollments(self.student, None, [])) self.assertEqual(len(courses_list), 2)
self.assert_no_xss(response, xss_content)
self._configure_message_timeout(10000)
DonationConfiguration(enabled=True).save()
for mode, min_price in course_modes: CourseModeFactory.create(mode_slug=mode, course_id=self.course.id, min_price=min_price)
self.client.login(username=self.student.username, password=self.PASSWORD) response = self.client.get(reverse("dashboard"))
self._configure_message_timeout(10000) DonationConfiguration(enabled=True).save()
CourseModeFactory.create(mode_slug="honor", course_id=self.course.id, min_price=100)
student = UserFactory.create() CourseEnrollmentFactory.create(user=student, course_id=self.course.id) self.client.login(username=student.username, password="test")
response = self.client.get(self.url) self.assertTrue(self.email_modal_link in response.content)
response = self.client.get(self.url) self.assertNotIn(self.email_modal_link, response.content)
self.assertFalse(BulkEmailFlag.feature_enabled(self.course.id)) response = self.client.get(self.url) self.assertNotIn(self.email_modal_link, response.content)
student = UserFactory.create() CourseEnrollmentFactory.create( user=student, course_id=SlashSeparatedCourseKey.from_deprecated_string(self.course_name) ) self.client.login(username=student.username, password="test")
response = self.client.get(self.url) self.assertFalse(self.email_modal_link in response.content)
response = self.client.get(self.url) self.assertFalse(self.email_modal_link in response.content)
super(TestCourseVerificationStatus, self).setUp()
self._assert_course_verification_status(None)
CourseEnrollmentFactory( course_id=self.course.id, user=self.user, mode="verified" )
self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
attempt.mark_ready() self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_VERIFY)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
self._assert_course_verification_status(VERIFY_STATUS_SUBMITTED)
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.approve()
self._assert_course_verification_status(VERIFY_STATUS_APPROVED)
response = self.client.get(self.dashboard_url) self.assertContains(response, attempt.expiration_datetime.strftime("%m/%d/%Y"))
self._setup_mode_and_enrollment(self.PAST, "verified")
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
self._setup_mode_and_enrollment(self.PAST, "verified")
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
self._setup_mode_and_enrollment(self.PAST, "verified")
self._assert_course_verification_status(VERIFY_STATUS_MISSED_DEADLINE)
self._setup_mode_and_enrollment(self.FUTURE, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit() attempt.deny("Not valid!")
self._assert_course_verification_status(None)
self._setup_mode_and_enrollment(self.FUTURE, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.status = "must_retry" attempt.system_error("Error!")
self._assert_course_verification_status(None)
self._setup_mode_and_enrollment(self.FUTURE, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
attempt.created_at = attempt.created_at - timedelta(days=364) attempt.save()
self._assert_course_verification_status(VERIFY_STATUS_NEED_TO_REVERIFY)
self._setup_mode_and_enrollment(self.PAST, "verified")
attempt = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt.mark_ready() attempt.submit()
self._assert_course_verification_status(VERIFY_STATUS_APPROVED)
self._assert_course_verification_status(VERIFY_STATUS_APPROVED)
response = self.client.get(self.dashboard_url) self.assertContains(response, attempt.expiration_datetime.strftime("%m/%d/%Y"))
course2 = CourseFactory.create() CourseModeFactory.create( course_id=course2.id, mode_slug="verified", expiration_datetime=self.PAST ) CourseEnrollmentFactory( course_id=course2.id, user=self.user, mode="verified" )
attempt2 = SoftwareSecurePhotoVerification.objects.create(user=self.user) attempt2.mark_ready() attempt2.submit() attempt2.approve() attempt2.save()
self.assertContains(response, unicode(self.course.id))
alt_text = self.BANNER_ALT_MESSAGES.get(status) if alt_text: self.assertContains(response, alt_text)
self.assertContains( response, "<article class=\"course {}\">".format(self.MODE_CLASSES[status]) )
if status is not None: if status in self.NOTIFICATION_MESSAGES: found_msg = False for message in self.NOTIFICATION_MESSAGES[status]: if message in response.content: found_msg = True break
all_messages = [] for msg_group in self.NOTIFICATION_MESSAGES.values(): all_messages.extend(msg_group)
for msg in all_messages: self.assertNotContains(response, msg)
self.bad_user_client = Client() self.good_user_client = Client() self.non_staff_client = Client() self.admin_client = Client()
self.some_url = '/'
UserProfile.objects.exists() return HttpResponse(mock_render_to_string(template_name, context))
request = RequestFactory().post('unused_url') request.user = self.user request.META['HTTP_HOST'] = "aGenericValidHostName" self.append_allowed_hosts("aGenericValidHostName")
user1_new_email = "valid_user1_email@example.com" user2_new_email = "valid_user2_email@example.com"
user2 = UserFactory.create(email=self.new_email, password="test2")
self.assertIsNone(self.do_email_change(self.user, user1_new_email)) self.assertIsNone(self.do_email_change(user2, user2_new_email))
request = RequestFactory().post('unused_url') request.user = self.user request.META['HTTP_HOST'] = "aGenericValidHostName" self.append_allowed_hosts("aGenericValidHostName")
self.registration = Registration() self.registration.register(self.user) self.registration.save()
self.assertFalse(self.user.is_active)
self.registration.activate() self.assertTrue(self.user.is_active) self.assertFalse(mock_segment_identify.called)
self.assertFalse(self.user.is_active)
self.registration.activate() self.assertTrue(self.user.is_active) mock_segment_identify.assert_called_with( self.user.id, expected_segment_payload, expected_segment_mailchimp_list )
([], '', CourseMode.DEFAULT_MODE_SLUG),
(['verified', 'audit'], 'course_modes_choose', CourseMode.DEFAULT_MODE_SLUG),
(['honor', 'verified', 'audit'], 'course_modes_choose', CourseMode.HONOR),
for mode_slug in course_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode_slug, mode_display_name=mode_slug, )
full_url = ( reverse(next_url, kwargs={'course_id': unicode(self.course.id)}) if next_url else next_url )
resp = self._change_enrollment('enroll') self.assertEqual(resp.status_code, 200) self.assertEqual(resp.content, full_url)
if enrollment_mode is None: self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
CourseEnrollment.enroll(self.user, self.course.id, mode="honor")
resp = self._change_enrollment('unenroll') self.assertEqual(resp.status_code, 200)
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
for mode_slug in course_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode_slug, mode_display_name=mode_slug, )
self._change_enrollment('enroll', email_opt_in=email_opt_in)
if email_opt_in is not None: opt_in = email_opt_in == 'true' mock_update_email_opt_in.assert_called_once_with(self.user, self.course.org, opt_in) else: self.assertFalse(mock_update_email_opt_in.called)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertFalse(is_enrolled)
is_enrolled = CourseEnrollment.is_enrolled(self.user, self.course.id) self.assertTrue(is_enrolled)
self.client.logout()
resp = self._change_enrollment('enroll') self.assertEqual(resp.status_code, 403)
resp = self._change_enrollment('unenroll') self.assertEqual(resp.status_code, 400)
staff = UserFactory.create(username="staff", email="staff@e.com", password="test") role = CourseStaffRole(self.course_limited.id) role.add_users(staff)
instructor = UserFactory.create(username="instructor", email="instructor@e.com", password="test") role = CourseInstructorRole(self.course_limited.id) role.add_users(instructor)
current_year = datetime.datetime.now().year self.set_year_of_birth(current_year - 10) self.assertFalse(self.profile.requires_parental_consent())
self.profile.profile_image_uploaded_at = datetime.datetime.now() self.profile.save() self.assertFalse(self.profile.has_profile_image)
self.set_year_of_birth(current_year - 10) self.profile.save() self.assertFalse(self.profile.has_profile_image)
with self.assertRaises(AttributeError): getattr(self.profile, '_changed_fields')
super(AutoAuthEnabledTestCase, self).setUp() self.url = '/auto_auth' self.client = Client()
user_profile = UserProfile.objects.get(user=user) self.assertEqual(user_profile.name, "Robot Name")
self.assertFalse(user.is_staff)
self._auto_auth({'username': 'test', 'course_id': course_id})
self.assertEqual(CourseEnrollment.objects.count(), 1) enrollment = CourseEnrollment.objects.get(course_id=course_key) self.assertEqual(enrollment.user.username, "test")
self._auto_auth({'username': 'test', 'course_id': course_id})
self._auto_auth({'username': 'test', 'course_id': course_id})
self.assertEqual(CourseEnrollment.objects.count(), 1) enrollment = CourseEnrollment.objects.get(course_id=course_key) self.assertEqual(enrollment.user.username, "test")
response = self._auto_auth({ 'username': 'test', 'course_id': course_id, 'redirect': True, 'staff': 'true', }, status_code=302)
self.assertEqual(CourseEnrollment.objects.count(), 1) enrollment = CourseEnrollment.objects.get(course_id=course_key) self.assertEqual(enrollment.user.username, "test")
if settings.ROOT_URLCONF == 'lms.urls': url_pattern = '/info' else: url_pattern = '/course/{}'.format(unicode(course_key))
response = self._auto_auth({ 'username': 'test', 'redirect': True, 'staff': 'true', }, status_code=302)
if settings.ROOT_URLCONF == 'lms.urls': url_pattern = '/dashboard' else: url_pattern = '/home'
url_pattern = '/u/test#about_me' response = self._auto_auth({ 'username': 'test', 'redirect_to': url_pattern, 'staff': 'true', }, status_code=302)
for cookie in ['csrftoken', 'sessionid']:
super(AutoAuthDisabledTestCase, self).setUp() self.url = '/auto_auth' self.client = Client()
self.course = CourseFactory()
CreditProvider.objects.create( provider_id=self.PROVIDER_ID, display_name=self.PROVIDER_NAME, provider_status_url=self.PROVIDER_STATUS_URL, enable_integration=True, )
credit_api.set_credit_requirements(
self.enrollment = CourseEnrollmentFactory( user=self.user,
response = self._load_dashboard() self.assertNotContains(response, "credit-eligibility-msg") self.assertNotContains(response, "purchase-credit-btn")
self._make_eligible()
response = self._load_dashboard() self.assertContains(response, "credit-eligibility-msg") self.assertContains(response, "purchase-credit-btn")
eligibility = CreditEligibility.objects.get(username=self.USERNAME) eligibility.deadline = datetime.datetime.now(pytz.UTC) + datetime.timedelta(days=29) eligibility.save()
self._make_eligible() self._purchase_credit()
self._make_eligible() self._purchase_credit() self._initiate_request()
response = self._load_dashboard() self.assertContains(response, "credit-request-pending-msg")
self._make_eligible() self._purchase_credit() request_uuid = self._initiate_request() self._set_request_status(request_uuid, "approved")
response = self._load_dashboard() self.assertContains(response, "credit-request-approved-msg")
self._make_eligible() self._purchase_credit() request_uuid = self._initiate_request() self._set_request_status(request_uuid, "rejected")
response = self._load_dashboard() self.assertContains(response, "credit-request-rejected-msg")
self._make_eligible() self._purchase_credit() CourseEnrollmentAttribute.objects.all().delete()
response = self._load_dashboard() self.assertContains(response, "credit-error-msg")
self._make_eligible()
with patch('student.views.get_credit_provider_display_names') as mock_method: mock_method.return_value = providers_list response = self._load_dashboard()
self.user = UserFactory.build(username='test', email='test@edx.org') self.user.set_password('test_password') self.user.save()
RegistrationFactory(user=self.user)
UserProfileFactory(user=self.user)
self.client = Client() cache.clear()
try: self.url = reverse('login_post') except NoReverseMatch: self.url = reverse('login')
self.user.is_active = False self.user.save()
self.user.is_active = False self.user.save()
cookie = self.client.cookies[settings.EDXMKTG_USER_INFO_COOKIE_NAME] user_info = json.loads(cookie.value)
self.assertEqual(user_info["version"], settings.EDXMKTG_USER_INFO_COOKIE_VERSION)
self.assertEqual(user_info["username"], self.user.username) self.assertEqual(user_info["email"], self.user.email)
for url in user_info["header_urls"].values(): self.assertIn("http://testserver/", url)
self.assertIn(settings.EDXMKTG_LOGGED_IN_COOKIE_NAME, self.client.cookies) self.assertIn(settings.EDXMKTG_USER_INFO_COOKIE_NAME, self.client.cookies)
logout_url = reverse('logout') response = self.client.post(logout_url)
for cookie_name in [settings.EDXMKTG_LOGGED_IN_COOKIE_NAME, settings.EDXMKTG_USER_INFO_COOKIE_NAME]: cookie = self.client.cookies[cookie_name] self.assertIn("01-Jan-1970", cookie.get('expires'))
response, _ = self._login_response('test@edx.org', 'test_password') self._assert_response(response, success=True)
self.user = User.objects.get(pk=self.user.pk)
response = client2.post(self.url, creds) self._assert_response(response, success=True)
url = reverse('dashboard')
self.assertEqual(response.status_code, 302)
self.assertFalse(hasattr(user, 'profile'))
user = User.objects.get(pk=user.pk)
self.assertTrue(hasattr(user, 'profile'))
response = client2.post(self.url, creds) self._assert_response(response, success=True)
url = reverse('dashboard')
self.assertEqual(response.status_code, 302)
self.user = User.objects.get(pk=self.user.pk)
response = client2.post(self.url, creds) self._assert_response(response, success=True)
if bypass_activation_email: self.assertFalse(mock_send_mail.called) else: self.assertTrue(mock_send_mail.called)
del params["username"] assert_username_error("Username must be minimum of two characters long")
for username in ["", "a"]: params["username"] = username assert_username_error("Username must be minimum of two characters long")
params["username"] = "this_username_has_31_characters" assert_username_error("Username cannot be more than 30 characters long")
params["username"] = "invalid username" assert_username_error("Usernames must contain only letters, numbers, underscores (_), and hyphens (-).")
del params["email"] assert_email_error("A properly formatted e-mail is required")
for email in ["", "a"]: params["email"] = email assert_email_error("A properly formatted e-mail is required")
params["email"] = "this_email_address_has_76_characters_in_it_so_it_is_unacceptable@example.com" assert_email_error("Email cannot be more than 75 characters long")
params["email"] = "not_an_email_address" assert_email_error("A properly formatted e-mail is required")
del params["password"] assert_password_error("A valid password is required")
for password in ["", "a"]: params["password"] = password assert_password_error("A valid password is required")
params["username"] = params["password"] = "test_username_and_password" assert_password_error("Username and password fields cannot match")
del params["name"] assert_name_error("Your legal name must be a minimum of two characters long")
for name in ["", "a"]: params["name"] = name assert_name_error("Your legal name must be a minimum of two characters long")
del params["honor_code"] assert_honor_code_error("To enroll, you must follow the honor code.")
for honor_code in ["", "false", "not_boolean"]: params["honor_code"] = honor_code assert_honor_code_error("To enroll, you must follow the honor code.")
params["honor_code"] = "tRUe" self.assert_success(params)
del params["honor_code"] params["username"] = "another_test_username" params["email"] = "another_test_email@example.com" self.assert_success(params)
del params["terms_of_service"] assert_terms_of_service_error("You must accept the terms of service.")
for terms_of_service in ["", "false", "not_boolean"]: params["terms_of_service"] = terms_of_service assert_terms_of_service_error("You must accept the terms of service.")
params["terms_of_service"] = "tRUe" self.assert_success(params)
assert_extra_field_error()
params[field] = "" assert_extra_field_error()
if min_length > 1: params[field] = "a" assert_extra_field_error()
THIRD_PARTY_AUTH_BACKENDS = ["google-oauth2", "facebook"] THIRD_PARTY_AUTH_PROVIDERS = ["Google", "Facebook"]
params = [('course_id', self.course_id)] response = self.client.get(self.url, params)
expected_url = _third_party_login_url( backend_name, "login", redirect_url=_finish_auth_url(params), ) self.assertContains(response, expected_url)
expected_url = _third_party_login_url( backend_name, "login", redirect_url=self.courseware_url ) self.assertContains(response, expected_url)
response = self.client.get(self.url, params)
post_login_handler = _finish_auth_url(params) js_success_var = 'var nextUrl = "{}";'.format(post_login_handler) self.assertContains(response, js_success_var)
response = self.client.get(self.url, params)
post_login_handler = _finish_auth_url(params) js_success_var = 'var nextUrl = "{}";'.format(post_login_handler) self.assertContains(response, js_success_var)
self._check_linkedin_visibility(False)
self._check_linkedin_visibility(True)
self._check_linkedin_visibility(False)
for url_name, url_path in header_urls.iteritems(): header_urls[url_name] = request.build_absolute_uri(url_path)
CACHE_TOOLBOX_DEFAULT_TIMEOUT = getattr( settings, 'CACHE_TOOLBOX_DEFAULT_TIMEOUT', 60 * 60 * 24 * 3, )
try: return getattr(self, descriptor.cache_name) except AttributeError: pass
try: return getattr(self, '_%s_cache' % related_name) except AttributeError: pass
return
session_user_id = SafeSessionMiddleware.get_user_id_from_session(request)
raise Exception
super(CacheBackedAuthenticationMiddleware, self).process_request(request)
instance = model(pk=pk, **data)
instance._state.adding = False
instance._state.db = using or DEFAULT_DB_ALIAS
cache.delete(key)
if field.primary_key: continue
file = getattr(instance, field.attname) data[field.attname] = file.name
return '%s.%s:%d' % ( model._meta.app_label, model._meta.model_name, getattr(instance_or_pk, 'pk', instance_or_pk), )
pass
from django.utils.translation import get_language
return view_func(request, *args, **kwargs)
if RateLimitConfiguration.current().enabled: return func(*args, **kwargs) else: msg = "Rate limiting is disabled because `RateLimitConfiguration` is not enabled." LOGGER.info(msg) return
if not issubclass(clz, APIView): msg = ( u"{clz} is not a Django Rest Framework APIView subclass." ).format(clz=clz) LOGGER.warning(msg) return clz
if hasattr(clz, 'check_throttles'): clz.check_throttles = _check_throttles_decorator(clz.check_throttles)
resolve('/')
URLCONF_MODULES = None
URLCONF_MODULES = ['myapp.url']
URLCONF_MODULES = ['myapp.url', 'another_app.urls']
TestCase._enter_atomics = enter_atomics_wrapper(TestCase._enter_atomics) TestCase._rollback_atomics = rollback_atomics_wrapper(TestCase._rollback_atomics)
key = cleaned_string(key) key_prefix = cleaned_string(key_prefix) version = cleaned_string(version)
combined = ":".join([key_prefix, version, key])
if len(combined) > 250: combined = fasthash(combined)
return combined
format = ugettext("LONG_DATE_FORMAT") if format == "LONG_DATE_FORMAT": format = DEFAULT_LONG_DATE_FORMAT
format = ugettext("DATE_TIME_FORMAT") if format == "DATE_TIME_FORMAT": format = DEFAULT_DATE_TIME_FORMAT
raise ValueError("strftime format ends with raw %")
part = dtime.strftime(code)
pgettext = real_pgettext
milestones_api.add_course_milestone(course_key, 'requires', milestone)
milestones_api.add_course_milestone(prerequisite_course_key, 'fulfills', milestone)
if prerequisite_course_keys: for prerequisite_course_key_string in prerequisite_course_keys: prerequisite_course_key = CourseKey.from_string(prerequisite_course_key_string) add_prerequisite_course(course_key, prerequisite_course_key)
if required_courses: pre_requisite_courses[course_key] = {'courses': required_courses}
seed_milestone_relationship_types() course_milestones = milestones_api.get_course_milestones(course_key=course_key, relationship="fulfills")
try: milestone_paths = get_course_milestones_fulfillment_paths( unicode(course.id), serialize_user(user) ) except InvalidMilestoneRelationshipTypeException: return required_content
raise
log.exception("Error in django view.") return render_to_response(template_path, context)
raise
client_args={"disable_ssl_certificate_validation": True}
zendesk_tags = list(tags.values()) + ["LMS"]
white_label_org = microsite.get_value('course_org_filter') if white_label_org: zendesk_tags = zendesk_tags + ["whitelabel_{org}".format(org=white_label_org)]
log.warning('Unable to find group named %s for Zendesk ticket with ID %s.', group_name, ticket_id)
user_id = context.get('user_id') course_title = context.get('course_title')
from __future__ import unicode_literals
from __future__ import unicode_literals
from functools import wraps import random
ENABLED = True
if self.read_committed is True: if connection.vendor == 'mysql': cursor = connection.cursor() cursor.execute("SET TRANSACTION ISOLATION LEVEL READ COMMITTED")
try: connection.commit() except DatabaseError: try: connection.rollback() except Error: connection.close() raise
try: connection.rollback() except Error: connection.close()
if connection.commit_on_success_block_level == 0: if connection.features.autocommits_when_autocommit_is_off: connection.autocommit = True else: connection.set_autocommit(True)
else: return CommitOnSuccessManager(using, read_committed)
if not self.ALLOW_NESTED and connection.in_atomic_block: raise transaction.TransactionManagementError('Cannot be inside an atomic block.')
if self.read_committed is True: if connection.vendor == 'mysql': cursor = connection.cursor() cursor.execute("SET TRANSACTION ISOLATION LEVEL READ COMMITTED")
else: return OuterAtomic(using, savepoint, read_committed)
stored_file_name = file_storage.save(stored_file_name, uploaded_file)
USER_SETTINGS_CHANGED_EVENT_NAME = u'edx.user.settings.changed'
return {}
if isinstance(value, Country): if value.code: return value.code else: return None return value
if hasattr(instance, '_changed_fields'): del instance._changed_fields
max_value_length = settings.TRACK_MAX_EVENT / 4
module = descriptor
settings.ALLOWED_HOSTS = [request.META['HTTP_HOST']] self.assertEqual(safe_get_host(request), request.META['HTTP_HOST'])
settings.ALLOWED_HOSTS = ["the_valid_website.com"] with self.assertRaises(SuspiciousOperation): safe_get_host(request)
UNICODE_CHAR_CODES = (range(30) + [127] + [129, 500, 2 ** 8 - 1, 2 ** 8 + 1, 2 ** 16 - 1])
self.assertEqual(safe_key(1, 'prefix', 'version'), 'prefix:version:1')
self.assertEqual(safe_key('test', 5, 'version'), '5:version:test')
self.assertEqual(safe_key('test', 'prefix', 5), 'prefix:5:test')
for length in [248, 249, 250, 251, 252]:
key = 'a' * length
key = safe_key(key, '', '')
self.assertTrue(self._is_valid_key(key), msg="Failed for key length {0}".format(length))
key = safe_key('a' * 300, 'prefix', 'version') self.assertTrue(self._is_valid_key(key))
key = safe_key('key', 'a' * 300, 'version') self.assertTrue(self._is_valid_key(key))
key = safe_key('key', 'prefix', 'a' * 300) self.assertTrue(self._is_valid_key(key))
key = unichr(unicode_char)
key = safe_key(key, '', '')
self.assertTrue(self._is_valid_key(key), msg="Failed for unicode character {0}".format(unicode_char))
prefix = unichr(unicode_char)
key = safe_key('test', prefix, '')
self.assertTrue(self._is_valid_key(key), msg="Failed for unicode character {0}".format(unicode_char))
version = unichr(unicode_char)
key = safe_key('test', '', version)
self.assertTrue(self._is_valid_key(key), msg="Failed for unicode character {0}".format(unicode_char))
if len(key) > 250: return False
for char in key: if ord(char) < 33 or ord(char) == 127: return False
verify_file_presence(False)
verify_file_presence(True)
self.assertEqual(expected, dtime.strftime(fmt.encode('utf8')).decode('utf8'))
response = organizations_helpers.get_organization_by_short_name('non_existing') self.assertIsNone(response)
RateLimitConfiguration.objects.create(enabled=True)
request = mock.Mock() with self.assertRaises(Throttled): self.view.check_throttles(request)
RateLimitConfiguration.objects.create(enabled=False)
request = mock.Mock() self.view.check_throttles(request)
self.assertFalse(zendesk_mock_class.return_value.mock_calls) self.assertFalse(datadog_mock.mock_calls)
self.assertFalse(zendesk_mock_class.mock_calls) self.assertFalse(datadog_mock.mock_calls)
PYTHON_LIB_ZIP = "python_lib.zip"
COURSE_REGEX = re.compile(r'^(.*?/courses/)(?!v[0-9]+/[^/]+){}'.format(settings.COURSE_ID_PATTERN))
#pylint: skip-file from __future__ import unicode_literals
from __future__ import unicode_literals
content = None try: content = self.load_asset_from_location(loc) except (ItemNotFoundError, NotFoundError): return HttpResponseNotFound()
safe_course_key = loc.course_key if safe_course_key.run is None: safe_course_key = safe_course_key.replace(run='only')
is_from_cdn = StaticContentServer.is_cdn_request(request) newrelic.agent.add_custom_parameter('contentserver.from_cdn', is_from_cdn)
locked = self.is_content_locked(content) newrelic.agent.add_custom_parameter('contentserver.locked', locked)
if not self.is_user_authorized(request, content, loc): return HttpResponseForbidden('Unauthorized')
last_modified_at_str = content.last_modified_at.strftime(HTTP_DATE_FORMAT) if 'HTTP_IF_MODIFIED_SINCE' in request.META: if_modified_since = request.META['HTTP_IF_MODIFIED_SINCE'] if if_modified_since == last_modified_at_str: return HttpResponseNotModified()
log.exception( u"%s in Range header: %s for content: %s", exception.message, header_value, unicode(loc) )
log.warning(u"Unknown unit in Range header: %s for content: %s", header_value, unicode(loc))
log.warning( u"More than 1 ranges in Range header: %s for content: %s", header_value, unicode(loc) )
if response is None: response = HttpResponse(content.stream_data()) response['Content-Length'] = content.length
response['Accept-Ranges'] = 'bytes' response['Content-Type'] = content.content_type
return True
content = get_cached_content(location) if content is None: try: content = AssetManager.find(location, as_stream=True) except (ItemNotFoundError, NotFoundError): raise
for byte_range_string in byte_ranges_string.split(','): byte_range_string = byte_range_string.strip()
if hasattr(settings, 'DEPRECATED_ADVANCED_COMPONENT_TYPES'): xblock_types.extend( xblock_type for xblock_type in settings.DEPRECATED_ADVANCED_COMPONENT_TYPES if xblock_type not in xblock_types )
from __future__ import unicode_literals
from __future__ import unicode_literals
XBlockDisableConfig.objects.create( disabled_blocks='', enabled=True )
wrapped_func = wrapped_func.__func__
UserSocialAuth._meta.app_label = "default" Nonce._meta.app_label = "default" Association._meta.app_label = "default" Code._meta.app_label = "default"
__BACKUP_ATTRIBUTE_NAME = '__monkey_patch'
Options.FORWARD_PROPERTIES = {'fields', 'many_to_many', 'concrete_fields', 'local_concrete_fields', '_forward_fields_map'}
if timeout_in_seconds: utc_now = datetime.utcnow()
last_touch = request.session.get(LAST_TOUCH_KEYNAME)
if last_touch: time_since_last_activity = utc_now - last_touch
if time_since_last_activity > timedelta(seconds=timeout_in_seconds): del request.session[LAST_TOUCH_KEYNAME] auth.logout(request) return
import lettuce.django
xf.XMODULE_FACTORY_LOCK.enable()
success = False num_attempts = 0 while (not success) and num_attempts < MAX_VALID_BROWSER_ATTEMPTS:
if not success: raise IOError("Could not acquire valid {driver} browser session.".format(driver=browser_driver))
'transcript': 'http://video.google.com/timedtext?lang=en&v=OEoXaMPEzfM',
scenario.steps = [] return
return False
if method in self.URL_HANDLERS: handlers_list = self.URL_HANDLERS[method] else: self.log_error("Unrecognized method '{method}'".format(method=method)) return
if self._match_pattern(handlers_list): return else: self.send_response(404, content="404 Not Found")
notes = deepcopy(notes[start:end])
elif self._is_correct_lti_request(): params = {k: v for k, v in self.post_dict.items() if k != 'oauth_signature'}
response = requests.post(url, data=data, headers=headers, verify=False)
if self.post_dict.get('roles'): role = '<h5>Role: {}</h5>'.format(self.post_dict['roles']) else: role = ''
return urllib.unquote(urllib.unquote(response_str))
'Content-Type': content_type,
logging.basicConfig(level=logging.DEBUG, format="%(levelname)s %(message)s")
DEFAULT_DELAY_SEC = 0.5
time.sleep(self.server.config.get('time_to_response', self.DEFAULT_DELAY_SEC))
callback = self.get_params['callback']
if comment_id in self.server.config.get('comments', {}): comment = self.server.config['comments'][comment_id] self.send_json_response(comment)
self.server.config['test_reset'] = 'This is a reset config test'
response = requests.delete(reset_config_url) self.assertEqual(response.status_code, 200)
self.assertEqual(self.server.config, {})
response = requests.get(self._get_url("api/v1/search")) self.assertEqual(response.status_code, 400)
response = requests.get(self._get_url("api/v1/annotations")) self.assertEqual(response.status_code, 400)
response = requests.get(self._get_url("api/v1/annotations"), params={"user": "dummy-user-id"})
response = requests.get(self._get_url("api/v1/annotations"), params={ "user": "dummy-user-id", "page": 2, "page_size": 3 })
self.test_cleanup()
patcher = mock.patch('terrain.stubs.xqueue.post') self.post = patcher.start() self.addCleanup(patcher.stop)
patcher = mock.patch('terrain.stubs.xqueue.Timer') timer = patcher.start() timer.side_effect = FakeTimer self.addCleanup(patcher.stop)
expected_body = json.dumps({'correct': True, 'score': 1, 'msg': '<div></div>'}) self._check_grade_response(callback_url, expected_header, expected_body)
response_content = {'test_response': 'test_content'} self.server.config['default'] = response_content
self._check_grade_response(callback_url, expected_header, json.dumps(response_content))
response_content = {'test_response': 'test_content'} self.server.config['This is only a test.'] = response_content
self._check_grade_response(callback_url, expected_header, json.dumps(response_content))
self.server.config['test_1'] = {'response': True} self.server.config['test_2'] = {'response': False}
self.assertFalse(self.post.called) self.assertTrue(logger.error.called)
self.assertEqual(resp.status_code, 200)
return grade_request['xqueue_header']
expected_callback_dict = { 'xqueue_header': expected_header, 'xqueue_body': expected_body, }
self.post.assert_called_with(callback_url, data=expected_callback_dict)
post_params = {key: json.dumps(val)} response = requests.put(self.url, data=post_params) self.assertEqual(response.status_code, 200)
for key, val in params.iteritems(): self.assertEqual(self.server.config.get(key), val)
response = requests.put(self.url, data={'test_unicode': u'\u2603 the snowman'}) self.assertEqual(response.status_code, 400)
response = requests.get(self.url, params={"test_param": 2}) self.assertEqual(response.status_code, 200)
response = requests.get(self.url) self.assertEqual(response.status_code, 400)
response = requests.get(self.url + "?test_param=") self.assertEqual(response.status_code, 400)
response = requests.post(self.url, data={"test_param": 2}) self.assertEqual(response.status_code, 200)
response = requests.post(self.url) self.assertEqual(response.status_code, 400)
response = requests.post(self.url, data={"test_param": None}) self.assertEqual(response.status_code, 400)
missing = [] for key in required_keys: if params.get(key) is None: missing.append(key)
else: return func(self, *args, **kwargs)
try: post_dict = urlparse.parse_qs(contents, keep_blank_values=True) return { key: list_val[0] for key, list_val in post_dict.items() }
return { key: value[0] if len(value) == 1 else value for key, value in urlparse.parse_qs(query).items() }
try: key = unicode(key, 'utf-8') value = unicode(value, 'utf-8') except UnicodeDecodeError: self.log_message("Could not decode request params as UTF-8")
else: self.send_response(200)
HANDLER_CLASS = StubHttpRequestHandler
self.config = dict()
server_thread = threading.Thread(target=self.serve_forever) server_thread.daemon = True server_thread.start()
LOGGER.debug('Starting service on port {0}'.format(self.port))
HTTPServer.shutdown(self)
self.socket.close()
if self._is_grade_request():
error_msg = "XQueue received invalid grade request" self._send_immediate_response(False, message=error_msg)
error_msg = "XQueue could not decode grade request" self._send_immediate_response(False, message=error_msg)
self._send_immediate_response(True)
delayed_grade_func = lambda: self._send_grade_response( callback_url, xqueue_header, self.post_dict['xqueue_body'] )
else: self._send_immediate_response(False, message="Invalid request URL")
response_str = json.dumps( {'return_code': 0 if success else 1, 'content': message} )
grade_response = None
else: self.log_error( "Multiple response patterns matched '{0}'".format(xqueue_body_json), ) return
if grade_response is None: grade_response = self.server.config.get( 'default', copy.deepcopy(self.DEFAULT_GRADE_RESPONSE) )
if isinstance(grade_response, dict) and 'msg' in grade_response: grade_response['msg'] = "<div>{0}</div>".format(grade_response['msg'])
if url is not None:
grader_payload = xqueue_body.get('grader_payload')
import lettuce.django
re.compile(r'^Schedule & Details Settings \|'): [ "jquery", "js/base", "js/models/course", "js/models/settings/course_details", "js/views/settings/main"],
re.compile(r'^Advanced Settings \|'): [ "jquery", "js/base", "js/models/course", "js/models/settings/advanced", "js/views/settings/advanced", "codemirror"],
re.compile(r'^Course Outline \|'): [ "js/base", "js/models/course", "js/models/location", "js/models/section"],
re.compile(r'^Pages \|'): [ 'js/models/explicit_url', 'coffee/src/views/tabs', 'xmodule', 'coffee/src/main', 'xblock/cms.runtime.v1' ],
world.wait(1) continue
world.wait(1) continue
if result['requireType'] == 'require': world.wait(1) continue
if dependencies[0] != "jquery": dependencies.insert(0, "jquery")
world.wait(1) continue
if text: wait_for(lambda _: css_text(css_selector, index=index))
if partial_text: wait_for(lambda _: css_html(css_selector, index=index), timeout=8)
if value: wait_for(lambda _: css_value(css_selector, index=index))
if is_css_present(css_selector): return retry_on_exception(lambda: css_find(css_selector, wait_time=timeout)[index].text) else: return ""
if is_css_present(css_selector): return retry_on_exception(lambda: css_find(css_selector)[index].value) else: return ""
world.wait_for_js_to_load()
world.browser.execute_script("jQuery.fx.off = true;")
if len(User.objects.filter(username=uname)) > 0: return
user = User.objects.get(username=username) world.scenario_dict['USER'] = user
if is_staff: user.is_staff = True user.save() CourseEnrollment.enroll(user, course_key)
registration = world.RegistrationFactory(user=user) registration.register(user) registration.activate() CourseEnrollment.enroll(user, course_key)
languages.sort() return languages
self.assertAcceptEquals( 'rel;q=1.0, rel;q=0.5', self.process_request(accept='rel-ter;q=1.0, rel;q=0.5') )
self.assertAcceptEquals( 'rel-ter;q=1.0, rel-ter;q=0.5', self.process_request(accept='rel-ter;q=1.0, rel;q=0.5') )
DarkLangConfig( released_languages=('es-419, en'), changed_by=self.user, enabled=True ).save()
DarkLangConfig( released_languages=('es, en'), changed_by=self.user, enabled=True ).save()
DarkLangConfig( released_languages=('es-419, es, es-es'), changed_by=self.user, enabled=True ).save() self.assertAcceptEquals( expected, self.process_request(accept=accept_header) )
self.assertSessionLangEquals( 'rel', self.process_request(preview_lang='rel') )
DARK_LANGUAGE_KEY = 'dark-lang'
from __future__ import unicode_literals
from django.db import migrations, models
from __future__ import unicode_literals
CHINESE_LANGUAGE_CODE_MAP = {
if LANGUAGE_SESSION_KEY in request.session: del request.session[LANGUAGE_SESSION_KEY]
delete_user_preference(request.user, DARK_LANGUAGE_KEY) user_pref = get_user_preference(request.user, LANGUAGE_KEY) if user_pref: request.session[LANGUAGE_SESSION_KEY] = user_pref
preview_lang = request.GET.get('preview-lang', None) if not preview_lang and auth_user: preview_lang = get_user_preference(request.user, DARK_LANGUAGE_KEY)
if not preview_lang: return
request.session[LANGUAGE_SESSION_KEY] = preview_lang
if auth_user: set_user_preference(request.user, DARK_LANGUAGE_KEY, preview_lang)
course_id = CourseKeyField(max_length=255, db_index=True, verbose_name=_("Course"))
mode_slug = models.CharField(max_length=100, verbose_name=_("Mode"))
mode_display_name = models.CharField(max_length=255, verbose_name=_("Display Name"))
currency = models.CharField(default="usd", max_length=8)
expiration_datetime_is_explicit = models.BooleanField(default=False)
expiration_date = models.DateField(default=None, null=True, blank=True)
suggested_prices = models.CommaSeparatedIntegerField(max_length=255, blank=True, default='')
description = models.TextField(null=True, blank=True)
bulk_sku = models.CharField( max_length=255, null=True, blank=True,
VERIFIED_MODES = [VERIFIED, PROFESSIONAL]
NON_VERIFIED_MODES = [HONOR, AUDIT, NO_ID_PROFESSIONAL_MODE]
CREDIT_MODES = [CREDIT_MODE]
UPSELL_TO_VERIFIED_MODES = [HONOR, AUDIT]
DEFAULT_SHOPPINGCART_MODE_SLUG = HONOR DEFAULT_SHOPPINGCART_MODE = Mode(HONOR, _('Honor'), 0, '', 'usd', None, None, None, None)
if new_datetime is not None: self.expiration_datetime_is_explicit = True self._expiration_datetime = new_datetime
missing_courses = set(course_id_list) - set(modes_by_course.keys()) for course_id in missing_courses: modes_by_course[course_id] = [cls.DEFAULT_MODE]
if not include_expired: found_course_modes = found_course_modes.filter( Q(_expiration_datetime__isnull=True) | Q(_expiration_datetime__gte=now) )
return professional_mode if professional_mode else verified_mode
if cls.has_professional_mode(modes_dict): return False
if cls.is_white_label(course_id, modes_dict=modes_dict): return False
return cls.AUDIT in modes_dict or cls.HONOR in modes_dict
course_id = CourseKeyField(max_length=255, db_index=True)
mode_slug = models.CharField(max_length=100)
mode_display_name = models.CharField(max_length=255)
min_price = models.IntegerField(default=0)
suggested_prices = models.CommaSeparatedIntegerField(max_length=255, blank=True, default='')
currency = models.CharField(default="usd", max_length=8)
expiration_date = models.DateField(default=None, null=True, blank=True)
url(r'^choose/{}/$'.format(settings.COURSE_ID_PATTERN), views.ChooseModeView.as_view(), name='course_modes_choose'),
embargo_redirect = embargo_api.redirect_if_blocked( course_key, user=request.user, ip_address=get_ip(request), url=request.path ) if embargo_redirect: return redirect(embargo_redirect)
if is_active and (enrollment_mode in CourseMode.VERIFIED_MODES + [CourseMode.NO_ID_PROFESSIONAL_MODE]): return redirect(reverse('dashboard'))
return redirect(reverse('dashboard'))
amount_value = decimal.Decimal(amount).quantize(decimal.Decimal('.01'), rounding=decimal.ROUND_DOWN)
if amount_value < mode_info.min_price: error_msg = _("No selected price or selected price is too low.") return self.get(request, course_id, error=error_msg)
for parameter, default in PARAMETERS.iteritems(): PARAMETERS[parameter] = request.GET.get(parameter, default)
course_key = CourseKey.from_string(course_id) CourseMode.objects.get_or_create(course_id=course_key, **PARAMETERS)
return HttpResponse("Mode '{mode_slug}' created for '{course}'.".format( mode_slug=PARAMETERS['mode_slug'], course=course_id ))
[(CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG, CourseMode.DEFAULT_SHOPPINGCART_MODE_SLUG)]
if self.cleaned_data.get("_expiration_datetime"): return self.cleaned_data.get("_expiration_datetime").replace(tzinfo=UTC)
if verification_deadline is not None and mode_slug not in CourseMode.VERIFIED_MODES: raise forms.ValidationError("Verification deadline can be set only for verified modes.")
if verification_deadline is not None: if upgrade_deadline is not None and verification_deadline < upgrade_deadline: raise forms.ValidationError("Verification deadline must be after the upgrade deadline.")
if course_key is not None and mode_slug in CourseMode.VERIFIED_MODES: verification_models.VerificationDeadline.set_deadline(course_key, verification_deadline)
expiration_datetime_custom.short_description = "Upgrade Deadline"
from __future__ import unicode_literals
from __future__ import unicode_literals from datetime import timedelta
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
response = self.client.post(reverse('admin:course_modes_coursemode_add'), data=data) self.assertRedirects(response, reverse('admin:course_modes_coursemode_changelist'))
course_mode = CourseMode.objects.get(pk=1) self.assertEqual(course_mode.expiration_datetime.replace(tzinfo=None), expiration.replace(tzinfo=None))
VerificationDeadline.set_deadline(self.course.id, self.VERIFICATION_DEADLINE)
deadline = self.UPGRADE_DEADLINE if mode == "verified" else None form = self._admin_form(mode, upgrade_deadline=deadline)
VerificationDeadline.set_deadline(self.course.id, self.VERIFICATION_DEADLINE)
form = self._admin_form(course_mode)
new_deadline = (self.VERIFICATION_DEADLINE + timedelta(days=1)).replace(microsecond=0) self._set_form_verification_deadline(form, new_deadline) form.save()
updated_deadline = VerificationDeadline.deadline_for_course(self.course.id) self.assertEqual(updated_deadline, new_deadline)
VerificationDeadline.set_deadline(self.course.id, self.VERIFICATION_DEADLINE)
form = self._admin_form("verified", upgrade_deadline=self.UPGRADE_DEADLINE)
self._set_form_verification_deadline(form, None) form.save()
self.assertIs(VerificationDeadline.deadline_for_course(self.course.id), None)
form = self._admin_form(course_mode) self._set_form_verification_deadline(form, self.VERIFICATION_DEADLINE) self._assert_form_has_error(form, "Verification deadline can be set only for verified modes.")
class CourseModeFactory(DjangoModelFactory): class Meta(object): model = CourseMode
for mode in ('audit', 'honor', 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
if enrollment_mode is not None: CourseEnrollmentFactory( is_active=is_active, mode=enrollment_mode, course_id=self.course.id, user=self.user )
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
if redirect: self.assertRedirects(response, reverse('dashboard')) else: self.assertEquals(response.status_code, 200)
CourseModeFactory.create(mode_slug=CourseMode.NO_ID_PROFESSIONAL_MODE, course_id=self.course.id, min_price=100)
CourseEnrollmentFactory( is_active=False, mode=CourseMode.NO_ID_PROFESSIONAL_MODE, course_id=self.course.id, user=self.user )
for mode in ('audit', 'honor', 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
for mode in ('audit', 'honor'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
CourseEnrollmentFactory( is_active=True, course_id=self.course.id, user=self.user )
response = self.client.get( reverse('course_modes_choose', args=[unicode(self.course.id)]), follow=False, )
for mode in available_modes: CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
CourseModeFactory.create(mode_slug=mode, course_id=self.course.id, min_price=1)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(choose_track_url)
start_flow_url = reverse('verify_student_start_flow', args=[unicode(self.course.id)]) self.assertRedirects(response, start_flow_url)
CourseEnrollmentFactory( user=self.user, is_active=True, mode=mode, course_id=unicode(self.course.id), )
response = self.client.get(choose_track_url) self.assertRedirects(response, reverse('dashboard'))
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE[course_mode])
CourseModeFactory.create(mode_slug='honor', course_id=self.course.id) CourseModeFactory.create(mode_slug='verified', course_id=self.course.id, min_price=1)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE['verified'])
self.assertIn('donation_for_course', self.client.session) self.assertIn(unicode(self.course.id), self.client.session['donation_for_course'])
for mode in (CourseMode.DEFAULT_MODE_SLUG, 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
params = { 'enrollment_action': 'enroll', 'course_id': unicode(self.course.id) } self.client.post(reverse('change_enrollment'), params)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE[CourseMode.DEFAULT_MODE_SLUG])
mode, is_active = CourseEnrollment.enrollment_mode_for_user(self.user, self.course.id) self.assertEqual(mode, CourseMode.DEFAULT_MODE_SLUG) self.assertEqual(is_active, True)
for mode in ('honor', 'verified'): CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
choose_track_url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.post(choose_track_url, self.POST_PARAMS_FOR_COURSE_MODE['unsupported'])
url = reverse('create_mode', args=[unicode(self.course.id)]) response = self.client.get(url)
base_url = reverse('create_mode', args=[unicode(self.course.id)]) self.client.get(base_url)
url = reverse('create_mode', args=[unicode(self.course.id)]) self.client.get(url, parameters)
for mode in ["honor", "verified"]: CourseModeFactory.create(mode_slug=mode, course_id=self.course.id)
url = reverse('course_modes_choose', args=[unicode(self.course.id)]) response = self.client.get(url)
self.assertNotContains(response, "How it Works") self.assertNotContains(response, "Find courses") self.assertNotContains(response, "Schools & Partners")
redirect_url = reverse('dashboard') + '?course_closed=1%2F1%2F15%2C+12%3A00+AM' self.assertRedirects(response, redirect_url)
self.url = reverse('course_modes_choose', args=[unicode(self.course.id)])
modes = CourseMode.modes_for_course(self.course_key) self.assertEqual([CourseMode.DEFAULT_MODE], modes)
self.assertEqual(0, CourseMode.min_course_price_for_currency(self.course_key, 'usd'))
self.create_mode('professional', 'Professional Education Verified Certificate', 10)
honor, _ = self.create_mode('honor', 'Honor') self.assertFalse(CourseMode.has_payment_options(self.course_key))
verified, _ = self.create_mode('verified', 'Verified', min_price=5) self.assertTrue(CourseMode.has_payment_options(self.course_key))
verified.delete() self.assertFalse(CourseMode.has_payment_options(self.course_key))
honor.suggested_prices = '5, 10, 15' honor.save() self.assertTrue(CourseMode.has_payment_options(self.course_key))
self.create_mode('no-id-professional', 'no-id-professional', min_price=5) self.assertTrue(CourseMode.has_payment_options(self.course_key))
for mode_slug, min_price in modes_and_prices: self.create_mode(mode_slug, mode_slug.capitalize(), min_price=min_price)
self.assertEqual(CourseMode.can_auto_enroll(self.course_key), can_auto_enroll)
self.assertEqual(CourseMode.auto_enroll_mode(self.course_key, modes), result)
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Honor No Expiration", mode_slug="honor_no_expiration", expiration_datetime=None )
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Honor Not Expired", mode_slug="honor_not_expired", expiration_datetime=future )
CourseModeFactory.create( course_id=self.course_key, mode_display_name="Verified Expired", mode_slug="verified_expired", expiration_datetime=past )
self.assertEqual(len(all_modes[other_course_key]), 1) self.assertEqual(all_modes[other_course_key][0], CourseMode.DEFAULT_MODE)
self.assertFalse(CourseMode.is_professional_mode(None))
if is_verified: self.assertTrue(CourseMode.is_verified_slug(mode_slug)) else: self.assertFalse(CourseMode.is_verified_slug(mode_slug))
for mode in available_modes: CourseModeFactory.create( course_id=self.course_key, mode_display_name=mode, mode_slug=mode, )
selectable_modes = CourseMode.modes_for_course_dict(self.course_key) self.assertItemsEqual(selectable_modes.keys(), expected_selectable_modes)
all_modes = CourseMode.modes_for_course_dict(self.course_key, only_selectable=False) self.assertItemsEqual(all_modes.keys(), available_modes)
return "SELECT MAX(id) FROM {table_name} GROUP BY {key_fields}".format( key_fields=', '.join(key_fields_escaped),
cache_timeout = 600
verbose_name=_("Changed by"),
patcher = patch('config_models.models.cache', Mock(get=Mock(return_value=None))) patcher.start() self.addCleanup(patcher.stop)
with transaction.atomic(): return wrapped_func(*args, **kwargs)
return self.model.current()
serializer.save(changed_by=self.request.user)
def has_delete_permission(self, request, obj=None): return False
def get_readonly_fields(self, request, obj=None):
queryset = self.model.objects.current_set()
if course_message: msg = u"{} <br /> {}".format(msg, course_message.message)
pass
cache.clear() self.course_key = CourseLocator(org='TestOrg', course='TestCourse', run='TestRun')
self.assertEqual(get_site_status_msg(None), None) self.assertEqual(get_site_status_msg(self.course_key), None)
from __future__ import unicode_literals
if not GlobalStatusMessage.current().enabled: return None
if not username: username = request.user.username if username != request.user.username and not has_api_key_permissions: return Response(status=status.HTTP_404_NOT_FOUND)
user = User.objects.get(username=username)
response = api.add_enrollment(username, unicode(course_id), mode=mode, is_active=is_active)
manage.py ... enroll_user_in_course -e test@example.com -c edX/Open_DemoX/edx_demo_course
pass
user_enroll = get_enrollment(self.username, self.course_id) self.assertTrue(user_enroll['is_active'])
log.exception(u"Error occurred while retrieving course enrollment details from the cache")
log.exception(u"Error occurred while caching course enrollment details for course %s", course_id) raise errors.CourseEnrollmentError(u"An unexpected error occurred while retrieving course enrollment details.")
include_expired = not is_active if is_active is not None else False
api_path = getattr(settings, "ENROLLMENT_DATA_API", DEFAULT_DATA_API)
deleted = [] valid = [] for enrollment in enrollments: if enrollment.get("course_details") is not None: valid.append(enrollment) else: deleted.append(enrollment)
self.data = data
([], 'honor'),
(['honor', 'verified', 'audit'], 'honor'),
(['professional'], 'professional'), (['no-id-professional'], 'no-id-professional')
fake_data_api.add_course(self.COURSE_ID, course_modes=course_modes) api.add_enrollment(self.USERNAME, self.COURSE_ID)
fake_data_api.add_course(self.COURSE_ID, course_modes=['professional']) api.add_enrollment(self.USERNAME, self.COURSE_ID, mode='verified')
([], 'honor'),
(['honor', 'verified', 'audit'], 'honor'),
(['professional'], 'professional'), (['no-id-professional'], 'no-id-professional')
fake_data_api.add_course(self.COURSE_ID, course_modes=['honor']) api.update_enrollment(self.USERNAME, self.COURSE_ID, mode='honor', is_active=False)
([]),
api.add_enrollment(self.USERNAME, self.COURSE_ID, mode='audit')
fake_data_api.add_course(self.COURSE_ID, course_modes=['honor', 'verified', 'audit'])
details = api.get_course_enrollment_details(self.COURSE_ID)
fake_data_api.reset() cached_details = api.get_course_enrollment_details(self.COURSE_ID)
self.assertEqual(len(details['course_modes']), 3) self.assertEqual(details, cached_details)
([], 'honor'),
(['honor', 'verified', 'audit'], 'honor'),
self._create_course_modes(course_modes) enrollment = data.create_course_enrollment( self.user.username, unicode(self.course.id), enrollment_mode, True )
self.assertEqual(course_mode, enrollment['mode']) self.assertEqual(is_active, enrollment['is_active'])
CourseEnrollment.enroll(self.user, self.course.id, mode="honor")
self.assertFalse(enrollment['is_active'])
self.assertFalse(CourseEnrollment.is_enrolled(self.user, self.course.id))
([]),
(['honor', 'verified', 'audit']),
([], []),
(['honor', 'verified', 'audit'], ['1', '2', '3']),
created_courses = [] for course_number in course_numbers: created_courses.append(CourseFactory.create(number=course_number))
created_enrollments.append(data.create_course_enrollment( self.user.username, unicode(course.id), 'honor', True ))
results = data.get_course_enrollments(self.user.username) self.assertEqual(results, created_enrollments)
([], 'honor'),
(['honor', 'verified', 'audit'], 'verified'),
result = data.get_course_enrollment(self.user.username, unicode(self.course.id)) self.assertIsNone(result)
([], 'credit'),
(['honor', 'verified', 'audit', 'credit'], 'credit'),
self.assertTrue(mock_audit_log.called)
mock_audit_log.reset_mock()
self.course = CourseFactory.create(emit_signals=True)
([], CourseMode.DEFAULT_MODE_SLUG),
([CourseMode.VERIFIED, CourseMode.AUDIT], CourseMode.DEFAULT_MODE_SLUG),
for mode_slug in course_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode_slug, mode_display_name=mode_slug, )
self.assert_enrollment_status()
CourseModeFactory.create( course_id=self.course.id, mode_slug='professional', mode_display_name='Professional Education', )
resp = self.assert_enrollment_status(expected_status=status.HTTP_400_BAD_REQUEST)
self.client.logout()
self.assert_enrollment_status(expected_status=status.HTTP_401_UNAUTHORIZED)
self.client.logout()
self.user = UserFactory.create( username="inactive", email="inactive@example.com", password=self.PASSWORD, is_active=True )
self.client.login(username="inactive", password=self.PASSWORD)
self.user.is_active = False self.user.save()
self.assert_enrollment_status()
self.client.logout() response = self.client.get(url, **{'HTTP_X_EDX_API_KEY': self.API_KEY}) self.assertEqual(response.status_code, status.HTTP_200_OK)
__ = CourseOverview.get_from_id(course.id)
resp = self.client.get(reverse('courseenrollments')) self.assertEqual(resp.status_code, status.HTTP_200_OK)
CourseModeFactory.create( course_id=self.course.id, mode_slug='professional', mode_display_name='professional', )
self.assert_enrollment_status(as_server=True, mode='professional')
CourseModeFactory.create( course_id=self.course.id, mode_slug=CourseMode.HONOR, mode_display_name=CourseMode.HONOR, )
CourseModeFactory.create( course_id=self.course.id, mode_slug=CourseMode.VERIFIED, mode_display_name=CourseMode.VERIFIED, expiration_datetime='1970-01-01 05:00:00' )
self.assertEqual(len(v_data['course_modes']), 2)
self.assertEqual(len(h_data['course_modes']), 1) self.assertEqual(h_data['course_modes'][0]['slug'], CourseMode.HONOR)
for mode in [CourseMode.DEFAULT_MODE_SLUG, CourseMode.VERIFIED]: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status(as_server=True)
self.assert_enrollment_status(as_server=True)
self.assert_enrollment_status(as_server=True)
for mode in [CourseMode.DEFAULT_MODE_SLUG, CourseMode.VERIFIED]: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status(as_server=True, mode=CourseMode.VERIFIED)
for mode in configured_modes: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status(as_server=True, mode=selected_mode)
self.assert_enrollment_status( as_server=True, mode=None, is_active='foo', expected_status=status.HTTP_400_BAD_REQUEST )
self.assert_enrollment_activation(False, selected_mode)
self.assert_enrollment_activation(False, selected_mode)
expected_status = ( status.HTTP_200_OK if CourseMode.DEFAULT_MODE_SLUG in configured_modes else status.HTTP_400_BAD_REQUEST ) self.assert_enrollment_status( as_server=True, is_active=False, expected_status=expected_status, )
self.assert_enrollment_status(as_server=True, mode=CourseMode.VERIFIED)
self.assert_enrollment_activation(False, CourseMode.VERIFIED)
for mode in [CourseMode.DEFAULT_MODE_SLUG, CourseMode.VERIFIED]: CourseModeFactory.create( course_id=self.course.id, mode_slug=mode, mode_display_name=mode, )
self.assert_enrollment_status()
response = self.assert_enrollment_status( as_server=True, mode=new_mode, is_active=new_is_active, expected_status=expected_status, )
self.assertEqual(is_active, new_is_active) self.assertEqual(course_mode, new_mode)
__ = CourseOverview.get_from_id(self.course.id)
self.assertEqual(response.status_code, 403)
resp_data = json.loads(response.content) user_message_url = get_absolute_url(user_message_path) self.assertEqual(resp_data['user_message_url'], user_message_url)
self.assertEqual(self._get_enrollments(), [])
with restrict_course(self.course.id) as redirect_path: self.assert_access_denied(redirect_path)
cache.clear()
self.user.profile.country = restricted_country.country self.user.profile.save()
unrestricted_country, __ = self._setup_embargo()
self.user.profile.country = unrestricted_country.country self.user.profile.save() self.assert_enrollment_status()
self.assertEqual(len(self._get_enrollments()), 1)
self.assertEqual(resp.status_code, 200)
valid_assocs = [a for a in associations if a.getExpiresIn() > 0] if valid_assocs: valid_assocs.sort(lambda a: a.getExpiresIn(), reverse=True) assoc = valid_assocs.sort[0]
return 0
return 0
response = None
response = external_auth.views.redirect_with_get('root', request.GET)
response = redirect(reverse('cas-login'))
response = external_auth.views.redirect_with_get('root', request.GET)
request.session['ExternalAuthMap'] = eamap
username = re.sub(r'\s', '', _flatten_to_ascii(eamap.external_name), flags=re.UNICODE)
context['ask_for_fullname'] = eamap.external_name.strip() == ''
try: validate_email(eamap.external_email) context['ask_for_email'] = False except ValidationError: context['ask_for_email'] = True
cert = request._req.subprocess_env.get(certkey, '')
if not settings.FEATURES['AUTH_USE_CERTIFICATES']: return HttpResponseForbidden()
return student.views.index(request)
return redirect_with_get('signin_user', request.GET)
if ( settings.FEATURES.get('AUTH_USE_SHIB') and course.enrollment_domain and course.enrollment_domain.startswith(SHIBBOLETH_DOMAIN_PREFIX) ): return redirect_with_get('shib-login', request.GET)
return redirect_with_get('signin_user', request.GET)
return redirect_with_get('register_user', request.GET)
if ( settings.FEATURES.get('AUTH_USE_SHIB') and course.enrollment_domain and course.enrollment_domain.startswith(SHIBBOLETH_DOMAIN_PREFIX) ): return redirect_with_get('shib-login', request.GET)
return redirect_with_get('register_user', request.GET)
sreg_response = sreg.SRegResponse.extractResponse(sreg_request, sreg_data) sreg_response.toMessage(response.fields)
pass
ax_response.toMessage(response.fields)
add_openid_simple_registration(request, response, data) add_openid_attribute_exchange(request, response, data)
webresponse = server.encodeResponse(response) http_response = HttpResponse(webresponse.body) http_response.status_code = webresponse.code
for k, v in webresponse.headers.iteritems(): http_response[k] = v
return True
if (not hasattr(openid_request, 'trust_root') or not openid_request.trust_root): log.error('no trust_root') return False
trust_root = TrustRoot.parse(openid_request.trust_root) if not trust_root: log.error('invalid trust_root') return False
if (not hasattr(openid_request, 'return_to') or not openid_request.return_to): log.error('empty return_to') return False
if not trust_root.validateURL(openid_request.return_to): log.error('invalid return_to') return False
if not any(r for r in trusted_roots if fnmatch.fnmatch(trust_root, r)): log.error('non-trusted root') return False
endpoint = get_xrds_url('login', request) if not endpoint: return default_render_failure(request, "Invalid OpenID request")
store = DjangoOpenIDStore() server = Server(store, endpoint)
if not validate_trust_root(openid_request): return default_render_failure(request, "Invalid OpenID trust root")
if openid_request.mode == 'checkid_immediate': return provider_respond(server, openid_request, openid_request.answer(False), {})
if 'openid_error' in request.session: error = True del request.session['openid_error']
else: return provider_respond(server, openid_request, server.handleRequest(openid_request), {})
if not validate_trust_root(openid_request): return default_render_failure(request, "Invalid OpenID trust root")
if user is not None and user.is_active: if 'openid_error' in request.session: del request.session['openid_error']
url = endpoint + urlquote(user.username) response = openid_request.answer(True, None, url)
results = { 'nickname': user.username, 'email': user.email, 'fullname': user.profile.name, }
return provider_respond(server, openid_request, response, results)
response = render_to_response('provider_login.html', { 'error': error, 'return_to': return_to })
response['X-XRDS-Location'] = get_xrds_url('xrds', request) return response
response['X-XRDS-Location'] = get_xrds_url('identity', request) return response
response['X-XRDS-Location'] = get_xrds_url('xrds', request) return response
from __future__ import unicode_literals
self.assertIn(SESSION_KEY, self.client.session)
response = self.client.get(reverse('signup'), follow=True) self.assertEqual(response.status_code, 404)
response = self.client.get(reverse('signin_user')) self.assertEqual(200, response.status_code) self.assertTrue('login-and-registration-container' in response.content)
dec_mock(request) self.assertTrue(self.mock.called) self.assertEqual(0, len(ExternalAuthMap.objects.all()))
self.mock.reset_mock() request = self._create_ssl_request(self.MOCK_URL) request.user = UserFactory() dec_mock(request) self.assertTrue(self.mock.called)
self.assertIn(SESSION_KEY, self.client.session)
IDP = 'https://idp.stanford.edu/' REMOTE_USER = 'test_user@stanford.edu'
self.assertEquals(len(audit_log_calls), 0)
self.assertEquals(len(audit_log_calls), 0)
response = client.get(path='/shib-login/', data={}, follow=False, **identity)
client = DjangoTestClient() response1 = client.get(path='/shib-login/', data={}, follow=False, **identity) postvars = {'email': u'post_email@stanford.edu',
request2 = self.request_factory.post('/create_account', data=postvars) request2.session = client.session request2.user = AnonymousUser()
if mail: self.assertEqual(user.email, mail) self.assertEqual(list(User.objects.filter(email=postvars['email'])), [])
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, course.id): course.enrollment_domain = domain self.store.update_item(course, self.test_user_id)
for course in [shib_course, open_enroll_course]: for student in [shib_student, other_ext_student, int_student]: request = self.request_factory.post('/change_enrollment')
data = parse_qs(body) response = self.client.post(url, data)
data = {} if headers and 'Accept' in headers: data['CONTENT_TYPE'] = headers['Accept'] response = self.client.get(url, data)
provider_url = reverse('openid-provider-xrds') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
with self.settings(OPENID_SSO_SERVER_URL=abs_provider_url):
provider_url = reverse('openid-provider-login') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
for key in kwargs: args["openid." + key] = kwargs[key]
for _ in xrange(30): self._send_bad_redirection_login()
self.assertEquals(response.status_code, 302) cache.clear()
return self.client.post(url, post_args)
return self.client.post(url, post_args)
user.profile.name = u'Jan ĄĘ'
self.attempt_login(200) user.is_active = False
self.client.post(url, post_args)
provider_url = reverse('openid-provider-xrds') factory = RequestFactory() request = factory.request() abs_provider_url = request.build_absolute_uri(location=provider_url)
try: value = result.get(timeout=4.0) success = True except TimeoutError: value = None success = False
response = self.client.get(self.ping_url)
self.assertEqual(response.status_code, 200)
result_dict = json.loads(response.content)
self.assertTrue(result_dict['success'])
self.assertEqual(result_dict['value'], "pong")
self.assertIsInstance(result_dict['task_id'], unicode) self.assertIsInstance(result_dict['time'], float) self.assertTrue(result_dict['time'] > 0.0)
import signals import exceptions
options = {'statsd': True}
dog_stats_api.start(**options)
full_url = "http://{site_name}".format(site_name=settings.SITE_NAME) parsed_url = urlparse(full_url)
return RequestFactory( SERVER_NAME=parsed_url.hostname, SERVER_PORT=parsed_url.port or 80, ).get("/")
course_id = CourseKeyField(max_length=255, db_index=True, unique=True)
embargoed = models.BooleanField(default=False)
return u"Course '{}' is {}Embargoed".format(self.course_id.to_deprecated_string(), not_em)
embargoed_countries = models.TextField( blank=True, help_text="A comma-separated list of country codes that fall under U.S. embargo restrictions" )
return ( cls.is_restricted_course(unicode(course_id)) and cls._get_restricted_courses_from_cache().get(unicode(course_id))["disable_access_check"] )
cache_key = cls.MESSAGE_URL_CACHE_KEY.format( access_point=access_point, course_key=course_key ) url = cache.get(cache_key)
if url is None: url = cls._get_message_url_path_from_db(course_key, access_point) cache.set(cache_key, url)
if not cls.is_restricted_course(course_key): return default_path
if country not in cls.ALL_COUNTRIES: return True
rules_for_course = CountryAccessRule.objects.select_related('country').filter( restricted_course__course_key=course_id )
if not whitelist_countries: whitelist_countries = cls.ALL_COUNTRIES
return list(whitelist_countries - blacklist_countries)
("restricted_course", "country")
RestrictedCourse.invalidate_cache_for_course(instance.course_key) CountryAccessRule.invalidate_cache_for_course(instance.course_key)
pass
CountryAccessRule.invalidate_cache_for_course(restricted_course.course_key)
post_save.connect(invalidate_country_rule_cache, sender=CountryAccessRule) post_save.connect(invalidate_country_rule_cache, sender=RestrictedCourse) post_delete.connect(invalidate_country_rule_cache, sender=CountryAccessRule) post_delete.connect(invalidate_country_rule_cache, sender=RestrictedCourse)
if settings.FEATURES.get('USE_CUSTOM_THEME') and message_key in messages.CUSTOM_THEME_OVERRIDES: message_dict = messages.CUSTOM_THEME_OVERRIDES
elif access_point == self.ENROLLMENT_ACCESS_POINT: message_dict = messages.ENROLL_MESSAGES elif access_point == self.COURSEWARE_ACCESS_POINT: message_dict = messages.COURSEWARE_MESSAGES
ipaddr.IPNetwork(address)
cache.clear()
CountryAccessRule.objects.all().delete()
country, __ = Country.objects.get_or_create(country='IR')
restricted_course, __ = RestrictedCourse.objects.get_or_create(course_key=course_key) restricted_course.enroll_msg_key = 'default' restricted_course.access_msg_key = 'default' restricted_course.disable_access_check = disable_access_check restricted_course.save()
CountryAccessRule.objects.get_or_create( restricted_course=restricted_course, country=country, rule_type='blacklist' )
mock_ip.return_value = 'IR'
redirect_url = reverse( 'embargo_blocked_message', kwargs={ 'access_point': access_point, 'message_key': 'default' } ) yield redirect_url
from __future__ import unicode_literals
from __future__ import unicode_literals
if not settings.FEATURES.get('EMBARGO'): return True
if user is not None and has_course_author_access(user, course_key): return True
user_country_from_ip = _country_code_from_ip(ip_address)
user_country_from_profile = _get_user_country_from_profile(user)
'description',
'template',
CUSTOM_THEME_OVERRIDES = { 'embargo': BlockedMessage( description='Embargo', template='static_templates/theme-embargo.html' ) }
cache.clear()
for whitelist_country in whitelist: CountryAccessRule.objects.create( rule_type=CountryAccessRule.WHITELIST_RULE, restricted_course=self.restricted_course, country=Country.objects.get(country=whitelist_country) )
if profile_country is not None: self.user.profile.country = profile_country self.user.profile.save()
with self._mock_geoip(ip_country): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
self.assertEqual(result, allow_access)
result = embargo_api.check_course_access(self.course.id, ip_address='0.0.0.0') self.assertTrue(result)
result = embargo_api.check_course_access(self.course.id, ip_address='0.0.0.0') self.assertFalse(result)
unrestricted_course = CourseFactory.create() with self.assertNumQueries(1): embargo_api.check_course_access(unrestricted_course.id, user=self.user, ip_address='0.0.0.0')
with self.assertNumQueries(0): embargo_api.check_course_access(unrestricted_course.id, user=self.user, ip_address='0.0.0.0')
result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='FE80::0202:B3FF:FE1E:8329') self.assertTrue(result)
result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0') self.assertTrue(result)
with self.assertNumQueries(3): embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
CountryAccessRule.objects.create( rule_type=CountryAccessRule.BLACKLIST_RULE, restricted_course=self.restricted_course, country=Country.objects.get(country='US') )
with self._mock_geoip('US'): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
self.assertFalse(result, msg="User should not have access because the user isn't staff.")
staff_role.add_users(self.user)
with self._mock_geoip('US'): result = embargo_api.check_course_access(self.course.id, user=self.user, ip_address='0.0.0.0')
url_path = embargo_api.message_url_path(self.course.id, access_point) self.assertEqual(url_path, expected_url_path)
with self.assertNumQueries(2): embargo_api.message_url_path(self.course.id, "enrollment")
with self.assertNumQueries(0): embargo_api.message_url_path(self.course.id, "enrollment")
url_path = embargo_api.message_url_path(self.course.id, access_point)
self.assertEqual(url_path, '/embargo/blocked-message/courseware/default/')
self._restrict_course(self.course.id) embargo_api.message_url_path(self.course.id, 'courseware')
RestrictedCourse.objects.get(course_key=self.course.id).delete()
message_cache_key = ( 'embargo.message_url_path.courseware.{course_key}' ).format(course_key=self.course.id) cache.delete(message_cache_key)
url_path = embargo_api.message_url_path(self.course.id, 'courseware') self.assertEqual(url_path, '/embargo/blocked-message/courseware/default/')
from config_models.models import cache from embargo.models import IPFilter from embargo.forms import RestrictedCourseForm, IPFilterForm
form = RestrictedCourseForm(data={'course_key': 'not/valid'}) self._assert_course_field_error(form)
self.assertFalse(form.is_valid())
cache.clear()
self._load_page(access_point, 'default')
self.assertFalse(EmbargoedCourse.is_embargoed(course_id))
cauth = EmbargoedCourse(course_id=course_id, embargoed=True) cauth.save()
self.assertTrue(EmbargoedCourse.is_embargoed(course_id)) self.assertEquals( unicode(cauth), u"Course '{course_id}' is Embargoed".format(course_id=course_id) )
good_states = ['AZ', 'FR'] blocked_states = ['US', 'AQ'] currently_blocked = EmbargoedState.current().embargoed_countries_list
cauth = EmbargoedState(embargoed_countries='US, AQ') cauth.save() currently_blocked = EmbargoedState.current().embargoed_countries_list
blocked_states.append('IM') cauth.embargoed_countries = 'US, AQ, IM' cauth.save() currently_blocked = EmbargoedState.current().embargoed_countries_list
with self.assertNumQueries(1): RestrictedCourse.is_restricted_course(course_id) RestrictedCourse.is_disabled_access_check(course_id)
with self.assertNumQueries(0): RestrictedCourse.is_restricted_course(course_id) RestrictedCourse.is_disabled_access_check(course_id)
with self.assertNumQueries(0): RestrictedCourse.is_restricted_course(new_course_id) RestrictedCourse.is_disabled_access_check(new_course_id)
abc = RestrictedCourse.objects.get(course_key=new_course_id) abc.delete() with self.assertNumQueries(1): RestrictedCourse.is_restricted_course(new_course_id)
with self.assertNumQueries(0): RestrictedCourse.is_restricted_course(new_course_id)
with self.assertNumQueries(1): CountryAccessRule.check_country_access(course_id, 'NZ')
course.delete() with self.assertNumQueries(1): CountryAccessRule.check_country_access(course_id, 'NZ')
us_rule.delete() self._assert_history([('AU', 'blacklist')])
au_rule.delete() self._assert_history([])
CountryAccessRule.objects.create( restricted_course=self.restricted_course, country=self.countries['US'], rule_type=CountryAccessRule.WHITELIST_RULE )
self.restricted_course.delete() self._assert_history_deleted()
self.restricted_course.enroll_msg_key = 'embargo' self.restricted_course.access_msg_key = 'embargo' self.restricted_course.save()
self._assert_history([], enroll_msg='embargo', access_msg='embargo')
self.assertEqual(record.course_key, self.course_key)
snapshot = json.loads(record.snapshot) self.assertEqual(snapshot['enroll_msg'], enroll_msg) self.assertEqual(snapshot['access_msg'], access_msg)
for (country, rule_type) in country_rules: self.assertIn( { 'country': country, 'rule_type': rule_type }, snapshot['country_rules'] )
self.assertEqual(len(snapshot['country_rules']), len(country_rules))
django_cache.clear() config_cache.clear()
RestrictedCourse.objects.create(course_key=self.course.id)
response = self.client.get(self.courseware_url) self.assertEqual(response.status_code, 200)
self.client.logout()
IPFilter.objects.create( blacklist=", ".join(blacklist), whitelist=", ".join(whitelist), enabled=is_enabled )
response = self.client.get( "/", HTTP_X_FORWARDED_FOR=request_ip, REMOTE_ADDR=request_ip )
IPFilter.objects.create( blacklist="192.168.10.20", enabled=True )
IPFilter.objects.create( whitelist="192.168.10.20", enabled=True )
self.assertEqual(response.status_code, 200)
self.user.is_staff = True
ip_address = "192.168.10.20" IPFilter.objects.create( blacklist=ip_address, enabled=True )
with restrict_course(self.course.id): response = self.client.get( url, HTTP_X_FORWARDED_FOR=ip_address, REMOTE_ADDR=ip_address ) self.assertEqual(response.status_code, 200)
re.compile(r'^/embargo/blocked-message/'),
re.compile(r'^/admin/'),
re.compile(r'^/api/course_structure/v[\d+]/courses/{}/$'.format(settings.COURSE_ID_PATTERN)),
if not settings.FEATURES.get('EMBARGO'): raise MiddlewareNotUsed()
for pattern in self.ALLOW_URL_PATTERNS: if pattern.match(request.path) is not None: return None
ip_blacklist_url = reverse( 'embargo_blocked_message', kwargs={ 'access_point': 'courseware', 'message_key': 'embargo' } ) return redirect(ip_blacklist_url)
return None
return self.country_access_rules(request.user, ip_address, request.path)
created_time = models.DateTimeField(auto_now_add=True)
updated_time = models.DateTimeField(auto_now=True)
course_key = CourseKeyField(max_length=255, db_index=True)
action = models.CharField(max_length=100, db_index=True)
state = models.CharField(max_length=50)
objects = CourseActionStateManager()
MAX_MESSAGE_LENGTH = 1000
should_display = models.BooleanField(default=False)
message = models.CharField(max_length=MAX_MESSAGE_LENGTH)
source_course_key = CourseKeyField(max_length=255, db_index=True)
display_name = models.CharField(max_length=255, default="", blank=True)
objects = CourseRerunUIStateManager()
from __future__ import unicode_literals
self.initiate_rerun()
CourseRerunState.objects.succeeded(course_key=self.course_key) self.expected_rerun_state.update({ 'state': CourseRerunUIStateManager.State.SUCCEEDED, }) rerun = self.verify_rerun_state()
self.dismiss_ui_and_verify(rerun)
self.initiate_rerun()
exception = Exception("failure in rerunning") try: raise exception except: CourseRerunState.objects.failed(course_key=self.course_key)
self.dismiss_ui_and_verify(rerun)
COURSE_ACTION_STATES = (CourseRerunState, )
for CourseState in self.course_actions_displayable_states + self.courses_with_state3_non_displayable: action_class.objects.update_state( CourseState.course_key, CourseState.state, should_display=CourseState.should_display, allow_not_found=True )
if user: state_object.updated_user = user
if kwargs: for key, value in kwargs.iteritems(): setattr(state_object, key, value)
actual_url = staticfiles_storage.url(path_overrides[module])
self.assertEqual(map(str.strip, result.splitlines()), self.OVERRIDES_JS)
css_include = compressed_css('style-main-v1') self.assertIn(u'lms-main-v1.css', css_include)
css_include = compressed_css('style-main-v1', raw=True) self.assertIn(u'lms-main-v1.css?raw', css_include)
with self.settings(PIPELINE_ENABLED=True): js_include = compressed_js('base_application') self.assertIn(u'lms-base-application.js', js_include)
with self.settings(PIPELINE_ENABLED=False): js_include = compressed_js('base_application') self.assertIn(u'/static/js/src/logger.js', js_include)
"error": "invalid_client", "error_description": "{} is not a public client".format(client_id),
self.request.social_strategy.clean_partial_pipeline() raise OAuthValidationError( { "error": "invalid_grant", "error_description": "access_token is not valid", } )
self.request.backend = social_utils.load_backend(self.request.social_strategy, self.BACKEND, redirect_uri)
self.data = { "access_token": self.access_token, "client_id": self.client_id, }
assert self.match_social_auth(social_auth) return social_auth.uid
details = pipeline_kwargs.get('details')
)
return getattr(settings, 'SOCIAL_AUTH_OAUTH_SECRETS', {}).get(self.backend_name, '')
return social_auth.uid[len(self.idp_slug) + 1:]
return getattr(settings, 'SOCIAL_AUTH_SAML_SP_PUBLIC_CERT', '')
return getattr(settings, 'SOCIAL_AUTH_SAML_SP_PRIVATE_KEY', '')
icon_class = None icon_image = None secondary = False
accepts_logins = False
return social_auth.uid[len(self.lti_consumer_key) + 1:]
django_settings.FIELDS_STORED_IN_SESSION = _FIELDS_STORED_IN_SESSION
django_settings.MIDDLEWARE_CLASSES += _MIDDLEWARE_CLASSES
django_settings.SOCIAL_AUTH_LOGIN_ERROR_URL = '/'
django_settings.SOCIAL_AUTH_LOGIN_REDIRECT_URL = _SOCIAL_AUTH_LOGIN_REDIRECT_URL
django_settings.SOCIAL_AUTH_STRATEGY = 'third_party_auth.strategy.ConfigurationModelStrategy'
django_settings.SOCIAL_AUTH_PROTECTED_USER_FIELDS = ['email']
django_settings.SOCIAL_AUTH_RAISE_EXCEPTIONS = False
self.strategy.clean_partial_pipeline()
validated_lti_params = self.get_validated_lti_params(self.strategy)
self.strategy.session_setdefault('auth_entry', 'login')
from django.conf import settings
return redirect(request.GET.get('next', 'dashboard'))
return super(ConfigurationModelStrategy, self).setting(name, default, backend)
from __future__ import unicode_literals
from __future__ import unicode_literals
AUTH_ENTRY_LOGIN = 'login' AUTH_ENTRY_REGISTER = 'register' AUTH_ENTRY_ACCOUNT_SETTINGS = 'account_settings'
AUTH_ENTRY_LOGIN_API = 'login_api' AUTH_ENTRY_REGISTER_API = 'register_api'
if should_force_account_creation(): return dispatch_to_register() return dispatch_to_login()
return dispatch_to_register()
return redirect_to_custom_form(strategy.request, auth_entry, kwargs)
return association_response
OAuth2AuthenticationAllowInactiveUser, SessionAuthenticationAllowInactiveUser,
if not request.user.is_superuser and not ApiKeyHeaderPermission().has_permission(request, self): return Response(status=status.HTTP_403_FORBIDDEN)
self.provider = Registry.get(provider_id) if not self.provider: raise Http404
uid = self.provider.get_social_auth_uid('uid') if uid is not 'uid': query_set = query_set.filter(uid__startswith=uid[:-3])
return False
LINKED_USERS = (ALICE_USERNAME, STAFF_USERNAME, ADMIN_USERNAME) PASSWORD = "edx"
"remote_id": 'remote_' + username,
user = UserFactory.create(is_staff=True, is_superuser=True) user.save() self.client.login(username=user.username, password='test')
providers = OAuth2ProviderConfig.objects.all() pcount = len(providers)
provider1 = self.configure_dummy_provider( enabled=True, icon_class='', icon_image=SimpleUploadedFile('icon.svg', '<svg><rect width="50" height="100"/></svg>'), )
providers = OAuth2ProviderConfig.objects.all() self.assertEquals(len(providers), 1) self.assertEquals(providers[pcount].id, provider1.id)
post_data = models.model_to_dict(provider1) del post_data['icon_image']
post_data['name'] = 'Another name'
response = self.client.post(update_url, post_data) self.assertEquals(response.status_code, 302)
providers = OAuth2ProviderConfig.objects.all() self.assertEquals(len(providers), pcount + 2) self.assertEquals(providers[pcount].id, provider1.id) provider2 = providers[pcount + 1]
self.assertEquals(provider2.icon_image, provider1.icon_image) self.assertEquals(provider2.name, post_data['name'])
for char in pipeline.make_random_password(length=100000): self.assertIn(char, pipeline._PASSWORD_CHARSET)
self.assertEqual(google_provider.id, google_state.provider.id) self.assertEqual(self.user, google_state.user) self.assertEqual(user_social_auth_google.id, google_state.association_id)
google_provider = self.configure_google_provider(enabled=True) linkedin_provider = self.configure_linkedin_provider(enabled=True) self.assertEqual(len(provider.Registry.enabled()), 2)
self.assertEqual(google_provider.id, google_state.provider.id) self.assertEqual(self.user, google_state.user)
from third_party_auth.tasks import SAML_XML_NS XMLDSIG_XML_NS = 'http://www.w3.org/2000/09/xmldsig#'
('saml_key', 'MIICsDCCAhmgAw'), ('saml_key_alt', 'MIICWDCCAcGgAw'),
patcher = patch( 'social.backends.twitter.TwitterOAuth.unauthorized_token', create=True, return_value="unauth_token" ) patcher.start() self.addCleanup(patcher.stop)
user = User.objects.get(email=EMAIL) self.assertEqual(user.username, EDX_USER_ID)
httpretty.enable()
uid_patch = patch('onelogin.saml2.utils.OneLogin_Saml2_Utils.generate_unique_id', return_value='TESTID') uid_patch.start() self.addCleanup(uid_patch.stop)
self.assertTrue(provider_redirect_url.startswith(TESTSHIB_SSO_URL)) return self.client.post( self.complete_url, content_type='application/x-www-form-urlencoded', data=self.read_data_file('testshib_response.txt'), )
self.assertEqual(provider_redirect_url, self.url_prefix + self.complete_url) return self.client.get(provider_redirect_url)
PROVIDER_NAME = "override" PROVIDER_BACKEND = "override" PROVIDER_ID = "override" USER_EMAIL = "override" USER_NAME = "override" USER_USERNAME = "override"
self.client.logout() self._test_return_login(user_is_activated=False)
self.client.logout() self._test_return_login()
provider = None
self.assertFalse(payload.get('success')) self.assertIn('There was an error receiving your login information', payload.get('value'))
self.assertTrue(payload.get('success'))
self.assertFalse(payload.get('success')) self.assertIn('incorrect', payload.get('value'))
self.assertIn(self.provider.name, response.content)
self.assertEqual(auth_settings._SOCIAL_AUTH_LOGIN_REDIRECT_URL, response.get('Location'))
self.assertIn(self.provider.name, response.content)
self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN))
self.assert_account_settings_context_looks_correct(account_settings_context(request), request.user, linked=False) self.assert_social_auth_does_not_exist_for_user(request.user, strategy)
self.assert_logged_in_cookie_redirect(actions.do_complete(
self.set_logged_in_cookies(request)
self.assert_redirect_to_dashboard_looks_correct(actions.do_complete(
self.assert_social_auth_exists_for_user(request.user, strategy) self.assert_account_settings_context_looks_correct(account_settings_context(request), request.user, linked=True)
self.set_logged_in_cookies(request)
self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN))
self.assert_account_settings_context_looks_correct(account_settings_context(request), user, linked=True) self.assert_social_auth_exists_for_user(request.user, strategy)
self.assert_redirect_to_dashboard_looks_correct(actions.do_disconnect( request.backend, request.user, None, redirect_field_name=auth.REDIRECT_FIELD_NAME))
self.assert_account_settings_context_looks_correct(account_settings_context(request), user, linked=False) self.assert_social_auth_does_not_exist_for_user(user, strategy)
actions.do_complete(backend, social_views._do_login, user=unlinked_user)
self.assert_login_response_before_pipeline_looks_correct(self.client.get('/login'))
self.assert_redirect_to_provider_looks_correct(self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN)))
self.assert_redirect_to_login_looks_correct(actions.do_complete(request.backend, social_views._do_login))
self.assert_login_response_in_pipeline_looks_correct(student_views.signin_user(strategy.request))
self.assert_json_success_response_looks_correct(student_views.login_user(strategy.request))
self.assert_logged_in_cookie_redirect(actions.do_complete(
self.set_logged_in_cookies(request)
request, strategy = self.get_request_and_strategy( auth_entry=pipeline.AUTH_ENTRY_REGISTER, redirect_uri='social:complete') strategy.request.backend.auth_complete = mock.MagicMock(return_value=self.fake_auth_complete(strategy))
self.assert_register_response_before_pipeline_looks_correct(self.client.get('/register'))
self.assert_redirect_to_provider_looks_correct(self.client.get( pipeline.get_login_url(self.provider.provider_id, pipeline.AUTH_ENTRY_LOGIN)))
self.assert_redirect_to_register_looks_correct(actions.do_complete(request.backend, social_views._do_login))
self.assert_register_response_in_pipeline_looks_correct( student_views.register_user(strategy.request), pipeline.get(request)['kwargs'])
with self.assertRaises(auth_models.User.DoesNotExist): self.get_user_by_email(strategy, email)
self.assert_social_auth_does_not_exist_for_user(created_user, strategy)
self.assert_logged_in_cookie_redirect(actions.do_complete(
self.assert_redirect_to_register_looks_correct(actions.do_complete(backend, social_views._do_login))
student_views.create_account(strategy.request) self.assert_json_failure_response_is_username_collision(student_views.create_account(strategy.request))
TOKEN_RESPONSE_DATA = None
USER_RESPONSE_DATA = None
response = self.client.get(complete_url) self.assertEqual(response.status_code, 302) self.assertEqual(response['Location'], 'http://example.none/misc/final-destination')
self.client.defaults['SERVER_NAME'] = 'example.none' self.url_prefix = 'http://example.none'
settings.apply_settings(self.settings) self.assertEqual([], provider.Registry.enabled())
settings.apply_settings(self.settings) self.assertFalse(self.settings.SOCIAL_AUTH_RAISE_EXCEPTIONS)
UID_FIELD = "id"
UID_FIELD = "email"
redirect_uri = super(ExceptionMiddleware, self).get_redirect_uri(request, exception)
auth_entry = request.session.get(pipeline.AUTH_ENTRY_KEY)
if auth_entry and auth_entry in pipeline.AUTH_DISPATCH_URLS: redirect_uri = pipeline.AUTH_DISPATCH_URLS[auth_entry]
url = path
if rest.endswith('?raw'): return original
base_url = AssetBaseUrlConfig.get_base_url() excluded_exts = AssetExcludedExtensionsConfig.get_excluded_extensions() url = StaticContent.get_canonicalized_asset_path(course_id, rest, base_url, excluded_exts)
else: course_path = "/".join((static_asset_path or data_directory, rest))
assert_equals('"/static/data_dir/file.png"', replace_static_urls(STATIC_SOURCE, DATA_DIRECTORY))
assert_equals( '"' + mock_static_content.get_canonicalized_asset_path.return_value + '"', replace_static_urls(STATIC_SOURCE, DATA_DIRECTORY, course_id=COURSE_KEY) )
unlock_content = cls.create_image(prefix, (32, 32), 'blue', '{}_unlock.png')
lock_content = cls.create_image(prefix, (32, 32), 'green', '{}_lock.png', locked=True)
contentstore().generate_thumbnail(unlock_content, dimensions=(16, 16)) contentstore().generate_thumbnail(lock_content, dimensions=(16, 16))
cls.create_image(prefix, (1, 1), 'red', 'special/{}_unlock.png')
cls.create_image(prefix, (1, 1), 'yellow', 'special/{}_lock.png', locked=True)
from __future__ import unicode_literals
from __future__ import unicode_literals
decorator `django.utils.decorators.decorator_from_middleware(middleware_class)`
log.debug(u"Referrer hostname is `None`, so it is not on the whitelist.")
from __future__ import unicode_literals
request = args[0] request.META['CROSS_DOMAIN_CSRF_COOKIE_USED'] = True
return ensure_csrf_cookie(func)(*args, **kwargs)
if not is_cross_domain_request_allowed(request): log.debug("Could not set cross-domain CSRF cookie.") return response
LANGUAGE_KEY = 'pref-lang'
Language = namedtuple('Language', 'code name')
released_languages = [ Language(tuple[0], tuple[1]) for tuple in settings.LANGUAGES if tuple[0] in released_language_codes ]
self.middleware.process_request(self.request) self.assertNotIn(LANGUAGE_SESSION_KEY, self.request.session)
set_user_preference(self.user, LANGUAGE_KEY, 'eo') self.middleware.process_request(self.request) self.assertEquals(self.request.session[LANGUAGE_SESSION_KEY], 'eo')
self.request.session[LANGUAGE_SESSION_KEY] = 'en' set_user_preference(self.user, LANGUAGE_KEY, 'eo') self.middleware.process_request(self.request)
for browser_lang in lang_headers: if browser_lang in system_released_languages: if request.session.get(LANGUAGE_SESSION_KEY, None) is None: request.session[LANGUAGE_SESSION_KEY] = unicode(browser_lang) break
domain = domain.split(':')[0] microsites = cls.objects.filter(site__domain__iexact=domain)
original = Microsite.objects.get(id=instance.id) _make_archive_copy(original)
history = HistoricalRecords()
history = HistoricalRecords()
microsite = Microsite.get_microsite_for_domain(domain)
try: microsite = Microsite.objects.get(key='default') except Microsite.DoesNotExist: pass
self._set_microsite_config_from_obj(microsite.site.domain, domain, microsite)
config = microsite.values return config.get(val_name, default)
return set(MicrositeOrganizationMapping.objects.all().values_list('organization', flat=True))
organizations = microsite_object.get_organizations()
if not organizations: raise Exception( 'Configuration error. Microsite {key} does not have any ORGs mapped to it!'.format( key=microsite_object.key ) )
config['course_org_filter'] = organizations[0] self.current_request_configuration.data = config
template_obj = MicrositeTemplate.get_template_for_microsite( microsite_get_value('site_domain'), uri )
if 'default' in settings.MICROSITE_CONFIGURATION: self._set_microsite_config('default', subdomain, domain) return
for value in settings.MICROSITE_CONFIGURATION.itervalues(): org_filter = value.get('course_org_filter', None) if org_filter == org: return value.get(val_name, default) return default
for microsite in settings.MICROSITE_CONFIGURATION.itervalues(): org_filter = microsite.get('course_org_filter') if org_filter: org_filter_set.add(org_filter)
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(microsite.get_backend(None, BaseMicrositeBackend), None)
with self.assertRaises(TypeError): microsite.get_backend('microsite_configuration.microsite.get_backend', BaseMicrositeBackend)
with self.assertRaises(ValueError): microsite.get_backend('microsite_configuration.microsite.invalid_method', BaseMicrositeBackend)
self.assertIsInstance( microsite.get_backend( 'microsite_configuration.backends.database.DatabaseMicrositeBackend', BaseMicrositeBackend ), DatabaseMicrositeBackend )
settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] = [ path for path in settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] if path != settings.MICROSITE_ROOT_DIR ]
settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] = [ path for path in settings.DEFAULT_TEMPLATE_ENGINE['DIRS'] if path != settings.MICROSITE_ROOT_DIR ]
microsite.set_by_domain('unknown') self.assertIsNone(microsite.get_value('platform_name'))
Microsite.objects.all().delete() microsite.clear() microsite.set_by_domain('unknown') self.assertIsNone(microsite.get_value('platform_name'))
microsite.clear() with patch('django.conf.settings.MICROSITE_CONFIGURATION', False): self.assertEqual( microsite.get_all_orgs(), set() )
microsite.set_by_domain('unknown') self.assertEqual(microsite.get_value('university'), 'default_university')
if microsite.has_override_value('SESSION_COOKIE_DOMAIN'):
def _set_cookie_wrapper(key, value='', max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False):
if key == settings.SESSION_COOKIE_NAME: domain = microsite.get_value('SESSION_COOKIE_DOMAIN', domain)
return response.set_cookie_wrapped_func( key, value, max_age=max_age, expires=expires, path=path, domain=domain, secure=secure, httponly=httponly )
response.set_cookie_wrapped_func = response.set_cookie response.set_cookie = _set_cookie_wrapper
return None
_strip_value(value, lookup)
if not self.blank and value is self.Empty: raise ValidationError(self.error_messages['blank']) else: return super(OpaqueKeyField, self).validate(value, model_instance)
from track.backends.django import TrackingLog
if values: engine = values['ENGINE'] options = values.get('OPTIONS', {}) backends[name] = _instantiate_backend_from_name(engine, options)
extra = kwargs.get('extra', {})
extra['w'] = extra.get('w', 0)
extra['tz_aware'] = extra.get('tz_aware', True)
msg = 'Error inserting to MongoDB event tracker backend' log.exception(msg)
self.assertEqual(str(results[0].time), '2013-01-01 17:01:00+00:00')
from __future__ import unicode_literals
context_fields_to_remove = set(CONTEXT_FIELDS_TO_INCLUDE) context_fields_to_remove.add('client_id') for field in context_fields_to_remove: if field in context: del context[field]
def __call__(self, event): context = event.get('context', {}) course_id = context.get('course_id')
shim.remove_shim_context(event)
full_event = dict(event, **task_info)
self.mock_tracker.reset_mock() try: views.server_track(request, str(sentinel.event_type), '{}')
self.mock_tracker.reset_mock() try: views.server_track(request, str(sentinel.event_type), '{}')
self.assert_no_events_emitted() try: response = segmentio.segmentio_event(request) self.assertEquals(response.status_code, 200)
del input_payload['current_time']
del expected_event['event']['currentTime']
full_segment_event = request.json
segment_properties = full_segment_event.get('properties', {})
segment_context = full_segment_event.get('context')
context = {}
segment_event_name = segment_properties['name'] disallowed_substring_names = [ a.lower() for a in getattr(settings, 'TRACKING_SEGMENTIO_DISALLOWED_SUBSTRING_NAMES', []) ]
context['client'] = dict(segment_context) context['agent'] = segment_context.get('userAgent', '')
for field in ('traits', 'integrations', 'userAgent'): if field in context['client']: del context['client'][field]
context.update(app_context)
super(TestTrackerInstantiation, self).setUp() self.get_backend = tracker._instantiate_backend_from_name
tracker._initialize_backends_from_django_settings()
self.assertEqual(context[context_key], 'test latin1 Ó é ñ'.decode('utf8'))
obj = UTC.localize(obj)
obj = obj.astimezone(UTC)
for prefix in sorted(self._prefix_registry, reverse=True): if key.startswith(prefix): return self._prefix_registry[prefix]
if self.name == "edx.video.seeked": self['name'] = "edx.video.position.changed"
'HTTP_REFERER': 'referer', 'HTTP_ACCEPT_LANGUAGE': 'accept_language',
if re.match(pattern, path): return False
context[context_key] = request.META.get(header_name, '').decode('latin1')
self.assertIsNotNone(get_template_request_context())
self.assertIsNone(get_template_request_context())
link_map = settings.MKTG_URL_LINK_MAP enable_mktg_site = microsite.get_value( 'ENABLE_MKTG_SITE', settings.FEATURES.get('ENABLE_MKTG_SITE', False) )
if name == 'ROOT': return settings.MKTG_URLS.get('ROOT') return settings.MKTG_URLS.get('ROOT') + settings.MKTG_URLS.get(name)
if link_map[name] is not None: return reverse(link_map[name])
template_name = microsite.get_template_path(template_name)
request_context = get_template_request_context() if request_context: for item in request_context: context_dictionary.update(item) for item in context_instance: context_dictionary.update(item) if context: context_dictionary.update(context)
KEY_CSRF_TOKENS = ('csrf_token', 'csrf') for key in KEY_CSRF_TOKENS: if key in context_dictionary: context_dictionary[key] = unicode(context_dictionary[key])
template = lookup_template(namespace, template_name) return template.render_unicode(**context_dictionary)
self._collection.clear() self._uri_cache.clear()
namespace_dirs = {namespace: list(look.directories) for namespace, look in LOOKUP.items()}
LOOKUP.clear()
for namespace, directories in namespace_dirs.items(): for directory in directories: add_lookup(namespace, directory)
self.base_loader = base_loader
template = Template(filename=file_path, module_directory=self.module_directory, input_encoding='utf-8', output_encoding='utf-8', default_filters=['decode.utf8'], encoding_errors='replace', uri=template_name) return template, None
return self.base_loader.load_template_source(template_name, template_dirs)
context_dictionary = {}
for processor in get_template_context_processors(): context.update(processor(request))
assign_default_role(instance.course_id, instance.user)
db_table = 'django_comment_client_role'
return self.name + " for " + (self.course_id.to_deprecated_string() if self.course_id else "all courses")
db_table = 'django_comment_client_permission'
from django.test import TestCase
self.staff_user = User.objects.create_user( "patty", "patty@fake.edx.org", ) self.staff_user.is_staff = True
from __future__ import unicode_literals
community_ta_role.inherit_permissions(moderator_role)
DATABASES = { 'default': { 'ENGINE': 'django.db.backends.sqlite3', 'ATOMIC_REQUESTS': True, },
STATICFILES_STORAGE = 'openedx.core.lib.django_require.staticstorage.OptimizedCachedRequireJsStorage'
TEST_ROOT = REPO_ROOT / "test_root" LOG_DIR = (TEST_ROOT / "log").abspath()
STATIC_ROOT = (TEST_ROOT / "staticfiles" / "cms").abspath()
os.environ['REQUIRE_BUILD_PROFILE_OPTIMIZE'] = 'none'
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
with open(CONFIG_ROOT / CONFIG_PREFIX + "env.json") as env_file: ENV_TOKENS = json.load(env_file)
DEFAULT_COURSE_ABOUT_IMAGE_URL = ENV_TOKENS.get('DEFAULT_COURSE_ABOUT_IMAGE_URL', DEFAULT_COURSE_ABOUT_IMAGE_URL)
GITHUB_REPO_ROOT = ENV_TOKENS.get('GITHUB_REPO_ROOT', GITHUB_REPO_ROOT)
SOCIAL_SHARING_SETTINGS = ENV_TOKENS.get('SOCIAL_SHARING_SETTINGS', SOCIAL_SHARING_SETTINGS)
EDXMKTG_LOGGED_IN_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_LOGGED_IN_COOKIE_NAME', EDXMKTG_LOGGED_IN_COOKIE_NAME) EDXMKTG_USER_INFO_COOKIE_NAME = ENV_TOKENS.get('EDXMKTG_USER_INFO_COOKIE_NAME', EDXMKTG_USER_INFO_COOKIE_NAME)
CSRF_COOKIE_SECURE = ENV_TOKENS.get('CSRF_COOKIE_SECURE', False)
THEME_NAME = ENV_TOKENS.get('THEME_NAME', None) COMPREHENSIVE_THEME_DIR = path(ENV_TOKENS.get('COMPREHENSIVE_THEME_DIR', COMPREHENSIVE_THEME_DIR))
GIT_REPO_EXPORT_DIR = ENV_TOKENS.get('GIT_REPO_EXPORT_DIR', '/edx/var/edxapp/export_course_repos')
LANGUAGES = ENV_TOKENS.get('LANGUAGES', LANGUAGES) LANGUAGE_CODE = ENV_TOKENS.get('LANGUAGE_CODE', LANGUAGE_CODE) USE_I18N = ENV_TOKENS.get('USE_I18N', USE_I18N)
for app in ENV_TOKENS.get('ADDL_INSTALLED_APPS', []): INSTALLED_APPS += (app,)
if "TRACKING_IGNORE_URL_PATTERNS" in ENV_TOKENS: TRACKING_IGNORE_URL_PATTERNS = ENV_TOKENS.get("TRACKING_IGNORE_URL_PATTERNS")
with open(CONFIG_ROOT / CONFIG_PREFIX + "auth.json") as auth_file: AUTH_TOKENS = json.load(auth_file)
CMS_SEGMENT_KEY = AUTH_TOKENS.get('SEGMENT_KEY')
AWS_QUERYSTRING_AUTH = AUTH_TOKENS.get('AWS_QUERYSTRING_AUTH', True)
DATADOG = AUTH_TOKENS.get("DATADOG", {}) DATADOG.update(ENV_TOKENS.get("DATADOG", {}))
VIDEO_CDN_URL = ENV_TOKENS.get('VIDEO_CDN_URL', {})
SEARCH_ENGINE = "search.elastic.ElasticSearchEngine"
OAUTH_OIDC_ISSUER = ENV_TOKENS['OAUTH_OIDC_ISSUER']
PARTNER_SUPPORT_EMAIL = ENV_TOKENS.get('PARTNER_SUPPORT_EMAIL', PARTNER_SUPPORT_EMAIL)
AFFILIATE_COOKIE_NAME = ENV_TOKENS.get('AFFILIATE_COOKIE_NAME', AFFILIATE_COOKIE_NAME)
REST_FRAMEWORK,
SECRET_KEY = 'dev key'
'ENABLE_DISCUSSION_SERVICE': True, 'ENABLE_TEXTBOOK': True, 'ENABLE_STUDENT_NOTES': True,
'STUDIO_REQUEST_EMAIL': '',
'CMS_SEGMENT_KEY': None,
'ENABLE_SERVICE_STATUS': False,
'AUTOPLAY_VIDEOS': False,
'ENABLE_CREATOR_GROUP': False,
'ENFORCE_PASSWORD_POLICY': False,
'ENABLE_MAX_FAILED_LOGIN_ATTEMPTS': False,
'EDITABLE_SHORT_DESCRIPTION': True,
'SQUELCH_PII_IN_LOGS': False,
'EMBARGO': False,
'USE_MICROSITES': False,
'ALLOW_UNICODE_COURSE_ID': False,
'PREVENT_CONCURRENT_LOGINS': False,
'ADVANCED_SECURITY': False,
'ENABLE_VIDEO_UPLOAD_PIPELINE': False,
'ENABLE_EDXNOTES': False,
'ENABLE_CONTENT_LIBRARIES': True,
'MILESTONES_APP': False,
'ENABLE_PREREQUISITE_COURSES': False,
'ENTRANCE_EXAMS': False,
'LICENSING': False,
'ENABLE_COURSEWARE_INDEX': False,
'ENABLE_LIBRARY_INDEX': False,
'ALLOW_COURSE_RERUNS': True,
'CERTIFICATES_HTML_VIEW': False,
'ENABLE_TEAMS': True,
'ENABLE_VIDEO_BUMPER': False,
'SHOW_BUMPER_PERIODICITY': 7 * 24 * 3600,
'ENABLE_CREDIT_ELIGIBILITY': ENABLE_CREDIT_ELIGIBILITY,
'ALLOW_HIDING_DISCUSSION_TAB': False,
'ENABLE_SPECIAL_EXAMS': False,
'SHOW_LANGUAGE_SELECTOR': False,
'CUSTOM_COURSE_URLS': False
GEOIP_PATH = REPO_ROOT / "common/static/data/geoip/GeoIP.dat" GEOIPV6_PATH = REPO_ROOT / "common/static/data/geoip/GeoIPv6.dat"
'debug': False
AUTHENTICATION_BACKENDS = ( 'ratelimitbackend.backends.RateLimitModelBackend', )
from lms.envs.common import ( COURSE_KEY_PATTERN, COURSE_ID_PATTERN, USAGE_KEY_PATTERN, ASSET_KEY_PATTERN )
CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_SECURE = False
simplefilter('ignore')
'openedx.core.djangoapps.safe_sessions.middleware.SafeSessionMiddleware',
'cache_toolbox.middleware.CacheBackedAuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
'lang_pref.middleware.LanguagePreferenceMiddleware',
'dark_lang.middleware.DarkLangMiddleware',
'django.middleware.locale.LocaleMiddleware',
'edxmako.middleware.MakoMiddleware',
'ratelimitbackend.middleware.RateLimitMiddleware',
'session_inactivity_timeout.middleware.SessionInactivityTimeout',
'django.middleware.clickjacking.XFrameOptionsMiddleware',
X_FRAME_OPTIONS = 'ALLOW'
P3P_HEADER = 'CP="Open EdX does not have a P3P policy."'
from xmodule.modulestore.inheritance import InheritanceMixin from xmodule.modulestore import prefer_xmodules from xmodule.x_module import XModuleMixin
XBLOCK_MIXINS = ( LmsBlockMixin, InheritanceMixin, XModuleMixin, EditInfoMixin, AuthoringMixin, )
XBLOCK_FIELD_DATA_WRAPPERS = ()
MODULESTORE_FIELD_OVERRIDE_PROVIDERS = ()
'python_bin': None, 'user': 'sandbox',
'limits': { 'CPU': 1, },
DEBUG = False SESSION_COOKIE_SECURE = False SESSION_SAVE_EVERY_REQUEST = False SESSION_SERIALIZER = 'django.contrib.sessions.serializers.PickleSerializer'
SITE_ID = 1 SITE_NAME = "localhost:8001" HTTPS = 'on' ROOT_URLCONF = 'cms.urls'
EDX_PLATFORM_REVISION = dealer.git.Backend(path=REPO_ROOT).revision
EDX_PLATFORM_REVISION = 'unknown'
STATIC_URL = '/static/' + EDX_PLATFORM_REVISION + "/" STATIC_ROOT = ENV_ROOT / "staticfiles" / EDX_PLATFORM_REVISION
]
MESSAGE_STORAGE = 'django.contrib.messages.storage.session.SessionStorage'
PIPELINE_CSS_COMPRESSOR = None PIPELINE_JS_COMPRESSOR = None
"spec", "spec_helpers",
"xmodule_js", "common_static",
REQUIRE_BASE_URL = "./"
REQUIRE_JS = "js/vendor/requiresjs/require.js"
REQUIRE_STANDALONE_MODULES = {}
REQUIRE_DEBUG = False
REQUIRE_EXCLUDE = ("build.txt",)
'API': 'https://www.youtube.com/iframe_api',
'METADATA_URL': 'https://www.googleapis.com/youtube/v3/videos',
'openedx.core.djangoapps.common_views',
'simple_history',
'config_models',
'service_status',
'django_nose',
'contentstore', 'contentserver', 'course_creators', 'external_auth',
'track', 'eventtracking.django.apps.EventTrackingConfig',
'datadog',
'edxmako', 'pipeline', 'static_replace', 'require',
'openedx.core.djangoapps.theming',
'openedx.core.djangoapps.site_configuration',
'django_comment_common',
'django.contrib.admin',
'course_modes',
'dark_lang',
'reverification',
'openedx.core.djangoapps.user_api', 'django_openid_auth',
'monitoring',
'course_action_state',
'openedx.core.djangoapps.credit',
'edx_proctoring',
'openedx.core.djangoapps.bookmarks',
'openedx.core.djangoapps.programs',
'openedx.core.djangoapps.self_paced',
'provider', 'provider.oauth2', 'edx_oauth2_provider',
'oauth2_provider',
'lms.djangoapps.verify_student',
'microsite_configuration',
'milestones',
'statici18n',
'cms.lib.xblock.tagging',
TRACKING_IGNORE_URL_PATTERNS = [r'^/event', r'^/login', r'^/heartbeat']
'submissions', 'openassessment', 'openassessment.assessment', 'openassessment.fileupload', 'openassessment.workflow', 'openassessment.xblock',
'edxval',
'organizations',
try: imp.find_module(app_name) except ImportError: try: __import__(app_name) except ImportError: continue INSTALLED_APPS += (app_name,)
ADVANCED_SECURITY_CONFIG = {}
MAX_ASSET_UPLOAD_FILE_SIZE_URL = ""
ADVANCED_PROBLEM_TYPES = [ { 'component': 'openassessment', 'boilerplate_name': None, }, ]
SEARCH_ENGINE = None ELASTIC_FIELD_MAPPINGS = { "start_date": { "type": "date" } }
DEPRECATED_ADVANCED_COMPONENT_TYPES = []
CREDIT_TASK_DEFAULT_RETRY_DELAY = 30
CREDIT_TASK_MAX_RETRIES = 5
CREDIT_PROVIDER_TIMESTAMP_EXPIRATION = 15 * 60
OAUTH_OIDC_ISSUER = 'https://www.example.com/oauth2'
OAUTH_ID_TOKEN_EXPIRATION = 5 * 60
PARTNER_SUPPORT_EMAIL = ''
AFFILIATE_COOKIE_NAME = 'affiliate_id'
from yaml import Loader, SafeLoader
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
CONFIG_ROOT = path(os.environ.get('CONFIG_ROOT', ENV_ROOT))
CONFIG_PREFIX = SERVICE_VARIANT + "." if SERVICE_VARIANT else ""
BROKER_POOL_LIMIT = 0 BROKER_CONNECTION_TIMEOUT = 1
CELERY_RESULT_BACKEND = 'djcelery.backends.cache:CacheBackend'
BROKER_HEARTBEAT = 10.0 BROKER_HEARTBEAT_CHECKRATE = 2
CELERYD_PREFETCH_MULTIPLIER = 1
if 'FEATURES' in ENV_TOKENS: del ENV_TOKENS['FEATURES']
STATIC_URL = STATIC_URL_BASE.encode('ascii') if not STATIC_URL.endswith("/"): STATIC_URL += "/" STATIC_URL += EDX_PLATFORM_REVISION + "/"
for app in ADDL_INSTALLED_APPS: INSTALLED_APPS += (app,)
for database_name in DATABASES: DATABASES[database_name]['ATOMIC_REQUESTS'] = False
from lms.envs.test import ( WIKI_ENABLED, PLATFORM_NAME, SITE_NAME, DEFAULT_FILE_STORAGE, MEDIA_ROOT, MEDIA_URL, )
MONGO_PORT_NUM = int(os.environ.get('EDXAPP_TEST_MONGO_PORT', '27017')) MONGO_HOST = os.environ.get('EDXAPP_TEST_MONGO_HOST', 'localhost')
TEST_RUNNER = 'openedx.core.djangolib.nose.NoseTestSuiteRunner'
STATIC_ROOT = TEST_ROOT / "staticfiles"
FEATURES['ENABLE_EXPORT_GIT'] = True GIT_REPO_EXPORT_DIR = TEST_ROOT / "export_course_repos"
STATICFILES_STORAGE = 'pipeline.storage.NonPackagingPipelineStorage' STATIC_URL = "/static/"
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
MIGRATION_MODULES = NoOpMigrationModules()
filterwarnings('ignore', message='No request passed to the backend, unable to rate-limit')
simplefilter('ignore')
LETTUCE_SERVER_PORT = 8003 XQUEUE_PORT = 8040 YOUTUBE_PORT = 8031 LTI_PORT = 8765 VIDEO_SOURCE_PORT = 8777
PASSWORD_HASHERS = ( 'django.contrib.auth.hashers.SHA1PasswordHasher', 'django.contrib.auth.hashers.MD5PasswordHasher', )
CMS_SEGMENT_KEY = None
FEATURES['EMBARGO'] = True
FEATURES['ENABLE_DISCUSSION_SERVICE'] = False
PARENTAL_CONSENT_AGE_LIMIT = 13
FEATURES['ENABLE_CONTENT_LIBRARIES'] = True
FEATURES['MILESTONES_APP'] = True
FEATURES['ENTRANCE_EXAMS'] = True ENTRANCE_EXAM_MIN_SCORE_PCT = 50
FEATURES['ENABLE_COURSEWARE_INDEX'] = True FEATURES['ENABLE_LIBRARY_INDEX'] = True SEARCH_ENGINE = "search.tests.mock_search_engine.MockSearchEngine"
FEATURES['ENABLE_TEAMS'] = True
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
INSTALLED_APPS += ('openedx.core.djangoapps.api_admin',)
OAUTH2_PROVIDER_APPLICATION_MODEL = 'oauth2_provider.Application'
DEBUG = True
REQUIRE_DEBUG = False
STATICFILES_STORAGE = 'pipeline.storage.PipelineCachedStorage'
INSTALLED_APPS += ('django_extensions',)
DEBUG = True
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
FEATURES['MILESTONES_APP'] = True
FEATURES['ENABLE_PREREQUISITE_COURSES'] = True
FEATURES['ENABLE_EDXNOTES'] = True
FEATURES['ENABLE_TEAMS'] = True
FEATURES['LICENSING'] = True
PARTNER_SUPPORT_EMAIL = 'partner-support@example.com'
DEPRECATED_BLOCK_TYPES = ['poll', 'survey']
MOCK_SEARCH_BACKING_FILE = ( TEST_ROOT / "index_file.dat" ).abspath()
SECRET_KEY = "very_secret_bok_choy_key"
try:
DEBUG_TOOLBAR_MONGO_STACKTRACES = True
del DEFAULT_FILE_STORAGE MEDIA_ROOT = "/edx/var/edxapp/uploads"
for pkg_name in ['track.contexts', 'track.middleware', 'dd.dogapi']: logging.getLogger(pkg_name).setLevel(logging.CRITICAL)
PIPELINE_ENABLED = False STATICFILES_STORAGE = 'openedx.core.storage.DevelopmentStorage'
CELERY_ALWAYS_EAGER = True
DEBUG_TOOLBAR_MONGO_STACKTRACES = False
XBLOCK_SETTINGS = { "VideoDescriptor": { "licensing_enabled": True } }
REQUIRE_DEBUG = DEBUG
if os.path.isfile(join(dirname(abspath(__file__)), 'private.py')):
MODULESTORE = convert_module_store_setting_if_needed(MODULESTORE)
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
DEBUG = True
import logging logging.basicConfig(filename=TEST_ROOT / "log" / "cms_acceptance.log", level=logging.ERROR)
logging.getLogger().setLevel(logging.ERROR)
'ADDITIONAL_OPTIONS': { 'trashcan': { 'bucket': 'trash_fs' } }
FEATURES['AUTOMATIC_AUTH_FOR_TESTING'] = True
USE_I18N = True
LETTUCE_SELENIUM_CLIENT = os.environ.get('LETTUCE_SELENIUM_CLIENT', 'local')
try:
import uuid SECRET_KEY = uuid.uuid4().hex
from lms.envs.dev import (WIKI_ENABLED)
'origin': 'git@github.com:MITx/content-mit-6002x.git',
CACHE_TIMEOUT = 0
SECRET_KEY = '85920908f28904ed733fe576320db18cabd7b6cd'
CELERY_ALWAYS_EAGER = True
DEBUG_TOOLBAR_MONGO_STACKTRACES = False
FEATURES['ENABLE_SERVICE_STATUS'] = True
import os CMS_SEGMENT_KEY = os.environ.get('SEGMENT_KEY')
try:
from .aws import * import os from django.core.exceptions import ImproperlyConfigured
if db != 'read_replica': DATABASES[db].update(get_db_overrides(db))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')
APP.config_from_object('django.conf:settings') APP.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
from ratelimitbackend import admin
url(r'^edge/(?P<org>[^/]+)/(?P<course>[^/]+)/course/(?P<coursename>[^/]+)$', 'contentstore.views.landing', name='landing'),
url(r'^api/user/', include('openedx.core.djangoapps.user_api.urls')),
url(r'^lang_pref/session_language', 'lang_pref.views.update_session_language', name='session_language'),
urlpatterns += patterns( '',
urlpatterns += patterns( 'contentstore.views',
'packages': ('openassessment',),
if settings.FEATURES.get('AUTOMATIC_AUTH_FOR_TESTING'): urlpatterns += ( url(r'^auto_auth$', 'student.views.auto_auth'), )
url(r'^programs/id_token/$', ProgramsIdTokenView.as_view(), name='programs_id_token'), url(r'^program/', ProgramAuthoringView.as_view(), name='programs'),
handler404 = 'contentstore.views.render_404' handler500 = 'contentstore.views.render_500'
urlpatterns += ( url(r'^404$', handler404), url(r'^500$', handler500), )
from .celery import APP as CELERY_APP
if settings.COMPREHENSIVE_THEME_DIR: enable_comprehensive_theme(settings.COMPREHENSIVE_THEME_DIR)
if settings.THEME_NAME == "": settings.THEME_NAME = None return
theme_root = settings.ENV_ROOT / "themes" / settings.THEME_NAME
settings.STATICFILES_DIRS.append( (u'themes/{}'.format(settings.THEME_NAME), theme_root / 'static') )
if isinstance(authored_data, CmsFieldData):
from __future__ import unicode_literals
context = { 'reorderable_items': set(), 'read_only': True } problem_html = get_preview_fragment(request, self.problem, context).content
self.assertNotRegexpMatches(problem_html, r"data-block-type=[\"\']acid_aside[\"\']")
video_html = get_preview_fragment(request, self.video, context).content self.assertNotRegexpMatches(video_html, "<select")
from __future__ import unicode_literals
def __init__(self, course_descriptor): self.graders = [ CourseGradingModel.jsonize_grader(i, grader) for i, grader in enumerate(course_descriptor.raw_grader)
index = int(grader.get('id', len(descriptor.raw_grader))) grader = CourseGradingModel.parse_grader(grader)
if graceperiodjson is not None: if 'grace_period' in graceperiodjson: graceperiodjson = graceperiodjson['grace_period']
if minimum_grade_credit is not None: minimum_grade_credit = minimum_grade_credit
descriptor.raw_grader = descriptor.raw_grader
filtered_list = list(cls.FILTERED_LIST)
if not settings.FEATURES.get('ENABLE_EXPORT_GIT'): filtered_list.append('giturl')
if not settings.FEATURES.get('ENABLE_EDXNOTES'): filtered_list.append('edxnotes')
if not settings.FEATURES.get('ENABLE_VIDEO_UPLOAD_PIPELINE'): filtered_list.append('video_upload_pipeline')
if not settings.FEATURES.get('ENABLE_TEAMS'): filtered_list.append('teams_configuration')
if not settings.FEATURES.get('CUSTOM_COURSES_EDX'): filtered_list.append('enable_ccx') filtered_list.append('ccx_connector')
if not filter_tabs: filtered_list.remove("tabs")
key_values = {}
if did_validate: updated_data = cls.update_from_dict(key_values, descriptor, user, save=False)
update_creator_state = Signal(providing_args=["caller", "user", "state"])
send_admin_notification = Signal(providing_args=["user"])
send_user_notification = Signal(providing_args=["user", "state"])
if instance.state == CourseCreator.DENIED or granted_state_change: send_user_notification.send( sender=sender, user=instance.user, state=instance.state )
if instance.state == CourseCreator.PENDING: send_admin_notification.send( sender=sender, user=instance.user )
return user[0].state
obj.admin = request.user obj.save()
message_template = 'emails/course_creator_revoked.txt'
from __future__ import unicode_literals
self.assertFalse(auth.user_has_role(self.user, CourseCreatorRole()))
for _ in xrange(30): response = self.client.post('/admin/login/', post_params) self.assertEquals(response.status_code, 200)
self.assertEquals(response.status_code, 403)
add_user_with_status_granted(self.admin, self.user) self.assertEqual('unrequested', get_course_creator_status(self.user))
self.assertFalse(auth.user_has_role(self.user, CourseCreatorRole()))
add_user_with_status_unrequested(self.user) self.assertEqual('granted', get_course_creator_status(self.user))
user_requested_access(self.user) self.assertEqual('granted', get_course_creator_status(self.user))
add_user_with_status_unrequested(self.admin) self.assertIsNone(get_course_creator_status(self.admin))
add_user_with_status_granted(self.admin, self.admin) self.assertIsNone(get_course_creator_status(self.admin))
return
return
_timed_exams = modulestore().get_items( course_key, qualifiers={ 'category': 'sequential', }, settings={ 'is_time_limited': True, } )
timed_exams = [ timed_exam for timed_exam in _timed_exams if is_item_in_course_tree(timed_exam) ]
if timed_exam.is_proctored_exam and not timed_exam.is_practice_exam: try: update_review_policy( exam_id=exam_id, set_by_user_id=timed_exam.edited_by, review_policy=timed_exam.exam_review_rules ) except ProctoredExamReviewPolicyNotFoundException:
remove_review_policy(exam_id=exam_id)
exams = get_all_exams_for_course(course_key)
msg = 'Disabling timed exam {exam_id}'.format(exam_id=exam['id']) log.info(msg) update_exam( exam_id=exam['id'], is_proctored=False, is_active=False, )
super(GitExportError, self).__init__(unicode(message))
on_course_publish(course_key)
from .tasks import update_search_index
from .tasks import update_library_index
admin = User.objects.get(username=username, email=email)
for user in get_users_with_role(CourseStaffRole.ROLE): add_user_with_status_unrequested(user)
try: course_key = CourseKey.from_string(args[0]) except InvalidKeyError: try: course_key = SlashSeparatedCourseKey.from_deprecated_string(args[0]) except InvalidKeyError: raise CommandError(unicode(GitExportError.BAD_COURSE))
split_modulestore = modulestore()._get_modulestore_by_type(ModuleStoreEnum.Type.split) active_version_collection = split_modulestore.db_connection.course_index structure_collection = split_modulestore.db_connection.structures
CourseInstructorRole(dest_course_id).add_users( *CourseInstructorRole(source_course_id).users_with_role() ) CourseStaffRole(dest_course_id).add_users( *CourseStaffRole(source_course_id).users_with_role() )
assets_deleted = content_store.remove_redundant_content_for_courses() success = True
searcher = SearchEngine.get_search_engine(index_name)
if setup_option or query_yes_no(self.CONFIRMATION_PROMPT, default="no"): course_keys = [course.id for course in modulestore().get_courses()] else: return
course_keys = map(self._parse_course_key, args)
help = "Create a course in one of {}".format([ModuleStoreEnum.Type.mongo, ModuleStoreEnum.Type.split]) args = "modulestore user org course run"
self.assertTrue(self.store.has_item(course.id.make_usage_key('html', 'multi_parent_html')))
call_command('delete_orphans', unicode(published_branch), '--commit')
published_branch = course.id.for_branch( ModuleStoreEnum.BranchName.published )
self.assertOrphanCount(course.id, 1) self.assertOrphanCount(published_branch, 1)
self.store.delete_item( orphan.location, self.user.id, skip_auto_publish=True )
self.assertOrphanCount(course.id, 0) self.assertOrphanCount(published_branch, 1) self.assertIn(orphan, self.store.get_items(published_branch))
self.module_store = modulestore()._get_modulestore_by_type(ModuleStoreEnum.Type.mongo)
self.course = CourseFactory.create( org=org, number=course_number, run=course_run )
self.assertTrue(self.store.has_changes(self.store.get_item(self.course.location)))
versions = get_course_versions(unicode(self.course.id)) draft_version = versions['draft-branch'] published_version = versions['published-branch']
self.assertNotEqual(draft_version, published_version)
call_command('force_publish', unicode(self.course.id), '--commit')
self.assertFalse(self.store.has_changes(self.store.get_item(self.course.location)))
versions = get_course_versions(unicode(self.course.id)) new_draft_version = versions['draft-branch'] new_published_version = versions['published-branch']
self.assertEqual(draft_version, new_draft_version) self.assertNotEqual(published_version, new_published_version)
self.assertEqual(new_draft_version, new_published_version)
self.good_dir = self.create_course_xml(self.content_dir, self.base_course_key)
self.course_dir = self.create_course_xml(self.content_dir, self.truncated_key)
call_command('import', self.content_dir, self.good_dir) store = modulestore() self.assertIsNotNone(store.get_course(self.base_course_key))
call_command('import', self.content_dir, self.course_dir) self.assertIsNotNone(store.get_course(self.truncated_key))
modulestore().mappings = {}
self.assertEqual(unicode(course.location.course_key), unicode(course.children[0].course_key))
with self.assertRaisesRegexp(CommandError, unicode(GitExportError.URL_BAD)): call_command('git_export', 'foo/bar/baz', 'silly', stderr=StringIO.StringIO())
with self.assertRaisesRegexp(CommandError, unicode(GitExportError.BAD_COURSE)): call_command('git_export', 'foo/bar:baz', 'silly', stderr=StringIO.StringIO())
with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.XML_EXPORT_FAIL)): git_export_utils.export_to_git( course_key, 'file://{0}'.format(self.bare_repo_dir))
with self.assertRaisesRegexp(GitExportError, unicode(GitExportError.CANNOT_PULL)): git_export_utils.export_to_git( course_key, 'https://user:blah@example.com/r.git')
course = self.store.get_course(course.id)
dangling_pointer = course.id.make_usage_key('chapter', 'DanglingPointer')
self.assertEqual(len(course.children), 2) self.assertIn(dangling_pointer, course.children)
course = self.store.get_course(course.id) self.assertEqual(len(course.children), 1) self.assertNotIn(dangling_pointer, course.children)
call_command( "migrate_to_split", str(self.course.id), str(self.user.id), )
self.temp_dir_1 = mkdtemp() self.temp_dir_2 = mkdtemp(dir="")
self.addCleanup(shutil.rmtree, self.temp_dir_1) self.addCleanup(shutil.rmtree, self.temp_dir_2)
errstring = "Invalid course_key: 'InvalidCourseID'." with self.assertRaisesRegexp(CommandError, errstring): call_command('export', "InvalidCourseID", self.temp_dir_1)
for output_dir in [self.temp_dir_1, self.temp_dir_2]: call_command('export', course_id, output_dir)
courses, failed_export_courses = export_courses_to_output_path(self.temp_dir) self.assertEqual(len(courses), 2) self.assertEqual(len(failed_export_courses), 0)
self.assertEqual(store, modulestore()._get_modulestore_for_courselike(new_key).get_modulestore_type())
from optparse import make_option from django.core.management.base import BaseCommand, CommandError from .prompt import query_yes_no
raise CommandError(e)
from __future__ import unicode_literals
log = logging.getLogger(__name__)
save_course_update_items(location, course_updates, course_update_items, user) if "status" in course_update_dict: del course_update_dict["status"] return course_update_dict
if 0 < passed_index <= len(course_update_items): course_update_item = course_update_items[passed_index - 1] course_update_item["status"] = CourseInfoModule.STATUS_DELETED course_update_items[passed_index - 1] = course_update_item
save_course_update_items(location, course_updates, course_update_items, user) return _get_visible_update(course_update_items)
return 0
modulestore().update_item(course_updates, user.id)
Push.alert( data=push_payload, channels={"$in": push_channels}, where={"deviceType": "android"}, )
source_subs_filedata = request.FILES['transcript-file'].read().decode('utf-8-sig') source_subs_filename = request.FILES['transcript-file'].name
if video_list: sub_attr = source_subs_name try: generate_subs_from_source({1: sub_attr}, source_subs_ext, source_subs_filedata, item)
copy_or_rename_transcript(video_name, sub_attr, item, user=request.user)
youtube_id = videos.get('youtube', None) if youtube_id: transcripts_presence['is_youtube_mode'] = True
subs = ''
if ( transcripts_presence['youtube_diff'] and transcripts_presence['youtube_local'] and
html5_id_to_remove = [x for x in videos['html5'] if x != html5_id] if html5_id_to_remove: remove_subs_from_store(html5_id_to_remove, item)
copy_or_rename_transcript(new_name, old_name, item, user=request.user)
error_response(response, "Can't find transcripts in storage for {}".format(old_name))
current_subs = data.get('current_subs') if current_subs is not None: for sub in current_subs: remove_subs_from_store(sub, item)
item = modulestore().get_item(usage_key)
if not has_course_author_access(request.user, item.location.course_key): raise PermissionDenied()
COMPONENT_TYPES = ['discussion', 'html', 'problem', 'video']
xblock_info = create_xblock_info(xblock, include_ancestor_info=is_unit_page)
index = 1 for child in subsection.get_children(): if child.location == unit.location: break index += 1
component_types = COMPONENT_TYPES[:]
if library: component_types = [component for component in component_types if component != 'discussion']
if library: return component_templates
usage_key = usage_key.replace(course_key=modulestore().fill_in_run(usage_key.course_key))
req = django_to_webob_request(request)
modulestore().update_item(descriptor, request.user.id)
if not settings.FEATURES.get(feature_name, False): return HttpResponseBadRequest() return view_func(request, *args, **kwargs)
if not has_course_author_access(request.user, course_key): return HttpResponse(status=403)
if request.method == 'GET': return _get_entrance_exam(request, course_key)
entrance_exam_minimum_score_pct = _get_default_entrance_exam_minimum_pct() if ee_min_score != '' and ee_min_score is not None: entrance_exam_minimum_score_pct = float(ee_min_score) return create_entrance_exam(request, course_key, entrance_exam_minimum_score_pct)
elif request.method == 'DELETE': return delete_entrance_exam(request, course_key)
else: return HttpResponse(status=405)
if entrance_exam_minimum_score_pct is None: entrance_exam_minimum_score_pct = _get_default_entrance_exam_minimum_pct()
course = modulestore().get_course(course_key) if course is None: return HttpResponse(status=400)
create_xblock( parent_locator=unicode(created_block.location), user=request.user, category='sequential', display_name=_('Entrance Exam - Subsection') ) add_entrance_exam_milestone(course.id, created_block)
remove_entrance_exam_graders(course_key, request.user)
CONTENT_RE = re.compile(r"(?P<start>\d{1,11})-(?P<stop>\d{1,11})/(?P<end>\d{1,11})")
try: data_root = path(settings.GITHUB_REPO_ROOT) subdir = base64.urlsafe_b64encode(repr(courselike_key)) course_dir = data_root / subdir filename = request.FILES['course-data'].name
session_status = request.session.setdefault("import_status", {}) courselike_string = unicode(courselike_key) + filename _save_request_status(request, courselike_string, 0)
if root_name == COURSE_ROOT: if courselike_module.entrance_exam_enabled: remove_entrance_exam_milestone_reference(request, courselike_key) log.info( "entrance exam milestone content reference for course %s has been removed", courselike_module.id )
try: matches = CONTENT_RE.search(request.META["HTTP_CONTENT_RANGE"]) content_range = matches.groupdict()
content_range = {'start': 0, 'stop': 1, 'end': 2}
try: log.info("Course import %s: Upload complete", courselike_key) _save_request_status(request, courselike_string, 1)
if session_status[courselike_string] != 4: _save_request_status(request, courselike_string, -abs(session_status[courselike_string]))
pass
requested_format = request.GET.get('_accept', request.META.get('HTTP_ACCEPT', 'text/html'))
return HttpResponse(status=406)
static_tab_loc = course_key.make_usage_key('static_tab', tab.url_slug) tab.locator = static_tab_loc
requested_tab_id_locators = request.json['tabs']
old_tab_list = course_item.tabs
non_displayed_tabs = set(old_tab_list) - set(new_tab_list) new_tab_list.extend(non_displayed_tabs)
course_item.tabs = new_tab_list modulestore().update_item(course_item, request.user.id)
tab_id_locator = request.json['tab_id_locator']
tab.is_hidden = request.json['is_hidden'] modulestore().update_item(course_item, request.user.id)
modulestore().update_item(course, ModuleStoreEnum.UserID.primitive_command)
if requested_sort == 'date_added': requested_sort = 'uploadDate' elif requested_sort == 'display_name': requested_sort = 'displayname' sort = [(requested_sort, sort_direction)]
thumbnail_location = asset.get('thumbnail_location', None) if thumbnail_location: thumbnail_location = course_key.make_asset_key( 'thumbnail', thumbnail_location[4])
try: modulestore().get_course(course_key) except ItemNotFoundError: logging.error("Could not find course: %s", course_key) return HttpResponseBadRequest()
upload_file = request.FILES['file'] filename = upload_file.name mime_type = upload_file.content_type size = get_file_size(upload_file)
(thumbnail_content, thumbnail_location) = contentstore().generate_thumbnail( content, tempfile_path=tempfile_path, )
del_cached_content(thumbnail_location) if thumbnail_content is not None: content.thumbnail_location = thumbnail_location
contentstore().save(content) del_cached_content(content.location)
try: content = contentstore().find(asset_key) except NotFoundError: raise AssetNotFoundException
contentstore('trashcan').save(content)
contentstore().delete(content.get_id()) del_cached_content(content.location)
'id': unicode(location)
def landing(request, org, course, coursename): return render_to_response('temp-course-landing.html', {})
def edge(request): return redirect('/')
return xblock.has_children
fields = {}
child_position = None if is_entrance_exams_enabled(): if category == 'chapter' and is_entrance_exam: fields['is_entrance_exam'] = is_entrance_exam
if '/' == asset_key_string[0]: asset_key_string = asset_key_string[1:] asset_key = AssetKey.from_string(asset_key_string) try: delete_asset(course_key, asset_key) except AssetNotFoundException: pass
certificate["version"] = CERTIFICATE_SCHEMA_VERSION if certificate.get("signatories") is None: certificate["signatories"] = [] certificate["editing"] = False return certificate
if certificate_data.get('course_title'): certificate_response["course_title"] = certificate_data['course_title']
return certificate
certificates = course.certificates.get('certificates', []) if only_active: certificates = [certificate for certificate in certificates if certificate.get('is_active', False)] return certificates
course.certificates['certificates'].pop(index) store.update_item(course, request.user.id) break
for certificate in certificates: certificate['is_active'] = is_active break
for certificate in certificates: is_active = certificate.get('is_active', False) break
if not GlobalStaff().has_user(request.user): raise PermissionDenied()
if not GlobalStaff().has_user(request.user): raise PermissionDenied()
for index, cert in enumerate(certificates_list): if certificate_id is not None: if int(cert['id']) == int(certificate_id): match_cert = cert
if auth.user_has_role(user, CourseInstructorRole(course_id)): return 'instructor' else: return 'staff'
KEY_EXPIRATION_IN_SECONDS = 86400
return _("{profile_name} URL").format(profile_name=profile)
course = get_course_and_check_access(course_key, user)
for video in videos: video["status"] = StatusDisplayStrings.get(video["status"])
NEVER = lambda x: False ALWAYS = lambda x: True
xblock.runtime.wrappers.append(partial( wrap_xblock, 'StudioRuntime', usage_id_serializer=unicode, request_token=request_token(request), ))
reorderable_items = set() if view_name == 'reorderable_container_child_preview': reorderable_items.add(xblock.location)
context = {
return modulestore().update_item(xblock, user.id)
with store.bulk_operations(xblock.location.course_key):
if publish == "discard_changes": store.revert_to_published(xblock.location, user.id) return JsonResponse({'id': unicode(xblock.location)})
xblock.children = children
xblock = _update_with_callback(xblock, user, old_metadata, old_content)
if publish == 'make_public': modulestore().publish(xblock.location, user.id)
return JsonResponse(result, encoder=EdxJSONEncoder)
if category not in ['html', 'problem', 'video']: return HttpResponseBadRequest( "Category '%s' not supported for Libraries" % category, content_type='text/plain' )
dest_usage_key = source_item.location.replace(name=uuid4().hex) category = dest_usage_key.block_type
if source_item.has_children and not children_handled: dest_module.children = dest_module.children or [] for child in source_item.children: dupe = _duplicate_item(dest_module.location, child, user=user)
if branch == ModuleStoreEnum.BranchName.published: revision = ModuleStoreEnum.RevisionOption.published_only store.delete_item(itemloc, user_id, revision=revision)
return store.create_item(user.id, usage_key.course_key, usage_key.block_type, block_id=usage_key.block_id)
if not isinstance(xblock.location, LibraryUsageLocator): modulestore().has_changes(modulestore().get_course(xblock.location.course_key, depth=None))
xblock_info = create_xblock_info( xblock, data=data, metadata=own_metadata(xblock), include_ancestor_info=include_ancestor_info ) if include_publishing_info: add_container_page_publishing_info(xblock, xblock_info) return xblock_info
has_changes = None if (is_xblock_unit or course_outline) and not is_library_block: has_changes = modulestore().has_changes(xblock)
graders = _filter_entrance_exam_grader(graders)
if course is None: course = modulestore().get_course(xblock.location.course_key)
xblock_actions = {'deletable': True, 'draggable': True, 'childAddable': True} explanatory_message = None
xblock_info.update(_get_gating_info(course, xblock))
if getattr(xblock, "in_entrance_exam", False): xblock_info["is_header_visible"] = False
return VisibilityState.needs_attention
reset_to_default = False try: reset_to_default = xblock.start.year < 1900 except ValueError: reset_to_default = True
return get_default_time_display(xblock.start) if xblock.start != DEFAULT_START_DATE else None
user_perms = get_user_permissions(request.user, course_key) if not user_perms & STUDIO_VIEW_USERS: raise PermissionDenied()
staff = set(CourseStaffRole(course_key).users_with_role()).union(instructors)
if is_library: role_hierarchy = (CourseInstructorRole, CourseStaffRole, LibraryUserRole) else: role_hierarchy = (CourseInstructorRole, CourseStaffRole)
if not ((requester_perms & STUDIO_EDIT_ROLES) or (user.id == request.user.id)): return permissions_error_response
auth.add_users(request.user, role, user) role_added = True
old_roles.add(role)
CourseEnrollment.enroll(user, course_key)
req = django_to_webob_request(request) try: resp = instance.handle(handler, req, suffix)
is_author_mode = True
partial( wrap_xblock, 'PreviewRuntime', display_name_only=display_name_only, usage_id_serializer=unicode, request_token=request_token(request) ),
partial(replace_static_urls, None, course_id=course_id), _studio_wrap_xblock,
wrappers.insert(0, wrap_with_license)
wrappers=wrappers, wrappers_asides=wrappers_asides, error_descriptor_class=ErrorDescriptor, get_user_role=lambda: get_user_role(request.user, course_id),
resp = self.client.ajax_post( first_update_url, payload, HTTP_X_HTTP_METHOD_OVERRIDE="PUT", REQUEST_METHOD="POST" )
refetched = self.client.get_json(first_update_url) self.assertHTMLEqual( content, json.loads(refetched.content)['content'], "get w/ provided id" )
content = '<ol/>' payload = get_response(content, 'January 11, 2013') self.assertHTMLEqual(content, payload['content'], "self closing ol")
self.assertContains( self.client.ajax_post(course_update_url, {'garbage': 1}), 'Failed to save', status_code=400 )
content = 'outside <strong>inside</strong> after' payload = get_response(content, 'June 22, 2000') self.assertHTMLEqual(content, payload['content'], "text outside tag")
content = '<garbage tag No closing brace to force <span>error</span>' payload = {'content': content, 'date': 'January 11, 2013'}
content = "<p><br><br></p>" payload = get_response(content, 'January 11, 2013') self.assertHTMLEqual(content, payload['content'])
self.assertContains(self.client.delete(course_update_url + '19'), "delete", status_code=400)
resp = self.client.get_json(course_update_url) payload = json.loads(resp.content) self.assertTrue(len(payload) == 1)
self.client.ajax_post(course_update_url)
self.assertEqual(resp.status_code, 200)
self.assertFalse(mock_push_update.called)
handouts_location = self.course.id.make_usage_key('course_info', 'handouts') course_handouts_url = reverse_usage_url('xblock_handler', handouts_location)
self.assertEqual(resp.status_code, 200)
libraries = [LibraryFactory.create() for _ in range(3)] lib_dict = dict([(lib.location.library_key, lib) for lib in libraries])
self.assertNotIn(extra_user.username, response.content)
response = self.client.get(manage_users_url) self.assertEqual(response.status_code, 200) self.assertIn(extra_user.username, response.content)
course_url = u'/course/{}'.format(unicode(self.course.id)) self.assertEqual(xblock_studio_url(self.course), course_url)
video = ItemFactory.create(parent_location=child_vertical.location, category="video", display_name="My Video") self.assertIsNone(xblock_studio_url(video))
library = LibraryFactory.create() expected_url = u'/library/{}'.format(unicode(library.location.library_key)) self.assertEqual(xblock_studio_url(library), expected_url)
child_vertical = ItemFactory.create(parent_location=vertical.location, category='vertical', display_name='Child Vertical') self.assertEqual(xblock_type_display_name(child_vertical), u'Vertical')
self.odd_course = CourseFactory.create( org='test.org_1-2', number='test-2.3_course', display_name='dotted.course.name-2', )
lib1 = LibraryFactory.create()
outline_response = self.client.get(link.get("href"), {}, HTTP_ACCEPT='text/html') self.assertEqual(outline_response.status_code, 200)
non_staff_client, _ = self.create_non_staff_authed_user_client() response = non_staff_client.delete(outline_url, {}, HTTP_ACCEPT='application/json') self.assertEqual(response.status_code, 403)
self.check_index_and_outline(course_staff_client)
self.assert_correct_json_response(json_response)
notification_url = reverse_course_url('course_notifications_handler', self.course.id, kwargs={ 'action_state_id': 1, })
self.assertEquals(resp.status_code, 400)
rerun_state = CourseRerunState.objects.update_state( course_key=self.course.id, new_state=state, allow_not_found=True ) CourseRerunState.objects.update_should_display( entry_id=rerun_state.id, user=UserFactory(), should_display=should_display )
user2 = UserFactory() add_instructor(rerun_course_key, self.user, user2)
rerun_state = CourseRerunState.objects.update_state( course_key=rerun_course_key, new_state=state, allow_not_found=True ) CourseRerunState.objects.update_should_display( entry_id=rerun_state.id, user=user2, should_display=should_display )
CourseRerunState.objects.get(id=rerun_state.id)
self.course.display_coursenumber = None updated_course = self.update_course(self.course, self.user.id)
self.assertEqual(updated_course.display_coursenumber, None)
course_outline_url = reverse_course_url('course_handler', updated_course.id) response = self.client.get_html(course_outline_url)
self.assertEqual(response.status_code, 200)
self.assertIn('display_course_number: ""', response.content)
self.assert_correct_json_response(json_response)
self.assertIsNone(course_outline_initial_state('no-such-locator', course_structure))
self.assertEqual(_get_release_date(response), 'Unscheduled') _assert_settings_link_present(response)
self.assertEqual(info['blocks'], [])
if delete_vertical: self.store.delete_item(vertical1.location, self.user.id) else: self.store.delete_item(problem1.location, self.user.id)
self.assertEqual( info['blocks'], [[reverse_usage_url('container_handler', vertical2.location), 'notes problem in vert2']] )
self.assertIn(self.SUCCESSFUL_RESPONSE, response.content) self.assertEqual(response.status_code, 200)
non_staff_client, _ = self.create_non_staff_authed_user_client() response = non_staff_client.get(index_url, {}, HTTP_ACCEPT='application/json') self.assertEqual(response.status_code, 403)
self.assertIn(self.SUCCESSFUL_RESPONSE, response.content) self.assertEqual(response.status_code, 200)
err = SearchIndexingError mock_index_dictionary.return_value = err
response = self.client.get(index_url, {}, HTTP_ACCEPT='application/json') self.assertEqual(response.status_code, 500)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
reindex_course_and_check_access(self.course.id, self.user)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = SearchIndexingError mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = SearchIndexingError mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
err = ItemNotFoundError mock_get_course.return_value = err
with self.assertRaises(SearchIndexingError): reindex_course_and_check_access(self.course.id, self.user)
user2 = UserFactory() with self.assertRaises(PermissionDenied): reindex_course_and_check_access(self.course.id, user2)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
response = perform_search( "unique", user=self.user, size=10, from_=0, course_id=unicode(self.course.id)) self.assertEqual(response['total'], 1)
err = Exception mock_index_dictionary.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
err = ItemNotFoundError mock_get_course.return_value = err
with self.assertRaises(SearchIndexingError): CoursewareSearchIndexer.do_course_reindex(modulestore(), self.course.id)
test_container_html(draft_container)
self.store.publish(self.vertical.location, self.user.id) draft_container = self.store.get_item(draft_container.location) test_container_html(draft_container)
self.assertRaises( Http404, views.container_handler, request, usage_key_string='i4x://InvalidOrg/InvalidCourse/vertical/static/InvalidContent', )
response = views.container_handler( request=request, usage_key_string=unicode(self.vertical.location) ) self.assertEqual(response.status_code, 200)
self.course = modulestore().get_course(self.course.id)
self.assertEqual(len(paths[milestone_key]), 0)
update_entrance_exam(request, self.course.id, {})
super(TabsPageTests, self).setUp()
self.url = reverse_course_url('tabs_handler', self.course.id)
self.test_tab = ItemFactory.create( parent_location=self.course.location, category="static_tab", display_name="Static_1" ) self.reload_course()
with self.assertRaises(NotImplementedError): self.client.get(self.url)
with self.assertRaises(NotImplementedError): self.client.ajax_post( self.url, data={'invalid_request': None}, )
orig_tab_ids = [tab.tab_id for tab in self.course.tabs] tab_ids = list(orig_tab_ids) num_orig_tabs = len(orig_tab_ids)
self.assertTrue(num_orig_tabs >= 5)
tab_ids[num_orig_tabs - 1], tab_ids[num_orig_tabs - 2] = tab_ids[num_orig_tabs - 2], tab_ids[num_orig_tabs - 1]
removed_tab = tab_ids.pop(num_orig_tabs / 2) self.assertTrue(len(tab_ids) == num_orig_tabs - 1)
resp = self.client.ajax_post( self.url, data={'tabs': [{'tab_id': tab_id} for tab_id in tab_ids]}, ) self.assertEqual(resp.status_code, 204)
self.reload_course() new_tab_ids = [tab.tab_id for tab in self.course.tabs] self.assertEqual(new_tab_ids, tab_ids + [removed_tab]) self.assertNotEqual(new_tab_ids, orig_tab_ids)
tab_ids[0], tab_ids[1] = tab_ids[1], tab_ids[0]
resp = self.client.ajax_post( self.url, data={'tabs': [{'tab_id': tab_id} for tab_id in invalid_tab_ids]}, ) self.check_invalid_tab_id_response(resp)
old_tab = CourseTabList.get_tab_by_type(self.course.tabs, tab_type)
self.assertNotEqual(old_tab.is_hidden, new_is_hidden_setting)
self.reload_course() new_tab = CourseTabList.get_tab_by_type(self.course.tabs, tab_type) self.assertEqual(new_tab.is_hidden, new_is_hidden_setting)
self.assertEquals(course.tabs[2], {'type': 'discussion', 'name': 'Discussion'})
self.assertEqual(content.content_type, 'application/pdf')
relative_path = 'just_a_test.jpg' absolute_path = base_url + relative_path
self.assert_correct_filter_response(self.url, 'asset_type', 'OTHER')
output = assets._get_asset_json("my_file", content_type, upload_date, location, thumbnail_location, True)
json.dumps(assets._get_asset_json( "sample_static.txt", content_type, upload_date, asset_location, None, lock)), "application/json"
module_store = modulestore() course_items = import_course_from_xml( module_store, self.user.id, TEST_DATA_DIR, ['toy'], static_content_store=contentstore(), verbose=True ) course = course_items[0] verify_asset_locked_state(False)
resp_asset = post_asset_update(True, course) self.assertTrue(resp_asset['locked']) verify_asset_locked_state(True)
resp_asset = post_asset_update(False, course) self.assertFalse(resp_asset['locked']) verify_asset_locked_state(False)
self.asset_name = 'delete_test' self.asset = self.get_sample_asset(self.asset_name)
resp_status = self.client.get( reverse_course_url( 'import_status_handler', self.course.id, kwargs={'filename': os.path.split(self.bad_tar)[1]} ) )
__, nonstaff_user = self.create_non_staff_authed_user_client() auth.add_users(self.user, CourseStaffRole(self.course.id), nonstaff_user)
self.assertNotEqual(display_name_before_import, display_name_after_import)
self.assertFalse(CourseInstructorRole(self.course.id).has_user(nonstaff_user)) self.assertTrue(CourseStaffRole(self.course.id).has_user(nonstaff_user))
self.assertFalse(CourseInstructorRole(self.course.id).has_user(nonstaff_user)) self.assertTrue(CourseStaffRole(self.course.id).has_user(nonstaff_user))
with self.settings(DATA_DIR='/not/the/data/dir'): try_tar(self._edx_platform_tar())
extract_dir_relative = path.relpath(extract_dir, settings.DATA_DIR)
extract_dir_relative = path.relpath(extract_dir, settings.DATA_DIR)
extract_dir_relative = path.relpath(extract_dir, settings.DATA_DIR)
import_library_from_xml( self.store, 'test_user', self.export_dir, ['exported_source_library'], static_content_store=contentstore(), target_id=source_library2_key, load_error_modules=False, raise_on_failure=True, create_if_not_present=True, )
self.assertCoursesEqual(source_library1_key, source_library2_key)
if resp.context: self.assertEqual(resp.context['course'], self.course)
no_ids = [] self.reload_course() for textbook in self.course.pdf_textbooks: del textbook["id"] no_ids.append(textbook) self.assertEqual(no_ids, textbooks)
self.save_course() self.url_nonexist = self.get_details_url("1=20")
for video in self.previous_uploads: self.assertIn(video["edx_video_id"], response.content)
assert_bad({})
assert_bad({"files": [{"content_type": "video/mp4"}]})
assert_bad({"files": [{"file_name": "test.mp4"}]})
mock_key.side_effect = mock_key_instances + [Mock()]
response_file = response_obj["files"][i] self.assertEqual(response_file["file_name"], file_info["file_name"]) self.assertEqual(response_file["upload_url"], mock_key_instance.generate_url())
context = { 'reorderable_items': set(), 'read_only': True } html = get_preview_fragment(request, html, context).content
context = { 'reorderable_items': set(), 'read_only': True } html = get_preview_fragment(request, html, context).content
invalid_json = "{u'name': 'Test Name', []}"
user_partititons = self.course.user_partitions
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 1) self.assertEqual(len(user_partititons[0].groups), 3)
user_partititons = self.course.user_partitions
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 1) self.assertEqual(user_partititons[0].name, 'Name 1')
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 2) self.assertEqual(user_partititons[0].name, 'Name 0')
user_partititons = self.course.user_partitions self.assertEqual(len(user_partititons), 2) self.assertEqual(user_partititons[0].name, 'Name 0')
actual = GroupConfiguration.get_or_create_content_group(self.store, self.course)
self.assertEqual(actual, expected)
actual = GroupConfiguration.get_content_groups_usage_info(self.store, self.course) self.assertEqual(actual.keys(), [0])
self.verify_validation_update_usage_info(expected_result, mocked_message)
self.create_programs_config() self.mock_programs_api(data={'results': []})
self.mock_programs_api()
self.create_programs_config()
user, client_name = mock_get_id_token.call_args[0] self.assertEqual(user, self.user) self.assertEqual(client_name, "programs")
self.assertNotContains(resp, self.ext_user.email)
ext_user = User.objects.get(email=self.ext_user.email) self.assertFalse(auth.user_has_role(ext_user, CourseStaffRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertFalse(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseInstructorRole(self.course.id)))
user = User.objects.get(email=self.user.email) self.assertFalse(auth.user_has_role(user, CourseStaffRole(self.course.id)))
ext_user = User.objects.get(email=self.ext_user.email) self.assertTrue(auth.user_has_role(ext_user, CourseStaffRole(self.course.id)))
self.assert_not_enrolled()
self.course.enable_subsection_gating = True self.save_course()
self.chapter = ItemFactory.create( parent_location=self.course.location, category='chapter', display_name='untitled chapter' )
self.clear_subs_content()
self.assertEqual(json.loads(resp.content).get('status'), 'Transcripts are supported only for "video" modules.')
self.assertIn("ufeff", filedata) self.ufeff_srt_file.write(filedata) self.ufeff_srt_file.seek(0)
link = reverse('download_transcripts') resp = self.client.get(link, {'locator': 'BAD_LOCATOR'}) self.assertEqual(resp.status_code, 404)
resp = self.create_xblock(category='vertical') usage_key = self.response_usage_key(resp)
resp = self.client.get(reverse_usage_url('xblock_handler', usage_key)) self.assertEqual(resp.status_code, 200)
self.assertIn('wrapper-xblock-message', html) self.assertNotRegexpMatches(html, r'wrapper-xblock[^-]+')
self.assertIn('<header class="xblock-header xblock-header-vertical">', html) self.assertIn('<article class="xblock-render">', html)
child_vertical_usage_key = self._create_vertical(parent_usage_key=root_usage_key) resp = self.create_xblock(parent_usage_key=child_vertical_usage_key, category='problem', boilerplate='multiplechoice.yaml') self.assertEqual(resp.status_code, 200)
html, __ = self._get_container_preview(root_usage_key)
self.assertIn('level-element', html)
root_usage_key = self._create_vertical()
resp = self.create_xblock(category='static_tab', parent_usage_key=course.location) usage_key = self.response_usage_key(resp)
resp = self.client.delete(reverse_usage_url('xblock_handler', usage_key)) self.assertEqual(resp.status_code, 204)
display_name = 'Nicely created' resp = self.create_xblock(display_name=display_name, category='chapter')
course = self.get_item_from_modulestore(self.usage_key) self.assertIn(chap_usage_key, course.children)
resp = self.create_xblock(parent_usage_key=chap_usage_key, category='vertical') vert_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(category='problem', boilerplate='nosuchboilerplate.yaml') self.assertEqual(resp.status_code, 200)
resp = self.create_xblock(category='static_tab') usage_key = self.response_usage_key(resp)
new_tab = self.get_item_from_modulestore(usage_key) self.assertEquals(new_tab.display_name, 'Empty')
self.assertTrue( self._check_equality(source_usage_key, usage_key, parent_usage_key, check_asides=check_asides), "Duplicated item differs from original" )
original_item = self.get_item_from_modulestore(source_usage_key) duplicated_item = self.get_item_from_modulestore(duplicate_usage_key)
duplicated_item.location = original_item.location duplicated_item.display_name = original_item.display_name duplicated_item.parent = original_item.parent
data = { 'parent_locator': unicode(parent_usage_key), 'duplicate_source_locator': unicode(source_usage_key) } if display_name is not None: data['display_name'] = display_name
resp = self.create_xblock(parent_usage_key=self.usage_key, category='chapter') self.chapter_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.chapter_usage_key, category='sequential') self.seq_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.seq_usage_key, category='problem', boilerplate='multiplechoice.yaml') self.problem_usage_key = self.response_usage_key(resp)
self.create_xblock(parent_usage_key=self.chapter_usage_key, category='sequential2')
verify_order(self.html_usage_key, self.seq_usage_key, 2) verify_order(self.seq_usage_key, self.chapter_usage_key, 0)
verify_order(self.html_usage_key, self.usage_key)
verify_name(self.html_usage_key, self.seq_usage_key, "Duplicate of 'Text'")
verify_name(self.seq_usage_key, self.chapter_usage_key, "Duplicate of sequential")
verify_name(self.seq_usage_key, self.chapter_usage_key, "customized name", display_name="customized name")
resp = self.create_xblock(parent_usage_key=self.usage_key, category='chapter') self.chapter_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.chapter_usage_key, category='sequential') self.seq_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=self.seq_usage_key, category='problem', boilerplate='multiplechoice.yaml') self.problem_usage_key = self.response_usage_key(resp)
display_name = 'chapter created' resp = self.create_xblock(display_name=display_name, category='chapter') chap_usage_key = self.response_usage_key(resp)
resp = self.create_xblock(parent_usage_key=chap_usage_key, category='sequential') self.seq_usage_key = self.response_usage_key(resp) self.seq_update_url = reverse_usage_url("xblock_handler", self.seq_usage_key)
resp = self.client.delete(reverse_usage_url("xblock_handler", chapter1_usage_key)) self.assertEqual(resp.status_code, 204)
course = self.get_item_from_modulestore(self.usage_key) self.assertNotIn(chapter1_usage_key, course.children) self.assertIn(chapter2_usage_key, course.children)
children = self.get_item_from_modulestore(self.seq_usage_key).children self.assertEqual(unit1_usage_key, children[1]) self.assertEqual(unit2_usage_key, children[2])
resp = self.client.ajax_post( self.seq2_update_url, data={'children': [unicode(unit_1_key), unicode(unit_2_key)]} ) self.assertEqual(resp.status_code, 200)
self.assertListEqual( self.get_item_from_modulestore(self.seq2_usage_key).children, [unit_1_key, unit_2_key], ) self.assertListEqual( self.get_item_from_modulestore(self.seq_usage_key).children,
self.assertListEqual( self.get_item_from_modulestore(self.seq2_usage_key).children, [] )
self.assertListEqual( self.get_item_from_modulestore(self.seq2_usage_key).children, [unit_1_key, unit_2_key] )
self.assertFalse(self._is_location_published(self.problem_usage_key))
self.client.ajax_post( self.problem_update_url, data={'publish': 'make_public'} )
draft = self.get_item_from_modulestore(self.problem_usage_key, verify_is_draft=True) self.assertNotEqual(draft.data, published.data)
unit_update_url = reverse_usage_url('xblock_handler', unit_usage_key) self.assertFalse(self._is_location_published(unit_usage_key)) self.assertFalse(self._is_location_published(html_usage_key))
data={'metadata': {'user_partition_id': str(partition_id)}}
split_test = self.get_item_from_modulestore(self.split_test_usage_key, verify_is_draft=True) self.assertEqual(partition_id, split_test.user_partition_id) return split_test
self.assertEqual(-1, split_test.user_partition_id) self.assertEqual(0, len(split_test.children))
split_test = self._update_partition_id(0)
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_vertical_0_location = split_test.children[0] initial_vertical_1_location = split_test.children[1]
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_group_id_to_child = split_test.group_id_to_child
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) self.assertEqual(initial_group_id_to_child, split_test.group_id_to_child)
split_test = self._update_partition_id(0) self.assertEqual(2, len(split_test.children)) initial_group_id_to_child = split_test.group_id_to_child
split_test = self._update_partition_id(-50) self.assertEqual(2, len(split_test.children)) self.assertEqual(initial_group_id_to_child, split_test.group_id_to_child)
split_test = self._update_partition_id(0)
split_test = self._assert_children(2) group_id_to_child = split_test.group_id_to_child.copy() self.assertEqual(2, len(group_id_to_child))
split_test.add_missing_groups(self.request) split_test = self._assert_children(3) self.assertEqual(group_id_to_child, split_test.group_id_to_child)
self.descriptor = self.modulestore.return_value.get_item.return_value
req_factory_method = getattr(self.request_factory, method.lower()) request = req_factory_method('/dummy-url') request.user = self.user
XBlockDisableConfig.objects.create( disabled_create_blocks='', enabled=True )
with check_mongo_calls(chapter_queries_1): self.client.get(outline_url, HTTP_ACCEPT='application/json')
self.assertEqual(xblock_info['is_header_visible'], False) self.assertEqual(xblock_info['display_name'], 'Subsection - Entrance Exam')
self.assertIsNone(xblock_info.get('is_header_visible', None))
self.validate_xblock_info_consistency(xblock_info, has_child_info=has_child_info, course_outline=course_outline)
self.validate_xblock_info_consistency(xblock_info, has_child_info=has_child_info)
self.validate_xblock_info_consistency(xblock_info, has_child_info=has_child_info)
self.validate_xblock_info_consistency(xblock_info, has_child_info=True, has_ancestor_info=True)
self.validate_xblock_info_consistency(xblock_info)
self.assertEqual(xblock_info['enable_proctored_exams'], True)
self.assertEqual(xblock_info['is_proctored_exam'], True) self.assertEqual(xblock_info['is_time_limited'], True) self.assertEqual(xblock_info['default_time_limit_minutes'], 100)
return modulestore().get_item(child.location)
self._verify_visibility_state(xblock_info, VisibilityState.ready)
xblock_info = self._get_xblock_info(chapter.location) self._verify_visibility_state(xblock_info, VisibilityState.ready) self.assertFalse(course.self_paced)
course.self_paced = True self.store.update_item(course, self.user.id) self.assertTrue(course.self_paced)
xblock_info = self._get_xblock_info(chapter.location) self._verify_visibility_state(xblock_info, VisibilityState.live)
drag_handle_html = '<span data-tooltip="Drag to reorder" class="drag-handle action"></span>' self.assertIn(drag_handle_html, html)
add_button_html = '<div class="add-xblock-component new-component-item adding"></div>' if can_add: self.assertIn(add_button_html, html) else: self.assertNotIn(add_button_html, html)
{ u'description': 'Test description', u'version': CERTIFICATE_SCHEMA_VERSION },
{},
result = self.client.get_html(self._url()) self.assertNotIn('Test certificate', result.content)
from edxmako.shortcuts import render_to_response from mako.exceptions import TopLevelLookupException from django.http import HttpResponseNotFound
if not course_key_string or not action_state_id: return HttpResponseBadRequest()
return _dismiss_notification(request, action_state_id)
return HttpResponseBadRequest()
remove_all_instructors(action_state.course_key)
action_state.delete()
if isinstance(course.id, CCXLocator): return False
raise AccessListFallback
courses_list[course_key] = course
return [lib for lib in modulestore().get_libraries() if has_studio_read_access(user, lib.location.library_key)]
courses, in_process_course_actions = _accessible_courses_summary_list(request)
courses, in_process_course_actions = _accessible_courses_summary_list(request)
start = request.json.get('start', CourseFields.start.default) run = request.json.get('run')
fields.update({ 'language': getattr(settings, 'DEFAULT_COURSE_LANGUAGE', 'en'), 'cert_html_view_enabled': True, })
new_course = modulestore().create_course( org, number, run, user.id, fields=fields, )
add_instructor(new_course.id, user, user)
initialize_permissions(new_course.id, user) return new_course
if not has_studio_write_access(request.user, source_course_key): raise PermissionDenied()
store = modulestore() with store.default_store('split'): destination_course_key = store.make_course_key(org, number, run)
if store.has_course(destination_course_key, ignore_case=True): raise DuplicateCourseError(source_course_key, destination_course_key)
add_instructor(destination_course_key, request.user, request.user)
CourseRerunState.objects.initiated(source_course_key, destination_course_key, request.user, fields['display_name'])
fields['advertised_start'] = None
json_fields = json.dumps(fields, cls=EdxJSONEncoder) rerun_course.delay(unicode(source_course_key), unicode(destination_course_key), request.user.id, json_fields)
return JsonResponse({ 'url': reverse_url('course_handler'), 'destination_course_key': unicode(destination_course_key) })
if not has_studio_write_access(request.user, usage_key.course_key): raise PermissionDenied()
courses = [course for course in courses if course.id != course_key] if courses: courses = _remove_in_process_courses(courses, in_process_course_actions) settings_context.update({'possible_pre_requisite_courses': courses})
credit_requirements = get_credit_requirements(course_key) paired_requirements = {} for requirement in credit_requirements: namespace = requirement.pop("namespace") paired_requirements.setdefault(namespace, []).append(requirement)
show_min_grade_warning = False if course_module.minimum_grade_credit > 0 else True settings_context.update( { 'is_credit_course': True, 'credit_requirements': paired_requirements, 'show_min_grade_warning': show_min_grade_warning, } )
encoder=CourseSettingsEncoder
elif not entrance_exam_enabled and course_entrance_exam_present: delete_entrance_exam(request, course_key)
return JsonResponse( CourseDetails.update_from_json(course_key, request.json, request.user), encoder=CourseSettingsEncoder )
encoder=CourseSettingsEncoder
if 'minimum_grade_credit' in request.json: update_credit_course_requirements.delay(unicode(course_key))
for tab_type in CourseTabPluginManager.get_tab_types(): if not tab_type.is_dynamic and tab_type.is_default: tab_enabled = tab_type.is_enabled(course_module, user=request.user) update_tab(course_tabs, tab_type, tab_enabled)
if course_tabs != course_module.tabs: course_module.tabs = course_tabs
is_valid, errors, updated_data = CourseMetadata.validate_and_update_from_json( course_module, request.json, user=request.user, )
_refresh_course_tabs(request, course_module)
modulestore().update_item(course_module, request.user.id)
except (TypeError, ValueError, InvalidTabsException) as err: return HttpResponseBadRequest( django.utils.html.escape(err.message), content_type="text/plain" )
tid = random.choice(string.digits) + tid
tid = tid + random.choice(string.ascii_lowercase)
try: new_configuration = GroupConfiguration(request.body, course).get_user_partition() except GroupConfigurationsValidationError as err: return JsonResponse({"error": err.message}, status=400)
try: new_configuration = GroupConfiguration(request.body, course, group_configuration_id).get_user_partition() except GroupConfigurationsValidationError as err: return JsonResponse({"error": err.message}, status=400)
add_user_with_status_unrequested(user) course_creator_status = get_course_creator_status(user)
if library_key_string: return _display_library(library_key_string, request)
add_instructor(new_lib.location.library_key, request.user, request.user)
return redirect_with_get('login', request.GET, False)
next_url = request.GET.get('next') if next_url: return redirect(next_url) else: return redirect('/course/')
return redirect(reverse('cas-login'))
CONTENT_GROUP_CONFIGURATION_DESCRIPTION = 'The groups in this configuration can be mapped to cohort groups in the LMS.'
self.assertFalse(user(email).is_active)
resp = self.client.get(reverse('activate', kwargs={'key': activation_key})) return resp
self.assertTrue(user(email).is_active)
cache.clear()
self.assertEqual(resp.status_code, 400)
self.assertEqual(resp.status_code, 200)
resp = self._login(self.email, self.pw) data = parse_json(resp) self.assertFalse(data['success'])
self.login(self.email, self.pw)
self.login(self.email, self.pw)
self.login(self.email, self.pw)
self.client.logout() resp = self._activate_user(self.email) self.assertEqual(resp.status_code, 200)
expected = 'You can now <a href="' + reverse('login') + '">login</a>.' self.assertIn(expected, resp.content)
simple_auth_pages = ( '/home/', )
self.test_create_account()
self.client = AjaxEnabledTestClient()
print 'Not logged in' for page in auth_pages: print "Checking '{0}'".format(page) self.check_page_get(page, expected=302)
self.login(self.email, self.pw)
resp = self.client.get_html('/home/') self.assertEqual(resp.status_code, 302)
course_url = '/home/' resp = self.client.get_html(course_url) self.assertEquals(resp.status_code, 200)
time.sleep(2)
self.assertRedirects(resp, settings.LOGIN_REDIRECT_URL + '?next=/home/')
self.course.discussion_blackouts = [[]] self.assertTrue(self.course.forum_posts_allowed)
self.user.is_staff = True
course = CourseFactory.create() course.display_coursenumber = escaping_content
response = self.client.get('/home') self.assertEqual(response.status_code, 200) self.assert_no_xss(response, escaping_content)
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), 1)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list_by_groups), 1)
self.assertEqual(courses_list, courses_list_by_groups)
course_location = self.store.make_course_key('Org1', 'Course1', 'Run1') course = self._create_course_with_access_groups(course_location, self.user)
ccx_course_key = CCXLocator.from_course_locator(course.id, '1') self._add_role_access_to_user(self.user, ccx_course_key)
courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), 1) self.assertNotIn( ccx_course_key, [course.id for course in courses_list] )
instructor_courses = UserBasedRole(self.user, CourseInstructorRole.ROLE).courses_with_role() staff_courses = UserBasedRole(self.user, CourseStaffRole.ROLE).courses_with_role() all_courses = (instructor_courses | staff_courses)
self.assertIn( ccx_course_key, [access.course_id for access in all_courses] )
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(courses_list, [])
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(courses_list_by_groups, [])
GlobalStaff().add_users(self.user) self.assertTrue(GlobalStaff().has_user(self.user))
courses_list_by_staff, __ = get_courses_accessible_to_user(self.request) self.assertEqual(len(courses_list_by_staff), TOTAL_COURSES_COUNT)
self.assertTrue(all(isinstance(course, CourseSummary) for course in courses_list_by_staff))
with check_mongo_calls(mongo_calls): _accessible_courses_summary_list(self.request)
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(courses_list, [])
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(courses_list_by_groups, []) self.assertEqual(courses_list, courses_list_by_groups)
courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), 1)
self.assertTrue(all(isinstance(course, CourseSummary) for course in courses_summary_list)) self.assertEqual(len(courses_summary_list), 1)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list_by_groups), 1)
self.assertEqual(courses_list, courses_list_by_groups)
delete_course_and_groups(course_key, self.user.id)
courses_list, __ = _accessible_courses_list(self.request)
courses_summary_list, __ = _accessible_courses_summary_list(self.request)
courses_list_by_groups, __ = _accessible_courses_list_from_groups(self.request)
self.assertEqual( [len(courses_list), len(courses_list_by_groups), len(courses_summary_list)], [0, 0, 0] )
user_course_ids = random.sample(range(TOTAL_COURSES_COUNT), USER_COURSES_COUNT)
with Timer() as iteration_over_courses_time_1: courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
with Timer() as iteration_over_courses_time_2: courses_list, __ = _accessible_courses_list(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
with Timer() as iteration_over_groups_time_1: courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
with Timer() as iteration_over_groups_time_2: courses_list, __ = _accessible_courses_list_from_groups(self.request) self.assertEqual(len(courses_list), USER_COURSES_COUNT)
self.assertGreaterEqual(iteration_over_courses_time_1.elapsed, iteration_over_groups_time_1.elapsed) self.assertGreaterEqual(iteration_over_courses_time_2.elapsed, iteration_over_groups_time_2.elapsed)
with check_mongo_calls(courses_list_from_group_calls): _accessible_courses_list_from_groups(self.request)
role.add_users(self.user)
self.assertEqual(len(courses_list), 2) self.assertTrue(all(isinstance(course, CourseSummary) for course in courses_list))
for course in courses_in_progress: CourseRerunState.objects.initiated( sourse_course_key, destination_course_key=course.id, user=self.user, display_name="test course" )
repo_dir = os.path.abspath(git_export_utils.GIT_REPO_EXPORT_DIR) os.mkdir(repo_dir) self.addCleanup(shutil.rmtree, repo_dir)
self.assertTrue(self.store.has_item(course.id.make_usage_key('html', "multi_parent_html")))
course = self.create_course_with_orphans(module_store)
self.assertIn(orphan_vertical.location, self.store.get_orphans(course.id))
self.assertIn(multi_parent_html.location, orphan_vertical.children) self.assertIn(multi_parent_html.location, vertical1.children)
self.assertIn(orphan_chapter.location, self.store.get_orphans(course.id))
vertical1_parent = self.store.get_parent_location(vertical1.location) self.assertEqual(unicode(vertical1_parent), unicode(chapter1.location))
vertical1.children.append(html.location) self.store.update_item(vertical1, self.user.id)
html_parent = self.store.get_parent_location(html.location) self.assertEquals(unicode(html_parent), unicode(vertical1.location))
self.assertEqual(course.display_name, u"Φυσικά το όνομα Unicode")
print "static_asset_path = {0}".format(course.static_asset_path) self.assertEqual(course.static_asset_path, 'test_import_course')
all_assets, count = content_store.get_all_content_for_course(course.id) self.assertEqual(len(all_assets), 0) self.assertEqual(count, 0)
with check_exact_number_of_calls(store, 'refresh_cached_metadata_inheritance_tree', 28):
with check_exact_number_of_calls(store, '_get_cached_metadata_inheritance_tree', 1):
__, __, course = self.load_test_import_course(target_id=course_id, module_store=module_store)
__, __, re_course = self.load_test_import_course(target_id=course.id, module_store=module_store)
transcripts_utils.download_youtube_subs(good_youtube_sub, self.course, settings)
raise SkipTest
transcripts_utils.download_youtube_subs(good_youtube_sub, self.course, settings)
transcripts_utils.generate_subs_from_source(youtube_subs, 'SRT', srt_filedata, self.course)
self.html_unit = ItemFactory.create( parent_location=self.vertical.location, category="html", display_name="Html Content", modulestore=store, publish_item=False, )
added_to_index = self.reindex_course(store) self.assertEqual(added_to_index, 3) response = self.search() self.assertEqual(response["total"], 3)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 5)
self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 1)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.delete_item(store, self.html_unit.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 3)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
self.publish_item(store, self.vertical.location) self.reindex_course(store) response = self.search() self.assertEqual(response["total"], 4)
sequential2 = ItemFactory.create( parent_location=self.chapter.location, category='sequential', display_name='Section 2', modulestore=store, publish_item=True, start=datetime(2015, 3, 1, tzinfo=UTC), )
new_indexed_count = self.index_recent_changes(store, before_time) self.assertEqual(new_indexed_count, 5)
indexed_count = self.reindex_course(store) self.assertEqual(indexed_count, 7)
CoursewareSearchIndexer.do_course_reindex(store, course.id)
course = store.get_course(course.id, depth=1)
chapter_to_delete = course.get_children()[0] self.delete_item(store, chapter_to_delete.location)
CoursewareSearchIndexer.do_course_reindex(store, course.id) deleted_count = 1 + load_factor + (load_factor ** 2) + (load_factor ** 3) self.assert_search_count(course_size - deleted_count)
print "Failed with load_factor of {}".format(load_factor)
cls.html_unit = ItemFactory.create( parent_location=cls.vertical.location, category="html", display_name="Html Content", publish_item=False, )
response = searcher.search( doc_type=CoursewareSearchIndexer.DOCUMENT_TYPE, field_dictionary={"course": unicode(self.course.id)} ) self.assertEqual(response["total"], 3)
response = searcher.search(field_dictionary={"library": library_search_key}) self.assertEqual(response["total"], 2)
WORKS_WITH_STORES = (ModuleStoreEnum.Type.split, )
data = "Some data" ItemFactory.create( parent_location=self.library.location, category="html", display_name="Html Content 3", data=data, modulestore=store, publish_item=False, )
self.delete_item(store, self.html_unit1.location) self.reindex_library(store) response = self.search() self.assertEqual(response["total"], 1)
with translation.override("fr"):
with wrap_ugettext_with_xyz(french_translation): self.assertEqual(i18n_service.ugettext(self.test_language), 'XYZ dummy language')
self.assertEqual(i18n_service.ugettext(self.test_language), 'dummy language')
self.user = User.objects.create_user(self.uname, self.email, self.password)
mongo_course1_id = self.import_and_populate_course()
mongo_course2_id = mongo_course1_id
course = CourseFactory.create( org=org, number=course_number, run=course_run, display_name=display_name, default_store=ModuleStoreEnum.Type.split )
self.assertEqual(result.get(), "succeeded") rerun_state = CourseRerunState.objects.find_first(course_key=split_rerun_id) self.assertEqual(rerun_state.state, CourseRerunUIStateManager.State.SUCCEEDED)
self._bind_module(lc_block) self.assertEqual(len(lc_block.children), num_to_create) self.assertEqual(len(lc_block.get_child_descriptors()), num_expected)
self._bind_module(lc_block) chosen_child = get_child_of_lc_block(lc_block) chosen_child_defn_id = chosen_child.definition_locator.definition_id lc_block.save()
lc_block = self._refresh_children(lc_block) check()
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) course_block = modulestore().get_item(lc_block.children[0])
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 1)
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 1)
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 2)
with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
lc_block = self._add_library_content_block(course, self.lib_key) lc_block = self._refresh_children(lc_block) self.assertEqual(len(lc_block.children), 0)
self.client.logout()
library2_key = self._create_library(library="lib2") self._login_as_non_staff_user()
access_role(library2_key).add_users(self.non_staff_user)
lib_key_pacific = self._create_library(org="PacificX", library="libP") lib_key_atlantic = self._create_library(org="AtlanticX", library="libA")
self._login_as_non_staff_user()
org_access_role(lib_key_pacific.org).add_users(self.non_staff_user)
block = self._add_simple_content_block()
self._login_as_non_staff_user() self.assertFalse(self._can_access_library(self.library))
if use_org_level_role: OrgLibraryUserRole(self.lib_key.org).add_users(self.non_staff_user) else: LibraryUserRole(self.lib_key).add_users(self.non_staff_user)
block = self._add_simple_content_block() with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
if library_role: library_role(self.lib_key).add_users(self.non_staff_user) if course_role: course_role(course.location.course_key).add_users(self.non_staff_user)
self._add_simple_content_block() with modulestore().default_store(ModuleStoreEnum.Type.split): course = CourseFactory.create()
if library_role: library_role(self.lib_key).add_users(self.non_staff_user) if course_role: course_role(course.location.course_key).add_users(self.non_staff_user)
self.problem = ItemFactory.create( category="problem", parent_location=self.library.location,
self.library = modulestore().get_library(self.lib_key)
with modulestore().default_store(ModuleStoreEnum.Type.split): self.course = CourseFactory.create()
for field_name in ["display_name", "weight"]: self.problem_in_course.fields[field_name].delete_from(self.problem_in_course)
modulestore().update_item(self.problem_in_course, self.user.id) self.problem_in_course = modulestore().get_item(self.problem_in_course.location)
self.library = store.get_library(self.lib_key)
self.lc_block = store.get_item(self.lc_block.location) self.problem_in_course = store.get_item(self.problem_in_course.location)
self.assertEqual(len(self.library.children), 2)
with modulestore().default_store(ModuleStoreEnum.Type.mongo): self.course = CourseFactory.create()
self.lc_block = self._add_library_content_block(self.course, self.lib_key)
self.assertEquals(self.get_about_page_link(), "//localhost:8000/courses/mitX/101/test/about")
link = utils.get_lms_link_for_item(location, True) self.assertEquals( link, "//preview.localhost/courses/mitX/101/test/jump_to/i4x://mitX/101/vertical/contacting_us" )
location = course_key.make_usage_key('course', 'test') link = utils.get_lms_link_for_item(location) self.assertEquals(link, "//localhost:8000/courses/mitX/101/test/jump_to/i4x://mitX/101/course/test")
vertical.start = self.future modulestore().update_item(vertical, self.dummy_user)
staff_lock = self._create_xblock_with_start_date( name + "_locked", start_date, publish, visible_to_staff_only=True ) self.assertFalse(utils.is_currently_visible_to_students(staff_lock))
self.sequential.children = [self.vertical.location] self.sequential = self.store.update_item(self.sequential, ModuleStoreEnum.UserID.test)
self.set_group_access(self.vertical, {1: []}) self.set_group_access(self.html, {2: None})
self.set_group_access(self.vertical, {1: []}) self.set_group_access(self.problem, {2: [3, 4]})
self._set_group_access({0: [1]}) expected[0]["groups"][1]["selected"] = True self.assertEqual(self._get_partition_info(), expected)
self._set_group_access({0: [3]})
partitions = self._get_partition_info() self.assertEqual(len(partitions), 1) self.assertEqual(partitions[0]["scheme"], "cohort")
partitions = self._get_partition_info() self.assertEqual(len(partitions), 1) self.assertEqual(partitions[0]["scheme"], "verification")
course = self.store.get_course(self.store.make_course_key( 'test_org', 'import_draft_order', 'import_draft_order' )) self.assertIsNotNone(course)
assets, count = content_store.get_all_content_for_course(course.id) self.assertEqual(count, 2)
for asset in assets: self.assertEquals(asset['displayname'], expected_displayname)
root_dir = path(mkdtemp_clean()) print 'Exporting to tempdir = {0}'.format(root_dir) export_course_to_xml(self.store, content_store, course.id, root_dir, 'test_export')
self.assertEqual(len(exported_static_files), 1) self.assertTrue(filesystem.exists(expected_displayname)) self.assertEqual(exported_static_files[0], expected_displayname)
shutil.rmtree(root_dir)
effort = self.store.get_item(course_key.make_usage_key('about', 'end_date')) self.assertEqual(effort.data, 'TBD')
all_assets, __ = content_store.get_all_content_for_course(course.id) self.assertGreater(len(all_assets), 0)
all_thumbnails = content_store.get_all_content_thumbnails_for_course(course.id) self.assertGreater(len(all_thumbnails), 0)
with filesystem.open('updates.html', 'r') as course_policy: on_disk = course_policy.read() self.assertEqual(course_updates.data, on_disk)
root_dir = path(mkdtemp_clean()) print 'Exporting to tempdir = {0}'.format(root_dir) export_course_to_xml(self.store, content_store, course.id, root_dir, 'test_export')
with filesystem.open('updates.html', 'r') as grading_policy: on_disk = grading_policy.read() self.assertEqual(on_disk, course_updates.data)
html_module_location = course_key.make_usage_key('html', 'nonportable_link') html_module = self.store.get_item(html_module_location) self.assertIn('/jump_to_id/nonportable_link', html_module.data)
export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_export')
self.verify_content_existence(self.store, root_dir, course_id, 'tabs', 'static_tab', '.html')
self.verify_content_existence(self.store, root_dir, course_id, 'about', 'about', '.html')
filesystem = OSFS(root_dir / 'test_export/policies/2012_Fall') self.assertTrue(filesystem.exists('grading_policy.json'))
with filesystem.open('grading_policy.json', 'r') as grading_policy: on_disk = loads(grading_policy.read()) self.assertEqual(on_disk, course.grading_policy)
self.assertTrue(filesystem.exists('policy.json'))
self.store.delete_course(course_id, self.user.id)
self.check_import(root_dir, content_store, course_id)
new_course_id = self.store.make_course_key('anotherX', 'anotherToy', 'Someday') self.check_import(root_dir, content_store, new_course_id) self.assertCoursesEqual(course_id, new_course_id)
import_course_from_xml( self.store, self.user.id, root_dir, ['test_export'], static_content_store=content_store, target_id=course_id, )
self.check_populated_course(course_id)
verticals = self.store.get_items(course_id, qualifiers={'category': 'vertical'})
export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_export')
export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_export')
word_cloud = ItemFactory.create(parent_location=parent.location, category="word_cloud", display_name="untitled") del word_cloud.data self.assertEquals(word_cloud.data, '')
root_dir = path(mkdtemp_clean()) export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_roundtrip')
import_course_from_xml(self.store, self.user.id, root_dir) imported_word_cloud = self.store.get_item(course_id.make_usage_key('word_cloud', 'untitled'))
self.assertEquals(imported_word_cloud.data, '')
root_dir = path(mkdtemp_clean()) export_course_to_xml(self.store, content_store, course_id, root_dir, 'test_roundtrip')
import_course_from_xml(self.store, self.user.id, root_dir, create_if_not_present=True)
open_assessment = ItemFactory.create( parent_location=vertical.location, category="openassessment", display_name="untitled", ) draft_open_assessment = self.store.convert_to_draft( open_assessment.location, self.user.id )
self.assertFalse(hasattr(draft_open_assessment, "xml_attributes"))
root_dir = path(mkdtemp_clean()) export_course_to_xml( self.store, content_store, course_id, root_dir, 'test_no_xml_attributes' )
resp = self.client.get_html(get_url('container_handler', self.vert_loc)) self.assertEqual(resp.status_code, 200)
self.assertNotIn(malicious_code, resp.content)
self.check_components_on_page( ADVANCED_COMPONENT_TYPES, ['Word cloud', 'Annotation', 'Text Annotation', 'Video Annotation', 'Image Annotation', 'split_test'], )
assets, count = content_store.get_all_content_for_course(self.course.id) self.assertEqual(count, 1) display_name = assets[0]['displayname'] self.assertEqual(display_name, invalid_displayname)
self.assertTrue(filesystem.exists(exported_asset_name)) self.assertEqual(len(exported_static_files), 1)
shutil.rmtree(root_dir)
assets, count = content_store.get_all_content_for_course(self.course.id) self.assertEqual(count, 2)
for asset in assets: self.assertEquals(asset['displayname'], asset_displayname)
filesystem = OSFS(root_dir / 'test_export/static') exported_static_files = filesystem.listdir() self.assertTrue(filesystem.exists(asset_displayname)) self.assertEqual(len(exported_static_files), 1)
shutil.rmtree(root_dir)
usage_key = self.course.id.make_usage_key('vertical', None)
problem = self.store.get_item(problem.location)
self.store.publish(problem.location, self.user.id)
problem = self.store.get_item(problem.location)
self.store.convert_to_draft(problem.location, self.user.id) problem = self.store.get_item(problem.location)
problem.save() self.assertIn('graceperiod', own_metadata(problem)) self.assertEqual(problem.graceperiod, new_graceperiod)
problem = self.store.get_item(problem.location)
self.store.publish(problem.location, self.user.id)
self.store.convert_to_draft(problem.location, self.user.id) problem = self.store.get_item(problem.location)
num_drafts = self._get_draft_counts(self.course) self.assertEqual(num_drafts, 0)
self.store.convert_to_draft(self.problem.location, self.user.id)
draft_problem = self.store.get_item(self.problem.location) self.assertTrue(getattr(draft_problem, 'is_draft', False))
course = self.store.get_course(self.course.id, depth=None)
num_drafts = self._get_draft_counts(course) self.assertEqual(num_drafts, 1)
self.assertGreater(len(items[0].question), 0)
resp = self.client.get_json( get_url('xblock_view_handler', self.vert_loc, kwargs={'view_name': 'container_preview'}) ) self.assertEqual(resp.status_code, 200)
chapter = self.store.get_item(self.chapter_loc) self.assertIn(self.seq_loc, chapter.children)
self.assertNotIn(self.seq_loc, chapter.children)
content = contentstore().find(asset_key, throw_on_not_found=False) self.assertIsNone(content)
content = contentstore('trashcan').find(asset_key, throw_on_not_found=False) self.assertIsNotNone(content)
restore_asset_from_trashcan(unicode(asset_key))
content = contentstore('trashcan').find(asset_key, throw_on_not_found=False) self.assertIsNotNone(content)
all_assets, __ = contentstore('trashcan').get_all_content_for_course(self.course.id) self.assertGreater(len(all_assets), 0)
empty_asset_trashcan([self.course.id])
all_assets, count = contentstore('trashcan').get_all_content_for_course(self.course.id) self.assertEqual(len(all_assets), 0) self.assertEqual(count, 0)
with self.assertRaises(InvalidVersionError): self.store.convert_to_draft(self.chapter_loc, self.user.id)
with self.store.default_store(ModuleStoreEnum.Type.split): resp = self.client.get_html('/c4x/InvalidOrg/InvalidCourse/asset/invalid.png') self.assertEqual(resp.status_code, 404)
self.store.delete_course(self.course.id, self.user.id)
items = self.store.get_items(self.course.id) self.assertEqual(len(items), 0)
assets, count = contentstore().get_all_content_for_course(self.course.id) self.assertEqual(len(assets), 0) self.assertEqual(count, 0)
resp = self.client.get(get_url('xblock_handler', handouts.location))
self.assertIn(self.seq_loc, course.system.module_data)
self.assertNotIn(self.vert_loc, course.system.module_data)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, course_key)) return test_course_data
with self.assertRaises(ItemNotFoundError): are_permissions_roles_seeded(course_id)
course_id = _get_course_id(self.store, test_course_data) delete_course_and_groups(course_id, self.user.id) with self.assertRaises(ItemNotFoundError): are_permissions_roles_seeded(course_id)
self.assertTrue(are_permissions_roles_seeded(second_course_id))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, course_id)) self.assertTrue(self.user.roles.filter(name="Student", course_id=course_id))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, course_id)) self.assertTrue(self.user.roles.filter(name="Student", course_id=course_id))
instructor_role = CourseInstructorRole(course_id)
delete_course_and_groups(course_id, self.user.id)
self.user = User.objects.get_by_natural_key(self.user.natural_key()[0])
pass
self.assertEqual(initially_enrolled, CourseEnrollment.is_enrolled(self.user, course_id))
resp = self.client.get_html( get_url(handler, course_key, 'course_key_string') ) self.assertEqual(resp.status_code, 200)
delete_item(category='html', name='test_html')
delete_item(category='vertical', name='test_vertical')
delete_item(category='sequential', name='test_sequence')
delete_item(category='chapter', name='chapter_2')
self.assertGreater(len(modules), 10)
course_module = self.store.get_course(target_id)
self.assertTrue(did_load_item)
discussion_item = self.store.create_item(self.user.id, course.id, 'discussion', 'new_component')
fetched = self.store.get_item(discussion_item.location)
refetched = self.store.get_item(discussion_item.location)
self.assertEqual(fetched.discussion_id, discussion_item.discussion_id) self.assertEqual(fetched.discussion_id, refetched.discussion_id)
self.assertNotEqual(discussion_item.discussion_id, '$$GUID$$')
for vertical in verticals: self.assertEqual(course.xqa_key, vertical.xqa_key) self.assertEqual(course.start, vertical.start)
parent = verticals[0] new_block = self.store.create_child( self.user.id, parent.location, 'html', 'new_component' )
new_block = self.store.get_item(new_block.location)
self.assertEqual(parent.graceperiod, new_block.graceperiod) self.assertEqual(parent.start, new_block.start) self.assertEqual(course.start, new_block.start)
new_block.graceperiod = timedelta(1) self.store.update_item(new_block, self.user.id)
new_block = self.store.get_item(new_block.location)
courses = import_course_from_xml( self.store, self.user.id, TEST_DATA_DIR, ['conditional_and_poll'], static_content_store=content_store, create_if_not_present=True )
self.assertEqual(course.course_image, 'images_course_image.jpg')
asset_key = course.id.make_asset_key('asset', course.course_image) content_store.find(asset_key)
rerun_course_data = {'source_course_key': unicode(source_course_key)} if not destination_course_data: destination_course_data = self.destination_course_data rerun_course_data.update(destination_course_data) destination_course_key = _get_course_id(self.store, destination_course_data)
course_url = get_url('course_handler', destination_course_key, 'course_key_string') response = self.client.ajax_post(course_url, rerun_course_data)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, destination_course_key))
self.assertInCourseListing(source_course_key) self.assertInCourseListing(destination_course_key)
source_videos = list(get_videos_for_course(source_course.id)) target_videos = list(get_videos_for_course(destination_course_key)) self.assertEqual(1, len(source_videos)) self.assertEqual(source_videos, target_videos)
self.assertFalse(CourseEnrollment.is_enrolled(self.user, non_existent_course_key))
self.assertInCourseListing(existent_course_key)
self.assertInUnsucceededCourseActions(destination_course_key)
with self.assertRaises(CourseActionStateItemNotFoundError): CourseRerunState.objects.find_first(course_key=destination_course_key)
self.assertInCourseListing(existent_course_key)
self.assertEquals(source_course.wiki_slug, source_wiki_slug)
self.assertEquals(destination_course.wiki_slug, destination_wiki_slug)
self._test_page("/logout", 302)
test_course = self.store.get_course(test_course.id.version_agnostic()) self.assertIn(test_chapter.location, test_course.children)
class DraftReorderTestCase(ModuleStoreTestCase):
exam_review_policy = get_review_policy_by_exam_id(exam['id']) self.assertEqual(exam_review_policy['review_policy'], sequence.exam_review_rules)
self.assertEqual(exam['hide_after_due'], sequence.hide_after_due)
sequence.default_time_limit_minutes += sequence.default_time_limit_minutes self.store.update_item(sequence, self.user.id)
listen_for_course_publish(self, self.course.id)
self._verify_exam_data(sequence, expected_active)
listen_for_course_publish(self, self.course.id)
exams = get_all_exams_for_course(unicode(self.course.id)) self.assertEqual(len(exams), 1)
exams = get_all_exams_for_course(unicode(self.course.id)) self.assertEqual(len(exams), expected_count)
self.user = UserFactory(is_staff=True) self.client = AjaxEnabledTestClient() self.client.login(username=self.user.username, password='test')
self.course_key = self.store.make_course_key('Org_1', 'Course_1', 'Run_1') self._create_course_with_given_location(self.course_key)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key)) self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
delete_course_and_groups(self.course_key, self.user.id) resp = self._create_course_with_given_location(self.course_key) self.assertEqual(resp.status_code, 200)
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key))
self.assertTrue(self.user.roles.filter(name="Student", course_id=self.course_key))
self.assertTrue(CourseEnrollment.is_enrolled(self.user, self.course_key)) delete_course_and_groups(self.course_key, self.user.id)
new_course_key = self.course_key.replace(course=self.course_key.course.upper()) resp = self._create_course_with_given_location(new_course_key) self.assertEqual(resp.status_code, 200)
self.assertTrue( self.user.roles.filter(name="Student", course_id=new_course_key) )
private_vertical = self.store.create_item(self.user.id, course_id, 'vertical', self.PRIVATE_VERTICAL) self.assertFalse(self.store.has_published_version(private_vertical))
content_store.set_attr(self.LOCKED_ASSET_KEY, 'locked', True)
vertical = get_and_verify_publish_state('vertical', self.TEST_VERTICAL, True) for child in vertical.get_children(): verify_item_publish_state(child, True)
self.assertTrue(getattr(vertical, "is_draft", False))
sequential = get_and_verify_publish_state('sequential', self.SEQUENTIAL, True) self.assertFalse(getattr(sequential, "is_draft", False))
private_vertical = get_and_verify_publish_state('vertical', self.PRIVATE_VERTICAL, False)
public_vertical = get_and_verify_publish_state('vertical', self.PUBLISHED_VERTICAL, True)
draft_html = self.store.get_item(course_id.make_usage_key('html', self.DRAFT_HTML)) self.assertTrue(getattr(draft_html, 'is_draft', False))
draft_video = self.store.get_item(course_id.make_usage_key('video', self.DRAFT_VIDEO)) self.assertTrue(getattr(draft_video, 'is_draft', False))
for vert in [vertical, private_vertical, public_vertical]: self.assertIn(vert.location, sequential.children)
self.assertIn(draft_html.location, public_vertical.children)
self.assertIn(draft_video.location, public_vertical.children)
course = self.store.get_course(course_id) self.assertGreater(len(course.textbooks), 0)
self.assertAssetsEqual(self.LOCKED_ASSET_KEY, self.LOCKED_ASSET_KEY.course_key, course_id)
html_module = self.store.get_item(course_id.make_usage_key('html', 'nonportable')) self.assertIn('/static/foo.jpg', html_module.data)
self.assertEqual( self.store.has_published_version(course1_item), self.store.has_published_version(course2_item) )
self.assertEqual(own_metadata(course1_item), own_metadata(course2_item))
self.assertEqual([], course_detail_json['pre_requisite_courses'])
resp = self.client.get_json(url) course_detail_json = json.loads(resp.content) self.assertEqual(pre_requisite_course_keys, course_detail_json['pre_requisite_courses'])
self.assertEquals(course.entrance_exam_minimum_score_pct, .5)
altered_grader = CourseGradingModel.fetch(self.course.id) self.assertDictEqual(test_grader.grade_cutoffs, altered_grader.grade_cutoffs, "Noop update")
altered_grader = CourseGradingModel.fetch(self.course.id) self.assertEqual(test_grader.grace_period, altered_grader.grace_period, "Noop update")
descriptor = modulestore().get_item(self.course.location) section_grader_type = CourseGradingModel.get_section_grader_type(self.course.location)
self.assertGreater(len(sections), 0, "No sections found")
self.assertFalse(is_valid) self.assertEqual(len(errors), 3) self.assertFalse(test_model)
fresh = modulestore().get_course(self.course.id) test_model = CourseMetadata.fetch(fresh)
self.assertNotIn(self.notes_tab, self.course.tabs)
course = modulestore().get_course(self.course.id) self.assertNotIn("notes", course.advanced_modules)
if world.is_css_present('div#login_error'): assert_false(world.css_visible('div#login_error'))
files = files_string.split(",") upload_css = 'a.upload-button' world.css_click(upload_css)
assert world.is_css_not_present(ASSET_NAMES_CSS)
_write_test_file(file_name, "This is an arbitrary file for testing uploads")
with open(os.path.abspath(path), 'w') as cur_file: cur_file.write(text)
world.browser.driver.get(url) assert_equal(world.css_text('body'), expected_text)
try: ranges.last.value = 'Failure' except InvalidElementStateException:
ranges = world.css_find(range_css) assert_equal(len(ranges), 2) assert_not_equal(ranges.last.value, 'Failure')
ele.value = grace_period
world.wait_for( lambda _: world.css_has_value(grace_period_css, grace_period) )
assert_true(world.css_contains_text(problem_css, category))
if world.is_css_present('{}.is-shown'.format(saving_mini_css)): world.css_find('{}.is-hiding'.format(saving_mini_css))
label = world.css_html(".level-element>header>div>div>span.xblock-display-name") assert_equal(display_name, label)
_verify_page_names('First', 'Empty')
draggables = world.css_find(css_class + ' .drag-handle') source = draggables.first target = draggables.last
world.visit('/') signin_css = 'a.action-signin' assert world.is_css_present(signin_css)
world.visit('/') assert_in(uname, world.css_text('span.account-username', timeout=10))
world.visit('/') course_link_css = 'a.course-link' world.css_click(course_link_css) course_title_css = 'span.course-title' assert_true(world.is_css_present(course_title_css))
if key is not None:
attach_file(filename, sub_path) modal_css = 'div.wrapper-modal-window-assetupload' button_css = '{} .action-upload'.format(modal_css) world.css_click(button_css)
world.wait_for_ajax_complete()
assert world.is_css_not_present(modal_css, wait_time=10)
roles = (CourseStaffRole, CourseInstructorRole)
index = world.get_setting_entry_index(DISPLAY_NAME) world.set_field_value(index, '3.4') verify_modified_display_name()
world.click_course_content() outline_css = 'li.nav-course-courseware-outline a' world.css_click(outline_css)
DELAY = 0.5
video_url = world.browser.url
_step.given('I have uploaded subtitles "{}"'.format(sub_id))
world.visit(video_url)
_step.given('I edit the component') world.wait_for_ajax_complete() _step.given('I save changes')
DELAY = 0.5
world.trigger_event(SELECTORS['url_inputs'], event='input', index=index)
DEPRECATED_SETTINGS = ["CSS Class for Course Reruns", "Hide Progress Tab", "XQA Key"]
assert_policy_entries( [ADVANCED_MODULES_KEY, DISPLAY_NAME_KEY, "Show Calculator"], ["[]", DISPLAY_NAME_VALUE, "false"])
key = world.css_value(KEY_CSS, index=i) if key == expected_key: return i
verify_date_or_time(COURSE_START_DATE_CSS, '12/20/2013') verify_date_or_time(COURSE_START_TIME_CSS, DUMMY_TIME)
verify_date_or_time(COURSE_START_TIME_CSS, DUMMY_TIME)
e._element.send_keys(Keys.ENTER)
assert_true(world.css_has_value(css, date_or_time))
verify_date_or_time(COURSE_END_TIME_CSS, DEFAULT_TIME) verify_date_or_time(ENROLLMENT_START_TIME_CSS, DEFAULT_TIME) verify_date_or_time(ENROLLMENT_END_TIME_CSS, DUMMY_TIME)
assert_equal('Paragraph', dropdowns[0].text) assert_equal('Font Family', dropdowns[1].text)
world.css_click(".mce-i-none")
assert_false(world.css_has_class('.CodeMirror', 'is-inactive')) assert_true(world.is_css_not_present('.tiny-mce')) type_in_codemirror(0, text)
world.css_click(button_class) perform_action_in_plugin(action)
buttons = world.css_find('div.mce-widget>button')
world.wait_for_visible('.mce-window')
action()
world.css_click('.mce-primary')
module_count_before = len(world.browser.find_by_css(module_css))
world.disable_jquery_animations() world.css_click(component_button_css)
tab2_css = 'div.ui-tabs-panel#tab2' world.wait_for_visible(tab2_css)
buttons = world.css_find('div.new-component-{} button'.format(category))
matched_buttons = [btn for btn in buttons if btn.text == component_type]
assert_equal(len(matched_buttons), 1) return matched_buttons[0]
world.retry_on_exception( _click_advanced, ignored_exceptions=AssertionError, )
link = world.retry_on_exception( lambda: _find_matching_button(category, component_type), ignored_exceptions=AssertionError )
world.retry_on_exception(lambda: link.click())
settings_button = world.browser.find_by_css('.settings-button') if len(settings_button) > 0: world.css_click('.settings-button')
reload_the_page(step) edit_component_and_select_settings()
reload_the_page(step)
CourseInstructorRole(course_key).add_users(new_instructor) auth.add_users(requesting_user, CourseStaffRole(course_key), new_instructor)
seed_permissions_roles(course_key)
CourseEnrollment.enroll(user_who_created_course, course_key)
assign_default_role(course_key, user_who_created_course)
try: remove_all_instructors(course_key) except Exception as err: log.error("Error in deleting course groups for {0}: {1}".format(course_key, err))
about_base = marketing_urls.get('ROOT', None)
about_base = re.sub(r"^https?://", "", about_base)
except ItemNotFoundError: return False
if published.visible_to_staff_only: return False
if 'detached' not in published._class_tags and published.start is not None: return datetime.now(UTC) > published.start
return True
if xblock.category == 'chapter': return xblock
if not parent_location: return xblock
if xblock.fields['visible_to_staff_only'].is_set_on(xblock): return xblock
if xblock.category == 'chapter': return None
if not parent_location: return None
if p.active and p.groups and (schemes is None or p.scheme.name in schemes):
groups = [] for g in p.groups:
partitions.append({ "id": p.id, "name": p.name, "scheme": p.scheme.name, "groups": groups, })
for p in user_partitions: has_selected = any(g["selected"] for g in p["groups"]) has_selected_groups = has_selected_groups or has_selected
from edxval.api import copy_course_videos
source_course_key = CourseKey.from_string(source_course_key_string) destination_course_key = CourseKey.from_string(destination_course_key_string) fields = deserialize_fields(fields) if fields else None
store = modulestore() with store.default_store('split'): store.clone_course(source_course_key, destination_course_key, user_id, fields=fields)
initialize_permissions(destination_course_key, User.objects.get(id=user_id))
CourseRerunState.objects.succeeded(course_key=destination_course_key)
copy_course_videos(source_course_key, destination_course_key)
CourseRerunState.objects.failed(course_key=destination_course_key) logging.exception(u'Course Rerun Error') return "duplicate course"
CourseRerunState.objects.failed(course_key=destination_course_key) logging.exception(u'Course Rerun Error')
modulestore().delete_course(destination_course_key, user_id)
pass
time_isoformat.split('+')[0], "%Y-%m-%dT%H:%M:%S.%f"
indexed_count = { "count": 0 }
items_index = []
if not item_index_dictionary and not item.has_children: return
cls.supplemental_index_information(modulestore, structure)
for item in structure.get_children(): prepare_item_index(item, groups_usage_info=groups_usage_info) searcher.index(cls.DOCUMENT_TYPE, items_index) cls.remove_deleted_items(searcher, structure_key, indexed_items)
log.exception( "Indexing error encountered, courseware index may be out of date %s - %r", structure_key, err ) error_list.append(_('General indexing error occurred'))
FROM_ABOUT_INFO = from_about_dictionary FROM_COURSE_PROPERTY = from_course_property FROM_COURSE_MODE = from_course_mode
about_dictionary = { item.location.name: item.data for item in modulestore.get_items(course.id, qualifiers={"category": "about"}) }
try: section_content = about_information.get_value(**about_context)
try: searcher.index(cls.DISCOVERY_DOCUMENT_TYPE, [course_info])
CONFIG_FILE = open(settings.REPO_ROOT / "docs" / "cms_config.ini") CONFIG = ConfigParser.ConfigParser() CONFIG.readfp(CONFIG_FILE)
from safe_lxml import defuse_xml_libs defuse_xml_libs()
import contracts contracts.disable_all()
from django.core.wsgi import get_wsgi_application application = get_wsgi_application()
call_command('flush', verbosity=0, interactive=False, load_initial_data=False)
cls.clear_caches()
cls.clear_caches()
for cache in settings.CACHES: caches[cache].clear()
sites.models.SITE_CACHE.clear()
def default(self, noDefaultEncodingObj): return noDefaultEncodingObj.value.replace("<script>", "sample-encoder-was-here")
for gen in xrange(3): gc.collect(gen) scanner.dump_all_objects( format_str.format("gc-gen-{}".format(gen)) )
if not hasattr(request, '_xblock_token'): request._xblock_token = uuid.uuid1().get_hex()
class_name = getattr(block, 'unmixed_class', block.__class__).__name__
css_classes.append('xmodule_display')
css_classes.append('xmodule_edit')
template_context['js_init_parameters'] = json.dumps(frag.json_init_args).replace("/", r"\/")
template_context['js_init_parameters'] = json.dumps(frag.json_init_args).replace("/", r"\/")
cursor.execute(query, [module_id.to_deprecated_string()])
edit_link = "//" + settings.CMS_BASE + '/container/' + unicode(block.location)
return wrap_fragment( frag, render_to_string( "edit_unit_link.html", {'frag_content': frag.content, 'edit_link': edit_link} ) )
filepath = filename
giturl = "" data_dir = ""
content = html_parsed[0].tail
return list(reversed(course_updates.items))
get_parents=None, get_children=get_children, filter_func=filter_func,
filter_func = filter_func or (lambda __: True)
stack = deque([_Node(start_node, get_children)])
visited = set()
current = stack[-1]
if current.node in visited or not filter_func(current.node): stack.pop() continue
try: next_child = current.children.next()
yield current.node visited.add(current.node) stack.pop()
stack.append(_Node(next_child, get_children))
filter_func = filter_func or (lambda __: True)
stack = deque([start_node])
while stack:
current_node = stack.pop()
if get_parents and current_node != start_node: parents = get_parents(current_node)
if not all(parent in yield_results for parent in parents): continue
elif not yield_descendants_of_unyielded and not any(yield_results[parent] for parent in parents): continue
if current_node not in yield_results:
unvisited_children = list(get_children(current_node))
unvisited_children = list( child for child in get_children(current_node) if child not in yield_results )
unvisited_children.reverse() stack.extend(unvisited_children)
should_yield_node = filter_func(current_node) if should_yield_node: yield current_node
yield_results[current_node] = should_yield_node
full_name = UserProfile.objects.get(user=user).name
full_name = UserProfile.objects.get(user=user).name
if local_loglevel not in LOG_LEVELS: local_loglevel = 'INFO'
service_variant = ''
url = settings.STATIC_URL + settings.DEFAULT_COURSE_ABOUT_IMAGE_URL
print public_key_str print private_key_str
try: mod = import_module(app + '.startup') except ImportError: continue
if hasattr(mod, 'run'): mod.run()
(plaintext, err_from_stderr) = process.communicate( input=html_message.encode('utf-8') )
COURSE_TAB_NAMESPACE = 'openedx.course_tab'
self.prefix = os.path.join(self.RESOURCE_PREFIX, module)
try: try: return super(PutAsCreateMixin, self).update(request, *args, **kwargs) except Http404: return super(PutAsCreateMixin, self).create(request, *args, **kwargs)
except ValidationError as err: return Response(err.messages, status=status.HTTP_400_BAD_REQUEST)
self.check_permissions(clone_request(self.request, 'POST'))
raise
object_results = map(ordered_objects, search_queryset_pks) paged_results.object_list = object_results
if not user or user.is_anonymous(): return None
return (user, None)
raise
if not hasattr(cls, "_plugins"): plugins = {}
exc = drf_exceptions.AuthenticationFailed({u'error_code': -1}) self.assertEqual(exc.detail, u"{u'error_code': -1}")
self.user.is_active = False self.user.save()
self.assertNotIn('error_code', json.loads(response.content))
self.assertNotIn('error_code', json.loads(response.content))
if field.source is None: field.bind(self.field_name, self)
GATING_NAMESPACE_QUALIFIER = '.gating'
log.warning("Multiple gating milestones found for prereq UsageKey %s", prereq_content_key)
return [ m['content_id'] for m in find_gating_milestones( course.id, None, 'requires', {'id': user.id} ) ]
self.chapter1 = ItemFactory.create( parent_location=self.course.location, category='chapter', display_name='untitled chapter 1' )
STRING_PAYLOAD = 'string_payload'
ROOT_EXTRA_FIELDS = 'root_extra_fields'
CONTEXT_EXTRA_FIELDS = 'context_extra_fields'
PAYLOAD_EXTRA_FIELDS = 'payload_extra_fields'
return { cls.STRING_PAYLOAD, cls.ROOT_EXTRA_FIELDS, cls.CONTEXT_EXTRA_FIELDS, }
if EventMatchTolerates.STRING_PAYLOAD in tolerate: expected = parse_event_payload(expected) actual = parse_event_payload(actual)
self._assert_num_requests(1)
get_edx_api_data(program_config, self.user, 'programs', cache_key=cache_key) get_edx_api_data(program_config, self.user, 'programs', resource_id=resource_id, cache_key=cache_key)
actual_collection = get_edx_api_data(program_config, self.user, 'programs', cache_key=cache_key) self.assertEqual(actual_collection, expected_collection)
self._assert_num_requests(2)
test_uuid = uuid.UUID(token, version=1) self.assertEqual(token, test_uuid.hex)
tip = resolved(joinpath(base, dirname(info.name))) return _is_bad_path(info.linkname, base=tip)
if not base.startswith(resolved(settings.DATA_DIR)): raise SuspiciousOperation("Attempted to import course outside of data dir")
from logging import getLogger
timeout_in_seconds = 60 * 60 * 24 self._cache.set( self._encode_root_cache_key(block_structure.root_block_usage_key), zp_data_to_cache, timeout=timeout_in_seconds, )
block_relations, transformer_data, block_data_map = zunpickle(zp_data_from_cache) block_structure = BlockStructureModulestoreData(root_block_usage_key) block_structure._block_relations = block_relations block_structure._transformer_data = transformer_data block_structure._block_data_map = block_data_map
if xblock.location in blocks_visited: return
blocks_visited.add(xblock.location)
for child in xblock.get_children():
TRANSFORMER_VERSION_KEY = '_version'
self.parents = []
self.children = []
self.root_block_usage_key = root_block_usage_key
self._block_relations = defaultdict(_BlockRelations)
self._add_block(self._block_relations, root_block_usage_key)
pruned_block_relations = defaultdict(_BlockRelations) old_block_relations = self._block_relations
for block_key in self.post_order_traversal(): if block_key in old_block_relations: self._add_block(pruned_block_relations, block_key)
for child in old_block_relations[block_key].children: if child in pruned_block_relations: self._add_to_relations(pruned_block_relations, block_key, child)
self._block_relations = pruned_block_relations
self.xblock_fields = {}
self.transformer_data = defaultdict(dict)
self._block_data_map = defaultdict(_BlockData)
self._transformer_data = defaultdict(dict)
for child in children: self._block_relations[child].parents.remove(usage_key)
for parent in parents: self._block_relations[parent].children.remove(usage_key)
self._block_relations.pop(usage_key, None) self._block_data_map.pop(usage_key, None)
if keep_descendants: for child in children: for parent in parents: self._add_relation(parent, child)
self._xblock_map = {}
self._requested_xblock_fields = set()
from collections import namedtuple from copy import deepcopy import ddt import itertools from nose.plugins.attrib import attr from unittest import TestCase
for parent, children in enumerate(children_map): self.assertSetEqual(set(block_structure.get_children(parent)), set(children))
for child, parents in enumerate(self.get_parents_map(children_map)): self.assertSetEqual(set(block_structure.get_parents(child)), set(parents))
for node in range(len(children_map)): self.assertIn(node, block_structure) self.assertNotIn(len(children_map) + 1, block_structure)
block_structure = BlockStructureModulestoreData(root_block_usage_key=0)
block_structure = BlockStructureModulestoreData(root_block_usage_key=0) for block in blocks: block_structure._add_xblock(block.location, block)
fields = ["field1", "field2", "field3"] block_structure.request_xblock_fields(*fields)
for block in blocks: for field in fields: self.assertIsNone(block_structure.get_xblock_field(block.location, field))
block_structure._collect_requested_xblock_fields()
for block in blocks: for field in fields: self.assertEquals( block_structure.get_xblock_field(block.location, field), block.field_map.get(field), )
for child in children_map[block_to_remove]: for parent in parents_map[block_to_remove]: removed_children_map[parent].append(child)
for child in children_map[block_to_remove]: if pruned_parents_map[child]: continue for block in traverse_post_order(child, get_children=lambda block: pruned_children_map[block]): missing_blocks.append(block) pruned_children_map[block] = []
([], []),
([TestTransformer1()], []),
([TestTransformer1(), TestTransformer2()], []),
([UnregisteredTestTransformer3()], [UnregisteredTestTransformer3.name()]),
([TestTransformer1(), UnregisteredTestTransformer3()], [UnregisteredTestTransformer3.name()]),
self.map = {} self.set_call_count = 0 self.timeout_from_last_call = 0
return cls.__name__
SIMPLE_CHILDREN_MAP = [[1, 2], [3, 4], [], [], []]
LINEAR_CHILDREN_MAP = [[1], [2], [3], []]
DAG_CHILDREN_MAP = [[1, 2], [3], [3, 4], [5, 6], [], [], []]
block_structure = block_structure_cls(root_block_usage_key=0)
for parent, children in enumerate(children_map): for child in children:
self.assertEquals( block_key in block_structure, block_key not in missing_blocks, 'Expected presence in block_structure for block_key {} to match absence in missing_blocks.'.format( unicode(block_key) ), )
if block_key not in missing_blocks: self.assertEquals( set(block_structure.get_children(block_key)), set(children), )
discussion_id_map_json = CompressedTextField(verbose_name='Discussion ID Map JSON', blank=True, null=True)
result[discussion_id] = UsageKey.from_string(result[discussion_id]).map_into_course(self.course_id)
cur_block = unordered_structure[block]
cs = CourseStructure.objects.get(course_id=self.course.id) self.assertEqual(cs.structure_json, structure_json)
course_id = self.course.id self.assertRaises(ValueError, update_course_structure, course_id)
from .tasks import update_course_structure
try: structure = CourseStructure.objects.get(course_id=course_key) structure.discussion_id_map_json = None structure.save() except CourseStructure.DoesNotExist: pass
update_course_structure.apply_async([unicode(course_key)], countdown=0)
from __future__ import unicode_literals
return dict( super(GradingPolicySerializer, self).to_representation( defaultdict(lambda: None, obj) ) )
if obj.get("parent") is None: data["parent"] = None
data["children"] = obj["children"]
tasks.update_course_structure.delay(unicode(course_key)) raise CourseStructureNotAvailableError
SignalHandler.course_published.connect(listen_for_course_publish)
SignalHandler.course_published.disconnect(listen_for_course_publish)
blocks_stack.extend(children)
from .models import CourseStructure
if not isinstance(course_key, basestring): raise ValueError('course_key must be a string. {} is not acceptable.'.format(type(course_key)))
VERSION = 4
version = IntegerField()
start = DateTimeField(null=True) end = DateTimeField(null=True) advertised_start = TextField(null=True) announcement = DateTimeField(null=True)
course_image_url = TextField() social_sharing_url = TextField(null=True) end_of_course_survey_url = TextField(null=True)
certificates_display_behavior = TextField(null=True) certificates_show_before_end = BooleanField(default=False) cert_html_view_enabled = BooleanField(default=False) has_any_active_web_certificate = BooleanField(default=False) cert_name_short = TextField() cert_name_long = TextField()
lowest_passing_grade = DecimalField(max_digits=5, decimal_places=2, null=True)
days_early_for_beta = FloatField(null=True) mobile_available = BooleanField(default=False) visible_to_staff_only = BooleanField(default=False)
enrollment_start = DateTimeField(null=True) enrollment_end = DateTimeField(null=True) enrollment_domain = TextField(null=True) invitation_only = BooleanField(default=False) max_student_enrollments_allowed = IntegerField(null=True)
catalog_visibility = TextField(null=True) short_description = TextField(null=True) course_video_url = TextField(null=True) effort = TextField(null=True) self_paced = BooleanField(default=False)
course_overview.delete() course_overview = None
if course_overview and not hasattr(course_overview, 'image_set'): CourseOverviewImageSet.create_for_course(course_overview)
course_overviews = CourseOverview.objects.all()
course_overviews = course_overviews.filter(org__iexact=org)
for tab in tabs: if tab.tab_id == "discussion" and django_comment_client.utils.is_discussion_enabled(self.id): return True return False
raw_image_url = self.course_image_url
urls = { 'raw': raw_image_url, 'small': raw_image_url, 'large': raw_image_url, }
if not url: return url
if netloc: return url
config = CourseOverviewImageConfig.current() if not config.enabled: return
if not course: course = modulestore().get_course(course_overview.id)
small_width = models.IntegerField(default=375) small_height = models.IntegerField(default=200)
large_width = models.IntegerField(default=750) large_height = models.IntegerField(default=400)
course_about_accessor = lambda object, field_name: CourseDetails.fetch_about_attribute(object.id, field_name)
for course_overview in [course_overview_cache_miss, course_overview_cache_hit]: course_overview_tabs = course_overview.tabs.all() course_resp_tabs = {tab.tab_id for tab in course_overview_tabs} self.assertEqual(self.COURSE_OVERVIEW_TABS, course_resp_tabs)
course = CourseFactory.create(default_store=modulestore_type, run="TestRun", **course_kwargs) self.check_course_overview_against_course(course)
course = CourseFactory.create(mobile_available=True, default_store=modulestore_type) course_overview_1 = CourseOverview.get_from_id(course.id) self.assertTrue(course_overview_1.mobile_available)
course.mobile_available = False with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred): self.store.update_item(course, ModuleStoreEnum.UserID.test)
course_overview_2 = CourseOverview.get_from_id(course.id) self.assertFalse(course_overview_2.mobile_available)
with self.assertRaises(CourseOverview.DoesNotExist): self.store.delete_course(course.id, ModuleStoreEnum.UserID.test) CourseOverview.get_from_id(course.id)
course = CourseFactory.create(default_store=modulestore_type, emit_signals=True)
with check_mongo_calls(0): CourseOverview.get_from_id(course.id)
with self.assertRaises(IOError): CourseOverview.load_from_module_store(self.store.make_course_key('Non', 'Existent', 'Course'))
with check_mongo_calls_range(max_finds=max_mongo_calls, min_finds=min_mongo_calls): _course_overview_2 = CourseOverview.get_from_id(course.id)
with mock.patch( 'openedx.core.djangoapps.content.course_overviews.models.CourseOverview.objects.get' ) as mock_getter:
for _ in range(2): self.assertIsInstance(CourseOverview.get_from_id(course.id), CourseOverview)
overview_v10 = CourseOverview.get_from_id(course.id) self.assertEqual(overview_v10.version, 10)
overview_v10.version = 9 overview_v10.save()
updated_overview = CourseOverview.get_from_id(course.id) self.assertEqual(updated_overview.version, 10)
updated_overview.version = 11 updated_overview.save()
unmodified_overview = CourseOverview.get_from_id(course.id) self.assertEqual(unmodified_overview.version, 11)
self.assertEqual( {c.id for c in CourseOverview.get_all_courses(org='TEST_ORG_1')}, {c.id for c in org_courses[1]}, )
fallback_url = settings.STATIC_URL + settings.DEFAULT_COURSE_ABOUT_IMAGE_URL course_overview = self._assert_image_urls_all_default(modulestore_type, course_image, fallback_url)
self.assertTrue(hasattr(course_overview, 'image_set'))
self.set_config(enabled=False)
fake_course_image = 'sample_image.png' course_overview = self._assert_image_urls_all_default(modulestore_type, fake_course_image)
self.assertFalse(hasattr(course_overview, 'image_set'))
self.assertTrue(hasattr(course_overview_before, 'image_set'))
course_overview_before.image_set.small_url = broken_small_url course_overview_before.image_set.large_url = broken_large_url course_overview_before.image_set.save()
self.set_config(False)
course_overview_after = CourseOverview.get_from_id(course.id)
self.assertTrue(hasattr(course_overview_after, 'image_set')) image_set = course_overview_after.image_set self.assertEqual(image_set.small_url, broken_small_url) self.assertEqual(image_set.large_url, broken_large_url)
expected_url = course_image_url(course) self.assertEqual( course_overview_after.image_urls, { 'raw': expected_url, 'small': expected_url, 'large': expected_url } )
AssetBaseUrlConfig.objects.create(enabled=True, base_url='fakecdn.edx.org') expected_cdn_url = "//fakecdn.edx.org" + expected_path_start
AssetBaseUrlConfig.objects.create(enabled=True, base_url='fakecdn.edx.org') expected_cdn_url = "//fakecdn.edx.org"
fake_course_image = 'sample_image.png' patched_create_thumbnail.side_effect = Exception("Kaboom!")
course_overview = self._assert_image_urls_all_default(modulestore_type, fake_course_image)
patched_create_thumbnail.assert_called()
self.assertTrue(hasattr(course_overview, 'image_set')) self.assertEqual(course_overview.image_set.small_url, '') self.assertEqual(course_overview.image_set.large_url, '')
with mock.patch('openedx.core.lib.courses.create_course_image_thumbnail') as patched_create_thumbnail: course_overview = CourseOverview.get_from_id(course_overview.id) patched_create_thumbnail.assert_not_called()
course_image_asset_key = StaticContent.compute_location(course.id, course.course_image) course_image_content = StaticContent(course_image_asset_key, image_name, 'image/jpeg', image_buff) contentstore().save(course_image_content)
if create_after_overview: self.set_config(enabled=False)
course_overview = CourseOverview.get_from_id(course.id)
if create_after_overview: self.assertFalse(hasattr(course_overview, 'image_set')) self.set_config(enabled=True) course_overview = CourseOverview.get_from_id(course.id)
course_image_asset_key = StaticContent.compute_location(course.id, course.course_image) course_image_content = StaticContent(course_image_asset_key, image_name, 'image/png', image_buff) contentstore().save(course_image_content)
config = CourseOverviewImageConfig.current() course_overview = CourseOverview.get_from_id(course.id) image_urls = course_overview.image_urls
self.assertTrue(image_url.endswith('src_course_image-png-{}x{}.jpg'.format(*target)))
src_x, src_y = src_dimensions target_x, target_y = target image_x, image_y = image.size
self.set_config(False) course = CourseFactory.create()
overview = CourseOverview.get_from_id(course.id) self.assertFalse(hasattr(overview, 'image_set'))
CourseOverviewImageSet.objects.create(course_overview=overview)
self.set_config(True) CourseOverviewImageSet.create_for_course(overview) self.assertTrue(hasattr(overview, 'image_set'))
self.assertEqual( course_overview.image_urls, { 'raw': expected_url, 'small': expected_url, 'large': expected_url, } ) return course_overview
from cms.djangoapps.contentstore.courseware_index import CourseAboutSearchIndexer CourseAboutSearchIndexer.remove_deleted_items(course_key)
self._assert_courses_not_in_overview(self.course_key_1, self.course_key_2) self.command.handle(all=True)
self._assert_courses_in_overview(self.course_key_1, self.course_key_2)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
]
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.problem_section = ItemFactory.create(parent_location=chapter.location, category='sequential', metadata={'graded': True, 'format': 'Homework'}, display_name=self.TEST_SECTION_NAME)
problem_vertical = ItemFactory.create( parent_location=self.problem_section.location, category='vertical', display_name='Problem Unit' )
values = ','.join(values)
values = set(values.split(',')) if values else set()
def __unicode__(self): return u'SystemUser'
from . import signals
from openedx.core.djangoapps.programs.models import ProgramsApiConfig
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
ROUTING_KEY = getattr(settings, 'CREDENTIALS_GENERATION_ROUTING_KEY', None)
if not config.is_certification_enabled: LOGGER.warning( 'Task award_program_certificates cannot be executed when program certification is disabled in API config', ) raise self.retry(countdown=countdown, max_retries=config.max_retries)
return
existing_program_ids = get_awarded_certificate_programs(student)
raise self.retry(exc=exc, countdown=countdown, max_retries=config.max_retries)
LOGGER.exception('Failed to award certificate for program %s to user %s', program_id, username) retry = True
LOGGER.info('Retrying task to award failed certificates to user %s', username) raise self.retry(countdown=countdown, max_retries=config.max_retries)
self.create_programs_config(enable_certification=False)
GeneratedCertificateFactory( user=self.bob, course_id=self.alternate_course_id, mode=MODES.verified, status=failing_status, )
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
utils.get_programs(self.user)
utils.get_programs(self.user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
for _ in range(2): utils.get_programs(staff_user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 3)
enrollments = self._create_enrollments(solo_course_id, shared_course_id) meter = utils.ProgramProgressMeter(self.user, enrollments)
meter = utils.ProgramProgressMeter(self.user, []) self._assert_progress(meter)
cache_key = programs_config.CACHE_KEY if programs_config.is_cache_enabled and not user.is_staff else None return get_edx_api_data(programs_config, user, 'programs', resource_id=program_id, cache_key=cache_key)
self.course_ids = [unicode(e.course_id) for e in enrollments]
authentication_classes = []
email_label = _(u"Email")
email_placeholder = _(u"username@domain.com")
email_instructions = _("The email address you used to register with {platform_name}").format( platform_name=settings.PLATFORM_NAME )
password_label = _(u"Password")
from student.views import login_user return shim_student_view(login_user, check_logged_in=True)(request)
authentication_classes = []
self.field_handlers = {} for field_name in self.DEFAULT_FIELDS + self.EXTRA_FIELDS: handler = getattr(self, "_add_{field_name}_field".format(field_name=field_name)) self.field_handlers[field_name] = handler
for field_name in self.DEFAULT_FIELDS: self.field_handlers[field_name](form_desc, required=True)
custom_form = get_registration_extension_form()
for field_name in self.EXTRA_FIELDS: if self._is_field_visible(field_name): self.field_handlers[field_name]( form_desc, required=self._is_field_required(field_name) )
email_label = _(u"Email")
email_placeholder = _(u"username@domain.com")
name_label = _(u"Full name")
name_placeholder = _(u"Jane Doe")
name_instructions = _(u"Your legal name, used for any certificates you earn.")
username_label = _(u"Public username")
u"The name that will identify you in your courses - " u"{bold_start}(cannot be changed later){bold_end}"
username_placeholder = _(u"JaneDoe")
password_label = _(u"Password")
education_level_label = _(u"Highest level of education completed")
gender_label = _(u"Gender")
yob_label = _(u"Year of birth")
mailing_address_label = _(u"Mailing address")
goals_label = _(u"Tell us why you're interested in {platform_name}").format( platform_name=settings.PLATFORM_NAME )
city_label = _(u"City")
state_label = _(u"State/Province/Region")
company_label = _(u"Company")
title_label = _(u"Title")
first_name_label = _(u"First Name")
last_name_label = _(u"Last Name")
country_label = _(u"Country") error_msg = _(u"Please select your Country.")
if self._is_field_visible("terms_of_service"): terms_text = _(u"Honor Code")
else: terms_text = _(u"Terms of Service and Honor Code")
label = _(u"I agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
error_msg = _(u"You must agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
terms_text = _(u"Terms of Service") terms_link = u"<a href=\"{url}\">{terms_text}</a>".format( url=marketing_link("TOS"), terms_text=terms_text )
label = _(u"I agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
error_msg = _(u"You must agree to the {platform_name} {terms_of_service}.").format( platform_name=get_themed_value("PLATFORM_NAME", settings.PLATFORM_NAME), terms_of_service=terms_link )
field_overrides = current_provider.get_register_form_data( running_pipeline.get('kwargs') )
form_desc.override_field_properties( "password", default="", field_type="hidden", required=False, label="", instructions="", restrictions={} )
authentication_classes = []
email_label = _(u"Email")
email_placeholder = _(u"username@domain.com")
email_instructions = _(u"The email address you used to register with {platform_name}").format( platform_name=settings.PLATFORM_NAME )
email_opt_in = request.data['email_opt_in'].lower() == 'true' update_email_opt_in(request.user, org, email_opt_in) return HttpResponse(status=status.HTTP_200_OK)
NAME_MIN_LENGTH = 2 NAME_MAX_LENGTH = 255
USERNAME_MIN_LENGTH = 2 USERNAME_MAX_LENGTH = 30
EMAIL_MIN_LENGTH = 3 EMAIL_MAX_LENGTH = 254
PASSWORD_MIN_LENGTH = 2 PASSWORD_MAX_LENGTH = 75
ALL_USERS_VISIBILITY = 'all_users'
PRIVATE_VISIBILITY = 'private'
self.configuration = kwargs.pop('configuration', None) if not self.configuration: self.configuration = settings.ACCOUNT_VISIBILITY_CONFIGURATION
self.custom_fields = kwargs.pop('custom_fields', [])
read_only_fields = () explicit_read_only_fields = ("profile_image", "requires_parental_consent")
profile_privacy = UserPreference.get_value(user, ACCOUNT_VISIBILITY_PREF_KEY) return profile_privacy if profile_privacy else configuration.get('default_visibility')
urls = _get_default_profile_image_urls()
visible_fields = _visible_fields
changing_email = False if "email" in update: changing_email = True new_email = update["email"] del update["email"]
old_name = None if "name" in update: old_name = existing_user_profile.name
read_only_fields = set(update.keys()).intersection( AccountUserSerializer.get_read_only_fields() + AccountLegacyProfileSerializer.get_read_only_fields() )
field_errors = {}
if field_errors: raise AccountValidationError(field_errors)
if "language_proficiencies" in update: old_language_proficiencies = legacy_profile_serializer.data["language_proficiencies"]
if 'account_privacy' in update: update_user_preferences( requesting_user, {'account_privacy': update["account_privacy"]}, existing_user )
_validate_username(username) _validate_password(password, username) _validate_email(email)
user = User(username=username, email=email, is_active=False) user.set_password(password)
registration = Registration() registration.register(user)
UserProfile(user=user).save()
return registration.activation_key
registration.activate()
form = PasswordResetFormNoActive({'email': email})
if form.is_valid(): form.save( from_email=theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL), domain_override=orig_host, use_https=is_secure ) else: raise UserNotFound
self.user.profile.year_of_birth = 1980 self.user.profile.profile_image_uploaded_at = TEST_PROFILE_IMAGE_UPLOAD_DT self.user.profile.save()
account_settings = get_account_settings(self.default_request, self.different_user.username) self.assertNotIn("email", account_settings)
naughty_update = { "username": "not_allowed", "gender": "undecided", "email": "not an email address" }
account_settings = get_account_settings(self.default_request) self.assertEqual("Mickey Mouse", account_settings["name"])
pending_change = PendingEmailChange.objects.filter(user=self.user) self.assertEqual(0, len(pending_change))
create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
user = User.objects.get(username=self.USERNAME) request = RequestFactory().get("/api/user/v1/accounts/") request.user = user account_settings = get_account_settings(request)
self.assertIsNotNone(account_settings['date_joined']) del account_settings['date_joined']
u'{user}@example.com'.format( user=(u'e' * (EMAIL_MAX_LENGTH - 11)) )
activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL) user = User.objects.get(username=self.USERNAME)
activate_account(activation_key) account = get_account_settings(request) self.assertTrue(account['is_active'])
create_account(self.USERNAME, self.USERNAME, self.EMAIL)
activation_key = create_account(self.USERNAME, self.PASSWORD, self.EMAIL) activate_account(activation_key)
request_password_change(self.EMAIL, self.ORIG_HOST, self.IS_SECURE)
self.assertEqual(len(mail.outbox), 1)
email_body = mail.outbox[0].body result = re.search(r'(?P<url>https?://[^\s]+)', email_body) self.assertIsNot(result, None)
self.assertEqual(len(mail.outbox), 0)
create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
self.assertEqual(len(mail.outbox), 1)
TEST_PROFILE_IMAGE_BACKEND = deepcopy(settings.PROFILE_IMAGE_BACKEND) TEST_PROFILE_IMAGE_BACKEND['options']['base_url'] = '/profile-images/'
response = client.patch(self.url, data=json.dumps(json_data), content_type=content_type) self.assertEqual(expected_status, response.status_code) return response
set_user_preference(self.user, ACCOUNT_VISIBILITY_PREF_KEY, preference_visibility) self.create_mock_profile(self.user) response = self.send_get(client)
response = self.send_get(client, query_parameters='view=shared') verify_fields_visible_to_all_users(response)
self.assertEqual(False, data["accomplishments_shared"])
self.user.is_active = False self.user.save() verify_get_own_information(9)
legacy_profile = UserProfile.objects.get(id=self.user.id) legacy_profile.year_of_birth = 2000 legacy_profile.save()
response = self.send_patch(client, {field: ""}) self.assertEqual("", response.data[field])
response = self.send_get(client) self.assertEqual("m", response.data["gender"])
self.assertIsNone(response.data[field_name])
response = self.send_patch(self.client, {field_name: ""}) self.assertIsNone(response.data[field_name])
get_response = self.send_get(self.client) self.assertEqual(new_name, get_response.data["name"])
self.assertEqual(old_email, response.data["email"]) self.assertEqual("change my email", response.data["goals"])
response = self.send_get(client, query_parameters='view=shared') self._verify_private_account_response( response, requires_parental_consent=True, account_privacy=PRIVATE_VISIBILITY )
mock_email_change.side_effect = [ValueError, "mock value error thrown"] self.client.login(username=self.user.username, password=self.test_password) old_email = self.user.email
OUTPUT_FIELD_NAMES = [ "email", "full_name", "course_id", "is_opted_in_for_email", "preference_set_datetime" ]
QUERY_INTERVAL = 1000
DEFAULT_DATETIME_STR = datetime.datetime(year=2014, month=12, day=1).isoformat(' ')
courses = self._get_courses_for_org(org_list) only_courses = options.get("courses")
org_list = list(set(org_list) | set(course.org for course in courses))
if not courses: raise CommandError( u"No courses found for orgs: {orgs}".format( orgs=", ".join(org_list) ) )
LOGGER.info( u"Retrieving data for courses: {courses}".format( courses=", ".join([unicode(course) for course in courses]) ) )
with open(file_path, "w") as file_handle: with self._log_execution_time(): self._write_email_opt_in_prefs(file_handle, org_list, courses)
LOGGER.info(u"Output file: {file_path}".format(file_path=file_path))
LOGGER.info(u"Retrieved {num_rows} records.".format(num_rows=row_count))
db_alias = ( 'read_replica' if 'read_replica' in settings.DATABASES else 'default' ) return connections[db_alias].cursor()
self._assert_output(output)
self._assert_output(output, (self.user, self.courses[0].id, True))
self._create_courses_and_enrollments( (self.TEST_ORG, True), ("other_org", True) )
self._set_opt_in_pref(self.user, "other_org", False)
output = self._run_command(self.TEST_ORG) self._assert_output( output, (self.user, self.courses[0].id, True), expect_pref_datetime=False )
self._create_courses_and_enrollments( (self.TEST_ORG, True), ("org_alias", True) )
self._set_opt_in_pref(self.user, self.TEST_ORG, True) self._set_opt_in_pref(self.user, "org_alias", False)
self._create_courses_and_enrollments((self.TEST_ORG, True)) self._set_opt_in_pref(self.user, self.TEST_ORG, opt_in_pref)
CourseEnrollment.unenroll(self.user, self.courses[0].id, skip_refund=True)
output = self._run_command(self.TEST_ORG) self._assert_output(output, (self.user, self.courses[0].id, opt_in_pref))
self._create_courses_and_enrollments( (self.TEST_ORG, True), (self.TEST_ORG, True), (self.TEST_ORG, True), ("org_alias", True) )
self._set_opt_in_pref(self.user, "org_alias", False)
CourseEnrollment.unenroll(self.user, self.courses[3].id, skip_refund=True)
with self.assertRaisesRegexp(CommandError, "^No courses found for orgs:"): self._run_command("other_org")
self._create_courses_and_enrollments( (self.TEST_ORG, True), (self.TEST_ORG, True), (self.TEST_ORG, True), )
only_courses = [self.courses[0].id, self.courses[1].id] self._run_command(self.TEST_ORG, only_courses=only_courses)
output = self._run_command(self.TEST_ORG, query_interval=4)
output_emails = [row["email"] for row in output] for email in output_emails: self.assertIn(email, output_emails)
self._create_courses_and_enrollments( ("MyOrg", True), ("myorg", True) )
self._set_opt_in_pref(self.user, "MyOrg", True) self._set_opt_in_pref(self.user, "myorg", False)
temp_dir_path = tempfile.mkdtemp() self.addCleanup(shutil.rmtree, temp_dir_path)
if other_names is None: other_names = []
if query_interval is not None: command.QUERY_INTERVAL = query_interval
command.handle(output_path, *org_list, courses=only_courses)
return rows
if include_default_option: field_dict["options"].append({ "value": "", "name": "--", "default": True })
field_dict.update(self._field_overrides.get(name, {}))
if "field_type" in kwargs: kwargs["type"] = kwargs["field_type"]
if "default" in kwargs: kwargs["defaultValue"] = kwargs["default"]
request.POST = request.POST.copy()
if "enrollment_action" in request.POST: del request.POST["enrollment_action"] if "course_id" in request.POST: del request.POST["course_id"]
response = view_func(request)
else: response.status_code = 403 response.content = msg
elif response.status_code != 200 or not success: if response.status_code == 200: response.status_code = 400 response.content = msg
else: response.content = msg
return response
from __future__ import unicode_literals
course_tag_api.set_course_tag(user, course_key, partition_key, group.id)
if value is None: return Response(status=status.HTTP_404_NOT_FOUND)
(27, True, u"True"),
(32, False, u"False"),
(14, True, u"True"),
(13, True, u"False"),
(12, True, u"False")
course = CourseFactory.create() create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
course = CourseFactory.create() create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
(27, True, False, u"False"),
(32, False, True, u"True"),
(13, True, False, u"False"),
(12, True, False, u"False")
course = CourseFactory.create() create_account(self.USERNAME, self.PASSWORD, self.EMAIL)
set_user_preference(self.user, "dict_pref", {"int_key": 10}) set_user_preference(self.user, "string_pref", "value")
self.send_delete(self.client) self.send_get(self.client, expected_status=404)
self.send_delete(self.client, expected_status=404)
if request.user.is_staff: return True user = get_object_or_404(User, username__iexact=url_username) if field_name in visible_fields(user.profile, user): return True raise Http404()
COURSE_SCOPE = 'course'
tag = course_tag_api.get_course_tag(self.user, self.course_id, self.test_key) self.assertIsNone(tag)
self.user = UserFactory.create()
group1_id = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition)
for __ in range(10): group2_id = RandomUserPartitionScheme.get_group_for_user( self.MOCK_COURSE_ID, self.user, self.user_partition ) self.assertEqual(group1_id, group2_id)
group = RandomUserPartitionScheme.get_group_for_user( self.MOCK_COURSE_ID, self.user, self.user_partition, assign=False )
group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition)
with self.assertRaisesRegexp(UserPartitionError, "Cannot assign user to an empty user partition"): RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, empty_partition)
old_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition) self.assertIn(old_group.id, [0, 1])
new_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertIn(new_group.id, [3, 4])
new_group_2 = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertEqual(new_group, new_group_2)
old_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, self.user_partition) self.assertIn(old_group.id, [0, 1])
new_group = RandomUserPartitionScheme.get_group_for_user(self.MOCK_COURSE_ID, self.user, user_partition) self.assertEqual(old_group.id, new_group.id)
try: intercepted_function(raise_error=FakeInputException) except FakeOutputException as ex: self.assertEqual(ex.message, expected_log_msg)
mock_logger.exception.assert_called_once_with(expected_log_msg)
self.assertNotIn("enrollment_action", self.captured_request.POST) self.assertNotIn("course_id", self.captured_request.POST)
self.assertEqual(self.captured_request.POST.get("course_id"), "edX/DemoX/Fall")
class UserPreferenceFactory(DjangoModelFactory): class Meta(object): model = UserPreference
response = self.client.get(self.url, content_type="application/json") self.assertHttpOK(response)
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
response = self.client.post(self.url, { "email": self.EMAIL, "password": self.PASSWORD, }) self.assertHttpOK(response)
response = self.client.get(reverse("dashboard")) self.assertHttpOK(response)
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
data = { "email": self.EMAIL, "password": self.PASSWORD, }
self.assertEqual( self.client.session.get_expire_at_browser_close(), expire_at_browser_close )
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
response = self.client.post(self.url, { "email": self.EMAIL, "password": "invalid" }) self.assertHttpForbidden(response)
response = self.client.post(self.url, { "email": "invalid@example.com", "password": self.PASSWORD, }) self.assertHttpForbidden(response)
UserFactory.create(username=self.USERNAME, email=self.EMAIL, password=self.PASSWORD)
response = self.client.post(self.url, { "email": self.EMAIL, }) self.assertHttpBadRequest(response)
response = self.client.post(self.url, { "password": self.PASSWORD, }) self.assertHttpBadRequest(response)
response = self.client.post(self.url, {})
response = self.client.get(self.url, content_type="application/json") self.assertHttpOK(response)
},
self._assert_reg_field( no_extra_fields_setting, { "name": "password", "type": "hidden", "required": False, } )
response = self.client.get(reverse("dashboard")) self.assertHttpOK(response)
user = User.objects.get(username=self.USERNAME) request = RequestFactory().get('/url') request.user = user account_settings = get_account_settings(request)
response = self.client.get(reverse("dashboard")) self.assertHttpOK(response)
data = { "email": self.EMAIL, "name": self.NAME, "username": self.USERNAME, "password": self.PASSWORD, }
data.update(invalid_fields)
response = self.client.post(self.url, data) self.assertHttpBadRequest(response)
response = self.client.post(self.url, data) self.assertHttpBadRequest(response)
with override_settings(REGISTRATION_EXTRA_FIELDS=extra_fields_setting): response = self.client.get(self.url) self.assertHttpOK(response)
form_desc = json.loads(response.content)
actual_field = None for field in form_desc["fields"]: if field["name"] == expected_field["name"]: actual_field = field break
with override_settings(REGISTRATION_EXTRA_FIELDS={"country": "required"}): response = self.client.get(self.url) self.assertHttpOK(response)
original_modified = tag.modified tag.value = "barfoo" tag.save() self.assertEquals(tag.value, "barfoo") self.assertNotEqual(original_modified, tag.modified)
set_user_preference(user, key, value) pref = UserPreference.get_value(user, key) self.assertEqual(pref, value)
pref = UserPreference.get_value(user, 'testkey_none') self.assertIsNone(pref)
self.assertEquals(self.middleware.process_request(self.request), None)
self.assertEquals( self.middleware.process_response(self.request, self.response), self.response ) exit_context.assert_called_with(UserTagsEventContextMiddleware.CONTEXT_NAME) exit_context.reset_mock()
get_tracker.side_effect = Exception self.assertEquals( self.middleware.process_response(self.request, self.response), self.response )
from __future__ import unicode_literals
if not self.theme_location: return False
func = with_comprehensive_theme(EDX_THEME_DIR)(func)
from django.template.base import ( TemplateSyntaxError, Library, token_kwargs, TemplateDoesNotExist ) from django.template.loader_tags import IncludeNode
path = path[len(self.storage.prefix):]
from __future__ import unicode_literals
site_configuration = SiteConfigurationFactory.create( site=self.site, )
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
self.assertEqual(len(site_configuration_history), 1)
site_configuration = SiteConfigurationFactory.create( site=self.site, )
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
self.assertEqual(len(site_configuration_history), 2)
site_configuration = SiteConfigurationFactory.create( site=self.site, )
site_configuration_history = SiteConfigurationHistory.objects.filter( site=site_configuration.site, ).all()
self.assertEqual(len(site_configuration_history), 1)
site_configuration = SiteConfigurationFactory.create( site=self.site, )
self.assertEqual(len(site_configuration_history), 1)
from __future__ import unicode_literals
from __future__ import unicode_literals
get_user_credentials(self.user)
get_user_credentials(self.user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
for _ in range(2): get_user_credentials(staff_user)
self.assertEqual(len(httpretty.httpretty.latest_requests), 3)
self.create_credentials_config() self.create_programs_config()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False)
self.assertEqual(len(actual), 2) self.assertEqual(actual, expected)
self.create_credentials_config() self.create_programs_config()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user, category='xseries') expected = self.expected_credentials_display_data()
self.assertEqual(len(actual), 2) self.assertEqual(actual, expected)
self.create_credentials_config() self.create_programs_config()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user, category='dummy_category') expected = self.expected_credentials_display_data()
self.mock_programs_api() self.mock_credentials_api(self.user, reset_url=False) actual = get_programs_credentials(self.user) expected = self.expected_credentials_display_data()
use_cache = credential_configuration.is_cache_enabled and not user.is_staff cache_key = credential_configuration.CACHE_KEY + '.' + user.username if use_cache else None
self.org = org self.course_id = course_id self.run = run self.language = None
course_details.license = getattr(course_descriptor, "license", "all-rights-reserved")
except ValueError: pass
for attribute in ABOUT_ATTRIBUTES: if attribute in jsondict: cls.update_about_item(descriptor, attribute, jsondict[attribute], user.id)
return CourseDetails.fetch(course_key)
GRADES_UPDATED = Signal(providing_args=["username", "grade_summary", "course_key", "deadline"])
from openedx.core.djangoapps.ccxcon import tasks tasks.update_ccxcon.delay(unicode(course_key))
from __future__ import unicode_literals
course_instructors = list_with_level(course, 'instructor') course_instructors_ids = [anonymous_id_for_user(user, course_key) for user in course_instructors] course_details = CourseDetails.fetch(course_key)
add_course_url = urlparse.urljoin(course.ccx_connector, CCXCON_COURSEXS_URL) resp = oauth_ccxcon.post( url=add_course_url, json=payload, headers=headers, timeout=CCXCON_REQUEST_TIMEOUT )
with cls.store.bulk_operations(course.id, emit_signals=False):
self.assertEqual(k_args, tuple()) self.assertEqual( k_kwargs.get('url'), urlparse.urljoin(self.course.ccx_connector, ccxconapi.CCXCON_COURSEXS_URL) )
mock_response.status_code = 200 mock_post.return_value = mock_response
self.assertEqual(k_args, tuple()) self.assertEqual( k_kwargs.get('url'), urlparse.urljoin(self.course.ccx_connector, ccxconapi.CCXCON_COURSEXS_URL) )
if cur_retry < 5: update_ccxcon.apply_async( kwargs={'course_id': course_id, 'cur_retry': cur_retry + 1},
xblock_class = XBlock.load_class(block_type, select=settings.XBLOCK_SELECT_FUNCTION) content = xblock_class.open_local_resource(uri)
from __future__ import unicode_literals
from __future__ import unicode_literals
requirements = CreditRequirement.objects.filter(course__course_key=course_key, active=True)
history = HistoricalRecords()
deadline = models.DateTimeField( default=default_deadline_for_credit_eligibility, help_text=ugettext_lazy("Deadline for purchasing and requesting credit.") )
status_by_req = defaultdict(lambda: False) for status in CreditRequirementStatus.get_statuses(requirements, username): status_by_req[status.requirement.id] = status.status
unique_together = ('username', 'course', 'provider') get_latest_by = 'created'
icrv_blocks = get_course_blocks(course_key, VERIFICATION_BLOCK_CATEGORY)
_set_verification_partitions(course_key, icrv_blocks)
used_ids = set(p.id for p in course.user_partitions) return generate_int_id(used_ids=used_ids)
partitions += _other_partitions(verified_partitions, partitions, course_key) course.set_user_partitions_for_scheme(partitions, scheme) modulestore().update_item(course, ModuleStoreEnum.UserID.system)
lookup_value = '[^/.]+'
provider_ids = self.request.GET.get('provider_ids', None)
provider = generics.get_object_or_404(CreditProvider, provider_id=provider_id)
course_key = request.data.get('course_key') try: course_key = CourseKey.from_string(course_key) except InvalidKeyError: raise InvalidCourseKey(course_key)
username = request.data.get('username') if not username: raise ValidationError({'detail': 'A username must be specified.'})
if not CreditEligibility.is_user_eligible_for_credit(course_key, username): raise UserNotEligibleException(course_key, username)
authentication_classes = () permission_classes = ()
serializer = CreditProviderCallbackSerializer(data=data, provider=provider) serializer.is_valid(raise_exception=True)
@method_decorator(csrf_exempt) def dispatch(self, request, *args, **kwargs): return super(CreditCourseViewSet, self).dispatch(request, *args, **kwargs)
course_key = self.kwargs.get(self.lookup_field) if course_key is not None: self.kwargs[self.lookup_field] = CourseKey.from_string(course_key)
from openedx.core.djangoapps.credit import api, tasks
from openedx.core.djangoapps.credit import api
status = 'satisfied' reason = {'final_grade': grade_summary['percent']}
status = 'failed' reason = { 'current_date': now, 'deadline': deadline }
status = 'failed' reason = { 'final_grade': grade_summary['percent'], 'minimum_grade': min_grade }
if not date_time: msg = '[{}] is not a valid timestamp'.format(value) log.warning(msg) raise serializers.ValidationError(msg)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, )
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, get_credit_requirement_status, )
return None
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, set_credit_requirement_status as api_set_credit_requirement_status )
if not is_credit_course(course_key): return
try: user = User.objects.get(id=user_id) except ObjectDoesNotExist: return None
from openedx.core.djangoapps.credit.api.eligibility import ( is_credit_course, remove_credit_requirement_status as api_remove_credit_requirement_status )
if not is_credit_course(course_key): return
log_msg = ( 'remove_credit_requirement_status was called with ' 'user_id={user_id}, course_key_or_id={course_key_or_id} ' 'req_namespace={req_namespace}, req_name={req_name}, '.format( user_id=user_id, course_key_or_id=course_key_or_id, req_namespace=req_namespace, req_name=req_name ) ) log.info(log_msg)
try: user = User.objects.get(id=user_id) except ObjectDoesNotExist: return None
is_verified, has_skipped, has_completed = _get_user_statuses(user, course_key, checkpoint)
cache_values = cache.get_many([ enrollment_cache_key, has_skipped_cache_key, verification_status_cache_key ])
is_verified = cache_values.get(enrollment_cache_key) if is_verified is None: is_verified = CourseEnrollment.is_enrolled_as_verified(user, course_key) cache.set(enrollment_cache_key, is_verified)
has_skipped = cache_values.get(has_skipped_cache_key) if has_skipped is None: has_skipped = SkippedReverification.check_user_skipped_reverification_exists(user, course_key) cache.set(has_skipped_cache_key, has_skipped)
verification_statuses = cache_values.get(verification_status_cache_key) if verification_statuses is None: verification_statuses = VerificationStatus.get_all_checkpoints(user.id, course_key) cache.set(verification_status_cache_key, verification_statuses)
checkpoint = verification_statuses.get(checkpoint) has_completed_check = bool(checkpoint)
notification_msg = MIMEMultipart('related') msg_alternative = MIMEMultipart('alternative') notification_msg.attach(msg_alternative) subject = _(u'Course Credit Eligibility')
email_body_plain = render_to_string('credit_notifications/credit_eligibility_email.txt', context) msg_alternative.attach(SafeMIMEText(email_body_plain, _subtype='plain', _charset='utf-8'))
if logo_image: notification_msg.attach(logo_image)
from_address = theming_helpers.get_value('default_from_email', settings.DEFAULT_FROM_EMAIL) to_address = user.email
msg = EmailMessage(subject, None, from_address, [to_address]) msg.attach(notification_msg) msg.send()
html_with_inline_css = pynliner.fromString('<style>' + css_content + '</style>' + html_without_css) return html_with_inline_css
providers_string = _("{first_provider} and {second_provider}").format( first_provider=providers[0], second_provider=providers[1] )
providers_string = _("{first_providers}, and {last_provider}").format( first_providers=u", ".join(providers[:-1]), last_provider=providers[-1] )
credit_request, created = CreditRequest.objects.get_or_create( course=credit_course, provider=credit_provider, username=username, )
user = User.objects.select_related('profile').get(username=username)
try: final_grade = CreditRequirementStatus.objects.get( username=username, requirement__namespace="grade", requirement__name="grade", requirement__course__course_key=course_key, status="satisfied" ).reason["final_grade"]
if len(unicode(final_grade)) > 7: final_grade = u'{:.5f}'.format(final_grade) else: final_grade = unicode(final_grade)
course_enrollment = CourseEnrollment.get_enrollment(user, course_key) enrollment_date = course_enrollment.created if course_enrollment else ""
completion_date = get_last_exam_completion_date(course_key, username)
parameters["signature"] = signature(parameters, shared_secret_key)
reqs = CreditRequirement.get_course_requirements(course_key)
req_to_update = next(( req for req in reqs if req.namespace == req_namespace and req.name == req_name ), None)
CreditRequirementStatus.add_or_update_requirement_status( username, req_to_update, status=status, reason=reason )
if status == "satisfied" and not eligible_before_update: is_eligible, eligibility_record_created = CreditEligibility.update_eligibility(reqs, username, course_key) if eligibility_record_created and is_eligible: try: send_credit_notifications(username, course_key)
req_to_remove = CreditRequirement.get_course_requirements(course_key, namespace=req_namespace, name=req_name)
CreditRequirementStatus.remove_requirement_status( username, req_to_remove )
if req["namespace"] == old_req.namespace and req["name"] == old_req.name: found_flag = True break
api.set_credit_requirements(self.course_key, requirements[1:])
visible_reqs = api.get_credit_requirements(self.course_key) self.assertEqual(len(visible_reqs), 1) self.assertEqual(visible_reqs[0]["namespace"], "grade")
credit_course = self.add_credit_course() CreditEligibility.objects.create( course=credit_course, username="staff", deadline=datetime.datetime.now(pytz.UTC) - datetime.timedelta(days=1) )
is_eligible = api.is_user_eligible_for_credit("staff", credit_course.course_key) self.assertFalse(is_eligible)
eligibilities = api.get_eligibilities_for_user("staff") self.assertEqual(eligibilities, [])
credit_course = self.add_credit_course() credit_course.enabled = False credit_course.save()
is_eligible = api.is_user_eligible_for_credit("staff", credit_course.course_key) self.assertFalse(is_eligible)
eligibilities = api.get_eligibilities_for_user("staff") self.assertEqual(eligibilities, [])
self.assert_grade_requirement_status(None, 0)
api.set_credit_requirement_status(username, self.course_key, "grade", "grade") self.assert_grade_requirement_status('satisfied', 0)
api.set_credit_requirement_status(username, self.course_key, "grade", "grade", status="failed") self.assert_grade_requirement_status('failed', 0)
self.assertEqual(req_status[1]["status"], None) self.assertEqual(req_status[1]["order"], 1)
self.add_credit_course()
api.remove_credit_requirement_status("bob", self.course_key, "grade", "grade")
req_status = api.get_credit_requirement_status(self.course_key, "bob", namespace="grade", name="grade") self.assertEqual(len(req_status), 0)
self.add_credit_course() CourseFactory.create(org='edX', number='DemoX', display_name='Demo_Course')
with self.assertNumQueries(12): api.set_credit_requirement_status( user.username, self.course_key, requirements[0]["namespace"], requirements[0]["name"] )
self.assertFalse(api.is_user_eligible_for_credit("bob", self.course_key))
with self.assertNumQueries(20): api.set_credit_requirement_status( "bob", self.course_key, requirements[1]["namespace"], requirements[1]["name"] )
self.assertTrue(api.is_user_eligible_for_credit("bob", self.course_key))
self.assertEqual(len(mail.outbox), 1) self.assertEqual( mail.outbox[0].subject, 'You are eligible for credit from Hogwarts School of Witchcraft and Wizardry' )
email_image = email_payload_first[1]
text_content_first = email_payload_first[0]._payload[0]._payload self.assertIn( 'credit from Hogwarts School of Witchcraft and Wizardry for', text_content_first )
self.assertEqual(len(mail.outbox), 2)
self.add_credit_course()
api.set_credit_requirement_status("bob", self.course_key, "grade", "grade")
req_status = api.get_credit_requirement_status(self.course_key, "bob", namespace="grade", name="grade") self.assertEqual(req_status, [])
self.assertEqual(len(mail.outbox), 1)
self.assertEqual(mail.outbox[0].subject, expected_subject)
self._configure_credit()
provider = CreditProvider.objects.get() provider.active = False provider.save()
result = api.get_credit_providers(['fake_provider_id']) self.assertEqual(result, [])
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
self.assertIn('request_uuid', parameters) self.assertEqual(len(parameters['request_uuid']), 32)
self.assertIn('timestamp', parameters) parsed_date = from_timestamp(parameters['timestamp']) self.assertLess(parsed_date, datetime.datetime.now(pytz.UTC))
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
self._assert_credit_status("pending")
api.update_credit_request_status(request["parameters"]["request_uuid"], self.PROVIDER_ID, status) self._assert_credit_status(status)
uuid = request["parameters"]["request_uuid"] with self.assertNumQueries(3): api.update_credit_request_status(uuid, self.PROVIDER_ID, "approved")
first_request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"])
self.assertEqual( first_request["parameters"]["request_uuid"], second_request["parameters"]["request_uuid"] )
self.assertEqual(second_request["parameters"]["user_full_name"], "Bobby")
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"])
api.update_credit_request_status(request["parameters"]["request_uuid"], self.PROVIDER_ID, status)
with self.assertRaises(RequestAlreadyCompleted): api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO['username'])
first_request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"])
other_course_key = CourseKey.from_string("edX/other/2015") self._configure_credit(course_key=other_course_key) second_request = api.create_credit_request(other_course_key, self.PROVIDER_ID, self.USER_INFO["username"])
self.assertEqual(first_request["parameters"]["course_num"], self.course_key.course) self.assertEqual(second_request["parameters"]["course_num"], other_course_key.course)
self.user.profile.mailing_address = None self.user.profile.save()
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"]) self.assertEqual(request["parameters"]["user_mailing_address"], "")
query = "UPDATE auth_userprofile SET country = NULL WHERE id = %s" connection.cursor().execute(query, [str(self.user.profile.id)])
request = api.create_credit_request(self.course_key, self.PROVIDER_ID, self.USER_INFO["username"]) self.assertEqual(request["parameters"]["user_country"], "")
grade_status = CreditRequirementStatus.objects.get( username=self.USER_INFO['username'], requirement__namespace="grade", requirement__name="grade" ) grade_status.reason = {} grade_status.save()
with self.assertRaises(CreditRequestNotFound): api.update_credit_request_status("invalid_uuid", self.PROVIDER_ID, "approved")
response_providers = get_credit_provider_display_names(self.course_key) self.assertListEqual(self.PROVIDERS_LIST, response_providers)
response_providers = get_credit_provider_display_names(self.course_key) self.assertListEqual(self.PROVIDERS_LIST, response_providers)
self.assertEqual(len(httpretty.httpretty.latest_requests), 1)
import datetime import json
self.course = CourseFactory.create() self.checkpoint_location = u'i4x://{org}/{course}/edx-reverification-block/first_uuid'.format( org=self.course.id.org, course=self.course.id.course )
user = self.create_user_and_enroll(enrollment_type) if verification_status: self.add_verification_status(user, verification_status)
user = self.create_user_and_enroll('verified')
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
user = self.create_user_and_enroll('verified') self.add_verification_status(user, VerificationStatus.APPROVED_STATUS) with self.assertNumQueries(4): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
user = self.create_user_and_enroll('verified') self.add_verification_status(user, VerificationStatus.DENIED_STATUS)
with self.assertNumQueries(4): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
user = self.create_user_and_enroll('honor') with self.assertNumQueries(3): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.ALLOW)
with self.assertNumQueries(3): self._assert_group_assignment(user, VerificationPartitionScheme.DENY)
with self.assertNumQueries(0): self._assert_group_assignment(user, VerificationPartitionScheme.DENY)
if self.list_path: self.path = reverse(self.list_path)
self.user = UserFactory(password=self.password, is_staff=True) self.client.login(username=self.user.username, password=self.password)
response = self.client.get(self.path) self.assertEqual(response.status_code, 403)
user.is_staff = True
response = client.get('/')
response = client.post(self.path, data=json.dumps(data), content_type=JSON, HTTP_X_CSRFTOKEN=csrf_token) self.assertEqual(response.status_code, 201)
response = self.client.get(self.path, **headers) self.assertEqual(response.status_code, 403)
user.is_staff = True
self.assertDictEqual(json.loads(response.content), data)
course_key = CourseKey.from_string(course_id) self.assertTrue(CreditCourse.objects.filter(course_key=course_key, enabled=enabled).exists())
self.assertDictEqual(json.loads(response.content), self._serialize_credit_course(cc1))
self.assertListEqual(json.loads(response.content), expected)
self.assertDictEqual(json.loads(response.content), data)
credit_course = CreditCourse.objects.get(course_key=credit_course.course_key) self.assertTrue(credit_course.enabled)
self.provider.enable_integration = True self.provider.save()
requirement = CreditRequirement.objects.create( course=course, namespace='grade', name='grade', )
CreditRequirementStatus.objects.create( username=username, requirement=requirement, status='satisfied', reason={'final_grade': final_grade} )
request = CreditRequest.objects.get(username=username, course__course_key=course_key) self.assertEqual(request.status, 'pending')
content = json.loads(response.content) parameters = content['parameters']
self.provider.enable_integration = True self.provider.save()
with override_settings(CREDIT_PROVIDER_SECRET_KEYS={}): response = self.post_credit_request(self.user.username, self.eligibility.course.course_key) self.assertEqual(response.status_code, 400)
self.client.logout()
response = self._credit_provider_callback(request_uuid, "approved", sig="invalid") self.assertEqual(response.status_code, 403)
self._assert_request_status(request_uuid, "pending")
self._credit_provider_callback(request_uuid, 'approved') self._assert_request_status(request_uuid, "approved")
self._credit_provider_callback(request_uuid, 'approved') self._assert_request_status(request_uuid, "approved")
CreditProvider.objects.create(provider_id=other_provider_id, enable_integration=True)
request_uuid = self._create_credit_request_and_get_uuid()
response = self._credit_provider_callback( request_uuid, 'approved', provider_id=other_provider_id, secret_key=other_provider_secret_key, keys={other_provider_id: other_provider_secret_key} )
self.assertEqual(response.status_code, 404)
self._assert_request_status(request_uuid, 'pending')
with override_settings(CREDIT_PROVIDER_SECRET_KEYS={}): response = self._credit_provider_callback(request_uuid, 'approved', keys={}) self.assertEqual(response.status_code, 403)
key = signature.get_shared_secret_key("asu") sig = signature.signature({}, key) self.assertEqual(sig, "7d70a26b834d9881cc14466eceac8d39188fc5ef5ffad9ab281a8327c2c0d093")
key = signature.get_shared_secret_key("asu") self.assertIs(key, None)
MODULESTORE = TEST_DATA_SPLIT_MODULESTORE
SignalHandler.pre_publish.disconnect(receiver=on_pre_publish) self.addCleanup(SignalHandler.pre_publish.connect, receiver=on_pre_publish)
self.assertEqual(len(partition.groups), 2) self.assertItemsEqual( [g.id for g in partition.groups], [ VerificationPartitionScheme.ALLOW, VerificationPartitionScheme.DENY, ] )
self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only ) self._update_partitions()
other_icrv = ItemFactory.create(parent=self.verticals[3], category='edx-reverification-block') self._update_partitions()
icrv_location = self.icrv.location self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only ) self._update_partitions()
self.store.delete_item( self.icrv.location, ModuleStoreEnum.UserID.test, revision=ModuleStoreEnum.RevisionOption.published_only )
with check_mongo_calls_range(max_finds=4, max_sends=2): self._update_partitions(reload_items=False)
with check_mongo_calls_range(max_finds=5, max_sends=3): self._update_partitions(reload_items=False)
ItemFactory.create(parent=self.verticals[3], category='edx-reverification-block') with check_mongo_calls_range(max_finds=6, max_sends=3): self._update_partitions(reload_items=False)
if reload_items:
self.assertEqual(self.course.user_partitions, [])
from __future__ import unicode_literals
self.assertFalse([ requirement for requirement in requirements if requirement['namespace'] == 'proctored_exam' ])
self.assertFalse([ requirement for requirement in requirements if requirement['namespace'] == 'proctored_exam' ])
create_exam( course_id=unicode(self.course.id), content_id='foo3', exam_name='A Proctored Exam', time_limit_mins=10, is_proctored=True, is_active=True, is_practice_exam=True )
self.assertFalse([ requirement for requirement in requirements if requirement['namespace'] == 'proctored_exam' ])
requirements = get_credit_requirements(self.course.id, namespace="reverification") self.assertEqual(len(requirements), 1)
with self.store.branch_setting(ModuleStoreEnum.Branch.draft_preferred, self.course.id): self.store.delete_item(self.subsection.location, ModuleStoreEnum.UserID.test)
on_course_publish(self.course.id) requirements = get_credit_requirements(self.course.id, namespace="reverification") self.assertEqual(len(requirements), 0)
start = datetime.now(UTC) self.add_icrv_xblock(related_assessment_name="Midterm A", start_date=start)
start = datetime.now(UTC) first_block = self.add_icrv_xblock(related_assessment_name="Midterm Start Date")
self.assertEqual(requirements[2]["name"], first_block.get_credit_requirement_name()) self.assertEqual(requirements[3]["name"], second_block.get_credit_requirement_name())
start = datetime.now(UTC) self.add_icrv_xblock(related_assessment_name="Midterm A", start_date=start)
CreditCourse.objects.create( course_key=self.course.id, enabled=True, )
CreditProvider.objects.create( provider_id="ASU", enable_integration=True, provider_url="https://credit.example.com/request", )
set_credit_requirements(self.course.id, requirements)
self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
credit_state = self.service.get_credit_state(self.user.id, self.course.id) self.assertEqual(credit_state['credit_requirement_status'][0]['status'], "satisfied")
self.service.remove_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' )
credit_state = self.service.get_credit_state(self.user.id, self.course.id) self.assertEqual(credit_state['credit_requirement_status'][0]['status'], None)
retval = self.service.set_credit_requirement_status( self.user.id, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
retval = self.service.remove_credit_requirement_status( 0, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
self.service.remove_credit_requirement_status( self.user.id, no_credit_course.id, 'grade', 'grade' )
credit_state = self.service.get_credit_state(self.user.id, self.course.id) self.assertNotIn('course_name', credit_state)
self.service.set_credit_requirement_status( self.user.id, no_credit_course.id, 'grade', 'grade' )
retval = self.service.set_credit_requirement_status( 0, self.course.id, 'grade', 'grade' ) self.assertIsNone(retval)
self.service.set_credit_requirement_status( self.user.id, unicode(self.course.id), 'grade', 'grade' )
return [ block for block in modulestore().get_items( course_key, qualifiers={"category": category}, revision=ModuleStoreEnum.RevisionOption.published_only, ) if _is_in_course_tree(block) ]
CREDIT_REQUIREMENT_XBLOCK_CATEGORIES = [ "edx-reverification-block", ]
@task(default_retry_delay=settings.CREDIT_TASK_DEFAULT_RETRY_DELAY, max_retries=settings.CREDIT_TASK_MAX_RETRIES)
sorted_block_requirements = sorted( block_requirements, key=lambda x: (x['start_date'] is None, x['start_date'], x['display_name']) )
from edx_proctoring.api import get_all_exams_for_course
course_id = CourseKeyField( max_length=255, db_index=True, help_text="Which course is this group associated with?", )
with outer_atomic(read_committed=True):
if created: return
@receiver(pre_delete, sender=CohortMembership)
always_cohort_inline_discussions = models.BooleanField(default=True)
global _local_random
ans = set()
course_cohort_settings = get_course_cohort_settings(course_key) if not course_cohort_settings.is_cohorted: return request_cache.data.setdefault(cache_key, None)
membership = CohortMembership.objects.create( user=user, course_user_group=get_random_cohort(course_key) ) return request_cache.data.setdefault(cache_key, membership.course_user_group)
if created: manual_cohorts = CourseUserGroup.objects.filter( course_id=course.id, group_type=CourseUserGroup.COHORT ).exclude(name__in=course.auto_cohort_groups) for cohort in manual_cohorts: CourseCohort.create(course_user_group=cohort)
migrate_cohort_settings(course)
return JsonResponse({"error": unicode(err)}, 400)
return JsonResponse({"error": "Cohort name must be specified."}, 400)
return JsonResponse({"error": "Assignment type must be specified."}, 400)
return JsonResponse( {"error": "If group_id is specified, user_partition_id must also be specified."}, 400 )
existing_group_id, _ = cohorts.get_group_info_for_cohort(cohort) if existing_group_id is not None: unlink_cohort_partition_group(cohort)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string)
cohort = cohorts.get_cohort_by_id(course_key, int(cohort_id))
return HttpResponseBadRequest('Requested page must be numeric')
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string) get_course_with_access(request.user, 'staff', course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string) get_course_with_access(request.user, 'staff', course_key)
course_key = SlashSeparatedCourseKey.from_deprecated_string(course_key_string) get_course_with_access(request.user, 'staff', course_key)
course_wide_entries = discussion_category_map.pop('entries')
config_course_cohorts(self.course1, is_cohorted=True, auto_cohorts=["Course1AutoGroup1", "Course1AutoGroup2"])
membership1 = CohortMembership( course_id=course_1_auto_cohort_1.course_id, user=self.user1, course_user_group=course_1_auto_cohort_1 ) membership1.save() membership2 = CohortMembership( course_id=course_1_auto_cohort_1.course_id, user=self.user2, course_user_group=course_1_auto_cohort_1 ) membership2.save()
call_command('post_cohort_membership_fix')
call_command('post_cohort_membership_fix', commit='commit')
self.assertEqual(self.user1.course_groups.count(), 1) self.assertEqual(CohortMembership.objects.filter(user=self.user1).count(), 1)
course_group.users.remove(user) user.course_groups.remove(course_group)
from __future__ import unicode_literals
import json
discussion_topics = { "Topic B": {"id": "Topic B"}, }
ItemFactory.create( parent_location=self.course.location, category="discussion", discussion_id="Topic_A", discussion_category="Chapter", discussion_target="Discussion", start=now )
self.verify_lists_expected_cohorts([])
config_course_cohorts(self.course, is_cohorted=True)
self.verify_lists_expected_cohorts([])
users = [UserFactory() for _ in range(3)] self._enroll_users(users, self.course.id)
for user in users: get_cohort(user, self.course.id)
config_course_cohorts_legacy(self.course, [], cohorted=True, auto_cohort_groups=["AutoGroup"])
with self.assertRaises(CourseUserGroup.DoesNotExist): get_cohort_by_name(self.course.id, "AutoGroup")
cohort_name = 'I AM A RANDOM COHORT' data = {'name': cohort_name, 'assignment_type': CourseCohort.RANDOM} response_dict = self.put_handler(self.course, data=data)
cohort_name = 'I AM A RANDOM COHORT' data = {'name': cohort_name, 'assignment_type': CourseCohort.RANDOM} response_dict = self.put_handler(self.course, data=data)
self.assertEqual(self.cohort1.name, response_dict.get("name"))
self.create_cohorted_discussions()
always_cohort_inline_discussions = True
modulestore().update_item(course, ModuleStoreEnum.UserID.test)
modulestore().update_item(course, ModuleStoreEnum.UserID.test)
import ddt from mock import call, patch from nose.plugins.attrib import attr import before_after
cohort.name = "NewName" cohort.save() self.assertFalse(mock_tracker.called)
CourseUserGroup.objects.create( name="TestOtherGroupType", course_id=self.course_key, group_type="dummy" ) self.assertFalse(mock_tracker.called)
cohort_list[0].users.add(*user_list) assert_events("added", user_list, cohort_list[:1]) mock_tracker.reset_mock()
cohort_list[0].users.remove(*user_list) assert_events("removed", user_list, cohort_list[:1]) mock_tracker.reset_mock()
cohort_list[0].users.add(*user_list) cohort_list[0].users.clear() assert_events("removed", user_list, cohort_list[:1]) mock_tracker.reset_mock()
non_cohort.users.add(*user_list) non_cohort.users.clear() self.assertFalse(mock_tracker.emit.called)
user_list[0].course_groups.add(*cohort_list) assert_events("added", user_list[:1], cohort_list) mock_tracker.reset_mock()
user_list[0].course_groups.remove(*cohort_list) assert_events("removed", user_list[:1], cohort_list) mock_tracker.reset_mock()
user_list[0].course_groups.add(*cohort_list) user_list[0].course_groups.clear() assert_events("removed", user_list[:1], cohort_list) mock_tracker.reset_mock()
user_list[0].course_groups.add(non_cohort) user_list[0].course_groups.clear() self.assertFalse(mock_tracker.emit.called)
fake_key = SlashSeparatedCourseKey('a', 'b', 'c') self.assertRaises(Http404, lambda: cohorts.is_course_cohorted(fake_key))
config_course_cohorts(course, is_cohorted=True)
config_course_cohorts( course, is_cohorted=True, auto_cohorts=["AutoGroup"] )
self.assertIsNone(cohorts.get_cohort(user, course.id, assign=False))
self.assertEquals(cohorts.get_cohort(user, course.id).name, "AutoGroup")
config_course_cohorts( course, is_cohorted=True, auto_cohorts=["AutoGroup"] )
config_course_cohorts( course, is_cohorted=True, auto_cohorts=["AutoGroup"] )
config_course_cohorts_legacy( course, discussions=[], cohorted=True, auto_cohort_groups=["OtherGroup"] )
config_course_cohorts( course, is_cohorted=True, auto_cohorts=[] )
config_course_cohorts_legacy( course, discussions=[], cohorted=True, auto_cohort_groups=["AutoGroup"] )
CohortFactory(course_id=course.id, name="ManualCohort") CohortFactory(course_id=course.id, name="ManualCohort2")
self.assertEqual(first_cohort.users.get(), course_user)
add_user_to_cohort(first_cohort, self.student.username) self.assert_student_in_group(None)
add_user_to_cohort(second_cohort, self.student.username) self.assert_student_in_group(self.groups[1])
remove_user_from_cohort(second_cohort, self.student.username) self.assert_student_in_group(None)
add_user_to_cohort(test_cohort, self.student.username) self.assert_student_in_group(None)
link_cohort_to_partition_group( test_cohort, self.user_partition.id, self.groups[0].id, ) self.assert_student_in_group(self.groups[0])
unlink_cohort_partition_group(test_cohort) link_cohort_to_partition_group( test_cohort, self.user_partition.id, self.groups[1].id, ) self.assert_student_in_group(self.groups[1])
unlink_cohort_partition_group( test_cohort, ) self.assert_student_in_group(None)
self.assert_student_in_group(None)
cohort = get_course_cohorts(self.course)[0]
link_cohort_to_partition_group( cohort, self.user_partition.id, self.groups[0].id, )
self.assert_student_in_group(self.groups[0])
link_cohort_to_partition_group( test_cohort, self.user_partition.id, self.groups[0].id, ) add_user_to_cohort(test_cohort, self.student.username) self.assert_student_in_group(self.groups[0])
new_groups = [Group(10, 'New Group 10'), Group(20, 'New Group 20'), Group(30, 'New Group 30')] new_user_partition = UserPartition(
self.assert_student_in_group(new_groups[0], new_user_partition)
new_user_partition = UserPartition(
new_user_partition = UserPartition(
self._verify_masquerade_for_all_groups()
return None
return None
DEFAULT_USER_MESSAGE = ugettext_noop(u'An error has occurred. Please try again.')
response.data["current_page"] = self.page.number
response.data["start"] = (self.page.number - 1) * self.get_page_size(self.request)
tasks.update_xblocks_cache.apply_async([unicode(course_key)], countdown=0)
try: fields = kwargs['context'].pop('fields', DEFAULT_FIELDS) or DEFAULT_FIELDS except KeyError: fields = DEFAULT_FIELDS super(BookmarkSerializer, self).__init__(*args, **kwargs)
required_fields = set(fields) all_fields = set(self.fields.keys()) for field_name in all_fields - required_fields: self.fields.pop(field_name)
from __future__ import unicode_literals
if Bookmark.objects.filter(user=user, course_key=course_key).count() >= settings.MAX_BOOKMARKS_PER_COURSE: return False
with self.assertNumQueries(1): bookmarks = api.get_bookmarks(user=self.user, course_key=course.id, serialized=False) self.assertEqual(len(bookmarks), count)
self.assert_bookmark_data_is_valid(bookmarks[-1], bookmarks_data[0], check_optional_fields=check_all_fields) self.assert_bookmark_data_is_valid(bookmarks[0], bookmarks_data[-1], check_optional_fields=check_all_fields)
response = self.send_get( client=self.client, url=reverse('bookmarks'), query_parameters='course_id=invalid' ) bookmarks_data = response.data['results']
self.other_sequential_2.children.append(self.other_vertical_1.location)
self.assertEqual(bookmark, bookmark2) self.assertEqual(bookmark.xblock_cache, bookmark2.xblock_cache) self.assert_bookmark_model_is_valid(bookmark2, bookmark_data)
block = modulestore().get_course(course.id, depth=None) for __ in range(depth - 1): children = block.get_children() block = children[-1]
usage_key = UsageKey.from_string('i4x://edX/apis/html/interactive') usage_key.replace(course_key=self.course.id) self.assertEqual(Bookmark.get_path(usage_key), [])
self.other_sequential_1.children = []
bookmark_service = BookmarksService(self.other_user) with self.assertNumQueries(1): self.assertFalse(bookmark_service.is_bookmarked(usage_key=self.sequential_1.location))
with self.assertNumQueries(0): self.assertFalse( self.bookmark_service.set_bookmarked(usage_key=UsageKey.from_string("i4x://ed/ed/ed/interactive")) )
blocks_stack.extend(children)
if not isinstance(course_id, basestring): raise ValueError('course_id must be a string. {} is not acceptable.'.format(type(course_id)))
managed = False
for field in ('client_type', 'client_secret', 'client_id', 'authorization_grant_type'): form.fields.pop(field)
Application.objects.filter(user=self.request.user).delete() return super(ApiRequestStatusView, self).form_valid(form)
if not username: return redirect(reverse('api_admin:catalog-search')) return redirect(reverse('api_admin:catalog-list', kwargs={'username': username}))
kwargs.setdefault('label_suffix', '') super(ApiAccessRequestForm, self).__init__(*args, **kwargs)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
final_attrs['value'] = force_text(value)
self.assertEqual(len([r for r in httpretty.httpretty.latest_requests if r.method == 'POST']), 0)
self.assertEqual(len([r for r in httpretty.httpretty.latest_requests if r.method == 'PATCH']), 0)
from smtplib import SMTPException
mock_model_log_exception.assert_called_once_with( 'Error sending API user notification email for request [%s].', self.api_access_request.id ) self.assertIsNotNone(self.api_access_request.id)
mock_model_log_exception.assert_called_once_with( 'Error sending API user notification email for request [%s].', self.api_access_request.id ) self.assertEqual(self.api_access_request.status, ApiAccessRequest.APPROVED)
full_name = UserProfile.objects.get(user=user).name
uploaded_file = request.FILES['file']
with closing(uploaded_file):
try: validate_uploaded_image(uploaded_file) except ImageValidationError as error: return Response( {"developer_message": error.message, "user_message": error.user_message}, status=status.HTTP_400_BAD_REQUEST, )
profile_image_names = get_profile_image_names(username) create_profile_images(uploaded_file, profile_image_names)
set_has_profile_image(username, True, _make_upload_dt())
return Response(status=status.HTTP_204_NO_CONTENT)
set_has_profile_image(username, False)
profile_image_names = get_profile_image_names(username) remove_profile_images(profile_image_names)
return Response(status=status.HTTP_204_NO_CONTENT)
if exif is None: image.save(string_io, format='JPEG') else: image.save(string_io, format='JPEG', exif=exif)
width, height = image_obj.size self.assertEqual(width, height) actual_sizes[width] = name
_view_name = None client_class = PatchedClient
self.reset_tracker()
profile = self.user.profile.__class__.objects.get(user=self.user) self.assertEqual(profile.has_profile_image, has_profile_image)
with make_image_file() as image_file: response = self.client.post(self.url, {'file': image_file}, format='multipart') self.check_response(response, 204)
self.reset_tracker()
self.reset_tracker() different_client = APIClient() different_client.login(username=different_user.username, password=TEST_PASSWORD) response = different_client.delete(self.url) self.check_response(response, 404)
patched_client_login = Client.login
self.assertIsNone(getattr(self.request, 'session', None)) self.assertIsNone(getattr(self.request, 'safe_cookie_verified_user_id', None))
self.assert_response(safe_cookie_data) self.assert_user_in_session()
self.assertEquals(self.request.COOKIES[settings.SESSION_COOKIE_NAME], session_id)
self.assertIsNotNone(self.request.session)
self.assertEquals(self.request.safe_cookie_verified_user_id, self.user.id)
self.assert_response_with_delete_cookie()
safe_cookie_data_1 = SafeCookieData.create(session_id, user_id) self.assertTrue(safe_cookie_data_1.verify(user_id))
serialized_value = unicode(safe_cookie_data_1)
safe_cookie_data_2 = SafeCookieData.parse(serialized_value) self.assertTrue(safe_cookie_data_2.verify(user_id))
self.assert_cookie_data_equal(safe_cookie_data_1, safe_cookie_data_2)
self.assertEqual( self.safe_cookie_data._compute_digest(self.user_id), self.safe_cookie_data._compute_digest(self.user_id), )
log.debug( "SafeCookieData received empty user_id '%s' for session_id '%s'.", user_id, session_id, )
return self._on_user_authentication_failed(request)
return process_request_response
if is_request_from_mobile_app(request): return HttpResponse(status=401)
log_func = log.debug if request.user.id is None else log.warning log_func(
safe_cookie_data = SafeCookieData.create( cookies[settings.SESSION_COOKIE_NAME].value, user_id, )
cookies[settings.SESSION_COOKIE_NAME] = unicode(safe_cookie_data)
if settings.ROOT_URLCONF != 'lms.urls': raise unittest.SkipTest('Test only valid in lms')
for resource_id, resource in self.test_recommendations.iteritems(): for xblock_name in self.XBLOCK_NAMES: result = self.call_event('add_resource', resource, xblock_name)
for resource, xblock_name in itertools.product(self.test_recommendations.values(), self.XBLOCK_NAMES): self.call_event('add_resource', resource, xblock_name)
self.check_event_response_by_key( 'add_resource', self.test_recommendations[self.resource_id], 'id', self.resource_id )
self.check_event_response_by_key('flag_resource', resource, 'reason', '')
self.check_event_response_by_key('flag_resource', resource, 'reason', 'reason 0')
for xblock_name in self.XBLOCK_NAMES: self.check_event_response_by_key('flag_resource', resource, 'reason', 'reason 0', xblock_name)
resp = json.loads(self.call_event('export_resources', {}).content)
self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
resource['event'] = test_case['event_second'] self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
resource['id'] = self.resource_id_second self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
self.check_event_response_by_key( 'handle_vote', resource, 'newVotes', test_case['new_votes'], self.XBLOCK_NAMES[1] )
self.check_event_response_by_key('handle_vote', resource, 'newVotes', test_case['new_votes'])
self.check_event_response_by_key('endorse_resource', resource, test_case['key'], test_case['val'])
self.check_event_response_by_http_status('remove_resource', resource, test_case['status'])
resource = {"id": self.resource_id, 'reason': ''} self.check_event_response_by_http_status(test_case['handler'], resource, test_case['status'])
self.attempt_upload_file_and_verify_result(test_case, 'upload_screenshot')
self.attempt_upload_file_and_verify_result(test_case, 'upload_screenshot')
self.login(email, password)
if settings.ROOT_URLCONF != 'lms.urls': raise unittest.SkipTest('Test only valid in lms') super(XBlockTestCase, cls).setUpClass()
html_response.debug = {'url': url, 'section': section, 'block_urlname': block_urlname} return html_response
test_configuration = [ { "urlname": "two_done_block_test_case_0", #"olx": self.olx_scenarios[0],
self.assertEqual(resp.data, {"state": desired_state})
self.check_response('done_0', 'done-unmarked') self.check_response('done_1', 'done-unmarked')
self.toggle_button('done_0', {}, False) self.toggle_button('done_1', {}, True)
self.check_response('done_0', 'done-unmarked') self.check_response('done_1', 'done-marked')
if settings.ROOT_URLCONF != 'lms.urls': raise unittest.SkipTest('Test only valid in lms')
from random import choice characters = 'abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)' SECRET_KEY = ''.join([choice(characters) for i in range(50)])
PROFILE_IMAGE_MAX_BYTES = 1000 PROFILE_IMAGE_MIN_BYTES = 1000
import django try: django.setup()
#templates_path.append('source/_templates')
#html_static_path.append('source/_static')
if on_rtd: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms' else: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms'
#templates_path.append('source/_templates')
#html_static_path.append('source/_static')
#)
if on_rtd: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms' else: os.environ['DJANGO_SETTINGS_MODULE'] = 'lms'
exclude_patterns = ['build', 'links.rst']
#needs_sphinx = '1.0'
templates_path = add_base(['_templates'])
source_suffix = '.rst'
#source_encoding = 'utf-8-sig'
master_doc = 'index'
project = u'edX' copyright = u'2013, EdX Doc Team'
version = '0.1' release = '0.1'
#today = '' #today_fmt = '%B %d, %Y'
exclude_patterns = []
#default_role = None
#add_function_parentheses = True
#add_module_names = True
#show_authors = False
pygments_style = 'sphinx'
#modindex_common_prefix = []
#keep_warnings = False
html_theme = 'default'
#html_theme_options = {}
#html_theme_path = []
#html_title = None
#html_short_title = None
#html_logo = None
#html_favicon = None
#html_static_path = add_base(['_static'])
#html_last_updated_fmt = '%b %d, %Y'
#html_use_smartypants = True
#html_sidebars = {}
#html_additional_pages = {}
#html_domain_indices = True
#html_use_index = True
#html_split_index = False
#html_show_sourcelink = True
#html_show_sphinx = True
#html_show_copyright = True
#html_use_opensearch = ''
#html_file_suffix = None
htmlhelp_basename = 'edxdoc'
#'papersize': 'letterpaper',
#'pointsize': '10pt',
#'preamble': '',
latex_documents = [ ( 'index', 'getting_started.tex', u'edX Studio Documentation', u'EdX Doc Team', 'manual', ), ]
#latex_logo = None
#latex_use_parts = False
#latex_show_pagerefs = False
#latex_show_urls = False
#latex_appendices = []
#latex_domain_indices = True
man_pages = [ ('index', 'getting_started', u'getting_started Documentation', [u'EdX Doc Team'], 1) ]
#man_show_urls = False
#texinfo_appendices = []
#texinfo_domain_indices = True
#texinfo_show_urls = 'footnote'
#texinfo_no_detailmenu = False
epub_title = u'getting_started' epub_author = u'EdX Doc Team' epub_publisher = u'EdX Doc Team' epub_copyright = u'2013, EdX Doc Team'
#epub_language = ''
#epub_scheme = ''
#epub_identifier = ''
#epub_uid = ''
#epub_cover = ()
#epub_guide = ()
#epub_pre_files = []
#epub_post_files = []
#epub_exclude_files = []
#epub_tocdepth = 3
#epub_tocdup = True
#epub_fix_images = False
#epub_max_image_width = 0
#epub_show_urls = 'inline'
#epub_use_index = True
intersphinx_mapping = {'http://docs.python.org/': None}
sys = 'cms' if sys == 'studio' else sys return cmd("python manage.py", sys, "--settings={}".format(settings), *args)
self.log_dir.makedirs_p() self.har_dir.makedirs_p() self.report_dir.makedirs_p()
self.load_data()
CourseFixture('foobar_org', '1117', 'seed_forum', 'seed_foo').install() print 'Forums permissions/roles data has been seeded'
pass
substring = [ "--with-xunitmp --xunitmp-file={}".format(self.xunit_report), "--processes={}".format(self.num_processes), "--no-color --process-timeout=1200" ]
bokchoy_utils.clear_mongo() self.cache.flush_all()
self.load_data()
self.load_courses()
msg = colorize('green', "Confirming servers are running...") print msg bokchoy_utils.start_servers(self.default_store, self.coveragerc)
if not self.test_spec: test_spec = self.test_dir else: test_spec = self.test_dir / self.test_spec
if self.serversonly: return ""
self.should_fetch_course = False
self.should_fetch_course = kwargs.get('should_fetch_course') self.imports_dir = path('test_root/courses/')
with self: if self.cmd: passed = self.run_test() if not passed: self.failed_suites.append(self)
self.dbs[db].remove()
for db_alias in self.dbs.keys(): sh("cp {db_cache} {db}".format(db_cache=self.db_caches[db_alias], db=self.dbs[db_alias]))
for db_alias in self.dbs.keys(): sh("cp {db} {db_cache}".format(db_cache=self.db_caches[db_alias], db=self.dbs[db_alias]))
if self.failed_only: opts += "--failed"
env_fail_fast_set = ( 'TESTS_FAIL_FAST' in os.environ and os.environ['TEST_FAIL_FAST'] )
self.processes = 0
default_test_id = ( "{system}/djangoapps/*" " common/djangoapps/*" " openedx/core/djangoapps/*" " openedx/tests/*" " openedx/core/lib/*" )
sh('find {dir} -type f -delete'.format(dir=directory))
reports_dir = Env.REPORT_DIR.makedirs_p() clean_dir(reports_dir)
output = os.popen('mongo --eval "print(\'running\')"').read() return output and "running" in output
return Env.BOK_CHOY_CACHE.set('test', 'test')
REPO_ROOT = path(__file__).abspath().parent.parent.parent
REPORT_DIR = REPO_ROOT / 'reports' METRICS_DIR = REPORT_DIR / 'metrics'
PYTHON_COVERAGERC = REPO_ROOT / ".coveragerc"
BOK_CHOY_STUB_DIR = REPO_ROOT / "common" / "djangoapps" / "terrain"
VIDEO_SOURCE_DIR = REPO_ROOT / "test_root" / "data" / "video"
BOK_CHOY_MONGO_DATABASE = "test" BOK_CHOY_CACHE = memcache.Client(['0.0.0.0:11211'], debug=0)
TEST_DIR = REPO_ROOT / ".testids"
I18N_REPORT_DIR = REPORT_DIR / 'i18n'
SERVICE_VARIANT = os.environ.get('SERVICE_VARIANT', None)
try: with open(env_path) as env_file: return json.load(env_file)
child_pids = p1_group.get_children(recursive=True)
if tasks.environment.dry_run: for cmd in cmd_list: tasks.environment.info(cmd) return
except Exception as err: print("Error running process {}".format(err), file=sys.stderr)
child_pids = p1_group.get_children(recursive=True)
proc.wait()
python_suite = suites.PythonTestSuite('Python Tests', **opts) js_suite = suites.JsTestSuite('JS Tests', mode='run', with_coverage=True)
all_unittests_suite = suites.TestSuite('All Tests', subsuites=[js_suite, python_suite]) all_unittests_suite.run()
sh("coverage combine --rcfile={}".format(rcfile))
err_msg = colorize( 'red', "No coverage info found. Run `paver test` before running " "`paver coverage`.\n" ) sys.stderr.write(err_msg) return
sh("coverage xml --rcfile={}".format(rcfile)) sh("coverage html --rcfile={}".format(rcfile)) call_task('diff_coverage', options=dict(options))
xml_reports = []
sh( "diff-cover {xml_report_str} --compare-branch={compare_branch} " "--html-report {diff_html_path}".format( xml_report_str=xml_report_str, compare_branch=compare_branch, diff_html_path=diff_html_path, ) )
PRIVATE_REQS = 'requirements/private.txt' if os.path.exists(PRIVATE_REQS): PYTHON_REQ_FILES.append(PRIVATE_REQS)
if os.path.isfile(path_item): with open(path_item, "rb") as file_handle: hasher.update(file_handle.read())
new_hash = compute_fingerprint(paths) if new_hash != old_hash: install_func()
PACKAGES_TO_UNINSTALL = [
for _ in range(3): uninstalled = False frozen = sh("pip freeze", capture=True)
sh("pip uninstall --disable-pip-version-check -y {}".format(package_name)) uninstalled = True
print "Couldn't uninstall unwanted Python packages!" return
with open(state_file_path, "w") as state_file: state_file.write(expected_version)
files_to_fingerprint = list(PYTHON_REQ_FILES)
files_to_fingerprint.append(sysconfig.get_python_lib())
src_dir = os.path.join(sys.prefix, "src") if os.path.isdir(src_dir): files_to_fingerprint.append(src_dir)
this_file = __file__ if this_file.endswith(".pyc"):
report_dir = (Env.REPORT_DIR / system).makedirs_p()
Env.METRICS_DIR.makedirs_p()
report_dir = (Env.REPORT_DIR / system).makedirs_p()
violations_count_str = "Number of pylint violations: " + str(num_violations) print violations_count_str
with open(Env.METRICS_DIR / "pylint", "w") as f: f.write(violations_count_str)
if num_violations > violations_limit > -1: raise BuildFailure("Failed. Too many pylint violations. " "The limit is {violations_limit}.".format(violations_limit=violations_limit))
pylint_pattern = re.compile(r".(\d+):\ \[(\D\d+.+\]).")
if len(violation_list_for_line) == 4: num_violations_report += 1
Env.METRICS_DIR.makedirs_p()
violations_count_str = "Number of pep8 violations: {count}".format(count=count) print violations_count_str print violations_list
with open(Env.METRICS_DIR / "pep8", "w") as f: f.write(violations_count_str + '\n\n') f.write(violations_list)
if count: failure_string = "Too many pep8 violations. " + violations_count_str failure_string += "\n\nViolations:\n{violations_list}".format(violations_list=violations_list) raise BuildFailure(failure_string)
Env.METRICS_DIR.makedirs_p() _prepare_report_dir(complexity_report_dir)
_write_metric(num_violations, (Env.METRICS_DIR / "jshint"))
if num_violations > violations_limit > -1: raise BuildFailure( "JSHint Failed. Too many violations ({count}).\nThe limit is {violations_limit}.".format( count=num_violations, violations_limit=violations_limit ) )
_write_metric(metrics_str, metrics_report) sh("cat {metrics_report}".format(metrics_report=metrics_report), ignore_error=True)
violations_count_str = "Number of {safecommit_script} violations: {num_violations}\n".format( safecommit_script=safecommit_script, num_violations=num_violations )
metrics_report = (Env.METRICS_DIR / "safecommit") _write_metric(violations_count_str, metrics_report) sh("cat {metrics_report}".format(metrics_report=metrics_report), ignore_error=True)
raise BuildFailure(file_not_found_message)
regex = r'\d+.\d+'
regex = r'^\d+'
except (AttributeError, ValueError): return None
except (AttributeError, ValueError): violations['total'] = None return violations
dquality_dir = (Env.REPORT_DIR / "diff_quality").makedirs_p()
diff_quality_percentage_pass = True
(count, violations_list) = _get_pep8_violations()
print _pep8_output(count, violations_list)
with open(dquality_dir / "diff_quality_pep8.html", "w") as f: f.write(_pep8_output(count, violations_list, is_html=True))
compare_branch = getattr(options, 'compare_branch', None) compare_branch_string = u'' if compare_branch: compare_branch_string = u'--compare-branch={0}'.format(compare_branch)
diff_threshold = int(getattr(options, 'percentage', -1)) percentage_string = u'' if diff_threshold > -1: percentage_string = u'--fail-under={0}'.format(diff_threshold)
if not run_diff_quality( violations_type="pylint", prefix=pythonpath_prefix, reports=pylint_reports, percentage_string=percentage_string, branch_string=compare_branch_string, dquality_dir=dquality_dir ): diff_quality_percentage_pass = False
if not run_diff_quality( violations_type="jshint", prefix=pythonpath_prefix, reports=jshint_reports, percentage_string=percentage_string, branch_string=compare_branch_string, dquality_dir=dquality_dir ): diff_quality_percentage_pass = False
if not diff_quality_percentage_pass: raise BuildFailure("Diff-quality failure(s).")
NPM_VENDOR_DIRECTORY = path("common/static/common/js/vendor")
restart_django_servers()
import sass
if tasks.environment.dry_run: tasks.environment.info("install npm_assets") return
NPM_VENDOR_DIRECTORY.mkdir_p()
for library in NPM_INSTALLED_LIBRARIES: sh('/bin/cp -rf node_modules/{library} {vendor_dir}'.format( library=library, vendor_dir=NPM_VENDOR_DIRECTORY, ))
if tasks.environment.dry_run: return
try: while True: observer.join(2) except KeyboardInterrupt: observer.stop() print("\nStopped asset watcher.")
using_firefox = (os.environ.get('SELENIUM_BROWSER', 'firefox') == 'firefox') validate_firefox = getattr(options, 'validate_firefox_version', using_firefox)
if settings == DEFAULT_SETTINGS: args.append('--skip-collect') call_task('pavelib.assets.update_assets', args=args)
args = [ 'lms', 'studio', '--settings={}'.format(asset_settings), '--skip-collect' ] call_task('pavelib.assets.update_assets', args=args)
if settings != DEFAULT_SETTINGS: collect_assets(['lms'], asset_settings_lms) collect_assets(['studio'], asset_settings_cms)
call_task('pavelib.assets.watch_assets', options={'background': True})
sh("NO_EDXAPP_SUDO=1 EDX_PLATFORM_SETTINGS_OVERRIDE={settings} /edx/bin/edxapp-migrate-{system} --traceback --pythonpath=. {fake}".format( settings=settings, system=system, fake=fake))
os.environ.clear() os.environ.update(_orig_environ)
self.assertEqual(sysex.exception.args, (1,))
self.assertEqual(sysex.exception.args, (1,))
call_task('pavelib.quality.run_safelint', options={"thresholds": '{"total": 5}'})
call_task('pavelib.quality.run_safelint', options={"thresholds": '{"rules": {"javascript-escape": 5}}'})
check_firefox_version()
self._mock_paver_needs = patch.object(pavelib.quality.run_jshint, 'needs').start() self._mock_paver_needs.return_value = 0
patcher = patch('pavelib.quality.sh') self._mock_paver_sh = patcher.start()
self.addCleanup(patcher.stop) self.addCleanup(self._mock_paver_needs.stop)
suite = BokChoyTestSuite('', default_store='invalid') name = 'tests' self.assertEqual( suite.cmd, self._expected_command(name=name, store='invalid') )
mock_sh = patch('pavelib.utils.test.suites.bokchoy_suite.sh') self._mock_sh = mock_sh.start()
self.addCleanup(mock_sh.stop)
tasks.environment = MockEnvironment()
os.environ['NO_PREREQ_INSTALL'] = 'true'
with open(self.f.name, 'w') as f: f.write("foo/hello/test.py:304:15: E203 whitespace before ':'")
self._mock_paver_needs = patch.object(pavelib.quality.run_quality, 'needs').start() self._mock_paver_needs.return_value = 0
self.f = tempfile.NamedTemporaryFile(delete=False) self.f.close()
self.addCleanup(self._mock_paver_needs.stop) self.addCleanup(os.remove, self.f.name)
self.assertEqual(_mock_pep8_violations.call_count, 1) self.assertEqual(self._mock_paver_sh.call_count, 2)
self.assertEqual(self._mock_paver_sh.call_count, 1)
paver.easy.sh("exit 1")
paver.easy.sh("exit 1")
self._mock_paver_needs = patch.object(pavelib.js_test.test_js, 'needs').start() self._mock_paver_needs.return_value = 0
self.addCleanup(self._mock_paver_needs.stop)
sh("i18n_tool generate")
for system in ['lms', 'cms']: sh(django_cmd(system, DEFAULT_SETTINGS, 'compilejsi18n'))
from safe_lxml import defuse_xml_libs defuse_xml_libs()
if not enable_contracts and not edx_args.contracts: contracts.disable_all()
django_args.append('--help')
version = __import__('django').get_version()
simple_option_desc_re = re.compile( r'([-_a-zA-Z0-9]+)(\s*.*?)(?=,\s+(?:/|-|--)|$)')
lang = node['language'] highlight_args['force'] = True
lang = node['language'] highlight_args['force'] = True
fname.replace('_', r'\_'),
raise nodes.SkipNode
def visit_table(self, node): self.context.append(self.compact_p) self.compact_p = True
sys.path.insert(1, dirname(dirname(abspath(__file__))))
extensions = [ "djangodocs", "sphinx.ext.intersphinx", "sphinx.ext.viewcode", "ticket_role", ]
if 'spelling' in sys.argv: extensions.append("sphinxcontrib.spelling")
spelling_lang = 'en_US'
spelling_word_list_filename = 'spelling_wordlist'
source_suffix = '.txt'
master_doc = 'contents'
project = 'Django' copyright = 'Django Software Foundation and contributors'
django_next_version = '1.11'
locale_dirs = ['locale/']
today_fmt = '%B %d, %Y'
exclude_patterns = ['_build']
add_function_parentheses = True
add_module_names = False
show_authors = False
pygments_style = 'trac'
html_theme = "djangodocs"
html_theme_path = ["_theme"]
html_last_updated_fmt = '%b %d, %Y'
html_use_smartypants = True
html_translator_class = "djangodocs.DjangoHTMLTranslator"
html_additional_pages = {}
htmlhelp_basename = 'Djangodoc'
latex_documents = [ ('contents', 'django.tex', 'Django Documentation', 'Django Software Foundation', 'manual'), ]
man_pages = [( 'ref/django-admin', 'django-admin', 'Utility script for the Django Web framework', ['Django Software Foundation'], 1 ), ]
texinfo_documents = [( master_doc, "django", "", "", "Django", "Documentation of the Django framework", "Web development", False )]
epub_title = project epub_author = 'Django Software Foundation' epub_publisher = 'Django Software Foundation' epub_copyright = copyright
epub_theme = 'djangodocs-epub'
epub_cover = ('', 'epub-cover.html')
ticket_url = 'https://code.djangoproject.com/ticket/%s'
raise UnserializableContentError("Control characters are not supported in XML 1.0")
class HTMLParseError(Exception): pass
if 'max-age' in cc and 'max_age' in kwargs: kwargs['max_age'] = min(int(cc['max-age']), kwargs['max_age'])
if 'private' in cc and 'public' in kwargs: del cc['private'] elif 'public' in cc and 'private' in kwargs: del cc['public']
cookies = response.cookies response = HttpResponseNotModified() response.cookies = cookies return response
if response and not (200 <= response.status_code < 300): if_none_match = None if_match = None
if response and response.status_code != 200: if_modified_since = None if_unmodified_since = None
cache_key += '.%s' % getattr(request, 'LANGUAGE_CODE', get_language())
cache.set(cache_key, [], cache_timeout) return _generate_cache_key(request, request.method, [], key_prefix)
logging_config_func = import_string(logging_config)
if logging_settings: logging_config_func(logging_settings)
no_exc_record = copy(record) no_exc_record.exc_info = None no_exc_record.exc_text = None
msg = self.style.HTTP_SUCCESS(msg)
msg = self.style.HTTP_SERVER_ERROR(msg)
TRAILING_PUNCTUATION_RE = re.compile(
DOTS = ['&middot;', '*', '\u2022', '&#149;', '&bull;', '&#8226;']
_js_escapes.update((ord('%c' % z), '\\u%04X' % z) for z in range(32))
segment = quote(segment, safe=RFC3986_SUBDELIMS + RFC3986_GENDELIMS + str('~')) return force_text(segment)
try: scheme, netloc, path, query, fragment = urlsplit(url) except ValueError: return unquote_quote(url)
query_parts = [(unquote(force_str(q[0])), unquote(force_str(q[1]))) for q in parse_qsl(query, keep_blank_values=True)] query = urlencode(query_parts)
unescaped = unescaped[:-len(trail)]
text += trail trail = ''
trimmed_something = True while trimmed_something: trimmed_something = False
match = TRAILING_PUNCTUATION_RE.match(middle) if match: middle = match.group(1) trail = match.group(2) + trail trimmed_something = True
lead, middle, trail = '', word, '' lead, middle, trail = trim_punctuation(lead, middle, trail)
result = [] non_capturing_groups = [] consume_next = True pattern_iter = next_char(iter(pattern)) num_args = 0
try: ch, escaped = next(pattern_iter) except StopIteration: return [('', [])]
result.append(".")
raise NotImplementedError('Awaiting Implementation')
walk_to_end(ch, pattern_iter)
non_capturing_groups.append(len(result))
raise ValueError("Non-reversible reg-exp portion: '(?%s'" % ch)
count, ch = get_quantifier(ch, pattern_iter) if ch: consume_next = False
result.append(ch)
return [('', [])]
try: ch, escaped = next(input_iter) except StopIteration: ch = None if ch == '?': ch = None return int(values[0]), ch
from __future__ import unicode_literals
python_2_unicode_compatible = six.python_2_unicode_compatible
return s
return s
smart_unicode = smart_text force_unicode = force_text
return quote(force_bytes(path).replace(b"\\", b"/"), safe=b"/~!*()'")
return self[key]
return self.getlist(key)
__delitem__ = complain __delslice__ = complain __iadd__ = complain __imul__ = complain __setitem__ = complain __setslice__ = complain append = complain extend = complain insert = complain pop = complain remove = complain sort = complain reverse = complain
PY2 = sys.version_info[0] == 2 PY3 = sys.version_info[0] == 3 PY34 = sys.version_info[0:2] >= (3, 4)
MAXSIZE = int((1 << 31) - 1)
class X(object):
MAXSIZE = int((1 << 31) - 1)
MAXSIZE = int((1 << 63) - 1)
delattr(obj.__class__, self.name)
_moved_attributes = []
return sys.modules[fullname]
if sys.platform == "win32": _moved_attributes += [ MovedModule("winreg", "_winreg"), ]
class metaclass(meta):
if sys.platform.startswith('java'): memoryview = memoryview else: memoryview = buffer buffer_types = (bytearray, memoryview)
date = datetime_safe.new_datetime(date) time_str = date.strftime('%Y-%m-%dT%H:%M:%S')
ttl = force_text(ttl)
ttl = force_text(ttl)
return latest_date or datetime.datetime.utcnow().replace(tzinfo=utc)
_version = "2.0"
for cat in item['categories']: handler.addQuickElement("category", cat)
content_type = 'application/atom+xml; charset=utf-8' ns = "http://www.w3.org/2005/Atom"
if item['description'] is not None: handler.addQuickElement("summary", item['description'], {"type": "html"})
for cat in item['categories']: handler.addQuickElement("category", "", {"term": cat})
if item['item_copyright'] is not None: handler.addQuickElement("rights", item['item_copyright'])
DefaultFeed = Rss201rev2Feed
if isinstance(obj, datetime.datetime): if is_naive(obj): self.timezone = get_default_timezone() else: self.timezone = obj.tzinfo
return self.data.tzinfo.tzname(self.data) or ""
pass
return ""
return offset.days * 86400 + offset.seconds
return ''
_format_cache = {} _format_modules_cache = {}
return getattr(settings, format_type)
pass
return LocalTimezone()
return timezone.zone
return timezone.tzname(None)
value = value.astimezone(timezone) if hasattr(timezone, 'normalize'): value = timezone.normalize(value) return value
return datetime.utcnow().replace(tzinfo=utc)
return timezone.localize(value, is_dst=is_dst)
if is_aware(value): raise ValueError( "make_aware expects a naive datetime, got %s" % value) return value.replace(tzinfo=timezone)
value = value.astimezone(timezone) if hasattr(timezone, 'normalize'): value = timezone.normalize(value) return value.replace(tzinfo=None)
x = 0 for digit in str(number): x = x * len(from_digits) + from_digits.index(digit)
default = 'DEFAULT'
try: if register_to: before_import_registry = copy.copy(register_to._registry)
if register_to: register_to._registry = before_import_registry
if module_has_submodule(app_config.module, module_to_search): raise
return False
return sys.modules[name] is not None
return False
return False
def capfirst(x): return x and force_text(x)[0].upper() + force_text(x)[1:] capfirst = keep_lazy_text(capfirst)
if text.endswith(truncate): return text return '%s%s' % (text, truncate)
continue
return self.add_truncation_text(text[:end_index or 0], truncate)
return text
pos = 0 end_text_pos = 0 current_len = 0 open_tags = []
break
current_len += 1 if current_len == truncate_len: end_text_pos = pos continue
continue
for tag in open_tags: out += '</%s>' % tag return out
_(', ').join(force_text(i) for i in list_[:-1]), force_text(last_word), force_text(list_[-1]))
if not os.path.exists(filename): os.makedirs(filename)
import re
ip_str = _explode_shorthand_ip_string(ip_str)
if unpack_ipv4: ipv4_unpacked = _unpack_ipv4(ip_str)
if '.' not in hextets[index]: hextets[index] = hextets[index].lstrip('0') if not hextets[index]: hextets[index] = '0'
if hextets[index] == '0': doublecolon_len += 1 if doublecolon_start == -1: doublecolon_start = index if doublecolon_len > best_doublecolon_len: best_doublecolon_len = doublecolon_len best_doublecolon_start = doublecolon_start else: doublecolon_len = 0 doublecolon_start = -1
return ip_str
return ip_str
if ':' not in ip_str: return False
if ip_str.count('::') > 1: return False
if ':::' in ip_str: return False
if ip_str.count(':') > 7: return False
if '::' not in ip_str and ip_str.count(':') != 7: if ip_str.count('.') != 3: return False
return ip_str
if '.' in ip_str.split(':')[-1]: fill_to = 7 else: fill_to = 8
delta -= datetime.timedelta(calendar.leapdays(d.year, now.year))
role, instructions = part.split('=') role = role.upper()
opts = tuple(s for s in styles if s in opt_dict.keys()) if opts: definition['opts'] = opts
if role in PALETTES[NOCOLOR_PALETTE] and definition: palette[role] = definition
if palette == PALETTES[NOCOLOR_PALETTE]: return None return palette
update_wrapper(_wrapper, func)
if hasattr(middleware, 'process_response'): def callback(response): return middleware.process_response(request, response) response.add_post_render_callback(callback)
return [etag_str]
return _is_safe_url(url, host) and _is_safe_url(url.replace('\\', '/'), host)
if keep_blank_values: nv.append('') else: continue
del Trans
pass
_translations = {} _active = local()
_default = None
CONTEXT_SEPARATOR = "\x04"
warnings.warn("localedirs is ignored when domain is 'django'.", RuntimeWarning) localedirs = None
raise IOError("No translation files found for default language %s." % settings.LANGUAGE_CODE)
self._catalog = {}
self.plural = other.plural self._info = other._info.copy() self._catalog = other._catalog.copy()
return settings.LANGUAGE_CODE
eol_message = message.replace(str('\r\n'), str('\n')).replace(str('\r'), str('\n'))
result = type(message)("")
result = force_text(message)
result = ungettext(singular, plural, number)
for supported_code in supported_lang_codes: if supported_code.startswith(generic_lang_code + '-'): return supported_code
raw_prefix = 'u' if six.PY3 else ''
_illegal_formatting = re.compile(r"((^|[^%])(%%)*%[sy])")
sites = [] i = 0 while 1: j = text.find(substr, i) if j == -1: break sites.append(j) i = j + 1 return sites
delta = 2000 - year off = 6 * (delta // 100 + delta // 400) year = year + off
def curry(_curried_func, *args, **kwargs): def _curried(*moreargs, **morekwargs): return _curried_func(*(args + moreargs), **dict(kwargs, **morekwargs)) return _curried
if hasattr(cls, method_name): continue meth = cls.__promise__(method_name) setattr(cls, method_name, meth)
def __wrapper__(self, *args, **kw): res = func(*self.__args, **self.__kw) return getattr(res, method_name)(*args, **kw) return __wrapper__
return str(self.__cast())
return __proxy__(args, kw)
_wrapped = None
self._wrapped = empty
self.__dict__["_wrapped"] = value
def __getstate__(self): return {}
return type(self)()
return copy.copy(self._wrapped)
result = type(self)() memo[id(self)] = result return result
__dir__ = new_method_proxy(dir)
__class__ = property(new_method_proxy(operator.attrgetter("__class__"))) __eq__ = new_method_proxy(operator.eq) __ne__ = new_method_proxy(operator.ne) __hash__ = new_method_proxy(hash)
return SimpleLazyObject(self._setupfunc)
return copy.copy(self._wrapped)
result = SimpleLazyObject(self._setupfunc) memo[id(self)] = result return result
try:
import pyinotify
return
request_finished.connect(update_watch)
update_watch() notifier.check_events(timeout=None) notifier.read_events() notifier.process_events() notifier.stop()
return EventHandler.modified_code
filename = traceback.extract_tb(tb)[-1][0]
key = hashlib.sha1(key_salt + secret).digest()
def constant_time_compare(val1, val2): return hmac.compare_digest(force_bytes(val1), force_bytes(val2))
if not old_method and new_method: setattr(base, old_method_name, wrapper(new_method))
from __future__ import unicode_literals
'div': both_before + [ Tok("punct", literals("/= /"), next='reg'), ] + both_after,
tok = '"REGEX"'
tok = tok.replace("\\", "U")
result = user_function(*args, **kwds) stats[MISSES] += 1 return result
key = make_key(args, kwds, typed)
obj = super(klass, cls).__new__(cls) obj._constructor_args = (args, kwargs) return obj
EscapeUnicode = EscapeText
SafeUnicode = SafeText
self._func_path = '.'.join([func.__class__.__module__, func.__class__.__name__])
self._func_path = '.'.join([func.__module__, func.__name__])
ns_resolver = RegexURLResolver(ns_pattern, resolver.url_patterns) return RegexURLResolver(r'^/', [ns_resolver])
self._regex = regex self._regex_dict = {}
urlconf_repr = '<%s list>' % self.urlconf_name[0].__class__.__name__
sub_match_dict = dict(match.groupdict(), **self.default_kwargs) sub_match_dict.update(sub_match.kwargs)
sub_match_args = sub_match.args if not sub_match_dict: sub_match_args = match.groups() + sub_match.args
from django.conf import urls callback = getattr(urls, 'handler%s' % view_type)
_prefixes = local()
_urlconfs = local()
try: dot = callback.rindex('.') except ValueError: return callback, '' return callback[:dot], callback[dot + 1:]
if hasattr(klass, '_default_manager'): return klass._default_manager.all() return klass
if hasattr(to, 'get_absolute_url'): return to.get_absolute_url()
to = force_text(to)
if to.startswith(('./', '../')): return to
try: return reverse(to, args=args, kwargs=kwargs) except NoReverseMatch: if callable(to): raise if '/' not in to and '.' not in to: raise
return to
try: content_length = int(META.get('CONTENT_LENGTH', 0)) except (ValueError, TypeError): content_length = 0
raise MultiPartParserError("Invalid content length: %r" % content_length)
possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size] self._chunk_size = min([2 ** 31 - 4] + possible_sizes)
from django.http import QueryDict
if self._content_length == 0: return QueryDict(encoding=self._encoding), MultiValueDict()
for handler in handlers: result = handler.handle_raw_input(self._input_data, self._meta, self._content_length, self._boundary, encoding) if result is not None: return result[0], result[1]
self._post = QueryDict(mutable=True) self._files = MultiValueDict()
stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))
old_field_name = None counters = [0] * len(handlers)
num_bytes_read = 0 num_post_keys = 0 read_size = None
self.handle_file_complete(old_field_name, counters) old_field_name = None
num_post_keys += 1 if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys): raise TooManyFieldsSent( 'The number of GET/POST parameters exceeded ' 'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.' )
if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None: read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read
num_bytes_read += len(field_name) + 2 if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE): raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')
msg = "Could not decode base64 data: %r" % e six.reraise(MultiPartParserError, MultiPartParserError(msg), sys.exc_info()[2])
break
exhaust(field_stream)
old_field_name = field_name
exhaust(stream)
exhaust(self._input_data)
for handler in handlers: retval = handler.upload_complete() if retval: break
self._files.appendlist( force_text(old_field_name, self._encoding, errors='replace'), file_obj) break
for handler in self._upload_handlers: if hasattr(handler, 'file'): handler.file.close()
if remaining is None: yield b''.join(self) return
while remaining != 0: assert remaining > 0, 'remaining bytes to read should never go negative'
self._rollback = len(boundary) + 6
unused_char = self._stream.read(1) if not unused_char: raise InputStreamExhausted() self._stream.unget(unused_char)
self._done = True return chunk
chunk = stream.read(max_header_size)
header_end = chunk.find(b'\r\n\r\n')
stream.unget(chunk) return (RAW, {}, stream)
stream.unget(chunk[header_end + 4:])
for line in header.split(b'\r\n'): try: name, (value, params) = _parse_header(line) except ValueError: continue
yield parse_boundary_stream(sub_stream, 1024)
name = name[:-1] if p.count(b"'") == 2: has_encoding = True
_cookie_allows_colon_in_names = six.PY3
if isinstance(value, Morsel): dict.__setitem__(self, key, value) else: super(SimpleCookie, self).__setitem__(key, value)
key, val = str(''), chunk
cookiedict[key] = http_cookies._unquote(val)
return responses.get(self.status_code, 'Unknown Status Code')
return matched.group('charset').replace('"', '')
value.encode(charset)
value = value.decode(charset)
value.decode(charset)
value = value.encode(charset)
value = str(Header(value, 'utf-8', maxlinelen=sys.maxsize).encode())
delta = delta + datetime.timedelta(seconds=1) expires = None max_age = max(0, delta.days * 86400 + delta.seconds)
if not expires: self.cookies[key]['expires'] = cookie_date(time.time() + max_age)
if isinstance(value, bytes): return bytes(value) if isinstance(value, six.text_type): return bytes(value.encode(self.charset))
return force_bytes(value, self.charset)
def close(self): for closable in self._closable_objects: try: closable.close() except Exception: pass self.closed = True signals.request_finished.send(sender=self._handler_class)
self.content = content
self.streaming_content = streaming_content
self._iterator = iter(value) if hasattr(value, 'close'): self._closable_objects.append(value)
_encoding = None _upload_handlers = []
if settings.DEBUG: return host
location = '//%s' % self.get_full_path()
location = urljoin(current_uri, location)
self._initialize_handlers()
data = BytesIO(self._body)
_mutable = True _encoding = None
try: query_string = query_string.decode(encoding) except UnicodeDecodeError: query_string = query_string.decode('iso-8859-1')
return host, ''
browsers = [] browser = None
if hasattr(cls, 'selenium'): cls.selenium.quit() super(SeleniumTestCase, cls)._tearDownClassInternal()
if tblib is None: err = err[0], err[1], None self.check_picklable(test, err) self.events.append(('addExpectedFailure', self.test_index, err))
if not hasattr(os, 'fork'): return 1 try: return int(os.environ['DJANGO_TEST_PROCESSES']) except KeyError: return multiprocessing.cpu_count()
connection.settings_dict.update(settings_dict) connection.close()
init_worker = _init_worker run_subsuite = _run_subsuite
break
tests = self.test_loader.discover(start_dir=label, **kwargs)
self.test_loader._top_level_dir = None
parallel_units = len(parallel_suite.subsuites) if self.parallel > parallel_units: self.parallel = parallel_units
if self.parallel > 1: suite = parallel_suite
dependencies_map = {}
mirrored_aliases[alias] = test_settings['MIRROR']
item = test_databases.setdefault( connection.creation.test_db_signature(), (connection.settings_dict['NAME'], set()) ) item[1].add(alias)
for alias, mirror_alias in mirrored_aliases.items(): connections[alias].creation.set_as_test_mirror( connections[mirror_alias].settings_dict)
if isinstance(self.children[-1], six.string_types): if self.children[-1].isspace(): self.children.pop()
if len(document.children) == 1: if not isinstance(document.children[0], six.string_types): document = document.children[0] return document
try:
COMPLEX_OVERRIDE_SETTINGS = {'DATABASES'}
timezone.get_default_timezone.cache_clear()
warnings.warn("Overriding setting %s can lead to unexpected behavior." % kwargs['setting'], stacklevel=5)
if self._middleware_chain is None: self.load_middleware()
request._dont_enforce_csrf_checks = not self.enforce_csrf_checks
response = self.get_response(request)
conditional_content_removal(request, response)
response.wsgi_request = request
if response.streaming: response.streaming_content = closing_iterator_wrapper( response.streaming_content, response.close) else: request_finished.disconnect(close_old_connections)
def is_file(thing): return hasattr(thing, "read") and callable(thing.read)
match = CONTENT_TYPE_RE.match(content_type) if match: charset = match.group(1) else: charset = settings.DEFAULT_CHARSET return force_bytes(data, encoding=charset)
if self.exc_info: exc_info = self.exc_info self.exc_info = None six.reraise(*exc_info)
response.client = self response.request = request
response.templates = data.get("templates", []) response.context = data.get("context")
response.resolver_match = SimpleLazyObject(lambda: resolve(request['PATH_INFO']))
if response.context and len(response.context) == 1: response.context = response.context[0]
if response.cookies: self.cookies.update(response.cookies)
request = HttpRequest()
request.session.save()
path = url.path if not path.startswith('/'): path = urljoin(response.request['PATH_INFO'], path)
raise RedirectCycleError("Redirect loop detected.", last_response=response)
raise RedirectCycleError("Too many redirects.", last_response=response)
test_func._overridden_settings = dict( test_func._overridden_settings, **self.options)
assert not kwargs self.operations = args[0]
test_func._modified_settings = list( test_func._modified_settings) + self.operations
value = self.options[name]
if not want.startswith('<?xml'): wrapper = '<root>%s</root>' want = wrapper % want got = wrapper % got
want_root = first_node(parseString(want)) got_root = first_node(parseString(got))
captured = input()
client_class = Client _overridden_settings = None _modified_settings = None
allow_database_queries = False
self.assertEqual( response.status_code, status_code, msg_prefix + "Response didn't redirect as expected: Response code was %d (expected %d)" % (response.status_code, status_code) )
if not path.startswith('/'): url = urljoin(response.request['PATH_INFO'], url) path = urljoin(response.request['PATH_INFO'], path)
if hasattr(response, 'render') and callable(response.render) and not response.is_rendered: response.render()
contexts = to_list(response.context) if not contexts: self.fail(msg_prefix + "Response did not use any contexts to render the response")
errors = to_list(errors)
if msg_prefix: msg_prefix += ": "
contexts = to_list(response.context) if not contexts: self.fail(msg_prefix + 'Response did not use any contexts to ' 'render the response')
errors = to_list(errors)
return template_name, None, msg_prefix
return _AssertTemplateUsedContext(self, context_mgr_template)
return _AssertTemplateNotUsedContext(self, context_mgr_template)
if callable_obj is None: return cm with cm: callable_obj(*args, **kwargs)
reset_sequences = False
available_apps = None
fixtures = None
allow_database_queries = True
if getattr(cls, 'multi_db', False): return [ alias for alias in connections if include_mirrors or not connections[alias].settings_dict['TEST']['MIRROR'] ] else: return [DEFAULT_DB_ALIAS]
if self.reset_sequences: self._reset_sequences(db_name)
call_command('loaddata', *self.fixtures, **{'verbosity': 0, 'database': db_name})
for db_name in self._databases_names(include_mirrors=False): inhibit_post_migrate = ( self.available_apps is not None or
self.serialized_rollback and hasattr(connections[db_name], '_test_serialized_contents')
self.setUpTestData() return super(TestCase, self)._fixture_setup()
test_item = test_func test_item.__unittest_skip__ = CheckCondition(condition)
for alias, conn in self.connections_override.items(): connections[alias] = conn
handler = self.static_handler(_MediaFilesHandler(WSGIHandler()))
self.httpd.shutdown() self.httpd.server_close()
if conn.vendor == 'sqlite' and conn.is_in_memory_db(conn.settings_dict['NAME']): conn.allow_thread_sharing = True connections_override[conn.alias] = conn
specified_address = os.environ.get( 'DJANGO_LIVE_TEST_SERVER_ADDRESS', 'localhost:8081-8179')
cls.server_thread.is_ready.wait() if cls.server_thread.error: cls._tearDownClassInternal() raise cls.server_thread.error
if hasattr(cls, 'server_thread'): cls.server_thread.terminate() cls.server_thread.join()
for conn in connections.all(): if conn.vendor == 'sqlite' and conn.is_in_memory_db(conn.settings_dict['NAME']): conn.allow_thread_sharing = False
if isinstance(f, models.FileField): file_field_list.append(f) else: f.save_form_data(instance, cleaned_data[f.name])
opts.fields = None
self.instance = opts.model() object_data = {}
for f in self.instance._meta.fields: field = f.name if field not in self.fields: exclude.append(f.name)
elif field in self._errors.keys(): exclude.append(f.name)
opts = self._meta
if hasattr(errors, 'error_dict'): error_dict = errors.error_dict else: error_dict = {NON_FIELD_ERRORS: errors}
if self._validate_unique: self.validate_unique()
self.instance.save() self._save_m2m()
self.save_m2m = self._save_m2m
form_class_attrs = { 'Meta': Meta, 'formfield_callback': formfield_callback }
return type(form)(class_name, (form,), form_class_attrs)
unique_fields = set()
try: kwargs['initial'] = self.initial_extra[i - self.initial_form_count()] except IndexError: pass
if not qs.ordered: qs = qs.order_by(self.model._meta.pk.name)
self._queryset = qs
if obj.pk is None: continue self.deleted_objects.append(obj) self.delete_existing(obj, commit=commit)
pk_value = None if form.instance._state.adding else form.instance.pk
form.data[form.add_prefix(self._pk_field.name)] = None
form.data[form.add_prefix(self.fk.name)] = None
return self.parent_instance
if not queryset._prefetch_related_lookups: queryset = queryset.iterator() for obj in queryset: yield self.choice(obj)
default_error_messages = { 'invalid_choice': _('Select a valid choice. That choice is not one of' ' the available choices.'), }
Field.__init__(self, required, widget, label, initial, help_text, *args, **kwargs) self.queryset = queryset
result.queryset = result.queryset return result
if hasattr(self, '_choices'): return self._choices
self.run_validators(value) return qs
TOTAL_FORM_COUNT = 'TOTAL_FORMS' INITIAL_FORM_COUNT = 'INITIAL_FORMS' MIN_NUM_FORM_COUNT = 'MIN_NUM_FORMS' MAX_NUM_FORM_COUNT = 'MAX_NUM_FORMS' ORDERING_FIELD_NAME = 'ORDER' DELETION_FIELD_NAME = 'DELETE'
DEFAULT_MIN_NUM = 0
DEFAULT_MAX_NUM = 1000
self.base_fields[MIN_NUM_FORM_COUNT] = IntegerField(required=False, widget=HiddenInput) self.base_fields[MAX_NUM_FORM_COUNT] = IntegerField(required=False, widget=HiddenInput) super(ManagementForm, self).__init__(*args, **kwargs)
return min(self.management_form.cleaned_data[TOTAL_FORM_COUNT], self.absolute_max)
if initial_forms > self.max_num >= 0: total_forms = initial_forms elif total_forms > self.max_num >= 0: total_forms = self.max_num
initial_forms = len(self.initial) if self.initial else 0
forms = [self._construct_form(i, **self.get_form_kwargs(i)) for i in range(self.total_form_count())] return forms
'use_required_attribute': False,
return [self.forms[i[0]] for i in self._ordering]
if self.forms: return self.forms[0].media else: return self.empty_form.media
forms = ' '.join(form.as_table() for form in self) return mark_safe('\n'.join([six.text_type(self.management_form), forms]))
declared_fields = OrderedDict() for base in reversed(new_class.__mro__): if hasattr(base, 'declared_fields'): declared_fields.update(base.declared_fields)
for attr, value in base.__dict__.items(): if value is None and attr in declared_fields: declared_fields.pop(attr)
self.label_suffix = label_suffix if label_suffix is not None else _(':') self.empty_permitted = empty_permitted
output.append(str_hidden)
error = ValidationError(error)
if self.empty_permitted and not self.has_changed(): return
self.cleaned_data[name] = self.initial.get(name, field.initial) continue
data.append(name) continue
sup_cls = super(cls, self) try: base = sup_cls.media except AttributeError: base = Media()
final_attrs['value'] = force_text(self.format_value(value))
input_attrs['id'] = '%s_%s' % (id_, i)
return FILE_INPUT_CONTRADICTION
default_attrs = {'cols': '40', 'rows': '10'} if attrs: default_attrs.update(attrs) super(Textarea, self).__init__(default_attrs)
def boolean_check(v): return not (v is False or v is None or v == '')
self.check_test = boolean_check if check_test is None else check_test
final_attrs['value'] = force_text(value)
return False
self.choices = list(choices)
selected_choices.remove(option_value)
renderer = kwargs.pop('renderer', None) if renderer: self.renderer = renderer super(RendererMixin, self).__init__(*args, **kwargs)
if id_: id_ += '_0' return id_
if years: self.years = years else: this_year = datetime.date.today().year self.years = range(this_year, this_year + 10)
if months: self.months = months else: self.months = MONTHS
if isinstance(empty_label, (list, tuple)): if not len(empty_label) == 3: raise ValueError('empty_label list/tuple must have 3 elements.')
info = super(UserList, self).__reduce_ex__(*args, **kwargs) return info[:3] + (None, None)
default_error_messages = { 'required': _('This field is required.'), } empty_values = list(validators.EMPTY_VALUES)
creation_counter = 0
self.localize = localize if self.localize: widget.is_localized = True
widget.is_required = self.required
extra_attrs = self.widget_attrs(widget) if extra_attrs: widget.attrs.update(extra_attrs)
self.creation_counter = Field.creation_counter Field.creation_counter += 1
initial_value = initial if initial is not None else '' data_value = data if data is not None else '' return initial_value != data_value
attrs['maxlength'] = str(self.max_length)
attrs['minlength'] = str(self.min_length)
kwargs.setdefault('widget', super(IntegerField, self).widget)
step = str(Decimal('1') / 10 ** self.decimal_places).lower()
try: file_name = data.name file_size = data.size except AttributeError: raise ValidationError(self.error_messages['invalid'], code='invalid')
image = Image.open(file) image.verify()
f.image = image f.content_type = Image.MIME.get(image.format)
six.reraise(ValidationError, ValidationError( self.error_messages['invalid_image'], code='invalid_image', ), sys.exc_info()[2])
raise ValidationError(self.error_messages['invalid'], code='invalid')
url_fields[0] = 'http'
url_fields[1] = url_fields[2] url_fields[2] = '' url_fields = split_url(urlunsplit(url_fields))
return self.to_python(initial) != self.to_python(data)
if callable(value): value = CallableChoiceIterator(value) else: value = list(value)
for k2, v2 in v: if value == k2 or text_value == force_text(k2): return True
for val in value: if not self.valid_value(val): raise ValidationError( self.error_messages['invalid_choice'], code='invalid_choice', params={'value': val}, )
for f in fields: f.required = False self.fields = fields
f.required = False
if self.required: raise ValidationError(self.error_messages['required'], code='required')
if field.error_messages['incomplete'] not in errors: errors.append(field.error_messages['incomplete']) continue
errors.extend(m for m in e.error_list if m not in errors)
if not isinstance(idx, six.integer_types + (slice,)): raise TypeError return list(self.__iter__())[idx]
class datetimeobject(datetime, object): pass
try: if timezone.is_naive(value): default_timezone = timezone.get_default_timezone() value = timezone.make_aware(value, default_timezone) except Exception: return ''
if len(language[0]) > 1: return translation.get_language_info(language[0]) else: return translation.get_language_info(str(language))
raise TemplateSyntaxError( "'blocktrans' is unable to format string returned by gettext: %r using %r" % (result, data) )
self = self_wr() if self._alive: self._alive = False if callback is not None: callback(self)
NO_RECEIVERS = object()
if settings.configured and settings.DEBUG: assert callable(receiver), "Signal receivers must be callable."
if not func_accepts_kwargs(receiver): raise ValueError("Signal receivers must accept keyword arguments (**kwargs).")
if receivers is NO_RECEIVERS: return []
self.sender_receivers_cache[sender] = receivers
receiver = receiver() if receiver is not None: non_weak_receivers.append(receiver)
def reset_queries(**kwargs): for conn in connections.all(): conn.queries_log.clear() signals.request_started.connect(reset_queries)
def close_old_connections(**kwargs): for conn in connections.all(): conn.close_if_unusable_or_obsolete() signals.request_started.connect(close_old_connections) signals.request_finished.connect(close_old_connections)
if hasattr(other, 'name'): return self.name == other.name return self.name == other
open.alters_data = True
if save: self.instance.save()
if hasattr(self, '_file'): self.close() del self.file
return {'name': self.name, 'closed': False, '_committed': True, '_file': None}
file = instance.__dict__[self.field.name]
elif isinstance(file, FieldFile) and not hasattr(file, 'field'): file.instance = instance file.field = self.field file.storage = self.field.storage
elif isinstance(file, FieldFile) and instance is not file.instance: file.instance = instance
return instance.__dict__[self.field.name]
attr_class = FieldFile
descriptor_class = FileDescriptor
if value is None: return None return six.text_type(value)
file.save(file.name, file, save=False)
if hasattr(self, '_dimensions_cache'): del self._dimensions_cache super(ImageFieldFile, self).delete(save)
if not cls._meta.abstract: signals.post_init.connect(self.update_dimension_fields, sender=cls)
has_dimension_fields = self.width_field or self.height_field if not has_dimension_fields: return
file = getattr(instance, self.attname)
if not file and not force: return
if file: width = file.width height = file.height else: width = None height = None
if self.width_field: setattr(instance, self.width_field, width) if self.height_field: setattr(instance, self.height_field, height)
from __future__ import unicode_literals
BLANK_CHOICE_DASH = [("", "")]
empty_strings_allowed = True empty_values = list(validators.EMPTY_VALUES)
creation_counter = 0 auto_creation_counter = -1
'unique_for_date': _("%(field_label)s must be unique for " "%(date_field_label)s %(lookup_type)s."),
hidden = False
def _description(self): return _('Field of type: %(field_type)s') % { 'field_type': self.__class__.__name__ } description = property(_description)
if auto_created: self.creation_counter = Field.auto_creation_counter Field.auto_creation_counter -= 1 else: self.creation_counter = Field.creation_counter Field.creation_counter += 1
if isinstance(other, Field): return self.creation_counter == other.creation_counter return NotImplemented
if isinstance(other, Field): return self.creation_counter < other.creation_counter return NotImplemented
obj = Empty() obj.__class__ = self.__class__ obj.__dict__ = self.__dict__.copy() return obj
return
for optgroup_key, optgroup_value in option_value: if value == optgroup_key: return
if not getattr(cls, self.attname, None): setattr(cls, self.attname, DeferredAttribute(self.attname, cls))
return bool(value)
defaults = {'max_length': self.max_length} defaults.update(kwargs) return super(CharField, self).formfield(**defaults)
return []
default_timezone = timezone.get_default_timezone() value = timezone.make_naive(value, default_timezone)
if not prepared: value = self.get_prep_value(value) return connection.ops.adapt_datefield_value(value)
return []
if not prepared: value = self.get_prep_value(value) return connection.ops.adapt_datetimefield_value(value)
from django.db.backends import utils return utils.format_number(value, self.max_digits, self.decimal_places)
return int(round(value.total_seconds() * 1000000))
kwargs['max_length'] = kwargs.get('max_length', 254) super(EmailField, self).__init__(*args, **kwargs)
return name, path, args, kwargs
defaults = { 'form_class': forms.EmailField, } defaults.update(kwargs) return super(EmailField, self).formfield(**defaults)
defaults = {'max_length': self.max_length, 'widget': forms.Textarea} defaults.update(kwargs) return super(TextField, self).formfield(**defaults)
return []
return value.time()
if not prepared: value = self.get_prep_value(value) return connection.ops.adapt_timefield_value(value)
defaults = { 'form_class': forms.URLField, } defaults.update(kwargs) return super(URLField, self).formfield(**defaults)
if isinstance(value, six.text_type): return six.memoryview(b64decode(force_bytes(value))) return value
return type( str('RelatedObjectDoesNotExist'), (self.field.remote_field.model.DoesNotExist, AttributeError), {} )
if related is not None: setattr(related, self.field.remote_field.get_cache_name(), None)
else: for lh_field, rh_field in self.field.related_fields: setattr(instance, lh_field.attname, getattr(value, rh_field.attname))
setattr(instance, self.cache_name, value)
if value is not None and not self.field.remote_field.multiple: setattr(value, self.field.remote_field.get_cache_name(), instance)
return type( str('RelatedObjectDoesNotExist'), (self.related.related_model.DoesNotExist, AttributeError), {} )
for index, field in enumerate(self.related.field.local_related_fields): setattr(value, field.attname, related_pk[index])
setattr(instance, self.cache_name, value)
setattr(value, self.related.field.get_cache_name(), instance)
manager = getattr(self.model, kwargs.pop('manager')) manager_class = create_reverse_many_to_one_manager(manager.__class__, rel) return manager_class(self.instance)
queryset.update(**{self.field.name: None})
objs = tuple(objs)
return self.rel.through
if instance.pk is None: raise ValueError("%r instance needs to have a primary key value before " "a many-to-many relationship can be used." % instance.__class__.__name__)
manager = getattr(self.model, kwargs.pop('manager')) manager_class = create_forward_many_to_many_manager(manager.__class__, rel, reverse) return manager_class(instance=self.instance)
if self.symmetrical: self._add_items(self.target_field_name, self.source_field_name, *objs)
objs = tuple(objs)
if created: self.add(obj) return obj, created
if created: self.add(obj) return obj, created
signals.m2m_changed.send( sender=self.through, action='pre_add', instance=self.instance, reverse=self.reverse, model=self.model, pk_set=new_ids, using=db, )
signals.m2m_changed.send( sender=self.through, action='post_add', instance=self.instance, reverse=self.reverse, model=self.model, pk_set=new_ids, using=db, )
if not objs: return
auto_created = True concrete = False editable = False is_relation = True
null = True
self.field_name = None
from django.db.models.sql.where import WhereNode, SubqueryConstraint, AND, OR
if relation == RECURSIVE_RELATIONSHIP_CONSTANT: relation = scope_model
if isinstance(relation, six.string_types): if "." not in relation: relation = "%s.%s" % (scope_model._meta.app_label, relation)
one_to_many = False one_to_one = False many_to_many = False many_to_one = False
apps.check_models_ready() return self.remote_field.model
if not isinstance(self.remote_field.model, ModelBase): return []
rel_opts = self.remote_field.model._meta rel_is_hidden = self.remote_field.is_hidden()
potential_clashes = rel_opts.fields + rel_opts.many_to_many for clash_field in potential_clashes:
potential_clashes = (r for r in rel_opts.related_objects if r.field is not self) for clash_field in potential_clashes:
return None
limit_choices_to = self.remote_field.limit_choices_to defaults.update({ 'limit_choices_to': limit_choices_to, })
many_to_many = False many_to_one = True one_to_many = False one_to_one = False
many_to_many = False many_to_one = True one_to_many = False one_to_one = False
to_field = to_field or (to._meta.pk and to._meta.pk.name)
many_to_many = False many_to_one = False one_to_many = False one_to_one = True
return []
many_to_many = True many_to_one = False one_to_many = False one_to_one = False
to = str(to)
if self_referential: seen_self = sum( from_model == getattr(field.remote_field, 'model', None) for field in self.remote_field.through._meta.fields )
else: assert from_model is not None, ( "ManyToManyField with intermediate " "tables cannot be checked if you don't pass the model " "where the field is attached to." )
if found: setattr(self, cache_attr, getattr(f, attr)) break else: found = True
setattr(cls, self.name, ManyToManyDescriptor(self.remote_field, reverse=False))
self.m2m_db_table = curry(self._get_m2m_db_table, cls._meta)
if not self.remote_field.is_hidden() and not related.related_model._meta.swapped: setattr(cls, related.get_accessor_name(), ManyToManyDescriptor(self.remote_field, reverse=True))
self.m2m_column_name = curry(self._get_m2m_attr, related, 'column') self.m2m_reverse_name = curry(self._get_m2m_reverse_attr, related, 'column')
return None
assert False, "Tried to Extract from an invalid type."
value = value.date()
return self.as_sql(compiler, connection, template='%(expressions)s::%(db_type)s')
if self.output_field.get_internal_type() == 'TextField': class ToNCLOB(Func): function = 'TO_NCLOB'
return super(ConcatPair, self).as_sql( compiler, connection, function='CONCAT_WS', template="%(function)s('', %(expressions)s)" )
c = self.copy() expressions = [ Coalesce(expression, Value('')) for expression in c.get_source_expressions() ] c.set_source_expressions(expressions) return c
if len(expressions) == 2: return ConcatPair(*expressions) return ConcatPair(expressions[0], self._paired(expressions[1:]))
return self.as_sql(compiler, connection, template='STATEMENT_TIMESTAMP()')
PathInfo = namedtuple('PathInfo', 'from_opts to_opts target_fields join_field m2m direct')
AND = 'AND' OR = 'OR' default = AND
clause, joins = query._add_q(self, reuse, allow_joins=allow_joins, split_subq=False) query.promote_joins(joins) return clause
for parent in inspect.getmro(self.__class__): if 'class_lookups' not in parent.__dict__: continue if lookup_name in parent.class_lookups: return parent.class_lookups[lookup_name]
pass
LOOKUP_SEP = '__'
return (unpickle_inner_exception, (attached_to, name), self.args)
parents = [b for b in bases if isinstance(b, ModelBase)] if not parents: return super_new(cls, name, bases, attrs)
app_config = apps.get_containing_app_config(module)
if is_proxy and base_meta and base_meta.swapped: raise TypeError("%s cannot proxy the swapped model '%s'." % (name, base_meta.swapped))
for obj_name, obj in attrs.items(): new_class.add_to_class(obj_name, obj)
new_fields = chain( new_class._meta.local_fields, new_class._meta.local_many_to_many, new_class._meta.private_fields ) field_names = {f.name for f in new_fields}
if not hasattr(new_class, attr_name): new_class.add_to_class(attr_name, field)
new_class._meta.parents.update(base_parents)
attr_meta.abstract = False new_class.Meta = attr_meta return new_class
if not inspect.isclass(value) and hasattr(value, 'contribute_to_class'): value.contribute_to_class(cls, name) else: setattr(cls, name, value)
for manager in opts.managers: originating_model = manager._originating_model if (cls is originating_model or cls._meta.proxy or originating_model._meta.abstract):
self.adding = True
self._state = ModelState()
for val, field in zip(args, fields_iter): if val is DEFERRED: continue setattr(self, field.attname, val)
continue
if len(update_fields) == 0: return
if not meta.auto_created: signals.post_save.send(sender=origin, instance=self, created=(not updated), update_fields=update_fields, raw=raw, using=using)
if name in exclude: break
date_checks = []
continue
continue
if not self._state.adding and self.pk is not None: qs = qs.exclude(pk=self.pk)
try: self.clean() except ValidationError as e: errors = e.update_error_dict(errors)
fields = (f for f in fields if isinstance(f.remote_field.model, ModelBase))
fields = (f for f in fields if isinstance(f.remote_field.through, ModelBase))
for parent in cls._meta.get_parent_list(): for f in parent._meta.get_fields(): if f not in used_fields: used_fields[f.name] = f
used_column_names = [] errors = []
forward_fields_map = { field.name: field for field in cls._meta._get_fields(reverse=False) }
fields = (f for f in fields if f != '?')
fields = ((f[1:] if f.startswith('-') else f) for f in fields)
fields = (f for f in fields if '__' not in f)
fields = {f for f in fields if f != 'pk'}
invalid_fields = []
with transaction.atomic(using=using, savepoint=False): for i, j in enumerate(id_list): ordered_obj.objects.filter(pk=j, **filter_args).update(_order=i)
model = model_id
exception = getattr(klass, exception_name) return exception.__new__(exception)
REPR_OUTPUT_SIZE = 20
EmptyResultSet = sql.EmptyResultSet
names = extra_names + field_names + annotation_names
names = extra_names + field_names + annotation_names
fields = list(queryset._fields) + [f for f in annotation_names if f not in queryset._fields]
from django.db.models.manager import Manager manager = Manager.from_queryset(cls)() manager._built_with_as_manager = True return manager
self._fetch_all() obj_dict = self.__dict__.copy() obj_dict[DJANGO_VERSION_PICKLE_KEY] = get_version() return obj_dict
try: arg.default_alias except (AttributeError, TypeError): raise TypeError("Complex aggregates require an alias") kwargs[arg.default_alias] = arg
self._for_write = True try: return self.get(**lookup), False except self.model.DoesNotExist: return self._create_object_from_params(lookup, params)
del_query._for_write = True
del_query.query.select_for_update = False del_query.query.select_related = False del_query.query.clear_ordering(force_empty=True)
self._result_cache = None return deleted, _rows_count
prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups) self._prefetch_done = True
field_names = list(fields)
if len(self._fields or self.model._meta.concrete_fields) > 1: raise TypeError('Cannot use multi-field values as a filter value.')
foreign_fields = getattr(field, 'foreign_related_fields', ()) if len(foreign_fields) == 1 and not foreign_fields[0].primary_key: return self.values(foreign_fields[0].name)
value_annotation = True
self._hints.update(hints)
if self._fields is not None: return True return check_rel_lookup_compatibility(self.model, opts, field)
db = self.db compiler = connections[db].ops.compiler('SQLCompiler')( self.query, connections[db], db )
if hasattr(self.query, 'cursor') and self.query.cursor: self.query.cursor.close()
for (query_name, model_name) in self.translations.items(): try: index = self._columns.index(query_name) self._columns[index] = model_name except ValueError: pass
obj_list = model_instances
if len(obj_list) == 0: break
obj_list = done_queries[prefetch_to] continue
first_obj = obj_list[0] to_attr = lookup.get_current_to_attr(level)[0] prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
raise ValueError("'%s' does not resolve to an item that supports " "prefetching - this is an invalid parameter to " "prefetch_related()." % lookup.prefetch_through)
leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
qs._prefetch_done = True obj._prefetched_objects_cache[cache_name] = qs
ADD = '+' SUB = '-' MUL = '*' DIV = '/' POW = '^' MOD = '%%'
BITAND = '&' BITOR = '|'
if isinstance(other, datetime.timedelta): other = DurationValue(other, output_field=fields.DurationField()) else: other = Value(other)
is_summary = False _output_field = None
return super(Expression, self).as_sql(compiler, connection)
expression_wrapper = '(%s)' sql = connection.ops.combine_expression(self.connector, expressions) return expression_wrapper % sql, expression_params
expression_wrapper = '(%s)' sql = connection.ops.combine_duration_expression(self.connector, expressions) return expression_wrapper % sql, expression_params
return self
return [self.result._output_field_or_none]
cols = [] for source in self.get_source_expressions(): cols.extend(source.get_group_by_cols()) return cols
AND = 'AND' OR = 'OR'
if empty_needed == 0: if self.negated: return '', [] else: raise EmptyResultSet if full_needed == 0: if self.negated: raise EmptyResultSet else: return '', []
sql_string = 'NOT (%s)' % sql_string
child.relabel_aliases(change_map)
contains_aggregate = False
contains_aggregate = False
GET_ITERATOR_CHUNK_SIZE = 100
MULTI = 'multi' SINGLE = 'single' CURSOR = 'cursor' NO_RESULTS = 'no results'
INNER = 'INNER JOIN' LOUTER = 'LEFT OUTER JOIN'
self.names_with_path = path_with_names
self._execute_query() if not connections[self.using].features.can_use_chunked_reads: result = list(self.cursor) else: result = self.cursor return iter(result)
self.max_depth = 5
self.values_select = []
self.deferred_loading = (set(), True)
change_map = {} conjunction = (connector == AND)
w = rhs.where.clone() w.relabel_aliases(change_map) self.where.add(w, connector)
self.select = [] for col in rhs.select: self.add_select(col.relabeled_clone(change_map))
if self._extra and rhs._extra: raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
self.order_by = rhs.order_by[:] if rhs.order_by else self.order_by self.extra_order_by = rhs.extra_order_by or self.extra_order_by
if not is_reverse_o2o(source): must_include[old_model].add(source) add_to_dict(must_include, cur_model, opts.pk)
continue
aliases.extend( join for join in self.alias_map.keys() if self.alias_map[join].parent_alias == alias and join not in aliases )
return
value, lookups, used_joins = self.prepare_lookup_value(value, lookups, can_reuse, allow_joins)
if isinstance(value, Iterator): value = list(value) self.check_related_objects(field, value, opts)
self._lookup_joins = join_list
num_lookups = len(lookups) if num_lookups > 1: raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
proxied_model = opts.concrete_model
path, final_field, targets, rest = self.names_to_path( names, opts, allow_many, fail_on_missing=True)
return Ref(name, self.annotation_select[name])
query = Query(self.model) query.add_filter(filter_expr) query.clear_ordering(True) trimmed_prefix, contains_louter = query.trim_start(names_with_path)
raise
self.deferred_loading = field_names.difference(existing), False
self.deferred_loading = field_names, False
try: return self._loaded_field_names_cache except AttributeError: collection = {} self.deferred_to_data(collection, self.get_loaded_field_names_cb) self._loaded_field_names_cache = collection return collection
self.votes = Counter()
order_by.append(( OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending), True)) continue
order_by.append(( OrderBy(self.query.annotations[col], descending=descending), False)) continue
order_by.extend(self.find_ordering_name( field, self.query.get_meta(), default_order=asc))
from_, f_params = self.get_from_clause()
self.query.reset_refcounts(refcounts_before)
obj.clear_ordering(True)
seen_models = {None: start_alias}
continue
if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1: result.append(', %s' % self.quote_name_unless_alias(alias))
return related_klass_infos
fields_found = set() if requested is None: if isinstance(self.query.select_related, dict): requested = self.query.select_related restricted = True else: restricted = False
return cursor
cursor.close()
return list(result)
cursor.close()
sql, params = val, []
sql, params = self.compile(val)
sql, params = field.get_placeholder(val, self, self.connection), [val]
sql, params = '%s', [val]
rows_of_fields_as_sql = ( (self.field_as_sql(field, v) for field, v in zip(fields, row)) for row in value_rows )
sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)
placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)
param_rows = [[p for ps in row for p in ps] for row in param_rows]
qn = self.connection.ops.quote_name opts = self.query.get_meta() result = ['INSERT INTO %s' % qn(opts.db_table)]
value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs] fields = [None]
can_bulk = (not self.return_id and self.connection.features.has_bulk_insert)
if r_fmt: result.append(r_fmt % col) params += [r_params] return [(" ".join(result), tuple(chain.from_iterable(params)))]
return tuple(tuple(ot) for ot in option_together)
return option_together
self.related_fkey_lookups = []
self.apps = self.default_apps
return self.apps.app_configs.get(self.app_label)
self.object_name = cls.__name__ self.model_name = self.object_name.lower() self.verbose_name = camel_case_to_spaces(self.object_name)
self.original_attrs = {}
if self.verbose_name_plural is None: self.verbose_name_plural = string_concat(self.verbose_name, 's')
self._ordering_clash = bool(self.ordering and self.order_with_respect_to)
if meta_attrs != {}: raise TypeError("'class Meta' got invalid attribute(s): %s" % ','.join(meta_attrs.keys()))
return swapped_for
manager._originating_model = base
for parent in self.model.mro()[1:]: if hasattr(parent, '_meta'): default_manager_name = parent._meta.default_manager_name break
try: res[field.attname] = field except AttributeError: pass
try: res[field.attname] = field except AttributeError: pass
return self._forward_fields_map[field_name]
return self.fields_map[field_name]
parent_link = parent._meta.get_ancestor_link(ancestor) if parent_link: return self.parents[parent] or parent_link
topmost_call = False if seen_models is None: seen_models = set() topmost_call = True seen_models.add(self.model)
cache_key = (forward, reverse, include_parents, include_hidden, topmost_call)
return self._get_fields_cache[cache_key]
fields = make_immutable_fields_list("get_fields()", fields)
self._get_fields_cache[cache_key] = fields return fields
creation_counter = 0
auto_created = False
obj = super(BaseManager, cls).__new__(cls) obj._constructor_args = (args, kwargs) return obj
return (
from django.db.models.query import QuerySet if isinstance(rhs, QuerySet): raise NotImplementedError("Bilateral transformations on nested querysets are not supported.")
value = Value(value, output_field=self.lhs.output_field)
sqls, sqls_params = self.batch_process_rhs(compiler, connection, rhs) placeholder = '(' + ', '.join(sqls) + ')' return (placeholder, sqls_params)
return self.batch_process_rhs(compiler, connection)
return ( f for f in opts.get_fields(include_hidden=True) if f.auto_created and not f.concrete and (f.one_to_one or f.one_to_many) )
self.data = OrderedDict()
self.fast_deletes = []
sub_objs = field.bulk_related_objects(new_objs, self.using) self.collect(sub_objs, source=model, nullable=True)
for model, instances in self.data.items(): self.data[model] = sorted(instances, key=attrgetter("pk"))
self.sort() deleted_counter = Counter()
for model, obj in self.instances_with_model(): if not model._meta.auto_created: signals.pre_delete.send( sender=model, instance=obj, using=self.using )
for qs in self.fast_deletes: count = qs._raw_delete(using=self.using) deleted_counter[qs.model._meta.label] += count
for instances in six.itervalues(self.data): instances.reverse()
if getattr(field, 'remote_field', None) is None: field_type = field.db_type(connection)
if field_type is None: return errors
integer_field_ranges = dict( BaseDatabaseOperations.integer_field_ranges, PositiveSmallIntegerField=(0, 65535), PositiveIntegerField=(0, 4294967295), )
if lookup_type == 'week_day': return "DAYOFWEEK(%s)" % field_name else: return "EXTRACT(%s FROM %s)" % (lookup_type.upper(), field_name)
return 'MATCH (%s) AGAINST (%%s IN BOOLEAN MODE)' % field_name
return force_text(getattr(cursor, '_last_executed', None), errors='replace')
return 18446744073709551615
if value == 0: raise ValueError('The database backend does not accept 0 as a ' 'value for AutoField.') return value
if timezone.is_aware(value): raise ValueError("MySQL backend does not support timezone-aware times.")
import MySQLdb.converters return MySQLdb.escape(value, MySQLdb.converters.conversions)
field.db_index = False
return self.cursor.execute(query, args)
if e.args[0] in self.codes_for_integrityerror: six.reraise(utils.IntegrityError, utils.IntegrityError(*tuple(e.args)), sys.exc_info()[2]) raise
if e.args[0] in self.codes_for_integrityerror: six.reraise(utils.IntegrityError, utils.IntegrityError(*tuple(e.args)), sys.exc_info()[2]) raise
self.close()
kwargs['client_flag'] = CLIENT.FOUND_ROWS kwargs.update(settings_dict['OPTIONS']) return kwargs
cursor.execute('SET SQL_AUTO_IS_NULL = 0')
self.needs_rollback, needs_rollback = False, self.needs_rollback try: self.cursor().execute('SET foreign_key_checks=1') finally: self.needs_rollback = needs_rollback
return self.connection.mysql_version >= (5, 6, 4) and Database.version_info >= (1, 2, 5)
with self.connection.cursor() as cursor: cursor.execute("SELECT 1 FROM mysql.time_zone LIMIT 1") return cursor.fetchone() is not None
VERSION_RE = re.compile(r'\S+ (\d+)\.(\d+)\.?(\d+)?')
return "DATE_TRUNC('%s', %s)" % (lookup_type, field_name)
sql = "DATE_TRUNC('%s', %s)" % (lookup_type, field_name) return sql, params
if lookup_type in ('iexact', 'icontains', 'istartswith', 'iendswith'): lookup = 'UPPER(%s)' % lookup
cursor.execute("SELECT CURRVAL(pg_get_serial_sequence('%s','%s'))" % ( self.quote_name(table_name), pk_name)) return cursor.fetchone()[0]
if cursor.query is not None: return cursor.query.decode('utf-8') return None
self.connection.close()
INETARRAY_OID = 1041 INETARRAY = psycopg2.extensions.new_array_type( (INETARRAY_OID,), 'INETARRAY', psycopg2.extensions.UNICODE, ) psycopg2.extensions.register_type(INETARRAY)
if not self.get_autocommit(): self.connection.commit()
self.connection.cursor().execute("SELECT 1")
if constraint not in constraints: constraints[constraint] = { "columns": [], "primary_key": pk, "unique": unique, "foreign_key": None, "check": check,
return "TO_CHAR(%s, 'D')" % field_name
return "EXTRACT(%s FROM %s)" % (lookup_type.upper(), field_name)
_tzname_re = re.compile(r'^[\w/:+-]+$')
return "CAST(%s AS TIMESTAMP)" % field_name
field = expression.output_field if value is None and field.empty_strings_allowed: value = '' if field.get_internal_type() == 'BinaryField': value = b'' return value
statement = cursor.statement
return super(DatabaseOperations, self).last_executed_query(cursor, statement, params)
break
if timezone.is_aware(value): raise ValueError("Oracle backend does not support timezone-aware times.")
if 'ORA-22858' in description or 'ORA-22859' in description: self._alter_field_type_workaround(model, old_field, new_field) else: raise
acceptable_ora_err = 'ORA-01543' if keepdb else None self._execute_allow_fail_statements(cursor, statements, parameters, verbosity, acceptable_ora_err)
allow_quiet_fail = acceptable_ora_err is not None and len(acceptable_ora_err) > 0 self._execute_statements(cursor, statements, parameters, verbosity, allow_quiet_fail=allow_quiet_fail) return True
('NLS_LANG', '.UTF8'), ('ORA_NCHAR_LITERAL_REPLACE', 'TRUE'),
if instance is None: raise AttributeError("operators not available as class attribute") instance.cursor().close() return instance.__dict__['operators']
pass
self.cursor.numbersAsStrings = True self.cursor.arraysize = 100
if hasattr(params, 'items'): return {k: v.force_bytes for k, v in params.items()} else: return [p.force_bytes for p in params]
pass
supports_select_for_update_with_limit = False supports_temporal_subtraction = True
TableInfo = namedtuple('TableInfo', ['name', 'type'])
FieldInfo = namedtuple('FieldInfo', 'name type_code display_size internal_size precision scale null_ok')
if f.remote_field.through is None: sequence_list.append({'table': f.m2m_db_table(), 'column': None})
raise NotImplementedError('Full-text search is not implemented for this database backend')
prep_for_iexact_query = prep_for_like_query
if callable(default): default = default() default = field.get_db_prep_save(default, self.connection) return default
self.deferred_sql.extend(self._model_indexes_sql(model))
for field in model._meta.local_many_to_many: if field.remote_field.through._meta.auto_created: self.create_model(field.remote_field.through)
for field in model._meta.local_many_to_many: if field.remote_field.through._meta.auto_created: self.delete_model(field.remote_field.through)
self.execute(self.sql_delete_table % { "table": self.quote_name(model._meta.db_table), })
TEST_DATABASE_PREFIX = 'test_'
from django.core.management import call_command
call_command( 'migrate', verbosity=max(verbosity - 1, 0), interactive=False, database=self.connection.alias, run_syncdb=True, )
if serialize: self.connection._test_serialized_contents = self.serialize_db_to_string()
self.connection.ensure_connection()
self._clone_test_db(number, verbosity, keepdb)
orig_settings_dict = self.connection.settings_dict new_settings_dict = orig_settings_dict.copy() new_settings_dict['NAME'] = '{}_{}'.format(orig_settings_dict['NAME'], number) return new_settings_dict
if not keepdb: self._destroy_test_db(test_database_name, verbosity)
if old_database_name is not None: settings.DATABASES[self.connection.alias]["NAME"] = old_database_name self.connection.settings_dict["NAME"] = old_database_name
data_types = {} data_types_suffix = {} data_type_check_constraints = {} ops = None vendor = 'unknown' SchemaEditorClass = None
self.close_at = None self.closed_in_transaction = False self.errors_occurred = False
self.allow_thread_sharing = allow_thread_sharing self._thread_ident = thread.get_ident()
self.run_on_commit = []
self.run_commit_hooks_on_set_autocommit_on = False
return pytz.timezone(self.settings_dict['TIME_ZONE'])
self.errors_occurred = False self.run_commit_hooks_on_set_autocommit_on = True
self.errors_occurred = False
return self.features.uses_savepoints and not self.get_autocommit()
self.run_on_commit = [ (sids, func) for (sids, func) in self.run_on_commit if sid not in sids ]
if self.get_autocommit() != self.settings_dict['AUTOCOMMIT']: self.close() return
if self.errors_occurred: if self.is_usable(): self.errors_occurred = False else: self.close() return
self.run_on_commit.append((set(self.savepoint_ids), func))
func()
executable_name = None
self.connection = connection
interprets_empty_strings_as_nulls = False
supports_nullable_unique_constraints = True
supports_partially_nullable_unique_constraints = True
related_fields_match_type = False allow_sliced_subqueries = True has_select_for_update = False has_select_for_update_nowait = False
test_db_allows_multiple_connections = True
supports_unspecified_pk = False
supports_forward_references = True
truncates_names = False
has_real_datatype = False supports_subqueries_in_group_by = True supports_bitwise_or = True
has_native_uuid_field = False
has_native_duration_field = False
supports_temporal_subtraction = False
driver_supports_timedelta_args = False
supports_microsecond_precision = True
supports_regex_backreferencing = True
supports_date_lookup_using_string = True
supports_timezones = True
has_zoneinfo_database = True
requires_explicit_null_ordering_when_grouping = False
nulls_order_largest = False
supports_1000_query_parameters = True
allows_auto_pk_0 = True
can_defer_constraint_checks = False
supports_mixed_date_datetime_comparisons = True
supports_tablespaces = False
supports_sequence_reset = True
can_introspect_max_length = True
can_introspect_null = True
can_introspect_default = True
can_introspect_foreign_keys = True
can_introspect_autofield = False
can_introspect_big_integer_field = True
can_introspect_binary_field = True
can_introspect_decimal_field = True
can_introspect_ip_address_field = False
can_introspect_positive_integer_field = False
can_introspect_small_integer_field = False
can_introspect_time_field = True
can_distinct_on_fields = False
autocommits_when_autocommit_is_off = False
atomic_transactions = True
can_rollback_ddl = False
supports_combined_alters = False
supports_foreign_keys = True
supports_column_check_constraints = True
supports_paramstyle_pyformat = True
requires_literal_defaults = False
connection_persists_old_columns = False
closed_cursor_error_class = ProgrammingError
has_case_insensitive_like = True
requires_sqlparse_for_splitting = True
bare_select_suffix = ''
implied_column_null = False
supports_select_for_update_with_limit = True
greatest_least_ignores_nulls = False
can_clone_databases = False
relations = {}
for field_desc in results.split(','): field_desc = field_desc.strip() if field_desc.startswith("UNIQUE"): continue
for field_index, field_desc in enumerate(results.split(',')): field_desc = field_desc.strip() if field_desc.startswith("UNIQUE"): continue
key_columns.append(tuple(s.strip('"') for s in m.groups()))
return [{ 'name': field[1], 'type': field[2], 'size': get_field_size(field[2]), 'null_ok': not field[3], 'default': field[4],
pass
return "django_date_extract('%s', %s)" % (lookup_type.lower(), field_name)
return "django_date_trunc('%s', %s)" % (lookup_type.lower(), field_name)
self._require_pytz() return "django_datetime_extract('%s', %s, %%s)" % ( lookup_type.lower(), field_name), [tzname]
self._require_pytz() return "django_datetime_trunc('%s', %s, %%s)" % ( lookup_type.lower(), field_name), [tzname]
return "django_time_extract('%s', %s)" % (lookup_type.lower(), field_name)
cursor = self.connection.connection.cursor() try: return cursor.execute(sql, params).fetchone() finally: cursor.close()
if timezone.is_aware(value): raise ValueError("SQLite backend does not support timezone-aware times.")
if connector == '^': return 'django_power(%s)' % ','.join(sub_expressions) return super(DatabaseOperations, self).combine_expression(connector, sub_expressions)
return (None, None)
c.execute('PRAGMA foreign_keys = %s' % int(self._initial_pragma_fk))
body = copy.deepcopy(body)
if override_uniques is None: override_uniques = [ [rename_mapping.get(n, n) for n in unique] for unique in model._meta.unique_together ]
if override_indexes is None: override_indexes = [ [rename_mapping.get(n, n) for n in index] for index in model._meta.index_together ]
@contextlib.contextmanager def altered_table_name(model, temporary_table_name): original_table_name = model._meta.db_table model._meta.db_table = temporary_table_name yield model._meta.db_table = original_table_name
self.alter_db_table(model, temp_model._meta.db_table, model._meta.db_table)
self.deferred_sql = [x for x in self.deferred_sql if temp_model._meta.db_table not in x] self.create_model(temp_model)
self.delete_model(model, handle_autom2m=False)
for sql in self.deferred_sql: self.execute(sql) self.deferred_sql = [] if restore_pk_field: restore_pk_field.primary_key = True
self.execute(self.sql_delete_table % { "table": self.quote_name(model._meta.db_table), })
if field.many_to_many and field.remote_field.through._meta.auto_created: return self.create_model(field.remote_field.through) self._remake_table(model, create_fields=[field])
self._remake_table(model, alter_fields=[(old_field, new_field)])
os.remove(test_database_name)
if not self.is_in_memory_db(self.settings_dict['NAME']): BaseDatabaseWrapper.close(self)
level = ''
return str(out)
_cursor = complain ensure_connection = complain _commit = complain _rollback = ignore _close = ignore _savepoint = ignore _savepoint_commit = complain _savepoint_rollback = ignore _set_autocommit = complain
try: self.close() except self.db.Database.Error: pass
return ( super(ModelOperation, self).reduce(operation, in_between, app_label=app_label) or not operation.references_model(self.new_name, app_label) )
ALTER_OPTION_KEYS = [ "get_latest_by", "managed", "ordering", "permissions", "default_permissions", "select_on_save", "verbose_name", "verbose_name_plural", ]
reversible = True
reduces_to_sql = True
atomic = False
elidable = False
self = object.__new__(cls) self._constructor_args = (args, kwargs) return self
for database_operation in self.database_operations: to_state = from_state.clone() database_operation.state_forwards(app_label, to_state) database_operation.database_forwards(app_label, schema_editor, from_state, to_state) from_state = to_state
pass
return ( super(FieldOperation, self).reduce(operation, in_between, app_label=app_label) or not operation.references_field(self.model_name, self.new_name, app_label) )
enum = None
for arg_name in operation_args[i:]:
try: migrations_module = import_module(migrations_package_name) except ImportError: pass else: try: return upath(module_dir(migrations_module)) except ValueError: pass
from __future__ import unicode_literals
continue
if key[0] == parent[0]: continue parent = self.check_key(parent, key[0]) if parent is not None: self.graph.add_dependency(migration, key, parent, skip_validation=True)
enum = None
value_repr = 'b' + value_repr
value = "(%s)" if len(strings) != 1 else "(%s,)" return value % (", ".join(strings)), imports
return string.rstrip(','), imports
return "set([%s])"
value_repr = value_repr[1:]
return "(%s)" if len(self.value) != 1 else "(%s,)"
value = value.__reduce__()[1][0]
if hasattr(value, 'deconstruct'): return DeconstructableSerializer(value)
stack.extendleft(children)
return self.defaults.get("ask_initial", False)
return None
return None
return None
code = input(prompt)
sys.exit(3)
return NOT_PROVIDED
sys.exit(3)
self._iterations = 0 while True: result = self.optimize_inner(operations, app_label) self._iterations += 1 if result == operations: return result operations = result
operations = []
dependencies = []
run_before = []
replaces = []
atomic = True
full_plan = self.migration_plan(self.loader.graph.leaf_nodes(), clean_start=True)
raise InvalidMigrationPlan( "Migration plans with both forwards and backwards migrations " "are not supported. Please split your migration process into " "separate plans of only forwards OR backwards migrations.", plan )
state = self._migrate_all_backwards(plan, full_plan, fake=fake)
break
migration.mutate_state(state, preserve=False) applied_migrations.remove(migration)
states[migration] = state state = migration.mutate_state(state, preserve=True) migrations_to_run.remove(migration)
migration.mutate_state(state, preserve=False)
applied, state = self.detect_soft_applied(state, migration) if applied: fake = True
with self.connection.schema_editor(atomic=migration.atomic) as schema_editor: state = migration.apply(state, schema_editor)
if self.progress_callback: self.progress_callback("apply_success", migration, fake) return state
if any(app == migration.app_label for app, name in migration.dependencies): return False, project_state
return False, project_state
if field.many_to_many: if field.remote_field.through._meta.db_table not in existing_table_names: return False, project_state else: found_add_field_migration = True continue
return (found_create_model_migration or found_add_field_migration), after_state
opts = m._meta if opts.proxy and m in related_fields_models: related_models.append(opts.concrete_model) return related_models
self.real_apps = real_apps or []
related_models = get_related_models_recursive(old_model)
related_models.add((app_label, model_name))
with self.apps.bulk_update(): for rel_app_label, rel_model_name in related_models: self.apps.unregister_model(rel_app_label, rel_model_name)
for model_state in self.apps.real_models: if (model_state.app_label, model_state.name_lower) in related_models: states_to_be_rendered.append(model_state)
for rel_app_label, rel_model_name in related_models: try: model_state = self.models[rel_app_label, rel_model_name] except KeyError: pass else: states_to_be_rendered.append(model_state)
self.apps.render_multiple(states_to_be_rendered)
path = ''
super(AppConfigStub, self).__init__(label, None)
ready = self.ready self.ready = False try: yield finally: self.ready = ready self.clear_cache()
clone.real_models = self.real_models return clone
if not model._default_manager.auto_created: if model._default_manager.use_in_migrations: default_manager = copy.copy(model._default_manager) default_manager._set_creation_counter()
else: default_manager = models.Manager() default_manager.model = model default_manager.name = model._default_manager.name managers.append((force_text(default_manager.name), default_manager))
return cls( model._meta.app_label, model._meta.object_name, fields, options, bases, managers, )
body.update(self.construct_managers())
return type( str(self.name), bases, body, )
return obj
deconstructed = deconstructed[1:]
self.generated_operations = {}
self.generate_renamed_models()
self._prepare_field_lists() self._generate_through_model_map()
self.generate_deleted_models() self.generate_created_models() self.generate_deleted_proxies() self.generate_created_proxies() self.generate_altered_options() self.generate_altered_managers()
self.generated_operations[app_label] = stable_topological_sort(ops, dependency_graph)
for app_label, migrations in self.migrations.items(): for migration in migrations: migration.dependencies = list(set(migration.dependencies))
for app_label, migrations in self.migrations.items(): for migration in migrations: migration.operations = MigrationOptimizer().optimize(migration.operations, app_label=app_label)
if not model_opts.managed: continue
continue
dependencies=[ (app_label, model_name, field_name, "order_wrt_unset"), (app_label, model_name, field_name, "foo_together_change"), ],
set(self.old_unmanaged_keys).intersection(self.new_model_keys)
set(self.old_model_keys).intersection(self.new_unmanaged_keys)
if self.savepoint and not connection.needs_rollback: sid = connection.savepoint() connection.savepoint_ids.append(sid) else: connection.savepoint_ids.append(None)
connection.in_atomic_block = False
pass
try: connection.commit() except DatabaseError: try: connection.rollback() except Error: connection.close() raise
if callable(using): return Atomic(DEFAULT_DB_ALIAS, savepoint)(using) else: return Atomic(using, savepoint)
if dj_exc_type not in (DataError, IntegrityError): self.wrapper.errors_occurred = True six.reraise(dj_exc_type, dj_exc_value, traceback)
def inner(*args, **kwargs): with self: return func(*args, **kwargs) return inner
if backend_name == 'django.db.backends.postgresql_psycopg2': backend_name = 'django.db.backends.postgresql'
pass
pass
continue
EMPTY_VALUES = (None, '', [], (), {})
if isinstance(regex, six.string_types): return re.compile(regex, flags) else: assert not flags, "flags must be empty if regex is passed pre-compiled" return regex
ipv4_re = r'(?:25[0-5]|2[0-4]\d|[0-1]?\d?\d)(?:\.(?:25[0-5]|2[0-4]\d|[0-1]?\d?\d)){3}'
scheme = value.split('://')[0].lower() if scheme not in self.schemes: raise ValidationError(self.message, code=self.code)
try: super(URLValidator, self).__call__(value) except ValidationError as e: if value: scheme, netloc, path, query, fragment = urlsplit(value) try:
r'\[([A-f0-9:\.]+)\]\Z', re.IGNORECASE)
if name is None: name = content.name
dirname, filename = os.path.split(filename) return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))
if settings.USE_TZ: tz = timezone.get_default_timezone() return timezone.make_aware(dt, tz).astimezone(timezone.utc) else: return dt
if hasattr(content, 'temporary_file_path'): file_move_safe(content.temporary_file_path(), full_path)
name = self.get_available_name(name) full_path = self.path(name)
break
return force_text(name.replace('\\', '/'))
try: os.remove(name) except OSError as e: if e.errno != errno.ENOENT: raise
return datetime.utcfromtimestamp(ts).replace(tzinfo=timezone.utc)
if hasattr(os.path, 'samefile'): try: return os.path.samefile(src, dst) except OSError: return False
return (os.path.normcase(os.path.abspath(src)) == os.path.normcase(os.path.abspath(dst)))
if _samefile(old_file_name, new_file_name): return
if not allow_overwrite and os.access(new_file_name, os.F_OK): raise IOError("Destination file %s exists and allow_overwrite is False" % new_file_name)
pass
if getattr(e, 'winerror', 0) != 32 and getattr(e, 'errno', 0) != 13: raise
if name is not None: name = os.path.basename(name)
if len(name) > 255: name, ext = os.path.splitext(name) ext = ext[:255] name = name[:255 - len(ext)] + ext
raise
return False
if sizeof(c_ulong) != sizeof(c_void_p): ULONG_PTR = c_int64 else: ULONG_PTR = c_ulong PVOID = c_void_p
class _OFFSET(Structure): _fields_ = [ ('Offset', DWORD), ('OffsetHigh', DWORD)]
LOCK_EX = LOCK_SH = LOCK_NB = 0
def lock(f, flags): return False
return True
if endswith_lf(line): yield line else: buffer_ = line
if content_length > settings.FILE_UPLOAD_MAX_MEMORY_SIZE: self.activated = False else: self.activated = True
unlink = os.unlink
is_compressed = False
compressed = zlib.compress(data) if len(compressed) < (len(data) - 1): data = compressed is_compressed = True
return force_str(signature)
age = time.time() - timestamp if age > max_age: raise SignatureExpired( 'Signature age %s > %s seconds' % (age, max_age))
from __future__ import unicode_literals
return errors
checks = [check for check in checks if not hasattr(check, 'tags') or Tags.database not in check.tags]
warnings.extend(check_resolver(pattern))
warnings.extend(get_warning_for_invalid_pattern(pattern))
from __future__ import unicode_literals
from __future__ import unicode_literals
DEBUG = 10 INFO = 20 WARNING = 30 ERROR = 40 CRITICAL = 50
obj = self.obj._meta.label
from __future__ import unicode_literals
from __future__ import unicode_literals
if not pending_models: return []
args.extend(getattr(operation, 'args', []) or []) keywords.update(getattr(operation, 'keywords', {}) or {}) operation = operation.func
from __future__ import unicode_literals
pass
if app_list_value is not None: if model not in app_list_value: app_list_value.append(model)
if format not in serializers.get_public_serializer_formats(): try: serializers.get_serializer(format) except serializers.SerializerDoesNotExist: pass
for tablename in tablenames: self.create_table(db, tablename, dry_run)
models.CharField(name='cache_key', max_length=255, unique=True, primary_key=True), models.TextField(name='value'), models.DateTimeField(name='expires', db_index=True),
self.fixture_count = 0 self.loaded_object_count = 0 self.fixture_object_count = 0 self.models = set()
for fixture_label in fixture_labels: if self.find_fixtures(fixture_label): break else: return
if objects_in_fixture == 0: warnings.warn( "No fixture data found for '%s'. (File format may be " "invalid.)" % fixture_name, RuntimeWarning )
fixture_files_in_dir.append((candidate, fixture_dir, fixture_name))
requires_system_checks = False leave_locale_alone = True
os.environ[str("DJANGO_COLORS")] = str("nocolor")
autoreload.raise_last_exception()
shutdown_message = options.get('shutdown_message', '') quit_command = 'CTRL-BREAK' if sys.platform == 'win32' else 'CONTROL-C'
BaseRunserverCommand = Command
table_name_filter = options.get('table_name_filter')
if column_name in indexes: if indexes[column_name]['primary_key']: extra_params['primary_key'] = True elif indexes[column_name]['unique']: extra_params['unique'] = True
field_type, field_params, field_notes = self.get_field_type(connection, table_name, row) extra_params.update(field_params) comment_notes.extend(field_notes)
'' if '.' in field_type else 'models.', field_type,
field_notes.append("Field renamed because it contained more than one '_' in a row.")
if type(field_type) is tuple: field_type, new_params = field_type field_params.update(new_params)
if field_type == 'CharField' and row[3]: field_params['max_length'] = int(row[3])
tup = '(' + ', '.join("'%s'" % column_to_field_name[c] for c in columns) + ')' unique_together.append(tup)
from django.conf import settings, global_settings
settings._setup()
migrations_to_squash = [ loader.get_migration(al, mn) for al, mn in loader.graph.forwards_plan((migration.app_label, migration.name)) if al == migration.app_label ]
replaces = [] for migration in migrations_to_squash: if migration.replaces: replaces.extend(migration.replaces) else: replaces.append((migration.app_label, migration.name))
writer = MigrationWriter(new_migration) with open(writer.path, "wb") as fh: fh.write(writer.as_string())
if os.name == 'nt': old_path = self.work_path new_path = self.path else: old_path = self.work_path[2:] new_path = self.path[2:]
if os.path.exists(self.work_path): os.unlink(self.work_path)
if lines_list and lines_list[-1]: lines_list.append('') return '\n'.join(lines_list)
lines = dropwhile(len, pot_lines)
if settings.configured: settings.USE_I18N = True else: settings.configure(USE_I18N=True)
locale_dirs = filter(os.path.isdir, glob.glob('%s/*' % self.default_locale_path)) all_locales = map(os.path.basename, locale_dirs)
if process_all: locales = all_locales else: locales = locale or all_locales locales = set(locales) - set(exclude)
for locale in locales: if self.verbosity > 0: self.stdout.write("processing locale %s\n" % locale) for potfile in potfiles: self.write_po_file(potfile, locale)
self.stdout.write(errors)
from __future__ import unicode_literals
options['no_color'] = True return super(Command, self).execute(*args, **options)
connection = connections[options['database']]
executor = MigrationExecutor(connection)
self.output_transaction = migration.atomic
plan = [(executor.loader.graph.nodes[targets[0]], options['backwards'])] sql_statements = executor.collect_sql(plan) return '\n'.join(sql_statements)
db_name = connection.creation.create_test_db(verbosity=verbosity, autoclobber=not interactive, serialize=False)
call_command('loaddata', *fixture_labels, **{'verbosity': verbosity})
reset_sequences = options.get('reset_sequences', True) allow_cascade = options.get('allow_cascade', False) inhibit_post_migrate = options.get('inhibit_post_migrate', False)
for app_config in apps.get_app_configs(): try: import_module('.management', app_config.name) except ImportError: pass
if sql_list and not inhibit_post_migrate: emit_post_migrate_signal(verbosity, interactive, database)
raise ImportError("No IPython")
imported_objects = {}
if options['command']: exec(options['command']) return
try: with open(path, 'a'): os.utime(path, None) except (IOError, OSError): return False return True
basedirs = set(map(os.path.abspath, filter(os.path.isdir, basedirs)))
locales = locale or all_locales locales = set(locales) - set(exclude)
from __future__ import unicode_literals
db = options['database'] connection = connections[db]
loader = MigrationLoader(connection) graph = loader.graph targets = graph.leaf_nodes() plan = [] seen = set()
for target in targets: for migration in graph.forwards_plan(target): if migration not in seen: node = graph.node_map[migration] plan.append(node) seen.add(migration)
options['secret_key'] = get_random_secret_key()
from __future__ import unicode_literals
for app_config in apps.get_app_configs(): if module_has_submodule(app_config.module, "management"): import_module('.management', app_config.name)
db = options['database'] connection = connections[db]
connection.prepare_database() executor = MigrationExecutor(connection, self.migration_progress_callback)
executor.loader.check_consistent_history(connection)
tables = connection.introspection.table_names(cursor) created_models = set()
return not ( (converter(opts.db_table) in tables) or (opts.auto_created and converter(opts.auto_created._meta.db_table) in tables) )
loader = MigrationLoader(None, ignore_no_migrations=True)
for db in connections: loader.check_consistent_history(connections[db])
conflicts = loader.detect_conflicts()
if app_labels: conflicts = { app_label: conflict for app_label, conflict in iteritems(conflicts) if app_label in app_labels }
if self.merge and not conflicts: self.stdout.write("No conflicts detected to merge.") return
if self.merge and conflicts: return self.handle_merge(loader, conflicts)
autodetector = MigrationAutodetector( loader.project_state(), ProjectState.from_apps(apps), questioner, )
changes = autodetector.changes( graph=loader.graph, trim_to_apps=app_labels or None, convert_apps=app_labels or None, migration_name=self.migration_name, )
command = command_name command_name = command.__class__.__module__.split('.')[-1]
try: app_name = get_commands()[command_name] except KeyError: raise CommandError("Unknown command: %r" % command_name)
command = app_name
if 'DJANGO_AUTO_COMPLETE' not in os.environ: return
parser = CommandParser(None, usage="%(prog)s subcommand [options] [args]", add_help=False) parser.add_argument('--settings') parser.add_argument('--pythonpath')
if subcommand in no_settings_commands: settings.configure()
else: django.setup()
from django.conf import settings if not settings.configured: settings.configure()
continue
absolute_path = self.download(template)
ext = self.splitext(guessed_filename)[1] content_type = info.get('content-type') if not ext and content_type: ext = mimetypes.guess_extension(content_type) if ext: guessed_filename += ext
if used_name != guessed_filename: guessed_path = path.join(tempdir, guessed_filename) shutil.move(the_path, guessed_path) return guessed_path
return the_path
return
help = ''
_called_from_command_line = False can_import_settings = True
return
is_a_tty = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty() if not supported_platform or not is_a_tty: return False return True
style.ERROR_OUTPUT = style.ERROR
if e.errno != errno.ENOENT: raise
filelist = random.sample(filelist, int(num_entries / self._cull_frequency)) for fname in filelist: self._delete(fname)
self.LibraryValueNotFoundException = value_not_found_exception
return 0
timeout = -1
return force_str(super(BaseMemcachedCache, self).make_key(key, version))
self._cache.delete(key)
if delta < 0: return self._cache.decr(key, -delta) try: val = self._cache.incr(key, delta)
except self.LibraryValueNotFoundException: val = None if val is None: raise ValueError("Key '%s' not found" % key) return val
if delta < 0: return self._cache.incr(key, -delta) try: val = self._cache.decr(key, delta)
except self.LibraryValueNotFoundException: val = None if val is None: raise ValueError("Key '%s' not found" % key) return val
_caches = {} _expire_info = {} _locks = {}
return False
DEFAULT_TIMEOUT = object()
MEMCACHE_MAX_KEY_LENGTH = 250
timeout = -1
return self.get(key, default, version=version)
return self.has_key(key)
return len(self.object_list)
if not isinstance(self.object_list, list): self.object_list = list(self.object_list) return self.object_list[index]
if self.paginator.count == 0: return 0 return (self.paginator.per_page * (self.number - 1)) + 1
if self.number == self.paginator.num_pages: return self.paginator.count return self.number * self.paginator.per_page
super(ValidationError, self).__init__(message, code, params)
getattr(self, 'error_dict')
ISO_8859_1, UTF_8 = str('iso-8859-1'), str('utf-8')
chunk = self._read_limited(size - len(self.buffer))
raw_query_string = get_bytes_from_wsgi(self.environ, 'QUERY_STRING', '') return http.QueryDict(raw_query_string, encoding=self._encoding)
script_url = _slashes_re.sub(b'/', script_url)
return value.encode(ISO_8859_1) if six.PY3 else value
self._middleware_chain = handler
set_urlconf(settings.ROOT_URLCONF)
if not getattr(response, 'is_rendered', True) and callable(getattr(response, 'render', None)): response = response.render()
for middleware_method in self._view_middleware: response = middleware_method(request, callback, callback_args, callback_kwargs) if response: return response
if response is None:
if resolver.urlconf_module is None: six.reraise(*exc_info) callback, param_dict = resolver.resolve_error_handler(500) return callback(request, **param_dict)
for middleware_method in self._request_middleware: response = middleware_method(request) if response: break
self.connection.close()
return
class CachedDnsName(object): def __str__(self): return self.get_fqdn()
utf8_charset = Charset.Charset('utf-8')
DEFAULT_ATTACHMENT_MIME_TYPE = 'application/octet-stream'
domain = DNS_NAME
from email.utils import formataddr if localpart and domain: addr = '@'.join([localpart, domain]) return formataddr((nm, addr))
from email.headerregistry import Address from email.errors import InvalidHeaderDefect, NonASCIILocalPartDefect
name, val = forbid_multi_line_headers(name, val, 'ascii') MIMEMessage.__setitem__(self, name, val)
MIMEText.__init__(self, _text, _subtype)
return 0
pass
if content is None: with open(path, 'rb') as f: content = f.read() mimetype = DEFAULT_ATTACHMENT_MIME_TYPE
if isinstance(content, EmailMessage): content = content.message() elif not isinstance(content, Message): content = message_from_string(content)
attachment = MIMEBase(basetype, subtype) attachment.set_payload(content) Encoders.encode_base64(attachment)
class ServerHandler(simple_server.ServerHandler, object): def handle_error(self): if not is_broken_pipe_error(): super(ServerHandler, self).handle_error()
return self.client_address[0]
for k, v in self.headers.items(): if '_' in k: del self.headers[k]
env['PATH_INFO'] = path.decode(ISO_8859_1) if six.PY3 else path
try: from yaml import CSafeLoader as SafeLoader from yaml import CSafeDumper as SafeDumper except ImportError: from yaml import SafeLoader, SafeDumper
return super(PythonSerializer, self).getvalue()
six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
Model = self._get_model_from_node(node, "model")
data = {} if node.hasAttribute('pk'): data[Model._meta.pk.attname] = Model._meta.pk.to_python( node.getAttribute('pk'))
m2m_data = {}
if self.ignore and field_name not in field_names: continue field = Model._meta.get_field(field_name)
return base.DeserializedObject(obj, m2m_data)
field_value = [getInnerText(k).strip() for k in keys] obj_pk = default_manager.db_manager(self.db).get_by_natural_key(*field_value).pk
obj_pk = model._meta.pk.to_python(n.getAttribute('pk'))
raise EntitiesForbidden(name, None, base, sysid, pubid, notation_name)
model_dependencies = [] models = set() for app_config, model_list in app_list: if model_list is None: model_list = app_config.get_models()
if is_protected_type(value): self._current[field.name] = value else: self._current[field.name] = field.value_to_string(obj)
for (field_name, field_value) in six.iteritems(d["fields"]):
continue
from __future__ import absolute_import, unicode_literals
self.options.update({'use_decimal': False})
self.json_kwargs['separators'] = (',', ': ')
return super(PythonSerializer, self).getvalue()
six.reraise(DeserializationError, DeserializationError(e), sys.exc_info()[2])
DateTimeAwareJSONEncoder = DjangoJSONEncoder
internal_use_only = False progress_class = ProgressBar
self.m2m_data = None
try: return loader(name, dirs) except TemplateDoesNotExist: pass
template = Template(template, origin, template_name, engine=self)
if isinstance(context, Context): return t.render(context) else: return t.render(Context(context))
raise TemplateDoesNotExist(', '.join(not_found))
_dec._decorated_function = getattr(func, '_decorated_function', func)
tupl = d.as_tuple() units = len(tupl[1]) - tupl[2] prec = abs(p) + units + 1
warnings.simplefilter('ignore', category=RemovedInDjango20Warning) return mark_for_escaping(value)
yes, no, maybe = bits[0], bits[1], bits[1]
from __future__ import absolute_import
from __future__ import absolute_import
continue
from __future__ import absolute_import
template_dirs = tuple(self.dirs) if self.app_dirs: template_dirs += get_app_template_dirs(self.app_dirname) return template_dirs
pass
from __future__ import absolute_import
from __future__ import absolute_import
raise parser.error_class( "Not expecting '%s' in this position in if tag." % self.id )
raise parser.error_class( "Not expecting '%s' as infix operator in if tag." % self.id )
return False
for key, op in OPERATORS.items(): op.id = key
id = "literal" lbp = 0
if self.current_token is not EndToken: raise self.error_class("Unused '%s' at end of if expression." % self.current_token.display()) return retval
context.render_context[self] = itertools_cycle(self.cyclevars)
with context.push(var=output): return self.filter_expr.resolve(context)
state_frame = self._get_context_stack_frame(context) if self not in state_frame: state_frame[self] = None
compare_to = [var.resolve(context, True) for var in self._varlist]
compare_to = nodelist_true_output = self.nodelist_true.render(context)
return nodelist_true_output or self.nodelist_true.render(context)
context[self.var_name] = obj return self.expression.resolve(context, True)
context[self.var_name] = [] return ''
url = '' try: url = reverse(view_name, args=args, kwargs=kwargs, current_app=current_app) except NoReverseMatch: if self.asvar is None: raise
self.extra_context = extra_context or {} if name: self.extra_context[name] = var
if token.contents == 'else': nodelist = parser.parse(('endif',)) conditions_nodelists.append((None, nodelist)) token = parser.next_token()
if token.contents != 'endif': raise TemplateSyntaxError('Malformed template tag at line {0}: "{1}"'.format(token.lineno, token.contents))
_builtin_context_processors = ('django.template.context_processors.csrf',)
return self.flatten() == other.flatten()
return False
self.template = None super(Context, self).__init__(dict_)
self.update({})
self.update({})
processors = (template.engine.template_context_processors + self._processors) updates = {} for processor in processors: updates.update(processor(self.request)) self.dicts[self._processors_index] = updates
self.dicts[self._processors_index] = {}
if hasattr(new_context, '_processors_index'): del new_context._processors_index return new_context
original_context = context context = RequestContext(request, **kwargs) if original_context: context.push(original_context)
UNKNOWN_SOURCE = '<unknown source>'
try: message = force_text(exception.args[0]) except (IndexError, UnicodeDecodeError): message = '(Could not get exception message)'
warnings.simplefilter('ignore', category=RemovedInDjango20Warning) obj = mark_for_escaping(new_obj) escape_isnt_last_filter = False
plen = len(provided) + 1 func = getattr(func, '_decorated_function', func)
if plen < (alen - dlen) or plen > alen: raise TemplateSyntaxError("%s requires %d arguments, %d provided" % (name, alen - dlen, plen))
self.literal = float(var)
if '.' not in var and 'e' not in var.lower(): self.literal = int(self.literal)
if var.endswith('.'): raise ValueError
value = self._resolve_lookup(context)
value = self.literal
except (TypeError, AttributeError, KeyError, ValueError, IndexError):
if isinstance(current, BaseContext) and getattr(type(current), bit): raise AttributeError current = getattr(current, bit)
if (isinstance(e, AttributeError) and not isinstance(current, BaseContext) and bit in dir(current)): raise
must_be_first = False child_nodelists = ('nodelist',) token = None
contains_nontext = False
return ''
kwarg_re = re.compile(r"(?:(\w+)=)?(.+)")
from __future__ import unicode_literals
continue
if func_supports_parameter(loader.get_template_sources, 'template_dirs'): args.append(template_dirs) for origin in loader.get_template_sources(*args): yield origin
return self.load_template(template_name, template_dirs)
if func_supports_parameter(self.get_template_sources, 'template_dirs'): args.append(template_dirs)
return source, display_name
default_name = tpl['BACKEND'].rsplit('.', 2)[-2]
params = params.copy() backend = params.pop('BACKEND') engine_cls = import_string(backend) engine = engine_cls(params)
return tuple(template_dirs)
self.blocks = defaultdict(list)
return parent
return parent.template
block_context.add_blocks(self.blocks)
return compiled_parent._render(context)
return relative_name
return self.tag_function
return self.tag_function(name)
def dec(func): return self.tag(name, func) return dec
self.tags[name] = compile_function return compile_function
def dec(func): return self.filter_function(func, **flags) return dec
return self.filter_function(name, **flags)
def dec(func): return self.filter(name, func, **flags) return dec
return dec
return dec(func)
csrf_token = context.get('csrf_token') if csrf_token is not None: new_context['csrf_token'] = csrf_token return t.render(new_context)
unhandled_params = unhandled_params[:-len(defaults)]
self.template_name = template self.context_data = context
super(SimpleTemplateResponse, self).__init__('', content_type, status, charset)
return 'NOTPROVIDED'
context_extras['sql_queries'] = lazy( lambda: list(itertools.chain(*[connections[x].queries for x in connections])), list )
sitemap_url = reverse('django.contrib.sitemaps.views.index')
sitemap_url = reverse('django.contrib.sitemaps.views.sitemap')
limit = 50000
protocol = None
if self.protocol is not None: protocol = self.protocol if protocol is None: protocol = 'http'
return self.queryset.filter()
response['Last-Modified'] = http_date(timegm(lastmod))
url = '%s:%s' % (protocol, url)
response['Last-Modified'] = http_date( timegm(feedgen.latest_post_date().utctimetuple()))
return escape(force_text(item))
try: code = six.get_function_code(attr) except AttributeError: code = six.get_function_code(attr.__call__)
data_source = DataSource(data_source)
_mapping = {}
from django.contrib.gis.utils import ogrinspect shp_file = 'data/mapping_hacks/world_borders.shp' model_name = 'WorldBorders'
layer = data_source[layer_key] ogr_fields = layer.fields
if imports:
mfield = field_name.lower() if mfield[-1:] == '_': mfield += 'field'
kwargs_str = get_kwargs_str(field_name)
if not isinstance(srs, SpatialReference): srs = SpatialReference(srs)
kwargs = {'srid': srs.srid, 'auth_name': auth_name, 'auth_srid': auth_srid or srs.srid, 'proj4text': srs.proj4, }
try: SpatialRefSys.objects.using(database).get(srid=srs.srid) except SpatialRefSys.DoesNotExist: SpatialRefSys.objects.using(database).create(**kwargs)
class LayerMapError(Exception): pass
if isinstance(data, six.string_types): self.ds = DataSource(data, encoding=encoding) else: self.ds = data self.layer = self.ds[layer]
self.mapping = mapping self.model = model
self.check_layer()
if connections[self.using].features.supports_transform: self.geo_field = self.geometry_field() else: transform = False
if transform: self.source_srs = self.check_srs(source_srs) self.transform = self.coord_transform() else: self.transform = transform
if encoding: from codecs import lookup lookup(encoding) self.encoding = encoding else: self.encoding = None
ogr_fields = self.layer.fields ogr_field_types = self.layer.field_types
fld_name = model_field.__class__.__name__
coord_dim = model_field.dim
self.geom_field = field_name self.coord_dim = coord_dim fields_val = model_field
if model_field.__class__ not in self.FIELD_TYPES: raise LayerMapError('Django field type "%s" has no OGR mapping (yet).' % fld_name)
idx = check_ogr_fld(ogr_name) ogr_field = ogr_field_types[idx]
sr = self.layer.srs
for attr in unique: if attr not in self.mapping: raise ValueError
if unique not in self.mapping: raise ValueError
kwargs = {}
for field_name, ogr_name in self.mapping.items(): model_field = self.fields[field_name]
try: val = self.verify_geom(feat.geom, model_field) except GDALException: raise LayerMapError('Could not retrieve geometry from feature.')
val = self.verify_fk(feat, model_field, ogr_name)
val = self.verify_ogr_field(feat[ogr_name], model_field)
kwargs[field_name] = val
val = force_text(ogr_field.value, self.encoding)
d = Decimal(str(ogr_field.value))
dtup = d.as_tuple() digits = dtup[1]
max_prec = model_field.max_digits - model_field.decimal_places
if d_idx < 0: n_prec = len(digits[:d_idx]) else: n_prec = len(digits) + d_idx
try: val = int(ogr_field.value) except ValueError: raise InvalidInteger('Could not construct integer from: %s' % ogr_field.value)
fk_kwargs = {} for field_name, ogr_name in rel_mapping.items(): fk_kwargs[field_name] = self.verify_ogr_field(feat[ogr_name], rel_model._meta.get_field(field_name))
if self.coord_dim != geom.coord_dim: geom.coord_dim = self.coord_dim
multi_type = self.MULTI_TYPES[geom.geom_type.num] g = OGRGeometry(multi_type) g.add(geom)
if self.transform: g.transform(self.transform)
return g.wkt
return CoordTransform(self.source_srs, target_srs)
opts = self.model._meta return opts.get_field(self.geom_field)
default_range = self.check_fid_range(fid_range)
if progress: if progress is True or not isinstance(progress, int): progress_interval = 1000 else: progress_interval = progress
m = self.model(**kwargs)
m.save(using=self.using) num_saved += 1 if verbose: stream.write('%s: %s\n' % ('Updated' if is_update else 'Saved', m))
if progress and num_feat % progress_interval == 0: stream.write('Processed %d features, saved %d ...\n' % (num_feat, num_saved))
return num_saved, num_feat
if i + 1 == n_i: step_slice = slice(beg, None) else: step_slice = slice(beg, end)
_save()
self.locations = self._build_kml_sources(locations)
from django.contrib.gis.sitemaps.kml import KMLSitemap, KMZSitemap
placemarks = klass._default_manager.using(using).annotate(kml=AsKML(field_name))
if compress: render = render_to_kmz else: render = render_to_kml return render('gis/kml/placemarks.kml', {'places': placemarks})
geo_context = {'LANGUAGE_BIDI': translation.get_language_bidi()} logger = logging.getLogger('django.contrib.gis')
if attrs: self.params.update(attrs) self.params['editable'] = self.params['modifiable'] else: self.params['editable'] = True
self.params['wkt'] = ''
self.params['map_options'] = self.map_options()
self.params['wkt'] = wkt
def ol_bounds(extent): return 'new OpenLayers.Bounds(%s)' % str(extent)
kwargs['widget'] = self.get_map_widget(db_field) return db_field.formfield(**kwargs)
_city_file = '' _country_file = ''
_city = None _country = None
if cache in self.cache_options: self._cache = cache else: raise GeoIP2Exception('Invalid GeoIP caching option: %s' % cache)
reader = geoip2.database.Reader(path, mode=cache) db_type = reader.metadata().database_type
self._city = reader self._city_file = path
self._country = reader self._country_file = path
if self._reader: self._reader.close()
if not isinstance(query, six.string_types): raise TypeError('GeoIP query must be a string, not type %s' % type(query).__name__)
if not (ipv4_re.match(query) or is_valid_ipv6_address(query)): query = socket.gethostbyname(query)
enc_query = self._check_query(query, city_or_country=True) return Country(self._country_or_city(enc_query))
try: import numpy except ImportError: numpy = False
try: ds = gdal.DataSource(data_source) except gdal.GDALException as msg: raise CommandError(msg)
field_type, geo_params = connection.introspection.get_geometry_type(table_name, geo_col) field_params.update(geo_params)
if isinstance(value, six.string_types): value = self.deserialize(value)
if gdal.HAS_GDAL: return 3857 else: return 900913
if not value.srid: try: value.srid = self.widget.map_srid except AttributeError: if self.srid: value.srid = self.srid return value
self.envelope = poly.envelope
self.points = self.latlng_from_coords(poly.shell.coords)
self.stroke_color, self.stroke_opacity, self.stroke_weight = stroke_color, stroke_opacity, stroke_weight
self.fill_color, self.fill_opacity = fill_color, fill_opacity
self.envelope = geom.envelope self.color, self.weight, self.opacity = color, weight, opacity super(GPolyline, self).__init__()
return hash(self.__class__) ^ hash(self.varname)
if not version: self.version = getattr(settings, 'GOOGLE_MAPS_API_VERSION', '2.x') else: self.version = version
if not api_url: self.api_url = getattr(settings, 'GOOGLE_MAPS_URL', GOOGLE_MAPS_URL) % self.version else: self.api_url = api_url
self.dom_id = dom_id self.extra_context = extra_context self.js_module = js_module self.template = template self.kml_urls = kml_urls
overlay_info = [[GMarker, markers, 'markers'], [GPolygon, polygons, 'polygons'], [GPolyline, polylines, 'polylines']]
self.calc_zoom = False if self.polygons or self.polylines or self.markers: if center is None or zoom is None: self.calc_zoom = True
if zoom is None: zoom = 4 self.zoom = zoom if center is None: center = (0, 0) self.center = center
template = kwargs.pop('template', 'gis/google/google-multi.js')
self.map_template = kwargs.pop('map_template', 'gis/google/google-single.js')
super(GoogleMapSet, self).__init__(**kwargs) self.template = template
if isinstance(args[0], (tuple, list)): self.maps = args[0] else: self.maps = args
self.dom_ids = ['map%d' % i for i in range(len(self.maps))]
return mark_safe('onload="%s.load()"' % self.js_module)
DTOR = pi / 180. RTOD = 180. / pi
self._tilesize = tilesize
self._nzoom = num_zoom
z *= 2
lon, lat = self.get_lon_lat(lonlat) npix = self._npix[zoom]
px_x = round(npix + (lon * self._degpp[zoom]))
px_y = round(npix + (0.5 * log((1 + fac) / (1 - fac)) * (-1.0 * self._radpp[zoom])))
return (px_x, px_y)
npix = self._npix[zoom]
lon = (px[0] - npix) / self._degpp[zoom]
lat = RTOD * (2 * atan(exp((px[1] - npix) / (-1.0 * self._radpp[zoom]))) - 0.5 * pi)
return (lon, lat)
delta = self._tilesize / 2
px = self.lonlat_to_pixel(lonlat, zoom)
ll = self.pixel_to_lonlat((px[0] - delta, px[1] - delta), zoom) ur = self.pixel_to_lonlat((px[0] + delta, px[1] + delta), zoom)
return Polygon(LinearRing(ll, (ll[0], ur[1]), ur, (ur[0], ll[1]), ll), srid=4326)
if not isinstance(geom, GEOSGeometry) or geom.srid != 4326: raise TypeError('get_zoom() expects a GEOS Geometry with an SRID of 4326.')
env = geom.envelope env_w, env_h = self.get_width_height(env.extent) center = env.centroid
tile_w, tile_h = self.get_width_height(self.tile(center, z).extent)
return self._nzoom - 1
if hasattr(lgeoip, 'GeoIP_lib_version'): GeoIP_lib_version = lgeoip.GeoIP_lib_version GeoIP_lib_version.argtypes = None GeoIP_lib_version.restype = c_char_p else: GeoIP_lib_version = None
GeoIPRecord_delete = lgeoip.GeoIPRecord_delete GeoIPRecord_delete.argtypes = [RECTYPE] GeoIPRecord_delete.restype = None
def check_record(result, func, cargs): if result: rec = result.contents record = {fld: getattr(rec, fld) for fld, ctype in rec._fields_}
encoding = geoip_encodings[record['charset']] for char_field in geoip_char_fields: if record[char_field]: record[char_field] = record[char_field].decode(encoding)
GeoIPRecord_delete(result) return record
GeoIP_open = lgeoip.GeoIP_open GeoIP_open.restype = DBTYPE GeoIP_delete = lgeoip.GeoIP_delete GeoIP_delete.argtypes = [DBTYPE] GeoIP_delete.restype = None
class geoip_char_p(c_char_p): pass
if os.name == 'nt': libc = CDLL('msvcrt') else: libc = CDLL(None) free = libc.free
free_regex = re.compile(r'^GEO-\d{3}FREE') lite_regex = re.compile(r'^GEO-\d{3}LITE')
_city_file = '' _country_file = ''
_city = None _country = None
if cache in self.cache_options: self._cache = cache else: raise GeoIPException('Invalid GeoIP caching option: %s' % cache)
if GeoIP_delete is None: return if self._country: GeoIP_delete(self._country) if self._city: GeoIP_delete(self._city)
if not isinstance(query, six.string_types): raise TypeError('GeoIP query must be a string, not type %s' % type(query).__name__)
return force_bytes(query)
return GeoIP_record_by_addr(self._city, c_char_p(enc_query))
return GeoIP_record_by_name(self._city, c_char_p(enc_query))
return {'country_code': self.country_code(query), 'country_name': self.country_name(query), }
@classmethod def open(cls, full_path, cache): return GeoIP(full_path, cache)
res.source_expressions[pos] = Transform(expr, base_srid).resolve_expression(*args, **kwargs)
self.source_expressions.pop(0) return super(AsKML, self).as_sql(compiler, connection)
for pos, expr in enumerate( self.source_expressions[self.geom_param_pos + 1:], start=self.geom_param_pos + 1): if isinstance(expr, GeomValue): expr.geography = True
if self.spheroid:
self.source_expressions[2] = Value(geo_field._spheroid)
self.function = 'ST_Length_Spheroid' self.source_expressions.append(Value(geo_field._spheroid))
return self.source_expressions[self.geom_param_pos + 1].value
value.srid = self.srid
self.source_expressions.append(Value(0))
s['select_field'] = AreaField('sq_m')
s['select_field'] = AreaField(Area.unit_attname(geo_field.units_name(connection)))
procedure_args = {'function': func}
geo_field = self._geo_field(field_name) if not geo_field: raise TypeError('%s output only available on GeometryFields.' % func)
procedure_args['geo_col'] = self._geocol_select(geo_field, field_name)
if not isinstance(model_att, six.string_types): model_att = att
fmt = '%%(function)s(%s)' % settings['procedure_fmt']
if settings.get('select_field'): select_field = settings['select_field'] if connection.ops.oracle: select_field.empty_strings_allowed = False else: select_field = Field()
self.query.add_annotation( RawSQL(fmt % settings['procedure_args'], settings['select_params'], select_field), model_att) return self
procedure_args, geo_field = self._spatial_setup(func, field_name=kwargs.get('field_name'))
connection = connections[self.db] geodetic = geo_field.geodetic(connection) geography = geo_field.geography
lookup_params = [geom or 'POINT (0 0)', 0]
backend = connection.ops
geom_args = bool(geom)
srid = self.query.get_context('transformed_srid') if srid: u, unit_name, s = get_srid_info(srid, connection) geodetic = unit_name.lower() in geo_field.geodetic_units
procedure_fmt = '%(geo_col)s,%(geom)s'
procedure_fmt += ",'%(spheroid)s'" procedure_args.update({'function': backend.length_spheroid, 'spheroid': params[1]})
if perimeter: procedure_args.update({'function': backend.perimeter3d}) elif length: procedure_args.update({'function': backend.length3d})
parent_model = geo_field.model._meta.concrete_model return self._field_column(compiler, geo_field, parent_model._meta.db_table)
for field in self.model._meta.fields: if isinstance(field, GeometryField): return field return False
return GISLookup._check_geo_field(self.model._meta, field_name)
if isinstance(value, Decimal): value = float(value) if value is not None and self.area_att: value = Area(**{self.area_att: value}) return value
geom_type = None
connection.ops.check_expression_support(self) self.function = connection.ops.spatial_aggregate_name(self.name) return super(GeoAggregate, self).as_sql(compiler, connection)
_srid_cache = {}
SpatialRefSys = connection.ops.spatial_ref_sys()
return None, None, None
_srid_cache[connection.alias] = {}
sel_fmt = connection.ops.select % sel_fmt
geodetic_units = ('decimal degree', 'degree')
self.spatial_index = spatial_index
self.srid = srid
kwargs['verbose_name'] = verbose_name
kwargs['srid'] = self.srid if self.spatial_index is not True: kwargs['spatial_index'] = self.spatial_index return name, path, args, kwargs
def _get_srid_info(self, connection): self._units, self._units_name, self._spheroid = get_srid_info(self.srid, connection)
return units_name.lower() in self.geodetic_units if units_name else self.srid == 4326
obj.srid = self.get_srid(obj)
geom_type = 'GEOMETRY'
self.dim = dim
self.geography = geography
def contribute_to_class(self, cls, name, **kwargs): super(GeometryField, self).contribute_to_class(cls, name, **kwargs)
setattr(cls, self.attname, SpatialProxy(Geometry, self))
class PointField(GeometryField): geom_type = 'POINT' form_class = forms.PointField description = _("Point")
if not connection.features.gis_enabled or not connection.features.supports_raster: raise ImproperlyConfigured('Raster fields require backends with raster support.')
if not prepared: value = connection.ops.deconstruct_raster(value) return super(RasterField, self).get_db_prep_value(value, connection, prepared)
from django.contrib.gis.gdal import GDALRaster setattr(cls, self.attname, SpatialProxy(GDALRaster, self))
use_for_related_fields = True
silence_use_for_related_fields_deprecation = True
field_list = lookup.split(LOOKUP_SEP)
field_list.reverse() fld_name = field_list.pop()
if isinstance(geo_fld, BaseSpatialField): return geo_fld else: return False
if only_lhs: self.band_rhs = 1 self.band_lhs = self.lhs.band_index + 1 return
return super(GISLookup, self).process_rhs(compiler, connection)
return connection.ops.gis_operators[self.lookup_name]
lookup_name = 'exact'
if len(self.rhs) > 2 and not self.rhs[2] == 'spheroid': self.process_band_indices()
return self
try: geo_value = instance.__dict__[self._field.attname] except KeyError: geo_value = super(SpatialProxy, self).__get__(instance, cls)
geo_obj = self._klass(geo_value) setattr(instance, self._field.attname, geo_obj)
gtype = self._field.geom_type
pass
if value.srid is None: value.srid = self._field.srid
pass
instance.__dict__[self._field.attname] = value return value
data_types_reverse = DatabaseIntrospection.data_types_reverse.copy() data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'
isinstance(field, GeometryField)
GDAL_TO_POSTGIS = [None, 4, 6, 5, 8, 7, 10, 11, None, None, None, None]
POSTGIS_TO_GDAL = [1, 1, 1, 3, 1, 3, 2, 5, 4, None, 6, 7, None, None]
STRUCT_SIZE = {
if self.is_geometry: self.ewkb = bytes(obj.ewkb) self._adapter = Binary(self.ewkb) else: self.ewkb = to_pgraster(obj)
if proto == ISQLQuote: return self else: raise Exception('Error implementing psycopg2 protocol. Is psycopg2 installed?')
return str('%s(%s)' % ( 'ST_GeogFromWKB' if self.geography else 'ST_GeomFromEWKB', self._adapter.getquoted().decode()) )
return "'%s'::raster" % self.ewkb
postgis_types_reverse = {}
('geography', ('GeometryField', {'geography': True})),
field_type = OGRGeomType(row[2]).django
dim = row[0] srid = row[1] field_params = {} if srid != 4326: field_params['srid'] = srid if dim != 2: field_params['dim'] = dim
BILATERAL = 'bilateral'
if isinstance(lookup.rhs, (tuple, list)): rhs_val = lookup.rhs[0] spheroid = lookup.rhs[-1] == 'spheroid' else: rhs_val = lookup.rhs spheroid = False
lhs_is_raster = lookup.lhs.field.geom_type == 'RASTER' rhs_is_raster = isinstance(rhs_val, GDALRaster)
sql_params.insert(1, lookup.lhs.output_field._spheroid)
value = dist_val[0]
geodetic = f.geodetic(self.connection) geography = f.geography
dist_param = value
if value is None: value_srid = None elif f.geom_type == 'RASTER' and isinstance(value, six.string_types): value_srid = get_pgraster_srid(value) else: value_srid = value.srid
sql, _ = compiler.compile(value) placeholder = placeholder % sql
with self.connection.temporary_connection() as cursor: cursor.execute('SELECT %s()' % func) return cursor.fetchone()[0]
version = self.postgis_lib_version() m = self.version_regex.match(version)
def geometry_columns(self): return PostGISGeometryColumns
def parse_raster(self, value): return from_pgraster(value)
for sql in self.geometry_sql: self.execute(sql) self.geometry_sql = []
for sql in self.geometry_sql: self.execute(sql) self.geometry_sql = []
return unpack('i', data[106:114])[0]
header, data = chunk(data, 122) header = unpack(POSTGIS_HEADER_STRUCTURE, header)
bands = [] pixeltypes = [] while data: pixeltype, data = chunk(data, 2) pixeltype = unpack('B', pixeltype)[0]
has_nodata = pixeltype >= 64 if has_nodata: pixeltype -= 64
pixeltype = POSTGIS_TO_GDAL[pixeltype] pack_type = GDAL_TO_STRUCT[pixeltype] pack_size = 2 * STRUCT_SIZE[pack_type]
nodata, data = chunk(data, pack_size) nodata = unpack(pack_type, nodata)[0]
band, data = chunk(data, pack_size * header[10] * header[11]) band_result = {'data': binascii.unhexlify(band)}
if has_nodata: band_result['nodata_value'] = nodata
bands.append(band_result)
pixeltypes.append(pixeltype)
if len(set(pixeltypes)) != 1: raise ValidationError("Band pixeltypes are not all equal.")
if rast is None or rast == '': return
result = pack(POSTGIS_HEADER_STRUCTURE, rasterheader)
pixeltype = GDAL_TO_POSTGIS[band.datatype()]
if band.nodata_value is not None: pixeltype += 64
bandheader = pack(structure, (pixeltype, band.nodata_value or 0))
band_data_hex = binascii.hexlify(band.data(as_memoryview=True)).upper()
result += bandheader + band_data_hex
return result.decode()
with self.cursor() as cursor: cursor.execute("CREATE EXTENSION IF NOT EXISTS postgis")
cs_bounds = models.PolygonField(null=True) objects = models.GeoManager()
if self._isClockwise(poly.exterior_ring): poly.exterior_ring = list(reversed(poly.exterior_ring))
for i, geom in enumerate(coll): if isinstance(geom, Polygon): coll[i] = self._fix_polygon(geom)
data_types_reverse = DatabaseIntrospection.data_types_reverse.copy() data_types_reverse[cx_Oracle.OBJECT] = 'GeometryField'
if lookup_type == 'dwithin': dist_param = 'distance=%s' % dist_param
sql, _ = compiler.compile(value) return placeholder % sql
def geometry_columns(self): from django.contrib.gis.db.backends.oracle.models import OracleGeometryColumns return OracleGeometryColumns
return truncate_name('%s_%s_id' % (model._meta.db_table, field.column), 30)
try: self._srs = gdal.SpatialReference(self.wkt) return self.srs except Exception as msg: pass
postgis = False spatialite = False mysql = False oracle = False spatial_version = None
select = None
geography = False geometry = False
disallowed_aggregates = ()
function_names = {}
geohash = False geojson = False gml = False kml = False svg = False
from_text = False from_wkb = False
def convert_extent(self, box, srid): raise NotImplementedError('Aggregate extent not implemented for this spatial backend.')
def geo_quote_name(self, name): return "'%s'" % name
def geometry_columns(self): raise NotImplementedError('Subclasses of BaseSpatialOperations must provide a geometry_columns() method.')
has_spatialrefsys_table = True
supports_add_srs_entry = True supports_geometry_field_introspection = True
supports_distances_lookups = True supports_left_right_lookups = False
supports_raster = False
supports_geometry_field_unique_index = True
@property def supports_collect_aggr(self): return aggregates.Collect not in self.connection.ops.disallowed_aggregates
setattr(self.__class__, 'has_%s_method' % method, property(partial(BaseSpatialFeatures.has_ops_method, method=method)))
sql, _ = compiler.compile(value) return placeholder % sql
return '%s(%s(%%s,%s), %s)' % (self.transform, self.from_text, value.srid, f.srid)
def geometry_columns(self): from django.contrib.gis.db.backends.spatialite.models import SpatialiteGeometryColumns return SpatialiteGeometryColumns
for sql in self.geometry_sql: self.execute(sql) self.geometry_sql = []
self.column_sql(model, field) for sql in self.geometry_sql: self.execute(sql) self.geometry_sql = []
if Database.version_info < (2, 5, 0): raise ImproperlyConfigured('Only versions of pysqlite 2.5+ are ' 'compatible with SpatiaLite and GeoDjango.')
supports_num_points_poly = False
return self.connection.ops.spatial_version >= (4, 1, 0)
D = Distance A = Area
def __add__(self, other): 'add another list-like object' return self.__class__(list(self) + list(other))
return False
return True
def count(self, val): "Standard list count method" count = 0 for i in self: if val == i: count += 1 return count
def append(self, val): "Standard list append method" self[len(self):] = [val]
if index.step is None: self._assign_simple_slice(start, stop, valueList) else: self._assign_extended_slice(start, stop, step, valueList)
newLen = len(self) newVals = dict(zip(indexList, valueList))
self._base_geom = geom from .geometry import GEOSGeometry if not isinstance(geom, GEOSGeometry): raise TypeError self.ptr = capi.geos_prepare(geom.ptr)
if isinstance(file_h, six.string_types): with open(file_h, 'rb') as file_h: buf = file_h.read() else: buf = file_h.read()
if len(args) == 1: if isinstance(args[0], (tuple, list)): init_geoms = args[0] else: init_geoms = args else: init_geoms = args
self._check_allowed(init_geoms)
collection = self._create_collection(len(init_geoms), iter(init_geoms)) super(GeometryCollection, self).__init__(collection, **kwargs)
return GEOSGeometry(capi.geom_clone(self._get_single_internal(index)), srid=self.srid)
class MultiPoint(GeometryCollection): _allowed = Point _typeid = 4
GeometryCollection._allowed = (Point, LineString, LinearRing, Polygon, MultiPoint, MultiLineString, MultiPolygon)
ext_ring = args[0] init_holes = args[1:] n_holes = len(init_holes)
prev_ptr = self.ptr srid = self.srid self.ptr = self._create_polygon(length, items) if srid: self.srid = srid capi.destroy_geom(prev_ptr)
return capi.get_intring(self.ptr, index - 1)
@property def num_interior_rings(self): "Returns the number of interior rings." return capi.get_nrings(self.ptr)
exterior_ring = property(_get_ext_ring, _set_ext_ring) shell = exterior_ring
coords = x
if isinstance(z, six.integer_types + (float,)): coords = [x, y, z] else: coords = [x, y]
super(Point, self).__init__(point, srid=srid)
raise GEOSException('Geometry resulting from slice deletion was invalid.')
@property def tuple(self): "Returns a tuple of the point." return self._cs.tuple
coords = tuple
free = GEOSFuncFactory('GEOSFree') free.argtypes = [c_void_p]
if result != 1: return None return last_arg_byref(cargs)
s = string_at(result, last_arg_byref(cargs)) free(result) return s
s = string_at(result) free(result) return s
geos_prepare = GEOSFuncFactory('GEOSPrepare', argtypes=[GEOM_PTR], restype=PREPGEOM_PTR) prepared_destroy = GEOSFuncFactory('GEOSPreparedGeom_destroy', argtypes=[PREPGEOM_PTR])
class PreparedPredicate(GEOSFuncFactory): argtypes = [PREPGEOM_PTR, GEOM_PTR] restype = c_char errcheck = staticmethod(check_predicate)
self.ptr = lgeos.initGEOS_r(notice_h, error_h)
class GEOSContext(threading.local): handle = None
self.cfunc = getattr(lgeos, func_name + '_r') self.threaded = True self.thread_context = thread_context
self.cfunc = getattr(lgeos, func_name) self.threaded = False
if not self.thread_context.handle: self.thread_context.handle = GEOSContextHandle() return self.cfunc(self.thread_context.handle.ptr, *args)
def _get_argtypes(self): return self.cfunc.argtypes
def _get_restype(self): return self.cfunc.restype
def _get_errcheck(self): return self.cfunc.errcheck
class UnaryPredicate(GEOSFuncFactory): "For GEOS unary predicate functions." argtypes = [GEOM_PTR] restype = c_char errcheck = staticmethod(check_predicate)
geos_relate = GEOSFuncFactory( 'GEOSRelate', argtypes=[GEOM_PTR, GEOM_PTR], restype=geos_char_p, errcheck=check_string )
geos_project = GEOSFuncFactory( 'GEOSProject', argtypes=[GEOM_PTR, GEOM_PTR], restype=c_double, errcheck=check_minus_one ) geos_interpolate = Topology('GEOSInterpolate', argtypes=[GEOM_PTR, c_double])
c_uchar_p = POINTER(c_ubyte)
class BinConstructor(GEOSFuncFactory): "Generates a prototype for binary construction (HEX, WKB) GEOS routines." argtypes = [c_char_p, c_size_t] restype = GEOM_PTR errcheck = staticmethod(check_geom)
class BinOutput(GEOSFuncFactory): "Generates a prototype for the routines that return a sized string." argtypes = [GEOM_PTR, POINTER(c_size_t)] restype = c_uchar_p errcheck = staticmethod(check_sized_string)
from_hex = BinConstructor('GEOSGeomFromHEX_buf') from_wkb = BinConstructor('GEOSGeomFromWKB_buf') from_wkt = GeomOutput('GEOSGeomFromWKT', [c_char_p])
to_hex = BinOutput('GEOSGeomToHEX_buf') to_wkb = BinOutput('GEOSGeomToWKB_buf') to_wkt = StringFromGeom('GEOSGeomToWKT')
create_point = GeomOutput('GEOSGeom_createPoint', [CS_PTR]) create_linestring = GeomOutput('GEOSGeom_createLineString', [CS_PTR]) create_linearring = GeomOutput('GEOSGeom_createLinearRing', [CS_PTR])
create_polygon = GeomOutput('GEOSGeom_createPolygon', None) create_empty_polygon = GeomOutput('GEOSGeom_createEmptyPolygon', None) create_collection = GeomOutput('GEOSGeom_createCollection', None)
get_extring = GeomOutput('GEOSGetExteriorRing', [GEOM_PTR]) get_intring = GeomOutput('GEOSGetInteriorRingN', [GEOM_PTR, c_int]) get_nrings = IntFromGeom('GEOSGetNumInteriorRings')
get_geomn = GeomOutput('GEOSGetGeometryN', [GEOM_PTR, c_int])
geom_clone = GEOSFuncFactory('GEOSGeom_clone', argtypes=[GEOM_PTR], restype=GEOM_PTR)
destroy_geom = GEOSFuncFactory('GEOSGeom_destroy', argtypes=[GEOM_PTR])
geos_get_srid = GEOSFuncFactory('GEOSGetSRID', argtypes=[GEOM_PTR], restype=c_int) geos_set_srid = GEOSFuncFactory('GEOSSetSRID', argtypes=[GEOM_PTR, c_int])
return last_arg_byref(cargs)
class CsInt(GEOSFuncFactory): "For coordinate sequence routines that return an integer." argtypes = [CS_PTR, POINTER(c_uint)] restype = c_int errcheck = staticmethod(check_cs_get)
self.errcheck = check_cs_get dbl_param = POINTER(c_double)
self.argtypes = [CS_PTR, c_uint, c_uint, dbl_param]
cs_clone = CsOutput('GEOSCoordSeq_clone', [CS_PTR]) create_cs = CsOutput('GEOSCoordSeq_create', [c_uint, c_uint]) get_cs = CsOutput('GEOSGeom_getCoordSeq', [GEOM_PTR])
cs_getordinate = CsOperation('GEOSCoordSeq_getOrdinate', ordinate=True, get=True) cs_setordinate = CsOperation('GEOSCoordSeq_setOrdinate', ordinate=True)
cs_getx = CsOperation('GEOSCoordSeq_getX', get=True) cs_gety = CsOperation('GEOSCoordSeq_getY', get=True) cs_getz = CsOperation('GEOSCoordSeq_getZ', get=True)
cs_setx = CsOperation('GEOSCoordSeq_setX') cs_sety = CsOperation('GEOSCoordSeq_setY') cs_setz = CsOperation('GEOSCoordSeq_setZ')
cs_getsize = CsInt('GEOSCoordSeq_getSize') cs_getdims = CsInt('GEOSCoordSeq_getDimensions')
class WKTReader_st(Structure): pass
wkt_reader_create = GEOSFuncFactory('GEOSWKTReader_create', restype=WKT_READ_PTR) wkt_reader_destroy = GEOSFuncFactory('GEOSWKTReader_destroy', argtypes=[WKT_READ_PTR])
wkt_writer_create = GEOSFuncFactory('GEOSWKTWriter_create', restype=WKT_WRITE_PTR) wkt_writer_destroy = GEOSFuncFactory('GEOSWKTWriter_destroy', argtypes=[WKT_WRITE_PTR])
wkb_reader_create = GEOSFuncFactory('GEOSWKBReader_create', restype=WKB_READ_PTR) wkb_reader_destroy = GEOSFuncFactory('GEOSWKBReader_destroy', argtypes=[WKB_READ_PTR])
argtypes = [WKB_READ_PTR, c_char_p, c_size_t] restype = GEOM_PTR errcheck = staticmethod(check_geom)
wkb_writer_create = GEOSFuncFactory('GEOSWKBWriter_create', restype=WKB_WRITE_PTR) wkb_writer_destroy = GEOSFuncFactory('GEOSWKBWriter_destroy', argtypes=[WKB_WRITE_PTR])
class WKBWriteFunc(GEOSFuncFactory): argtypes = [WKB_WRITE_PTR, GEOM_PTR, POINTER(c_size_t)] restype = c_uchar_p errcheck = staticmethod(check_sized_string)
class WKBWriterGet(GEOSFuncFactory): argtypes = [WKB_WRITE_PTR] restype = c_int
try: self._destructor(self._ptr) except (AttributeError, TypeError):
class _WKTReader(IOBase): _constructor = wkt_reader_create _destructor = wkt_reader_destroy ptr_type = WKT_READ_PTR
class WKTWriter(IOBase): _constructor = wkt_writer_create _destructor = wkt_writer_destroy ptr_type = WKT_WRITE_PTR
def _get_byteorder(self): return wkb_writer_get_byteorder(self.ptr)
@property def outdim(self): return wkb_writer_get_outdim(self.ptr)
@property def srid(self): return bool(ord(wkb_writer_get_include_srid(self.ptr)))
class ThreadLocalIO(threading.local): wkt_r = None wkt_w = None wkb_r = None wkb_w = None ewkb_w = None
def wkt_r(): if not thread_context.wkt_r: thread_context.wkt_r = _WKTReader() return thread_context.wkt_r
_ptr = None
ptr_type = c_void_p
def _get_ptr(self): if self._ptr: return self._ptr else: raise GEOSException('NULL GEOS %s pointer encountered.' % self.__class__.__name__)
if ptr is None or isinstance(ptr, self.ptr_type): self._ptr = ptr else: raise TypeError('Incompatible pointer type')
ptr = property(_get_ptr, _set_ptr)
if len(args) == 1: coords = args[0] else: coords = args
srid = kwargs.get('srid')
cs = GEOSCoordSeq(capi.create_cs(ncoords, ndim), z=bool(ndim == 3))
super(LineString, self).__init__(self._init_func(cs.ptr), srid=srid)
cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz) for i, c in enumerate(items): cs[i] = c
raise GEOSException('Geometry resulting from slice deletion was invalid.')
@property def tuple(self): "Returns a tuple version of the geometry from the coordinate sequence." return self._cs.tuple coords = tuple
class LinearRing(LineString): _minlength = 4 _init_func = capi.create_linearring
g = wkb_r().read(force_bytes(geo_input))
if not gdal.HAS_GDAL: raise ValueError('Initializing geometry from JSON input requires GDAL.') g = wkb_r().read(gdal.OGRGeometry(geo_input).wkb)
g = geo_input
g = wkb_r().read(geo_input)
raise TypeError('Improper geometry input type: %s' % str(type(geo_input)))
self.ptr = g
self._post_init(srid)
if srid and isinstance(srid, int): self.srid = srid
self._set_cs()
def __getstate__(self): return bytes(self.wkb), self.srid
def __or__(self, other): "Returns the union of this Geometry and the other." return self.union(other)
def __and__(self, other): "Returns the intersection of this Geometry and the other." return self.intersection(other)
def __sub__(self, other): "Return the difference this Geometry and the other." return self.difference(other)
def __xor__(self, other): "Return the symmetric difference of this Geometry and the other." return self.sym_difference(other)
@property def geom_type(self): "Returns a string representing the Geometry type, e.g. 'Polygon'" return capi.geos_type(self.ptr).decode()
def contains(self, other): "Returns true if other.within(this) returns true." return capi.geos_contains(self.ptr, other.ptr)
return wkb_w(dim=3 if self.hasz else 2).write_hex(self)
if clone: return self.clone() else: return
srid = None
def _topology(self, gptr): "Helper routine to return Geometry from the given pointer." return GEOSGeometry(gptr, srid=self.srid)
@property def area(self): "Returns the area of the Geometry." return capi.geos_area(self.ptr, byref(c_double()))
try: from django.conf import settings lib_path = settings.GEOS_LIBRARY_PATH except (AttributeError, EnvironmentError, ImportError, ImproperlyConfigured): lib_path = None
if lib_names: for lib_name in lib_names: lib_path = find_library(lib_name) if lib_path is not None: break
NOTICEFUNC = CFUNCTYPE(None, c_char_p, c_char_p)
class GEOSGeom_t(Structure): pass
GEOM_PTR = POINTER(GEOSGeom_t) PREPGEOM_PTR = POINTER(GEOSPrepGeom_t) CS_PTR = POINTER(GEOSCoordSeq_t) CONTEXT_PTR = POINTER(GEOSContextHandle_t)
def get_pointer_arr(n): "Gets a ctypes pointer array (of length `n`) for GEOSGeom_t opaque pointer." GeomArr = GEOM_PTR * n return GeomArr()
geos_version = GEOSFuncFactory('GEOSversion', restype=c_char_p)
@property def size(self): "Returns the size of this coordinate sequence." return capi.cs_getsize(self.ptr, byref(c_uint()))
def clone(self): "Clones this coordinate sequence." return GEOSCoordSeq(capi.cs_clone(self.ptr), self.hasz)
for field in obj._meta.fields: if hasattr(field, 'geom_type'): self.geometry_field = field.name break
force = True stats_args.insert(2, c_int(force)) func = capi.get_band_statistics
try: func(*stats_args) result = smin.value, smax.value, smean.value, sstd.value except GDALException: result = (None, None, None, None)
ctypes_array = GDAL_TO_CTYPES[self.datatype()] * (shape[0] * shape[1])
access_flag = 0 data_array = ctypes_array()
access_flag = 1
capi.band_io(self._ptr, access_flag, offset[0], offset[1], size[0], size[1], byref(data_array), shape[0], shape[1], self.datatype(), 0, 0)
GDAL_PIXEL_TYPES = {
GDAL_INTEGER_TYPES = [1, 2, 3, 4, 5]
if isinstance(ds_input, six.string_types) and json_regex.match(ds_input): ds_input = json.loads(ds_input)
driver = Driver(ds_input.get('driver', 'MEM'))
if 'width' not in ds_input or 'height' not in ds_input: raise GDALException('Specify width and height attributes for JSON or dict input.')
if 'srid' not in ds_input: raise GDALException('Specify srid for JSON or dict input.')
self.srs = ds_input.get('srid')
if 'origin' in ds_input: self.origin.x, self.origin.y = ds_input['origin']
self._ptr = ds_input
if not self._write: raise GDALException('Raster needs to be opened in write mode to change values.') capi.flush_ds(self._ptr)
gtf = (c_double * 6)() capi.get_ds_geotransform(self._ptr, byref(gtf)) return list(gtf)
values = (c_double * 6)(*values) capi.set_ds_geotransform(self._ptr, byref(values)) self._flush()
if 'width' not in ds_input: ds_input['width'] = self.width
if 'driver' not in ds_input: ds_input['driver'] = self.driver.name
ds_input['nr_of_bands'] = len(self.bands)
target = GDALRaster(ds_input, write=True)
for index, band in enumerate(self.bands): target.bands[index].nodata_value = band.nodata_value
algorithm = GDAL_RESAMPLE_ALGORITHMS[resampling]
target._flush()
algorithm = GDAL_RESAMPLE_ALGORITHMS[resampling]
target_srs = SpatialReference(srid)
if driver: data['driver'] = driver
return self.warp(data, resampling=resampling, max_error=max_error)
srs_type = 'epsg'
srs = srs_input
buf = c_char_p(b'') srs = capi.new_srs(buf)
if not srs: raise SRSException('Could not create spatial reference from: %s' % srs_input) else: self.ptr = srs
if srs_type == 'user': self.import_user_input(srs_input) elif srs_type == 'epsg': self.import_epsg(srs_input)
@property def linear_name(self): "Returns the name of the linear units." units, name = capi.linear_units(self.ptr, byref(c_char_p())) return name
def import_epsg(self, epsg): "Imports the Spatial Reference from the EPSG code (an integer)." capi.from_epsg(self.ptr, epsg)
@property def wkt(self): "Returns the WKT representation of this Spatial Reference." return capi.to_wkt(self.ptr, byref(c_char_p()))
try:
self.ensure_registered()
if dr_input.lower() in self._alias: name = self._alias[dr_input.lower()] else: name = dr_input
for iface in (vcapi, rcapi): driver = iface.get_driver_by_name(force_bytes(name)) if driver: break
if not driver: raise GDALException('Could not initialize GDAL/OGR Driver on input: %s' % str(dr_input)) self.ptr = driver
if not cls.driver_count(): vcapi.register_all() rcapi.register_all()
self._feat = feat self._index = index
fld_ptr = capi.get_feat_field_defn(feat.ptr, index) if not fld_ptr: raise GDALException('Cannot create OGR Field, invalid pointer given.') self.ptr = fld_ptr
self.__class__ = OGRFieldTypes[self.type]
if isinstance(self, OFTReal) and self.precision == 0: self.__class__ = OFTInteger self._double = True
def as_double(self): "Retrieves the Field's value as a double (float)." return capi.get_field_as_double(self._feat.ptr, self._index)
@property def name(self): "Returns the name of this Field." name = capi.get_field_name(self.ptr) return force_text(name, encoding=self._feat.encoding, strings_only=True)
return self.as_string()
class OFTInteger(Field): _double = False _bit64 = False
return int(self.as_double())
class OFTString(Field): pass
class OFTIntegerList(Field): pass
def arg_byref(args, offset=-1): "Returns the pointer argument's by-reference value." return args[offset]._obj.value
ptr = result if not ptr: s = None else: s = string_at(result)
check_err(result) ptr = ptr_byref(cargs, offset) s = ptr.value
def check_envelope(result, func, cargs, offset=-1): "Checks a function that returns an OGR Envelope by reference." env = ptr_byref(cargs, offset) return env
semi_major = srs_double(lgdal.OSRGetSemiMajor) semi_minor = srs_double(lgdal.OSRGetSemiMinor) invflattening = srs_double(lgdal.OSRGetInvFlattening)
morph_to_esri = void_output(lgdal.OSRMorphToESRI, [c_void_p]) morph_from_esri = void_output(lgdal.OSRMorphFromESRI, [c_void_p])
identify_epsg = void_output(lgdal.OSRAutoIdentifyEPSG, [c_void_p])
linear_units = units_func(lgdal.OSRGetLinearUnits) angular_units = units_func(lgdal.OSRGetAngularUnits)
to_xml = string_output(lgdal.OSRExportToXML, [c_void_p, POINTER(c_char_p), c_char_p], offset=-2, decoding='ascii')
isgeographic = int_output(lgdal.OSRIsGeographic, [c_void_p]) islocal = int_output(lgdal.OSRIsLocal, [c_void_p]) isprojected = int_output(lgdal.OSRIsProjected, [c_void_p])
new_ct = srs_output(std_call('OCTNewCoordinateTransformation'), [c_void_p, c_void_p]) destroy_ct = void_output(std_call('OCTDestroyCoordinateTransformation'), [c_void_p], errcheck=False)
void_output = partial(void_output, cpl=True) const_string_output = partial(const_string_output, cpl=True) double_output = partial(double_output, cpl=True)
func.argtypes = argtypes
func.restype = c_void_p func.errcheck = check_geom
func.restype = c_int
func.restype = gdal_char_p
func.restype = c_int
func.restype = c_int func.errcheck = partial(check_errcode, cpl=cpl)
def env_func(f, argtypes): "For getting OGREnvelopes." f.argtypes = argtypes f.restype = None f.errcheck = check_envelope return f
getx = pnt_func(lgdal.OGR_G_GetX) gety = pnt_func(lgdal.OGR_G_GetY) getz = pnt_func(lgdal.OGR_G_GetZ)
add_geom = void_output(lgdal.OGR_G_AddGeometry, [c_void_p, c_void_p]) import_wkt = void_output(lgdal.OGR_G_ImportFromWkt, [c_void_p, POINTER(c_char_p)])
destroy_geom = void_output(lgdal.OGR_G_DestroyGeometry, [c_void_p], errcheck=False)
assign_srs = void_output(lgdal.OGR_G_AssignSpatialReference, [c_void_p, c_void_p], errcheck=False) get_geom_srs = srs_output(lgdal.OGR_G_GetSpatialReference, [c_void_p])
geom_transform = void_output(lgdal.OGR_G_Transform, [c_void_p, c_void_p]) geom_transform_to = void_output(lgdal.OGR_G_TransformTo, [c_void_p, c_void_p])
get_envelope = env_func(lgdal.OGR_G_GetEnvelope, [c_void_p, POINTER(OGREnvelope)])
class GDALException(Exception): pass
OGRException = GDALException
self.num = num
self._envelope = args[0]
if len(args[0]) != 4: raise GDALException('Incorrect number of tuple elements (%d).' % len(args[0])) else: self._from_sequence(args[0])
self._from_sequence([float(a) for a in args])
try: from django.conf import settings lib_path = settings.GDAL_LIBRARY_PATH except (AttributeError, EnvironmentError, ImportError, ImproperlyConfigured): lib_path = None
lib_names = ['gdal111', 'gdal110', 'gdal19', 'gdal18', 'gdal17']
if lib_names: for lib_name in lib_names: lib_path = find_library(lib_name) if lib_path is not None: break
lgdal = CDLL(lib_path)
if os.name == 'nt': from ctypes import WinDLL lwingdal = WinDLL(lib_path)
_version_info = std_call('GDALVersionInfo') _version_info.argtypes = [c_char_p] _version_info.restype = c_char_p
CPLErrorHandler = CFUNCTYPE(None, c_int, c_int, c_char_p)
class Layer(GDALBase): "A class that wraps an OGR Layer, needs to be instantiated from a DataSource object."
self._random_read = self.test_capability(b'RandomRead')
if index < 0: raise OGRIndexError('Negative indices are not allowed on OGR Layers.') return self._make_feature(index)
start, stop, stride = index.indices(self.num_feat) return [self._make_feature(fid) for fid in range(start, stop, stride)]
capi.reset_reading(self._ptr) for i in range(self.num_feat): yield Feature(capi.get_next_feature(self._ptr), self)
try: return Feature(capi.get_feature(self.ptr, feat_id), self) except GDALException: pass
for feat in self: if feat.fid == feat_id: return feat
xmin, ymin, xmax, ymax = map(c_double, filter) capi.set_spatial_filter_rect(self.ptr, xmin, ymin, xmax, ymax)
class OGRGeometry(GDALBase): "Generally encapsulates an OGR geometry."
if str_instance and hex_regex.match(geom_input): geom_input = six.memoryview(a2b_hex(geom_input.upper().encode())) str_instance = False
if not g: raise GDALException('Cannot create OGR Geometry from input: %s' % str(geom_input)) self.ptr = g
if srs: self.srs = srs
self.__class__ = GEO_CLASSES[self.geom_type.num]
def __getstate__(self): srs = self.srs if srs: srs = srs.wkt else: srs = None return bytes(self.wkb), srs
def __or__(self, other): "Returns the union of the two geometries." return self.union(other)
def __and__(self, other): "Returns the intersection of this Geometry and the other." return self.intersection(other)
def __sub__(self, other): "Return the difference this Geometry and the other." return self.difference(other)
def __xor__(self, other): "Return the symmetric difference of this Geometry and the other." return self.sym_difference(other)
@property def dimension(self): "Returns 0 for points, 1 for lines, and 2 for surfaces." return capi.get_dims(self.ptr)
def _get_srid(self): srs = self.srs if srs: return srs.srid return None
@property def geos(self): "Returns a GEOSGeometry object from this OGRGeometry." from django.contrib.gis.geos import GEOSGeometry return GEOSGeometry(self.wkb, self.srid)
buf = (c_ubyte * sz)() capi.to_wkb(self.ptr, byteorder, byref(buf)) return six.memoryview(string_at(buf, sz))
def clone(self): "Clones this OGR Geometry." return OGRGeometry(capi.clone_geom(self.ptr), self.srs)
capi.geom_close_rings(self.ptr)
return func(self.ptr, other.ptr)
class Point(OGRGeometry):
class LinearRing(LineString): pass
@property def shell(self): "Returns the shell of this Polygon."
return sum(self[i].point_count for i in range(self.geom_count))
p = OGRGeometry(OGRGeomType('Point')) capi.get_centroid(self.ptr, p.ptr) return p
class GeometryCollection(OGRGeometry): "The Geometry Collection class."
return sum(self[i].point_count for i in range(self.geom_count))
class MultiPoint(GeometryCollection): pass
_ptr = None
ptr_type = c_void_p
def _get_ptr(self): if self._ptr: return self._ptr else: raise GDALException('GDAL %s pointer no longer valid.' % self.__class__.__name__)
@property def encoding(self): return self._layer._ds.encoding
geom_ptr = capi.get_feat_geom_ref(self.ptr) return OGRGeometry(geom_api.clone_geom(geom_ptr))
g = feature.geom
desc = feature['description']
for field in feature: nm = field.name
t = field.type
class DataSource(GDALBase): "Wraps an OGR Data Source object."
if write: self._write = 1 else: self._write = 0 self.encoding = encoding
raise GDALException('Invalid data source file "%s"' % ds_input)
if messages is None: break if messages: self._used_storages.add(storage) all_messages.extend(messages) if all_retrieved: break
elif storage in self._used_storages: storage._store([], response) self._used_storages.remove(storage)
return Message(*obj[1:])
max_cookie_size = 2048 not_finished = '__messagesnotfinished__'
messages.pop()
return json.loads(value, cls=MessageDecoder)
self.used = True return None
level = int(level) if level < self.level: return self.added_new = True message = Message(level, message, extra_tags=extra_tags) self._queued_messages.append(message)
if not modeladmin.has_delete_permission(request): raise PermissionDenied
deletable_objects, model_count, perms_needed, protected = get_deleted_objects( queryset, opts, request.user, modeladmin.admin_site, using)
self.wait_for('body')
pass
with self.disable_implicit_wait(): self.wait_until( lambda driver: len(driver.find_elements_by_css_selector(options_selector)) == 0 )
parameter_name = None
cls._field_list_filters.insert( cls._take_priority_index, (test, list_filter_class)) cls._take_priority_index += 1
if timezone.is_aware(now): now = timezone.localtime(now)
return self.field.label_tag( contents=mark_safe(contents), attrs=attrs, label_suffix='' if self.is_checkbox else None, )
if callable(field): class_name = field.__name__ if field.__name__ != '<lambda>' else '' else: class_name = field
from __future__ import unicode_literals
operations = [ migrations.AlterField( model_name='logentry', name='action_time', field=models.DateTimeField( verbose_name='action time', default=timezone.now, editable=False, ), ), ]
from __future__ import unicode_literals
for line in fieldset: for field in line: count += 1
count += 1
return _static(path)
if field_name == 'action_checkbox': yield { "text": text, "class_attrib": mark_safe(' class="action-checkbox-column"'), "sortable": False, } continue
yield { "text": text, "class_attrib": format_html(' class="column-{}"', field_name), "sortable": False, } continue
o_list_primary.insert(0, param) o_list_toggle.append(param)
if link_in_col(first, field_name, cl): table_tag = 'th' if first else 'td' first = False
def __init__(self, form, *items): self.form = form super(ResultList, self).__init__(*items)
site_title = ugettext_lazy('Django site admin')
site_header = ugettext_lazy('Django administration')
index_title = ugettext_lazy('Site administration')
site_url = '/'
admin_obj = admin_class(model, self) if admin_class is not ModelAdmin and settings.DEBUG: system_check_errors.extend(admin_obj.check())
from django.contrib.auth.views import redirect_to_login return redirect_to_login( request.get_full_path(), reverse('admin:login', current_app=self.name) )
if not getattr(view, 'csrf_exempt', False): inner = csrf_protect(inner) return update_wrapper(inner, view)
from django.contrib.contenttypes import views as contenttype_views
has_permission=False, **(extra_context or {})
index_path = reverse('admin:index', current_app=self.name) return HttpResponseRedirect(index_path)
if True not in perms.values(): continue
app_list = sorted(app_dict.values(), key=lambda x: x['name'].lower())
for app in app_list: app['models'].sort(key=lambda x: x['name'])
site = AdminSite()
ALL_VAR = 'all' ORDER_VAR = 'o' ORDER_TYPE_VAR = 'ot' PAGE_VAR = 'p' SEARCH_VAR = 'q' ERROR_FLAG = 'e'
for ignored in IGNORED_PARAMS: if ignored in lookup_params: del lookup_params[ignored] return lookup_params
spec = list_filter(request, lookup_params, self.model, self.model_admin)
field, field_list_filter_class = list_filter
field, field_list_filter_class = list_filter, FieldListFilter.create
use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, field_path)
result_count = paginator.count
if self.model_admin.show_full_result_count: full_result_count = self.root_queryset.count() else: full_result_count = None can_show_all = result_count <= self.list_max_show_all multi_page = result_count > self.list_per_page
self.show_admin_actions = not self.show_full_result_count or bool(full_result_count) self.full_result_count = full_result_count self.result_list = result_list self.can_show_all = can_show_all self.multi_page = multi_page self.paginator = paginator
if order_field.startswith('-') and pfx == "-": ordering.append(order_field[1:]) else: ordering.append(pfx + order_field)
ordering.extend(queryset.query.order_by)
(self.filter_specs, self.has_filters, remaining_lookup_params, filters_use_distinct) = self.get_filters(request)
qs = self.root_queryset for filter_spec in self.filter_specs: new_qs = filter_spec.queryset(request, qs) if new_qs is not None: qs = new_qs
qs = qs.filter(**remaining_lookup_params)
raise
ordering = self.get_ordering(request, qs) qs = qs.order_by(*ordering)
qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)
if filters_use_distinct | search_use_distinct: return qs.distinct() else: return qs
forms.MultiWidget.__init__(self, widgets, attrs)
related_url = reverse( 'admin:%s_%s_changelist' % ( rel_to._meta.app_label, rel_to._meta.model_name, ), current_app=self.admin_site.name, )
attrs['class'] = 'vManyToManyRawIdAdminField'
from django.contrib.contenttypes.models import ContentType return ContentType.objects.get_for_model(obj, for_concrete_model=False)
overrides = copy.deepcopy(FORMFIELD_FOR_DBFIELD_DEFAULTS) for k, v in self.formfield_overrides.items(): overrides.setdefault(k, {}).update(v) self.formfield_overrides = overrides
if db_field.choices: return self.formfield_for_choice_field(db_field, request, **kwargs)
return db_field.formfield(**kwargs)
if not db_field.remote_field.through._meta.auto_created: return None db = kwargs.get('using')
return reverse('admin:view_on_site', kwargs={ 'content_type_id': get_content_type_for_model(obj).pk, 'object_id': obj.pk })
break
return True
if field.primary_key: return True
for many_to_many in opts.many_to_many: if many_to_many.m2m_target_field_name() == to_field: return True
add_form_template = None change_form_template = None change_list_template = None delete_confirmation_template = None delete_selected_confirmation_template = None object_history_template = None
actions = [] action_form = helpers.ActionForm actions_on_top = True actions_on_bottom = False actions_selection_counter = True checks_class = ModelAdminChecks
exclude.extend(self.form._meta.exclude)
if self.actions is None or IS_POPUP_VAR in request.GET: return OrderedDict()
actions = filter(None, actions)
actions = OrderedDict( (name, (func, name, desc)) for func, name, desc in actions )
if callable(action): func = action action = action.__name__
elif hasattr(self.__class__, action): func = getattr(self.__class__, action)
else: try: func = self.admin_site.get_action(action) except KeyError: return None
return list(list_display)[:1]
"_saveasnew" in request.POST and self.save_as_continue and self.has_change_permission(request, obj)
try: action_index = int(request.POST.get('index', 0)) except ValueError: action_index = 0
data = request.POST.copy() data.pop(helpers.ACTION_CHECKBOX_NAME, None) data.pop("index", None)
if action_form.is_valid(): action = action_form.cleaned_data['action'] select_across = action_form.cleaned_data['select_across'] func = self.get_actions(request)[action][0]
queryset = queryset.filter(pk__in=selected)
if isinstance(response, HttpResponseBase): return response else: return HttpResponseRedirect(request.get_full_path())
if isinstance(f, models.ManyToManyField): initial[k] = initial[k].split(",")
if request.method == 'POST' and not form_validated and "_saveasnew" in request.POST: context['show_save'] = False context['show_save_and_continue'] = False add = False
actions = self.get_actions(request) if actions: list_display = ['action_checkbox'] + list(list_display)
action_failed = False selected = request.POST.getlist(helpers.ACTION_CHECKBOX_NAME)
formset = cl.formset = None
elif cl.list_editable: FormSet = self.get_changelist_formset(request) formset = cl.formset = FormSet(queryset=cl.result_list)
if formset: media = self.media + formset.media else: media = self.media
if actions: action_form = self.action_form(auto_id=None) action_form.fields['action'].choices = self.get_action_choices(request) else: action_form = None
(deleted_objects, model_count, perms_needed, protected) = get_deleted_objects( [obj], opts, request.user, self.admin_site, using)
opts = model._meta app_label = opts.app_label action_list = LogEntry.objects.filter( object_id=unquote(object_id), content_type=get_content_type_for_model(model) ).select_related().order_by('action_time')
exclude.extend(self.form._meta.exclude)
_('%(class_name)s %(instance)s') % { 'class_name': p._meta.verbose_name, 'instance': p}
for field in opts.fields: if field.remote_field and field.remote_field.model != self.parent_model: opts = field.remote_field.model._meta break
return no_edit_link
return format_html('{}: <a href="{}">{}</a>', capfirst(opts.verbose_name), admin_url, obj)
return no_edit_link
((field.many_to_one and not field.related_model) or field.one_to_many)):
label = field.related_model._meta.verbose_name
from __future__ import unicode_literals
return []
return []
return []
try: field = model._meta.get_field(item) except FieldDoesNotExist: try: field = getattr(model, item) except AttributeError: field = None
field = item
errors = super(InlineModelAdminChecks, self)._check_exclude(obj) if errors: return []
if self._check_relation(obj, parent_model): return []
MODEL_METHODS_EXCLUDE = ('_', 'add_', 'delete', 'save', 'set_')
self.template_name = 'admin_doc/missing_docutils.html' return self.render_to_response(admin.site.each_context(request))
pass
pass
pass
pattern = named_group_matcher.sub(lambda m: m.group(1), pattern)
pattern = non_named_group_matcher.sub("<var>", pattern)
session_data = None
max_age=settings.SESSION_COOKIE_AGE, salt='django.contrib.sessions.backends.signed_cookies',
self.create()
from django.contrib.sessions.models import Session return Session
self.save(must_create=True)
continue
data = None
if not set(session_key).issubset(set(VALID_KEY_CHARS)): raise InvalidSessionKey( "Invalid characters in session key")
expiry_age = self.get_expiry_age(expiry=self._expiry_date(session_data)) if expiry_age <= 0: session_data = {} self.delete() self.create()
session_data = self._get_session(no_load=must_create)
shutil.move(output_file_name, session_file_name) renamed = True
session.create = lambda: None session.load()
VALID_KEY_CHARS = string.ascii_lowercase + string.digits
if isinstance(e, SuspiciousOperation): logger = logging.getLogger('django.security.%s' % e.__class__.__name__) logger.warning(force_text(e)) return {}
self._session_cache = {} self.accessed = True self.modified = True
try: expiry = kwargs['expiry'] except KeyError: expiry = self.get('_session_expiry')
try: expiry = kwargs['expiry'] except KeyError: expiry = self.get('_session_expiry')
try: del self['_session_expiry'] except KeyError: pass return
from __future__ import unicode_literals
from __future__ import unicode_literals
return mark_safe("%d%s" % (value, suffixes[value % 10]))
return value
return value
'a second ago', '%(count)s seconds ago', delta.seconds
'a minute ago', '%(count)s minutes ago', count
'an hour ago', '%(count)s hours ago', count
'a second from now', '%(count)s seconds from now', delta.seconds
'a minute from now', '%(count)s minutes from now', count
'an hour from now', '%(count)s hours from now', count
if hasattr(self.base_field, 'from_db_value'): self.from_db_value = self._from_db_value super(ArrayField, self).__init__(**kwargs)
vals = json.loads(value) value = [self.base_field.to_python(val) for val in vals]
return [tuple(value) for value in values]
if hasattr(self, 'base_field'): self.base_field = self.base_field() super(RangeField, self).__init__(*args, **kwargs)
register_hstore_handler(schema_editor.connection)
if id_: id_ += '_0' return id_
for key, val in value.items(): value[key] = six.text_type(val) return value
initial_value = self.to_python(initial) return super(HStoreField, self).has_changed(initial_value, data)
message=string_concat( SimpleLazyObject(lambda: prefix % params), SimpleLazyObject(lambda: error.message % error_params), ), code=code, params=dict(error_params, **params),
if not location: self.base_location = None self.location = None
return name
if '?#' in name and not unparsed_name[3]: unparsed_name[2] += '?' return urlunsplit(unparsed_name)
if url.startswith(('http:', 'https:', '//', '#', 'data:')): return matched
if url.startswith('/') and not url.startswith(settings.STATIC_URL): return matched
url_path, fragment = urldefrag(url)
assert url_path.startswith(settings.STATIC_URL) target_name = url_path[len(settings.STATIC_URL):]
source_name = name if os.sep == '/' else name.replace(os.sep, '/') target_name = posixpath.join(posixpath.dirname(source_name), url_path)
hashed_url = self.url(unquote(target_name), force=True)
if fragment: transformed_url += ('?#' if '?#' in url else '#') + fragment
return template % unquote(transformed_url)
if dry_run: return
hashed_files = OrderedDict()
adjustable_paths = [ path for path in paths if matches_patterns(path, self._patterns.keys()) ]
def path_level(name): return len(name.split(os.sep))
storage, path = paths[name] with storage.open(path) as original_file:
hashed_name = self.hashed_name(name, original_file)
if hasattr(original_file, 'seek'): original_file.seek(0)
hashed_files[self.hash_key(name)] = hashed_name yield name, hashed_name, processed
self.hashed_files.update(hashed_files)
self.hashed_files[hash_key] = cache_name
self.hashed_files = _MappingCache(default_cache)
if settings.DEBUG and not urlpatterns: urlpatterns += staticfiles_urlpatterns()
if getattr(storage, 'prefix', None): prefixed_path = os.path.join(storage.prefix, path) else: prefixed_path = path
os.unlink(full_path)
target_last_modified = self.storage.get_modified_time(prefixed_path)
pass
source_last_modified = source_storage.get_modified_time(path)
handles_files = True
return _static(path)
return _do_static(parser, token)
searched_locations = []
if storage.exists(path): matched_path = storage.path(path) if matched_path: return matched_path
if not isinstance(self.storage, (Storage, LazyObject)): self.storage = self.storage() super(BaseStorageFinder, self).__init__(*args, **kwargs)
return [] if all else None
if host not in SITE_CACHE: SITE_CACHE[host] = self.get(domain__iexact=host) return SITE_CACHE[host]
domain, port = split_domain_port(host) if not port: raise if domain not in SITE_CACHE: SITE_CACHE[domain] = self.get(domain__iexact=domain) return SITE_CACHE[domain]
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
response_gone_class = http.HttpResponseGone response_redirect_class = http.HttpResponsePermanentRedirect
if response.status_code != 404: return response
return response
self._cache = {}
ct = self.get(pk=id) self._add_to_cache(self.db, ct)
key = (ct.app_label, ct.model) self._cache.setdefault(using, {})[key] = ct self._cache.setdefault(using, {})[ct.id] = ct
if absurl.startswith(('http://', 'https://', '//')): return http.HttpResponseRedirect(absurl)
object_domain = None
if object_domain is None: try: object_domain = Site.objects.get_current(request).domain except Site.DoesNotExist: pass
object_domain = RequestSite(request).domain
if object_domain is not None: protocol = request.scheme return http.HttpResponseRedirect('%s://%s%s' % (protocol, object_domain, absurl)) else: return http.HttpResponseRedirect(absurl)
return []
for gfk in gfks: if gfk.ct_field == obj.ct_field and gfk.fk_field == obj.ct_fk_field: return []
exclude.extend(self.form._meta.exclude)
from __future__ import unicode_literals
from __future__ import unicode_literals
content_type.model = old_model
ContentType.objects.clear_cache()
try: ContentType = apps.get_model('contenttypes', 'ContentType') except LookupError: available = False else: if not router.allow_migrate_model(using, ContentType): return available = True
if backward: break else: available = True continue
if apps is not global_apps: global_apps.get_model('contenttypes', 'ContentType').objects.clear_cache()
auto_created = False concrete = False editable = False hidden = False
raise Exception("Impossible arguments to GFK.get_content_type!")
auto_created = False
if not cls._meta.abstract:
manager = getattr(self.model, kwargs.pop('manager')) manager_class = create_generic_related_manager(manager.__class__, rel) return manager_class(instance=self.instance)
queryset.delete()
objs = tuple(objs)
from __future__ import unicode_literals
return iri_to_uri(get_script_prefix().rstrip('/') + self.url)
f.title = mark_safe(f.title) f.content = mark_safe(f.content)
from __future__ import unicode_literals
if self.starts_with: flatpages = flatpages.filter( url__startswith=self.starts_with.resolve(context))
if self.user: user = self.user.resolve(context) if not user.is_authenticated: flatpages = flatpages.filter(registration_required=False) else: flatpages = flatpages.filter(registration_required=False)
if len(bits) >= 3 and len(bits) <= 6:
if len(bits) % 2 == 0: prefix = bits[1] else: prefix = None
if bits[-2] != 'as': raise template.TemplateSyntaxError(syntax_message) context_name = bits[-1]
if len(bits) >= 5: if bits[-4] != 'for': raise template.TemplateSyntaxError(syntax_message) user = bits[-3] else: user = None
super(FlatpageFallbackMiddleware, self).__init__(get_response)
except Http404: return response except Exception: if settings.DEBUG: raise return response
try: ts_b36, hash = token.split("-") except ValueError: return False
if not constant_time_compare(self._make_token_with_timestamp(user, ts), token): return False
if (self._num_days(self._today()) - ts) > settings.PASSWORD_RESET_TIMEOUT_DAYS: return False
ts_b36 = int_to_base36(timestamp)
return date.today()
if self.is_active and self.is_superuser: return True
return _user_has_perm(self, perm, obj)
if self.is_active and self.is_superuser: return True
return get_user_model()._meta.pk.to_python(request.session[SESSION_KEY])
continue
break
user.backend = backend_path return user
user_login_failed.send(sender=__name__, credentials=_clean_credentials(credentials))
request.session.flush()
language = request.session.get(LANGUAGE_SESSION_KEY)
self._password = None
self._password = None self.save(update_fields=["password"])
self.password = make_password(None)
if not is_safe_url(url=redirect_to, host=request.get_host()): return resolve_url(settings.LOGIN_REDIRECT_URL) return redirect_to
if not is_safe_url(url=next_page, host=request.get_host()): next_page = request.path
return HttpResponseRedirect(next_page)
uid = force_text(urlsafe_base64_decode(uidb64)) user = UserModel._default_manager.get(pk=uid)
update_session_auth_hash(request, form.user) return HttpResponseRedirect(post_change_redirect)
kwargs['queryset'] = qs.select_related('content_type')
if lookup.startswith('password'): return False return super(UserAdmin, self).lookup_allowed(lookup, value)
return initial
return self.initial["password"]
subject = ''.join(subject.splitlines()) body = loader.render_to_string(email_template_name, context)
continue
password = None user_data = {} fake_user_data = {}
default_username = get_default_username() try:
if field.remote_field: fake_user_data[field_name] = field.remote_field.model(input_value)
continue
searched_perms = list() ctypes = set() for klass in app_config.get_models(): ctype = ContentType.objects.db_manager(using).get_for_model(klass)
all_perms = set(Permission.objects.using(using).filter( content_type__in=ctypes, ).values_list( "content_type", "codename" ))
return ''
return ''
from django.contrib.auth import models as auth_app
if auth_app.User._meta.swapped: return ''
try: auth_app.User._meta.get_field('username').run_validators(default_username) except exceptions.ValidationError: return ''
if check_db and default_username: try: auth_app.User._default_manager.get(username=default_username) except auth_app.User.DoesNotExist: pass else: return '' return default_username
UserModel().set_password(password)
create_unknown_user = True
from __future__ import unicode_literals
]
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
db.reset_queries()
if not is_correct and not hasher_changed and must_update: hasher.harden_runtime(password, encoded)
pass
algorithm, variety, raw_params, salt, data = bits version = 0x10
if self.digest is not None: password = binascii.hexlify(self.digest(force_bytes(password)).digest()) else: password = force_bytes(password)
diff = 2**(self.rounds - int(rounds)) - 1 while diff > 0: self.encode(password, force_bytes(salt)) diff -= 1
if user.has_perms(perms): return True if raise_exception: raise PermissionDenied return False
from __future__ import unicode_literals
return []
header = "REMOTE_USER" force_logout_if_no_header = True
user = auth.authenticate(remote_user=username) if user: request.user = user auth.login(request, user)
auth.logout(request)
raise TypeError("PermLookupDict is not iterable.")
raise TypeError("PermWrapper is not iterable.")
return bool(self[perm_name])
urlconf_module = arg
urlconf_module, app_name, namespace = view return RegexURLResolver(regex, urlconf_module, kwargs, app_name=app_name, namespace=namespace)
if self._wrapped is empty: return '<LazySettings [Unevaluated]>' return '<LazySettings "%(settings_module)s">' % { 'settings_module': self._wrapped.SETTINGS_MODULE, }
for setting in dir(global_settings): if setting.isupper(): setattr(self, setting, getattr(global_settings, setting))
self.SETTINGS_MODULE = settings_module
SETTINGS_MODULE = None
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DATETIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'd/m/Y' TIME_FORMAT = 'P' DATETIME_FORMAT = 'd/m/Y P' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd/m/Y' SHORT_DATETIME_FORMAT = 'd/m/Y P'
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'j F, Y' TIME_FORMAT = 'g:i A' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'j M, Y'
from __future__ import unicode_literals
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'g:i A' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'j M Y'
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DATETIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'd F Y' TIME_FORMAT = 'g:i A' SHORT_DATE_FORMAT = 'j M Y'
from __future__ import unicode_literals
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'h:i A' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'j M Y'
from __future__ import unicode_literals
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = r'Yeko M\re\n d\a' TIME_FORMAT = 'H:i' SHORT_DATE_FORMAT = 'Y M j'
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'G:i' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd.m.Y'
DECIMAL_SEPARATOR = ','
from __future__ import unicode_literals
DATE_FORMAT = 'j. F Y' TIME_FORMAT = 'H:i' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j. F' SHORT_DATE_FORMAT = 'j.n.Y'
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.' NUMBER_GROUPING = 3
from __future__ import unicode_literals
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DATETIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = ' '
from __future__ import unicode_literals
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'd F Y' TIME_FORMAT = 'g.i.A' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'Y-m-d'
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DATETIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.' NUMBER_GROUPING = 3
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] TIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'g:i A' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd-m-Y'
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
DATE_FORMAT = 'j F Y' TIME_FORMAT = 'H:i' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'j M Y'
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DATETIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'j F, Y' TIME_FORMAT = 'g:i A' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'j M, Y'
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
from __future__ import unicode_literals
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_FORMAT = 'j F، Y' TIME_FORMAT = 'g:i A' YEAR_MONTH_FORMAT = 'F Y' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd‏/m‏/Y'
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
DATE_FORMAT = 'd F Y' TIME_FORMAT = 'H:i' MONTH_DAY_FORMAT = 'j F' SHORT_DATE_FORMAT = 'd.m.Y'
DECIMAL_SEPARATOR = ','
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
'%d/%m/%Y', '%d/%m/%y'
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
from __future__ import unicode_literals
from __future__ import unicode_literals
DATE_INPUT_FORMATS = [
] DATETIME_INPUT_FORMATS = [
from __future__ import unicode_literals
DECIMAL_SEPARATOR = ',' THOUSAND_SEPARATOR = '.'
from __future__ import unicode_literals
from __future__ import unicode_literals
DECIMAL_SEPARATOR = '.' THOUSAND_SEPARATOR = ','
def gettext_noop(s): return s
DEBUG_PROPAGATE_EXCEPTIONS = False
USE_ETAGS = False
ADMINS = []
INTERNAL_IPS = []
ALLOWED_HOSTS = []
TIME_ZONE = 'America/Chicago'
USE_TZ = False
LANGUAGE_CODE = 'en-us'
LANGUAGES_BIDI = ["he", "ar", "fa", "ur"]
USE_I18N = True LOCALE_PATHS = []
LANGUAGE_COOKIE_NAME = 'django_language' LANGUAGE_COOKIE_AGE = None LANGUAGE_COOKIE_DOMAIN = None LANGUAGE_COOKIE_PATH = '/'
USE_L10N = False
MANAGERS = ADMINS
DEFAULT_CONTENT_TYPE = 'text/html' DEFAULT_CHARSET = 'utf-8'
FILE_CHARSET = 'utf-8'
SERVER_EMAIL = 'root@localhost'
DATABASES = {}
DATABASE_ROUTERS = []
EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
EMAIL_HOST = 'localhost'
EMAIL_PORT = 25
EMAIL_HOST_USER = '' EMAIL_HOST_PASSWORD = '' EMAIL_USE_TLS = False EMAIL_USE_SSL = False EMAIL_SSL_CERTFILE = None EMAIL_SSL_KEYFILE = None EMAIL_TIMEOUT = None
INSTALLED_APPS = []
DEFAULT_FROM_EMAIL = 'webmaster@localhost'
EMAIL_SUBJECT_PREFIX = '[Django] '
APPEND_SLASH = True
PREPEND_WWW = False
FORCE_SCRIPT_NAME = None
SECRET_KEY = ''
DEFAULT_FILE_STORAGE = 'django.core.files.storage.FileSystemStorage'
MEDIA_ROOT = ''
MEDIA_URL = ''
STATIC_ROOT = None
STATIC_URL = None
FILE_UPLOAD_HANDLERS = [ 'django.core.files.uploadhandler.MemoryFileUploadHandler', 'django.core.files.uploadhandler.TemporaryFileUploadHandler', ]
DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000
FILE_UPLOAD_TEMP_DIR = None
FILE_UPLOAD_PERMISSIONS = None
FILE_UPLOAD_DIRECTORY_PERMISSIONS = None
DATE_FORMAT = 'N j, Y'
DATETIME_FORMAT = 'N j, Y, P'
TIME_FORMAT = 'P'
YEAR_MONTH_FORMAT = 'F Y'
MONTH_DAY_FORMAT = 'F j'
SHORT_DATE_FORMAT = 'm/d/Y'
SHORT_DATETIME_FORMAT = 'm/d/Y P'
DATE_INPUT_FORMATS = [
TIME_INPUT_FORMATS = [
DATETIME_INPUT_FORMATS = [
FIRST_DAY_OF_WEEK = 0
DECIMAL_SEPARATOR = '.'
USE_THOUSAND_SEPARATOR = False
NUMBER_GROUPING = 0
THOUSAND_SEPARATOR = ','
DEFAULT_TABLESPACE = '' DEFAULT_INDEX_TABLESPACE = ''
X_FRAME_OPTIONS = 'SAMEORIGIN'
MIDDLEWARE_CLASSES = [ 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', ]
CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.locmem.LocMemCache', } } CACHE_MIDDLEWARE_KEY_PREFIX = '' CACHE_MIDDLEWARE_SECONDS = 600 CACHE_MIDDLEWARE_ALIAS = 'default'
PASSWORD_RESET_TIMEOUT_DAYS = 3
CSRF_FAILURE_VIEW = 'django.views.csrf.csrf_failure'
CSRF_COOKIE_NAME = 'csrftoken' CSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52 CSRF_COOKIE_DOMAIN = None CSRF_COOKIE_PATH = '/' CSRF_COOKIE_SECURE = False CSRF_COOKIE_HTTPONLY = False CSRF_HEADER_NAME = 'HTTP_X_CSRFTOKEN' CSRF_TRUSTED_ORIGINS = []
MESSAGE_STORAGE = 'django.contrib.messages.storage.fallback.FallbackStorage'
LOGGING_CONFIG = 'logging.config.dictConfig'
LOGGING = {}
DEFAULT_EXCEPTION_REPORTER_FILTER = 'django.views.debug.SafeExceptionReporterFilter'
TEST_RUNNER = 'django.test.runner.DiscoverRunner'
TEST_NON_SERIALIZED_APPS = []
FIXTURE_DIRS = []
STATICFILES_DIRS = []
STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.StaticFilesStorage'
STATICFILES_FINDERS = [ 'django.contrib.staticfiles.finders.FileSystemFinder', 'django.contrib.staticfiles.finders.AppDirectoriesFinder', ]
MIGRATION_MODULES = {}
SECURE_BROWSER_XSS_FILTER = False SECURE_CONTENT_TYPE_NOSNIFF = False SECURE_HSTS_INCLUDE_SUBDOMAINS = False SECURE_HSTS_SECONDS = 0 SECURE_REDIRECT_EXEMPT = [] SECURE_SSL_HOST = None SECURE_SSL_REDIRECT = False
t = Engine().from_string(CSRF_FAILURE_TEMPLATE)
raise
raise
raise
raise
return http.HttpResponseBadRequest(template.render())
raise
def _reject(self, request, reason): return None
get_token(request) return retval
def wrapped_view(*args, **kwargs): return view_func(*args, **kwargs) wrapped_view.csrf_exempt = True return wraps(view_func, assigned=available_attrs(view_func))(wrapped_view)
def get_last_modified(): if last_modified_func: dt = last_modified_func(request, *args, **kwargs) if dt: return timegm(dt.utctimetuple())
def etag(etag_func): return condition(etag_func=etag_func)
names = []
url = force_text(self.success_url)
model = self.model
model = self.object.__class__
model = self.get_queryset().model
def put(self, *args, **kwargs): return self.post(*args, **kwargs)
def post(self, request, *args, **kwargs): return self.delete(request, *args, **kwargs)
return {date_field: date}
qs = qs.none()
qs = self.get_queryset() if queryset is None else queryset
lookup_kwargs = self._make_single_date_lookup(date) qs = qs.filter(**lookup_kwargs)
start, end = get_current(date), get_next(date)
if allow_empty: if is_previous: result = get_current(start - datetime.timedelta(days=1)) else: result = end
if not allow_future: if generic_view.uses_datetime_field: now = timezone.now() else: now = timezone_today() lookup['%s__lte' % date_field] = now
try: result = getattr(qs[0], date_field) except IndexError: return None
if generic_view.uses_datetime_field: if settings.USE_TZ: result = timezone.localtime(result) result = result.date()
return get_current(result)
for key, value in six.iteritems(kwargs): setattr(self, key, value)
update_wrapper(view, cls, updated=())
update_wrapper(view, cls.dispatch, assigned=()) return view
if queryset is None: queryset = self.get_queryset()
pk = self.kwargs.get(self.pk_url_kwarg) slug = self.kwargs.get(self.slug_url_kwarg) if pk is not None: queryset = queryset.filter(pk=pk)
if slug is not None and (pk is None or self.query_pk_and_slug): slug_field = self.get_slug_field() queryset = queryset.filter(**{slug_field: slug})
if pk is None and slug is None: raise AttributeError("Generic detail view %s must be called with " "either an object pk or a slug." % self.__class__.__name__)
obj = queryset.get()
names = []
if self.object and self.template_name_field: name = getattr(self.object, self.template_name_field, None) if name: names.insert(0, name)
if not names: raise
}, "formats": { },
return [os.path.join(app.path, 'locale') for app in app_configs]
}, "formats": { },
continue
continue
DEBUG_ENGINE = Engine(debug=True)
cleansed = value
cleansed = CallableSettingWrapper(cleansed)
return import_string(settings.DEFAULT_EXCEPTION_REPORTER_FILTER)()
for k, v in cleansed.items(): cleansed[k] = CLEANSED_SUBSTITUTE return cleansed
for param in sensitive_post_parameters: if param in cleansed: cleansed[param] = CLEANSED_SUBSTITUTE return cleansed
is_multivalue_dict = isinstance(value, MultiValueDict)
value = self.get_cleansed_multivaluedict(request, value)
for name, value in tb_frame.f_locals.items(): cleansed[name] = CLEANSED_SUBSTITUTE
for name, value in tb_frame.f_locals.items(): if name in sensitive_variables: value = CLEANSED_SUBSTITUTE else: value = self.cleanse_special_types(request, value) cleansed[name] = value
for name, value in tb_frame.f_locals.items(): cleansed[name] = self.cleanse_special_types(request, value)
cleansed['func_args'] = CLEANSED_SUBSTITUTE cleansed['func_kwargs'] = CLEANSED_SUBSTITUTE
if isinstance(v, six.binary_type):
exceptions = [] exc_value = self.exc_value while exc_value: exceptions.append(exc_value) exc_value = explicit_or_implicit_cause(exc_value)
if not exceptions: return frames
exc_value = exceptions.pop() tb = self.tb if six.PY2 or not exceptions else exc_value.__traceback__
if six.PY2: tb = tb.tb_next elif not tb.tb_next and exceptions: exc_value = exceptions.pop() tb = exc_value.__traceback__ else: tb = tb.tb_next
if installed_apps is None and hasattr(sys.modules[__name__], 'apps'): raise RuntimeError("You must supply an installed_apps argument.")
self.app_configs = OrderedDict()
self.stored_app_configs = []
self.apps_ready = self.models_ready = self.ready = False
self._lock = threading.Lock()
self._pending_operations = defaultdict(list)
if installed_apps is not None: self.populate(installed_apps)
with self._lock: if self.ready: return
if self.app_configs: raise RuntimeError("populate() isn't reentrant")
for app_config in self.app_configs.values(): all_models = self.all_models[app_config.label] app_config.import_models(all_models)
if swapped and swapped == to_string: return model._meta.swappable if model._meta.swappable and model._meta.label == to_string: return model._meta.swappable
def apply_next_model(model): next_function = partial(apply_next_model.func, model) self.lazy_model_operation(next_function, *more_models) apply_next_model.func = function
self.name = app_name
self.module = app_module
if not hasattr(self, 'label'): self.label = app_name.rpartition(".")[2]
if not hasattr(self, 'verbose_name'): self.verbose_name = self.label.title()
if not hasattr(self, 'path'): self.path = self._path_from_module(app_module)
self.models_module = None
self.models = None
module = import_module(entry)
module = None
if not mod_path: raise
entry = module.default_app_config
return cls(entry, module)
mod = import_module(mod_path) try: cls = getattr(mod, cls_name) except AttributeError: if module is None: import_module(entry) else: raise
if not issubclass(cls, AppConfig): raise ImproperlyConfigured( "'%s' isn't a subclass of AppConfig." % entry)
try: app_name = cls.name except AttributeError: raise ImproperlyConfigured( "'%s' must supply a name attribute." % entry)
app_module = import_module(app_name)
return cls(app_name, app_module)
self.models = all_models
return constant_time_compare( _unsalt_cipher_token(request_csrf_token), _unsalt_cipher_token(csrf_token), )
def _accept(self, request): request.csrf_processing_done = True return None
request.csrf_cookie_needs_reset = True
if getattr(callback, 'csrf_exempt', False): return None
if '' in (referer.scheme, referer.netloc): return self._reject(request, REASON_MALFORMED_REFERER)
if referer.scheme != 'https': return self._reject(request, REASON_INSECURE_REFERER)
good_hosts = list(settings.CSRF_TRUSTED_ORIGINS) good_hosts.append(good_referer)
return self._reject(request, REASON_NO_CSRF_COOKIE)
request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')
raise
signals.got_request_exception.send(sender=self.handler.__class__, request=request) response = self.handler.handle_uncaught_exception(request, get_resolver(get_urlconf()), sys.exc_info())
return response
if not request.COOKIES and response.cookies and has_vary_header(response, 'Cookie'): return response
cache_key = get_cache_key(request, self.key_prefix, 'GET', cache=self.cache) if cache_key is None: request._cache_update_cache = True
request._cache_update_cache = False return response
if response.get('X-Frame-Options') is not None: return response
if getattr(response, 'xframe_options_exempt', False): return response
super(LocaleMiddleware, self).__init__(get_response)
language_url = request.get_full_path(force_append_slash=path_needs_slash).replace( script_prefix, '%s%s/' % (script_prefix, language), 1 ) return self.response_redirect_class(language_url)
if 'HTTP_USER_AGENT' in request.META: for user_agent_regex in settings.DISALLOWED_USER_AGENTS: if user_agent_regex.search(request.META['HTTP_USER_AGENT']): raise PermissionDenied('Forbidden user agent')
if self.should_redirect_with_slash(request): path = self.get_full_path_with_slash(request) else: path = request.get_full_path()
if redirect_url or path != request.get_full_path(): redirect_url += path return self.response_redirect_class(redirect_url)
if response.status_code == 404: if self.should_redirect_with_slash(request): return self.response_redirect_class(self.get_full_path_with_slash(request))
return bool(re.match("^https?://%s/" % re.escape(domain), referer))
if not referer: return True
if settings.APPEND_SLASH and uri.endswith('/') and referer == uri[:-1]: return True
if not self.is_internal_request(domain, referer) and '?' in referer: return True
parsed_referer = urlparse(referer) if parsed_referer.netloc in ['', domain] and parsed_referer.path == uri: return True
if not response.streaming and len(response.content) < 200: return response
if response.has_header('Content-Encoding'): return response
response.streaming_content = compress_sequence(response.streaming_content) del response['Content-Length']
from __future__ import unicode_literals
verbose_name = "\xc3\x85ngstr\xc3\xb6m's Articles"
movie_id = models.AutoField(primary_key=True) name = models.CharField(max_length=60)
return b'Name\xff: %s'.decode() % self.name
class Model1(models.Model): pkey = models.IntegerField(unique=True, db_index=True)
query.get_compiler(using=db).as_sql()
a = Article.objects.create( headline="Look at me!", pub_date=datetime.datetime.now() ) self.assertIs(a.get_status_display(), None)
a = Article.objects.get(pk=a.pk) self.assertEqual(a.misc_data, '') self.assertIs(type(a.misc_data), six.text_type)
b = BrokenUnicodeMethod.objects.create(name="Jerry") self.assertEqual(repr(b), "<BrokenUnicodeMethod: [Bad Unicode data]>")
m3 = Model3.objects.get(model2=1000) m3.model2
self.assertQuerysetEqual( qs & qs2, [('Second Revision', 'First Revision')], transform=lambda r: (r.title, r.base.title), ordered=False )
self.assertQuerysetEqual( qs.order_by('-second_extra').values_list('first', flat=True), ['a', 'a'], lambda x: x)
test_environ = os.environ.copy() if sys.platform.startswith('java'): python_path_var_name = 'JYTHONPATH' else: python_path_var_name = 'PYTHONPATH'
address_predefined = 'DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ old_address = os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS')
self.assertEqual('DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ, address_predefined) self.assertEqual(os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS'), old_address)
self.assertEqual(os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'], 'blah')
if address_predefined: os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = old_address else: del os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS']
call_command(self.cmd, addrport="beef:7654") self.assertServerSettings('beef', '7654')
call_command(self.cmd, addrport="deadbeef:7654") self.assertServerSettings('deadbeef', '7654')
self.assertIn("Not checking migrations", self.output.getvalue())
self.assertEqual(set(self.run_manage(args1)), set(self.run_manage(args2)))
with self.assertRaises(TypeError): command.run_from_argv(['', ''])
err.truncate(0) with self.assertRaises(CommandError): command.run_from_argv(['', '', '--traceback'])
self.assertTrue(mock_connections.close_all.called)
out, err = self.run_django_admin(args) self.assertNoOutput(out) self.assertOutput(err, "already exists")
out, err = self.run_django_admin(args) self.assertNoOutput(out) self.assertOutput(err, "already exists")
from __future__ import unicode_literals
class UserProfile(models.Model): user = models.OneToOneField(User, models.CASCADE, primary_key=True)
from __future__ import unicode_literals
with self.assertNumQueries(0): self.assertEqual(obj.name, "first") self.assertEqual(obj.other_value, 0)
SESSION_KEY = '2b1189a188b44ad18c35e1baac6ceead'
proxy = Proxy.objects.create(name="proxy", value=42)
item = SimpleItem.objects.create(name='first', value=47) feature = Feature.objects.create(item=item) SpecialFeature.objects.create(feature=feature)
specimens = models.Manager()
db_table = "Fixtures_regress_plant"
class Channel(models.Model): name = models.CharField(max_length=255)
class SpecialArticle(Article): pass
class CommonFeature(Article):
@python_2_unicode_compatible class Widget(models.Model): name = models.CharField(max_length=255)
class TestManager(models.Manager): def get_by_natural_key(self, key): return self.get(name=key)
def natural_key(self): return (self.name,) natural_key.dependencies = ['fixtures_regress.store']
class Circle1(models.Model): name = models.CharField(max_length=255)
class Thingy(models.Model): name = models.CharField(max_length=255)
natural_key.dependencies = ["fixtures_regress.M2MComplexCircular2A"]
from __future__ import unicode_literals
management.call_command( 'loaddata', 'sequence', verbosity=0, )
self.assertEqual( list(articles.values_list('id', flat=True)), [1, 2, 3, 4, 5, 6, 7, 8] )
data = out.getvalue()
data = re.sub('0{6,}[0-9]', '', data)
self.assertEqual(sorted_deps[-1], M2MThroughAB)
self.assertEqual(sorted_deps[:3], [A, B, C]) self.assertEqual(sorted_deps[3:], [AtoB, BtoC, CtoA])
relative_prefix = os.path.relpath(current_dir, os.getcwd()).replace('\\', '/') fixtures = [relative_prefix + '/fixtures/absolute.json']
from __future__ import absolute_import
content = template.render({}, request) self.assertEqual(content, 'yes')
content = template.render({'processors': 'no'}, request) self.assertEqual(content, 'no')
'APP_DIRS': True, 'OPTIONS': {'loaders': []},
self.assertEqual(rf2.name, 'new foo') self.assertEqual(rf2.value, 'new bar')
self.assertDone([1, 3])
paginator = Paginator([], 2) self.assertEqual(paginator.validate_number(1), 1)
for x in range(1, 10): a = Article(headline='Article %s' % x, pub_date=datetime(2005, 7, 29)) a.save()
self.vw.default_parts.add(self.sunroof)
models.signals.m2m_changed.connect( self.m2m_changed_signal_receiver, Person.fans.through ) models.signals.m2m_changed.connect( self.m2m_changed_signal_receiver, Person.friends.through )
members = models.ManyToManyField(Person, through=Membership) user_members = models.ManyToManyField(User, through='UserMembership')
class A(models.Model): a_text = models.CharField(max_length=20)
@python_2_unicode_compatible class Car(models.Model): make = models.CharField(max_length=20, unique=True, null=True) drivers = models.ManyToManyField('Driver', through='CarDriver')
no = models.IntegerField(verbose_name='number', blank=True, null=True)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.django_book, self.djangonaut_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.guitar_book, self.django_book, self.djangonaut_book])
request = self.request_factory.get('/', {'date_registered__isnull': 'True'}) changelist = self.get_changelist(request, Book, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(queryset.count(), 1) self.assertEqual(queryset[0], self.bio_book)
self.test_datefieldlistfilter()
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.django_book])
modeladmin = BookAdminWithCustomQueryset(self.alfred, Book, site) request = self.request_factory.get('/') changelist = self.get_changelist(request, Book, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.guitar_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.django_book, self.bio_book, self.djangonaut_book])
request = self.request_factory.get('/', {'books_authored__isnull': 'True'}) changelist = self.get_changelist(request, User, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.lisa])
request = self.request_factory.get('/', {'books_contributed__isnull': 'True'}) changelist = self.get_changelist(request, User, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.alfred])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.guitar_book, self.djangonaut_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.django_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book, self.djangonaut_book])
request = self.request_factory.get('/', {}) changelist = self.get_changelist(request, Book, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), list(Book.objects.all().order_by('-id')))
request = self.request_factory.get('/', {'publication-decade': 'the 80s'}) changelist = self.get_changelist(request, Book, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [])
request = self.request_factory.get('/', {'publication-decade': 'the 90s'}) changelist = self.get_changelist(request, Book, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book])
request = self.request_factory.get('/', {'publication-decade': 'the 00s'}) changelist = self.get_changelist(request, Book, modeladmin)
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.guitar_book, self.djangonaut_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.djangonaut_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.bio_book])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.jack, self.john])
queryset = changelist.get_queryset(request) self.assertEqual(list(queryset), [self.john])
from django.conf.urls import url
self.a.friends.add(self.b, self.c)
self.d.friends.add(self.a, self.c)
self.b.friends.add(self.a)
self.b.friends.remove(self.a)
self.a.friends.clear()
self.assertQuerysetEqual( self.a.friends.all(), [] )
self.assertQuerysetEqual( self.c.friends.all(), [ "David", ], attrgetter("name") )
self.assertQuerysetEqual( self.d.friends.all(), [ "Chuck", ], attrgetter("name") )
self.d.stalkers.add(self.a)
self.a.idols.add(self.d)
self.d.stalkers.add(self.a)
self.a.idols.add(self.a)
with self.assertRaises(AttributeError): setattr(self.a, 'full_name', 'Paul McCartney')
with self.assertRaisesMessage(TypeError, "'full_name' is an invalid keyword argument"): Person(full_name='Paul McCartney')
a2 = Person(full_name_2='Paul McCartney') a2.save() self.assertEqual(a2.first_name, 'Paul')
number2 = models.IntegerField(blank=True, default=Numbers.get_static_number)
class SomeModel(models.Model): somefield = models.IntegerField()
original = Event.DoesNotExist("Doesn't exist") unpickled = pickle.loads(pickle.dumps(original))
self.assertEqual(original.__class__, unpickled.__class__) self.assertEqual(original.args, unpickled.args)
self.assert_pickles( Event.objects.select_related('group').order_by('title').values_list('title', 'group__name') )
groups = pickle.loads(pickle.dumps(groups)) self.assertQuerysetEqual(groups, [g], lambda x: x)
groups = pickle.loads(pickle.dumps(groups)) self.assertQuerysetEqual(groups, [g], lambda x: x)
qs = Happening.objects.annotate(latest_time=models.Max('when')) self.assert_pickles(qs)
dummy1 = _("This is a translatable string.")
dummy2 = _("This is another translatable string.")
number = 3 dummy3 = ungettext("%(number)s Foo", "%(number)s Foos", number) % {'number': number}
dummy5 = _('This literal should be included.')
from __future__ import unicode_literals
for key, value in settings.items(): _format_cache[(key, lang)] = value try: yield finally: reset_format_cache()
result = ungettext_lazy( '%(name)s has %(num)d good result', '%(name)s has %(num)d good results',
t = Template('{% load i18n %}{% blocktrans context "unexisting" %}May{% endblocktrans %}') rendered = t.render(Context()) self.assertEqual(rendered, 'May')
class sideeffect_str(str): def split(self, *args, **kwargs): res = str.split(self, *args, **kwargs) trans_real._translations['en-YY'] = None return res
self.assertLess(translation_count, len(trans_real._translations))
self.assertEqual('10000', nformat(self.l, decimal_sep='.', decimal_pos=0, grouping=0, force_grouping=True))
self.assertEqual('', get_format('THOUSAND_SEPARATOR'))
self.assertEqual(0, get_format('FIRST_DAY_OF_WEEK'))
self.assertInHTML( '<input type="text" name="products_delivered" ' 'value="12.000" id="id_products_delivered" required />', form6.as_ul() )
self.assertEqual(sanitize_separators(123), 123)
self.assertEqual(sanitize_separators('10.10'), '10.10')
self.assertEqual(list(iter_format_modules('de')), [default_mod])
self.assertEqual( list(iter_format_modules('de', 'i18n.other.locale')), [test_mod, default_mod])
self.assertEqual( list(iter_format_modules('de', ['i18n.other.locale', 'i18n.other2.locale'])), [test_mod, test_mod2, default_mod])
r.META = {'HTTP_ACCEPT_LANGUAGE': 'es-us'} self.assertEqual(g(r), 'es')
r.COOKIES = {settings.LANGUAGE_COOKIE_NAME: 'es-us'} r.META = {} self.assertEqual(g(r), 'es')
self.assertUgettext('Date/time', 'Datum/Zeit')
with self.modify_settings(INSTALLED_APPS={'append': 'i18n.resolution'}): activate('de')
self.assertUgettext('Date/time', 'Datum/Zeit')
activate('de')
self.assertUgettext('Date/time', 'Datum/Zeit (APP)')
self.assertEqual(ugettext("Title"), "Title")
self.client.get('/fr/simple/') self.assertNotIn(LANGUAGE_SESSION_KEY, self.client.session)
self.assertFalse(check_for_language('tr-TR.UTF-8')) self.assertFalse(check_for_language('tr-TR.UTF8')) self.assertFalse(check_for_language('de-DE.utf-8'))
clear_url_caches()
clear_url_caches()
self.assertEqual(translate_url('/en/account/register/', 'nl'), '/nl/profiel/registeren/') self.assertEqual(translation.get_language(), 'en')
response = self.client.get('/account/register/', HTTP_ACCEPT_LANGUAGE='pl-pl') self.assertRedirects(response, '/en/account/register/')
self.assertListEqual(response.redirect_chain, [('/en/account/register/', 302)]) self.assertRedirects(response, '/en/account/register/', 302)
from __future__ import unicode_literals
pofile = os.path.join(proj_dir, 'locale', 'fr', 'LC_MESSAGES', 'django.po')
from django.utils.translation import ugettext as _
from __future__ import unicode_literals
from __future__ import unicode_literals
execute_from_command_line(['django-admin', 'help', 'compilemessages'])
from __future__ import unicode_literals
cwd_prefix = '%s%s' % (os.curdir, os.sep)
cwd_prefix = ''
self.assertFalse(os.path.exists('./templates/template_with_error.tpl.py'))
self.assertIn('msgctxt "Context wrapped in double quotes"', po_contents) self.assertIn('msgctxt "Context wrapped in single quotes"', po_contents)
self.assertIn('msgctxt "Special blocktrans context wrapped in double quotes"', po_contents) self.assertIn('msgctxt "Special blocktrans context wrapped in single quotes"', po_contents)
self.assertLocationCommentPresent(self.PO_FILE, 'Translatable literal #6b', 'templates', 'test.html')
execute_from_command_line(['django-admin', 'help', 'makemessages'])
CUSTOM_DAY_FORMAT = 'd/m/Y CUSTOM'
dummy1 = _("This is a translatable string.")
dummy2 = _("This is another translatable string.")
dates = [] with self.assertNumQueries(0): article_datetimes_iterator = Article.objects.datetimes('pub_date', 'day', order='DESC').iterator()
from __future__ import unicode_literals
f = TypedMultipleChoiceField(choices=[(1, "+1"), (-1, "-1")], coerce=int, required=False) self.assertEqual([], f.clean([]))
f = TypedMultipleChoiceField(choices=[(1, "+1"), (-1, "-1")], coerce=int, required=False, empty_value=None) self.assertIsNone(f.clean([]))
from __future__ import unicode_literals
with self.assertRaisesMessage(ValidationError, "'Enter a valid URL.'"): f.clean('http://%s' % ("X" * 200,))
with self.assertRaisesMessage(ValidationError, "'Enter a valid URL.'"): f.clean('http://%s' % ("X" * 60,))
self.assertEqual(url, f.clean(url))
from __future__ import unicode_literals
from datetime import date, datetime
self.assertHTMLEqual( a['mydate'].as_hidden(), '<input type="hidden" name="mydate" value="2008-4-1" id="id_mydate" />', )
class GetDateShowHiddenInitial(Form): mydate = DateField(widget=SelectDateWidget, show_hidden_initial=True)
f = DateField() with self.assertRaisesMessage(ValidationError, "'Enter a valid date.'"): f.clean('a\x00b')
self.assertEqual(e.__class__, ValueError)
f = CharField() self.assertEqual(f.widget_attrs(TextInput()), {}) self.assertEqual(f.widget_attrs(Textarea()), {})
from __future__ import unicode_literals
from __future__ import unicode_literals
f = TypedChoiceField(choices=[(1, "+1"), (-1, "-1")], coerce=bool) self.assertTrue(f.clean('-1'))
from __future__ import unicode_literals
self.assertFalse(f.has_changed('', None))
self.assertTrue(f.has_changed('', {'filename': 'resume.txt', 'content': 'My resume'}))
self.assertFalse(f.has_changed('resume.txt', None))
self.assertTrue(f.has_changed('resume.txt', {'filename': 'resume.txt', 'content': 'My resume'}))
from __future__ import unicode_literals
from __future__ import unicode_literals
addr = 'viewx3dtextx26qx3d@yahoo.comx26latlngx3d15854521645943074058' self.assertEqual(addr, f.clean(addr))
NULLBOOL_CHOICES = (('1', 'Yes'), ('0', 'No'), ('', 'Unknown'))
from __future__ import unicode_literals
self.assertInHTML('<option value="0">empty_label</option>', w.render('mydate', ''), count=3)
self.assertEqual(w.render('mydate', '2010-02-30').count('selected="selected"'), 3)
self.assertEqual(w1.choices, [1, 2, 3])
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertIsInstance(form.cleaned_data['multi_choice_optional'], models.query.QuerySet) self.assertIsInstance(form.cleaned_data['multi_choice'], models.query.QuerySet)
class BoundaryForm(ModelForm): class Meta: model = BoundaryModel fields = '__all__'
class DefaultsForm(ModelForm): class Meta: model = Defaults fields = '__all__'
tests = [ (EmptyCharLabelNoneChoiceForm, 'choice_string_w_none', None), (EmptyIntegerLabelChoiceForm, 'choice_integer', None), (EmptyCharLabelChoiceForm, 'choice', ''), ]
activate('nl')
with self.assertRaises(forms.ValidationError): f.clean('1:30:05 PM')
result = f.clean('13:30:05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '13:30:05')
result = f.clean('13:30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
result = f.clean('13:30:05.000155') self.assertEqual(result, time(13, 30, 5, 155))
with self.assertRaises(forms.ValidationError): f.clean('1:30:05 PM')
result = f.clean('13:30:05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '13:30:05')
result = f.clean('13:30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
result = f.clean('13.30.05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:05")
result = f.clean('13.30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
result = f.clean('13.30.05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:05")
result = f.clean('13.30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('13:30:05')
result = f.clean('1:30:05 PM') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '01:30:05 PM')
result = f.clean('1:30 PM') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM")
with self.assertRaises(forms.ValidationError): f.clean('13:30:05')
result = f.clean('1:30:05 PM') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '01:30:05 PM')
result = f.clean('01:30 PM') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM")
result = f.clean('13.30.05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:05 PM")
result = f.clean('13.30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM")
result = f.clean('13.30.05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:05 PM")
result = f.clean('13.30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM")
with self.assertRaises(forms.ValidationError): f.clean('1:30:05 PM')
result = f.clean('13:30:05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:05")
result = f.clean('13:30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('1:30:05 PM')
result = f.clean('13:30:05') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:05")
result = f.clean('13:30') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('13:30:05')
result = f.clean('1:30:05 PM') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:05")
result = f.clean('1:30 PM') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('13:30:05')
result = f.clean('1:30:05 PM') self.assertEqual(result, time(13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:05")
result = f.clean('1:30 PM') self.assertEqual(result, time(13, 30, 0))
text = f.widget.format_value(result) self.assertEqual(text, "13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('21/12/2010')
self.assertEqual(f.clean('2010-12-21'), date(2010, 12, 21))
result = f.clean('21.12.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010')
result = f.clean('21.12.10') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
with self.assertRaises(forms.ValidationError): f.clean('21/12/2010')
result = f.clean('21.12.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010')
result = f.clean('21.12.10') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12.21.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12-21-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12.21.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12-21-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21')
result = f.clean('21.12.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010')
result = f.clean('21-12-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21')
result = f.clean('21.12.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010')
result = f.clean('21-12-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12.21.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12-21-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12.21.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
result = f.clean('12-21-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010")
with self.assertRaises(forms.ValidationError): f.clean('21.12.2010')
result = f.clean('2010-12-21') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
result = f.clean('12/21/2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
with self.assertRaises(forms.ValidationError): f.clean('21.12.2010')
result = f.clean('2010-12-21') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
result = f.clean('12/21/2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21')
result = f.clean('21.12.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
result = f.clean('21-12-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21')
result = f.clean('21.12.2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
result = f.clean('21-12-2010') self.assertEqual(result, date(2010, 12, 21))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21")
with self.assertRaises(forms.ValidationError): f.clean('1:30:05 PM 21/12/2010')
self.assertEqual(f.clean('2010-12-21 13:30:05'), datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010 13:30:05')
result = f.clean('21.12.2010 13:30') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('1:30:05 PM 21/12/2010')
text = f.widget.format_value(result) self.assertEqual(text, '21.12.2010 13:30:05')
result = f.clean('21.12.2010 13:30') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:00")
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:05")
result = f.clean('13.30 12-21-2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:00")
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:05")
result = f.clean('13.30 12-21-2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "21.12.2010 13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21 13:30:05')
result = f.clean('1:30:05 PM 21/12/2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '01:30:05 PM 21/12/2010')
result = f.clean('1:30 PM 21-12-2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM 21/12/2010")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21 13:30:05')
result = f.clean('1:30:05 PM 21/12/2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, '01:30:05 PM 21/12/2010')
result = f.clean('1:30 PM 21-12-2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM 21/12/2010")
text = f.widget.format_value(result) self.assertEqual(text, "01:30:05 PM 21/12/2010")
result = f.clean('12-21-2010 13:30') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM 21/12/2010")
text = f.widget.format_value(result) self.assertEqual(text, "01:30:05 PM 21/12/2010")
result = f.clean('12-21-2010 13:30') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "01:30:00 PM 21/12/2010")
with self.assertRaises(forms.ValidationError): f.clean('13:30:05 21.12.2010')
result = f.clean('2010-12-21 13:30:05') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:05")
result = f.clean('12/21/2010 13:30:05') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:05")
with self.assertRaises(forms.ValidationError): f.clean('13:30:05 21.12.2010')
result = f.clean('2010-12-21 13:30:05') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:05")
result = f.clean('12/21/2010 13:30:05') self.assertEqual(result, datetime(2010, 12, 21, 13, 30, 5))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:05")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21 13:30:05')
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:05")
result = f.clean('1:30 PM 21-12-2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:00")
with self.assertRaises(forms.ValidationError): f.clean('2010-12-21 13:30:05')
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:05")
result = f.clean('1:30 PM 21-12-2010') self.assertEqual(result, datetime(2010, 12, 21, 13, 30))
text = f.widget.format_value(result) self.assertEqual(text, "2010-12-21 13:30:00")
from __future__ import unicode_literals
ChoiceFormSet = formset_factory(Choice)
FavoriteDrinksFormSet = formset_factory(FavoriteDrinkForm, formset=BaseFavoriteDrinksFormSet, extra=3)
class SplitDateTimeForm(Form): when = SplitDateTimeField(initial=datetime.datetime.now)
formset = self.make_choiceformset() self.assertFalse(formset.is_valid()) self.assertFalse(formset.has_changed())
blank_formset = self.make_choiceformset([('', '')]) self.assertFalse(blank_formset.has_changed())
invalid_formset = self.make_choiceformset([('Calexico', '')]) self.assertFalse(invalid_formset.is_valid()) self.assertTrue(invalid_formset.has_changed())
valid_formset = self.make_choiceformset([('Calexico', '100')]) self.assertTrue(valid_formset.is_valid()) self.assertTrue(valid_formset.has_changed())
ChoiceFormSet = formset_factory(Choice, extra=1, min_num=1)
self.assertFalse(formset.forms[0].empty_permitted) self.assertTrue(formset.forms[1].empty_permitted)
ChoiceFormSet = formset_factory(Choice, extra=0, min_num=3)
data['check-1-DELETE'] = '' formset = CheckFormSet(data, prefix='check') self.assertFalse(formset.is_valid())
LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=3, max_num=0) formset = LimitedFavoriteDrinkFormSet() form_output = []
initial = [ {'name': 'Gin Tonic'}, ] LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, extra=3, max_num=2) formset = LimitedFavoriteDrinkFormSet(initial=initial) form_output = []
ChoiceFormset = formset_factory(Choice, extra=3) formset = ChoiceFormset()
forms = list(formset) self.assertEqual(forms, formset.forms) self.assertEqual(len(formset), len(forms))
self.assertEqual(formset[0], forms[0]) try: formset[3] self.fail('Requesting an invalid formset index should raise an exception') except IndexError: pass
class BaseReverseFormSet(BaseFormSet): def __iter__(self): return reversed(self.forms)
class CustomErrorList(ErrorList): pass
self.assertEqual(len(formset.forms), 3) self.assertFalse(formset.is_valid())
self.assertEqual(len(formset.forms), 4)
class BaseCustomFormSet(BaseFormSet): def clean(self): raise ValidationError("This is a non-form error")
class ArticleForm(Form): title = CharField() pub_date = DateField()
self.assertFalse(empty_forms[0].is_bound) self.assertFalse(empty_forms[1].is_bound)
self.assertHTMLEqual(empty_forms[0].as_p(), empty_forms[1].as_p())
from __future__ import unicode_literals
extra_attrs = {'class': 'special'}
GENDERS = (('\xc5', 'En tied\xe4'), ('\xf8', 'Mies'), ('\xdf', 'Nainen'))
class CopyForm(Form): degree = IntegerField(widget=Select(choices=((1, gettext_lazy('test')),)))
class DataForm(Form): data = CharField(max_length=10)
class HiddenForm(Form): data = IntegerField(widget=HiddenInput)
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(article.content, "\r\nTst\r\n")
from __future__ import unicode_literals
p = Person({'first_name': 'John', 'last_name': 'Lennon', 'birthday': '1940-10-9'})
class OptionalPersonForm(Form): first_name = CharField() last_name = CharField() birth_date = DateField(required=False)
class ContactForm(Form): subject = CharField() message = CharField(widget=Textarea)
class ContactForm(Form): subject = CharField() message = CharField(widget=Textarea(attrs={'rows': 80, 'cols': 20}))
class FrameworkForm(Form): name = CharField() language = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')])
class FrameworkForm(Form): name = CharField() language = ChoiceField()
class FrameworkForm(Form): name = CharField() language = ChoiceField(choices=[('P', 'Python'), ('J', 'Java')], widget=RadioSelect)
class BeatleForm(Form): name = CharField()
class SongForm(Form): name = CharField() composers = MultipleChoiceField()
class MessageForm(Form): when = SplitDateTimeField()
class EscapingForm(Form): special_name = CharField(label="<em>Special</em> Field") special_safe_name = CharField(label=mark_safe("<em>Special</em> Field"))
for field, error_list in self._errors.items(): if not isinstance(error_list, self.error_class): self._errors[field] = self.error_class(error_list)
self.assertFalse(form.is_valid())
self.assertIsInstance(form._errors, forms.ErrorDict)
class Person(Form): first_name = CharField() last_name = CharField()
class MyForm(Form): def __init__(self, data=None, auto_id=False, field_list=[]): Form.__init__(self, data, auto_id=auto_id)
class Person(Form): first_name = CharField(required=False) last_name = CharField(required=False)
class TestForm(Form): foo = CharField(widget=HiddenInput) bar = CharField(widget=HiddenInput)
class UserRegistration(Form):
class UserRegistration(Form): username = CharField(max_length=10, widget=TextInput(attrs={'maxlength': 20})) password = CharField(max_length=10, widget=PasswordInput)
class UserRegistration(Form): username = CharField(max_length=10, label='Your username') password1 = CharField(widget=PasswordInput) password2 = CharField(widget=PasswordInput, label='Contraseña (de nuevo)')
class UserRegistration(Form): username = CharField(max_length=10, label='') password = CharField(widget=PasswordInput)
class UserRegistration(Form): username = CharField(max_length=10, label=None) password = CharField(widget=PasswordInput)
class UserRegistration(Form): username = CharField(max_length=10, initial='django') password = CharField(widget=PasswordInput)
def initial_django(): return 'django'
class PedanticField(forms.Field): def to_python(self, value): raise ValidationError('Whatever')
self.assertIs(form['name'], name)
now_no_ms = now.replace(microsecond=0) if now == now_no_ms: now = now.replace(microsecond=1)
class UserRegistration(Form): username = CharField(max_length=10, help_text='e.g., user@example.com') password = CharField(widget=PasswordInput, help_text='Wählen Sie mit Bedacht.')
class Person(Form): first_name = CharField() last_name = CharField() birthday = DateField()
class Person(Form): first_name = CharField() last_name = CharField() birthday = DateField()
class Person(Form): first_name = CharField() last_name = CharField() birthday = DateField()
class Person(Form): first_name = CharField() prefix = 'foo'
class Person(Form): name = CharField() is_cool = NullBooleanField()
class FileForm(Form): file1 = FileField()
class SongForm(Form): artist = CharField() name = CharField()
form = SongForm(data, empty_permitted=True) self.assertTrue(form.is_valid()) self.assertEqual(form.errors, {}) self.assertEqual(form.cleaned_data, {})
data = {'artist': None, 'song': ''} form = SongForm(data, empty_permitted=True) self.assertTrue(form.is_valid())
class PriceForm(Form): amount = FloatField() qty = IntegerField()
if value == '{}': return {} return super(CustomJSONField, self).to_python(value)
((), {}, '<label for="id_field">Field:</label>'),
(('custom',), {}, '<label for="id_field">custom:</label>'),
((), {'attrs': {'class': 'pretty'}}, '<label for="id_field" class="pretty">Field:</label>')
return { 'username': data['username'].lower(), 'password': 'this_is_not_a_secret', }
from __future__ import unicode_literals
self.assertHTMLEqual( str(ErrorList(ValidationError(VeryBadError()).messages)), '<ul class="errorlist"><li>A very bad error.</li></ul>' )
from django.forms import CharField, Form, Media, MultiWidget, TextInput from django.template import Context, Template from django.test import SimpleTestCase, override_settings from django.utils.encoding import force_text
class MyWidget(TextInput): pass
class MyWidget4(TextInput): class Media: css = {'all': ('/path/to/css1', '/path/to/css1')} js = ('/path/to/js1', '/path/to/js1')
class MyWidget8(MyWidget1): class Media: css = { 'all': ('/path/to/css3', 'path/to/css1') } js = ('/path/to/js1', '/path/to/js4')
class MyMultiWidget(MultiWidget): def __init__(self, attrs=None): widgets = [MyWidget1, MyWidget2, MyWidget3] super(MyMultiWidget, self).__init__(widgets, attrs)
class FormWithMedia(Form): field1 = CharField(max_length=20, widget=MyWidget1()) field2 = CharField(max_length=20, widget=MyWidget2())
manager1 = OnlyFred() manager2 = OnlyBarney() objects = models.Manager()
restricted = Value42()
class AbstractBase3(models.Model): comment = models.CharField(max_length=50)
@python_2_unicode_compatible class Child1(AbstractBase1): data = models.CharField(max_length=25)
default = models.Manager()
@python_2_unicode_compatible class RelatedModel(models.Model): test_gfk = GenericRelation('RelationModel', content_type_field='gfk_ctype', object_id_field='gfk_id') exact = models.NullBooleanField()
msg = "Manager isn't available; AbstractBase2 is abstract" with self.assertRaisesMessage(AttributeError, msg): AbstractBase2.restricted.all()
msg = "Manager isn't available; AbstractBase1 is abstract" with self.assertRaisesMessage(AttributeError, msg): AbstractBase1.objects.all()
related = RelatedModel.objects.create(exact=False) relation = related.test_fk.create() self.assertEqual(related.test_fk.get(), relation)
with warnings.catch_warnings(record=True) as warns: warnings.simplefilter('always', RemovedInDjango20Warning) MyModel._base_manager self.assertEqual(len(warns), 0)
class MyModel2(models.Model): objects = MyManager()
class MyRelModel2(models.Model): objects = MyManager()
class MyRelModel2(models.Model): objects = MyManager()
with warnings.catch_warnings(record=True) as warns: warnings.simplefilter('always', RemovedInDjango20Warning)
with warnings.catch_warnings(record=True) as warns: warnings.simplefilter('always', RemovedInDjango20Warning)
class MyModel4(AbstractParent, ConcreteParentWithManager): pass self.assertIs(MyModel4.default, MyModel4._default_manager) self.assertIsNone(getattr(MyModel4, 'objects', None))
class MyModel5(ConcreteParentWithManager): class Meta: manager_inheritance_from_future = True self.assertIs(MyModel5.default, MyModel5._default_manager) self.assertIsNone(getattr(MyModel5, 'objects', None))
ver_string = get_version(ver_tuple) six.assertRegex(self, ver_string, r'1\.4(\.dev[0-9]+)?')
c.value = 2 c.save() c.value = 3 c.save(force_update=True)
c.value = 4 with self.assertRaises(ValueError): c.save(force_insert=True, force_update=True)
c1 = Counter(name="two", value=2) with self.assertRaises(ValueError): with transaction.atomic(): c1.save(force_update=True) c1.save(force_insert=True)
c.value = 5 with self.assertRaises(IntegrityError): with transaction.atomic(): c.save(force_insert=True)
obj = WithCustomPK(name=1, value=1) with self.assertRaises(DatabaseError): with transaction.atomic(): obj.save(force_update=True)
ed, created = book.authors.get_or_create(name="Ed") self.assertTrue(created) self.assertEqual(book.authors.count(), 1)
ed, created = book.authors.get_or_create(name="Ed") self.assertFalse(created) self.assertEqual(book.authors.count(), 1)
fred, created = book.authors.get_or_create(name="Fred") self.assertTrue(created)
self.assertEqual(book.authors.count(), 2)
Author.objects.create(name="Ted")
self.assertEqual(Author.objects.count(), 3) self.assertEqual(book.authors.count(), 2)
_, created = ed.books.get_or_create(name="Ed's Recipes", publisher=p) self.assertTrue(created)
self.assertEqual(ed.books.count(), 2) self.assertEqual(fred.books.count(), 1)
_, created = ed.books.get_or_create(name='The Great Book of Ed', publisher_id=p.id) self.assertTrue(created)
_, created = ed.books.get_or_create(name='The Great Book of Ed', publisher_id=p.id) self.assertFalse(created)
self.assertEqual(p.books.count(), 3)
p, created = Person.objects.update_or_create(**params) self.assertFalse(created)
url(r'^\$', include([ url(r'^bar/$', lambda x: x, name='bar'), ])),
from __future__ import unicode_literals
from __future__ import unicode_literals
registry2 = CheckRegistry() registry2.register(f) registry2.register(f2, "tag1", "tag2") registry2.register(f3, "tag2", deploy=True)
result = check_url_config(None) self.assertEqual(len(result), 1) warning = result[0] self.assertEqual(warning.id, 'urls.W001')
from django.db import models from django.utils.encoding import python_2_unicode_compatible
age_sum = distinct_authors.aggregate(Sum('age')) self.assertEqual(age_sum['age__sum'], 103)
excluded_books = annotated_books.exclude(publisher__name="__UNLIKELY_VALUE__")
str(excluded_books.query)
self.assertIsNone(annotated_books.query.alias_map["aggregation_book"].join_type) self.assertIsNone(excluded_books.query.alias_map["aggregation_book"].join_type)
self.assertEqual( re.findall(r'order by (\w+)', qstr), [', '.join(f[1][0] for f in forced_ordering).lower()] )
class Greatest(Func): function = 'GREATEST'
form = Form() form_set = FormSet(instance=User())
form = Form() form_set = FormSet(instance=Restaurant())
self.assertEqual(formset[0].instance.user_id, "guido")
self.assertEqual(formset[0].instance.profile_id, 1)
Form(instance=None) FormSet(instance=None)
dalnet = Network.objects.create(name="DALnet") formset = HostFormSet(data, instance=dalnet, save_as_new=True)
self.test_init_database()
self.test_init_database()
self.test_init_database()
odd_ids = [user.pk for user in User.objects.all() if user.pk % 2] self.assertEqual(len(odd_ids), 0)
self.assertTrue(formset.is_valid()) formset.save() self.assertEqual(UserSite.objects.count(), 0)
self.assertEqual(signer.unsign(ts, max_age=datetime.timedelta(seconds=11)), value) with self.assertRaises(signing.SignatureExpired): signer.unsign(ts, max_age=10)
from __future__ import unicode_literals
file = SimpleUploadedFile("mode_test.txt", b"content") self.assertFalse(hasattr(file, 'mode')) gzip.GzipFile(fileobj=file)
f.DEFAULT_CHUNK_SIZE = 4 self.assertEqual(list(f), [b'one\n', b'two\n', b'three'])
f.DEFAULT_CHUNK_SIZE = 4 self.assertEqual(list(f), [b'one\r\n', b'two\r\n', b'three'])
f.DEFAULT_CHUNK_SIZE = 4 self.assertEqual(list(f), [b'one\r', b'two\r', b'three'])
with self.assertRaises(IOError): file_move_safe(self.file_a, self.file_b, allow_overwrite=False)
self.assertIsNone(file_move_safe(self.file_a, self.file_b, allow_overwrite=True))
response_gone_class = http.HttpResponseForbidden response_redirect_class = http.HttpResponseRedirect
class BinaryTree(models.Model): name = models.CharField(max_length=100) parent = models.ForeignKey('self', models.SET_NULL, null=True, blank=True)
class SomeParentModel(models.Model): name = models.CharField(max_length=1)
self.assertContains(response, '<h2>Author-book relationships</h2>') self.assertContains(response, 'Add another Author-book relationship') self.assertContains(response, 'id="id_Author_books-TOTAL_FORMS"')
self.assertContains( response, '<tr><td colspan="4"><ul class="errorlist nonfield">' '<li>The two titles must be the same</li></ul></td></tr>' )
self.assertContains( response, '<div class="js-inline-admin-formset inline-group" id="question_set-group"' ) self.assertContains(response, '<p>Callable in QuestionInline</p>')
author_book_auto_m2m_intermediate = Author.books.through.objects.get(author=author, book=book) self.author_book_auto_m2m_intermediate_id = author_book_auto_m2m_intermediate.pk
self.assertNotContains(response, '<h2>Author-book relationships</h2>') self.assertNotContains(response, 'Add another Author-Book Relationship') self.assertNotContains(response, 'id="id_Author_books-TOTAL_FORMS"')
self.assertNotContains(response, '<h2>Inner2s</h2>') self.assertNotContains(response, 'Add another Inner2') self.assertNotContains(response, 'id="id_inner2_set-TOTAL_FORMS"')
self.assertNotContains(response, '<h2>Author-book relationships</h2>') self.assertNotContains(response, 'Add another Author-Book Relationship') self.assertNotContains(response, 'id="id_Author_books-TOTAL_FORMS"')
self.assertNotContains(response, '<h2>Inner2s</h2>') self.assertNotContains(response, 'Add another Inner2') self.assertNotContains(response, 'id="id_inner2_set-TOTAL_FORMS"')
self.assertNotContains(response, '<h2>Author-book relationships</h2>') self.assertNotContains(response, 'Add another Author-Book Relationship') self.assertNotContains(response, 'id="id_Author_books-TOTAL_FORMS"')
self.selenium.find_element_by_link_text('Add another Profile').click()
self.assertEqual(ProfileCollection.objects.all().count(), 1) self.assertEqual(Profile.objects.all().count(), 3)
self.selenium.find_element_by_link_text('Add another Profile').click() self.selenium.find_element_by_link_text('Add another Profile').click()
class ChildModel1Inline(admin.TabularInline): model = ChildModel1
class BinaryTreeAdmin(admin.TabularInline): model = BinaryTree
class SightingInline(admin.TabularInline): model = Sighting
class SomeChildModelForm(forms.ModelForm):
site.register(Holder, HolderAdmin, inlines=[InnerInline]) site.register(Holder2, HolderAdmin, inlines=[InnerInline2]) site.register(Holder3, inlines=[InnerInline3])
('security', '/%2Fexample.com/security/', ['/example.com'], {}),
with self.assertRaises(NoReverseMatch): reverse(None)
self.assertEqual('/%257Eme/places/1/', reverse('places', args=[1]))
resolver = get_resolver('urlpatterns_reverse.namespace_urls') sub_resolver = resolver.namespace_dict['test-ns1'][1] self.assertIn('<RegexURLPattern list>', repr(sub_resolver))
class FakeObj(object): def get_absolute_url(self): return "/hi-there/"
redirect("urlpatterns_reverse.nonimported_module.view") self.assertNotIn("urlpatterns_reverse.nonimported_module", sys.modules)
from .views import nested_view self.assertEqual(reverse(nested_view), '/includes/nested_path/')
match_func, match_args, match_kwargs = resolve(path) self.assertEqual(match_func, func) self.assertEqual(match_args, args) self.assertEqual(match_kwargs, kwargs)
self.assertEqual(match[0], func) self.assertEqual(match[1], args) self.assertEqual(match[2], kwargs)
with self.assertRaisesMessage(TypeError, 'view must be a callable'): url(r'uncallable-object/$', views.uncallable)
msg = '(regex_error/$" is not a valid regular expression' with self.assertRaisesMessage(ImproperlyConfigured, msg): reverse(views.empty_view)
self.assertEqual(get_callable(empty_view), empty_view)
self.assertEqual(include(self.url_patterns, 'namespace'), (self.url_patterns, None, 'namespace'))
self.assertEqual( include(self.url_patterns, 'namespace', 'app_name'), (self.url_patterns, 'app_name', 'namespace') )
self.assertEqual( include((self.url_patterns, 'app_name', 'namespace')), (self.url_patterns, 'app_name', 'namespace') )
url(r'^(?:foo|bar)(\w+)/$', empty_view, name="disjunction"),
url('(.+)/security/$', empty_view, name='security'),
from django.conf.urls import url
raise AttributeError('I am here to confuse django.urls.get_callable')
from __future__ import unicode_literals
r = HttpResponse(12345) self.assertEqual(r.content, b'12345')
r = HttpResponse() r.content = 12345 self.assertEqual(r.content, b'12345')
r = HttpResponse() r.content = ['idan', 'alex', 'jacob'] self.assertEqual(r.content, b'idanalexjacob')
r = HttpResponse() r.content = ['1', '2', 3, '\u079e'] self.assertEqual(r.content, b'123\xde\x9e')
with self.assertRaises(AttributeError): response.content = "Hello dear" self.assertNotIn('content-type', response)
chunks = list(r) self.assertEqual(chunks, [b'hello', b'world']) for chunk in chunks: self.assertIsInstance(chunk, six.binary_type)
self.assertEqual(list(r), [])
self.assertFalse(hasattr(r, 'content'))
with self.assertRaises(AttributeError): r.content = 'xyz'
self.assertTrue(hasattr(r, 'streaming_content'))
r = StreamingHttpResponse(iter(['hello', 'world'])) self.assertEqual( six.binary_type(r), b'Content-Type: text/html; charset=utf-8')
self.assertEqual(list(r), [b'hello', b'world'])
r = StreamingHttpResponse(iter(['hello', 'world'])) with self.assertRaises(Exception): r.write('!')
with self.assertRaises(Exception): r.tell()
request_finished.disconnect(close_old_connections)
file1 = open(filename) r = HttpResponse(file1) self.assertTrue(file1.closed) r.close()
file1 = open(filename) file2 = open(filename) r = HttpResponse(file1) r.content = file2 self.assertTrue(file1.closed) self.assertTrue(file2.closed)
file1 = open(filename) r = StreamingHttpResponse(file1) self.assertFalse(file1.closed) r.close() self.assertTrue(file1.closed)
class First(models.Model): second = models.IntegerField()
class Third(models.Model): name = models.CharField(max_length=20) third = models.ForeignKey('self', models.SET_NULL, null=True, related_name='child_set')
@python_2_unicode_compatible class Category(models.Model): name = models.CharField(max_length=20)
class SchoolManager(models.Manager): def get_queryset(self): return super(SchoolManager, self).get_queryset().filter(is_public=True)
self.assertQuerysetEqual(Reporter.objects.filter(article__reporter=self.r).distinct(), john_smith)
self.assertQuerysetEqual( Article.objects.filter(reporter_id__exact=self.r.id), ["<Article: John's second test>", "<Article: This is a test>"] )
self.r.cached_query = Article.objects.filter(reporter=self.r) self.assertEqual(repr(deepcopy(self.r)), "<Reporter: John Smith>")
self.assertIs(r1.article_set.__class__, r1.article_set.__class__)
self.assertIs(r1.article_set.__class__, r2.article_set.__class__)
c = Child.objects.get(name="Child") p = c.parent
self.assertIs(c.parent, p)
del c._parent_cache self.assertIsNot(c.parent, p)
p2 = Parent.objects.create(name="Parent 2") c.parent = p2 self.assertIs(c.parent, p2)
p.bestchild = None self.assertIsNone(p.bestchild)
p.save() self.assertIsNone(p.bestchild)
p = Parent.objects.get(name="Parent") self.assertIsNone(p.bestchild)
setattr(c, "parent", None)
with self.assertRaises(ValueError): setattr(c, "parent", First(id=1, second=1))
Child(name='xyzzy', parent=None)
with self.assertRaises(IntegrityError), transaction.atomic(): Child.objects.create(name='xyzzy', parent=None)
p = Parent.objects.get(name="Parent") c = Child(parent=p) self.assertIs(c.parent, p)
cat = models.ForeignKey(Category, models.CASCADE) self.assertEqual('id', cat.remote_field.get_related_field().name)
self.assertQuerysetEqual(School.objects.all(), ["<School: School object>"])
self.assertEqual(private_student.school, private_school)
self.assertFalse(hasattr(Article(), 'reporter'))
with self.assertRaises(Exception) as cm: admin.autodiscover() self.assertEqual(str(cm.exception), "Bad admin module")
with self.assertRaises(Exception) as cm: admin.autodiscover() self.assertEqual(str(cm.exception), "Bad admin module")
class TablespacesTests(TestCase):
self._old_models = apps.app_configs['model_options'].models.copy()
self.assertNumContains(sql, 'tbl_tbsp', 1) self.assertNumContains(sql, settings.DEFAULT_INDEX_TABLESPACE, 1)
self.assertNumContains(sql, 'tbl_tbsp', 2)
self.assertEqual(sql_for_table(Scientist), sql_for_table(ScientistRef))
self.assertNumContains(sql, 'tbl_tbsp', 1) self.assertNumContains(sql, settings.DEFAULT_INDEX_TABLESPACE, 2)
self.assertNumContains(sql, 'tbl_tbsp', 3)
self.assertNumContains(sql, 'idx_tbsp', 1)
self.assertEqual(sql_for_table(Article), sql_for_table(ArticleRef))
self.assertNumContains(sql, 'tbl_tbsp', 0) self.assertNumContains(sql, 'idx_tbsp', 2)
self.assertEqual(list(self.book.model_options_bookstores.all()), [])
from django.db import models from django.utils.encoding import python_2_unicode_compatible
book = qs.get(other_rating=4) self.assertEqual(book['other_rating'], 4)
return None
from __future__ import unicode_literals
class Model(models.Model): field = models.AutoField(primary_key=False)
another = models.IntegerField(primary_key=True)
from __future__ import unicode_literals
return db == 'default'
from __future__ import unicode_literals
model = models.IntegerField()
class Model(models.Model): foreign_key = models.ForeignKey('Rel1', models.CASCADE)
friends = models.ManyToManyField('self', through="Relationship")
friends = models.ManyToManyField('self', through="Relationship", symmetrical=True)
friends = models.ManyToManyField( 'self', symmetrical=True, through="Relationship", through_fields=('first', 'second'), )
country_id = models.IntegerField() city_id = models.IntegerField()
class Meta: swappable = 'TEST_SWAPPABLE_MODEL'
if six.PY3: invalid_related_names.append('，')
if six.PY3: related_names.extend(['試', '試驗+'])
def test_complex_clash(self): class Target(models.Model): tgt_safe = models.CharField(max_length=10) clash = models.CharField(max_length=10) model = models.CharField(max_length=10)
from __future__ import unicode_literals
@isolate_apps('invalid_models_tests') class UniqueTogetherTests(SimpleTestCase):
unique_together = ('one', 'two')
class VeryLongModelNamezzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz(models.Model): title = models.CharField(max_length=11)
class m2msimple(models.Model): id2 = models.ForeignKey(ModelWithLongField, models.CASCADE)
pass
f_id = models.IntegerField()
f = models.ForeignKey(Target, models.CASCADE)
parent = models.OneToOneField(Place, models.CASCADE)
from __future__ import unicode_literals
from django.contrib.auth.models import User from django.db import models from django.utils.encoding import python_2_unicode_compatible
no = models.IntegerField(verbose_name="Number", blank=True, null=True)
class BandAdmin(ModelAdmin): fields = ['name']
class BandAdmin(ModelAdmin): fields = ['name']
class BandAdmin(ModelAdmin): fieldsets = [(None, {'fields': ['name']})]
class BandAdmin(ModelAdmin): exclude = ['bio']
class BandAdmin(ModelAdmin): exclude = ('bio',)
class BandAdmin(ModelAdmin): fields = ['name', 'bio'] exclude = ['bio']
class RequestMiddleware(TestMiddleware): def process_request(self, request): super(RequestMiddleware, self).process_request(request) return HttpResponse('Request Middleware')
class BadRequestMiddleware(TestMiddleware): def process_request(self, request): super(BadRequestMiddleware, self).process_request(request) raise TestException('Test Request Exception')
class NoTemplateResponseMiddleware(TestMiddleware): def process_template_response(self, request, response): super(NoTemplateResponseMiddleware, self).process_template_response(request, response)
pass
self.assert_middleware_usage(middleware, True, True, True, True, False)
self.client.handler.load_middleware() response = self.client.get('/middleware_exceptions/exception_in_render/') self.assertEqual(response.content, b'Exception caught')
del settings.ROOT_URLCONF with self.assertRaises(AttributeError): self.client.get("/middleware_exceptions/view/")
data_abstract = models.CharField(max_length=10) fk_abstract = models.ForeignKey(Relation, models.CASCADE, related_name='fk_abstract_rel')
data_not_concrete_abstract = models.ForeignObject( Relation, on_delete=models.CASCADE, from_fields=['abstract_non_concrete_id'], to_fields=['id'], related_name='fo_abstract_rel', )
content_type_abstract = models.ForeignKey(ContentType, models.CASCADE, related_name='+') object_id_abstract = models.PositiveIntegerField() content_object_abstract = GenericForeignKey('content_type_abstract', 'object_id_abstract')
generic_relation_abstract = GenericRelation(Relation)
data_base = models.CharField(max_length=10) fk_base = models.ForeignKey(Relation, models.CASCADE, related_name='fk_base_rel')
data_not_concrete_base = models.ForeignObject( Relation, on_delete=models.CASCADE, from_fields=['base_non_concrete_id'], to_fields=['id'], related_name='fo_base_rel', )
content_type_base = models.ForeignKey(ContentType, models.CASCADE, related_name='+') object_id_base = models.PositiveIntegerField() content_object_base = GenericForeignKey('content_type_base', 'object_id_base')
generic_relation_base = GenericRelation(Relation)
data_inherited = models.CharField(max_length=10) fk_inherited = models.ForeignKey(Relation, models.CASCADE, related_name='fk_concrete_rel')
data_not_concrete_inherited = models.ForeignObject( Relation, on_delete=models.CASCADE, from_fields=['model_non_concrete_id'], to_fields=['id'], related_name='fo_concrete_rel', )
content_type_concrete = models.ForeignKey(ContentType, models.CASCADE, related_name='+') object_id_concrete = models.PositiveIntegerField() content_object_concrete = GenericForeignKey('content_type_concrete', 'object_id_concrete')
generic_relation_concrete = GenericRelation(Relation)
baseperson = models.ForeignKey(BasePerson, models.CASCADE, related_name='relating_baseperson') baseperson_hidden = models.ForeignKey(BasePerson, models.CASCADE, related_name='+')
person = models.ForeignKey(Person, models.CASCADE, related_name='relating_person') person_hidden = models.ForeignKey(Person, models.CASCADE, related_name='+')
proxyperson = models.ForeignKey(ProxyPerson, models.CASCADE, related_name='relating_proxyperson') proxyperson_hidden = models.ForeignKey(ProxyPerson, models.CASCADE, related_name='relating_proxyperson_hidden+')
basepeople = models.ManyToManyField(BasePerson, related_name='relating_basepeople') basepeople_hidden = models.ManyToManyField(BasePerson, related_name='+')
people = models.ManyToManyField(Person, related_name='relating_people') people_hidden = models.ManyToManyField(Person, related_name='+')
class CommonAncestor(models.Model): pass
fields = Person._meta.get_fields() with self.assertRaisesMessage(AttributeError, msg): fields += ["errors"]
all_models_with_cache = (m for m in self.all_models if not m._meta.abstract) for m in all_models_with_cache: self.assertNotIn('_relation_tree', m._meta.__dict__)
self.assertTrue(self.all_models[0]._meta._relation_tree)
self.assertEqual(AbstractPerson._meta._relation_tree, EMPTY_RELATION_TREE)
all_models_but_abstractperson = (m for m in self.all_models if m is not AbstractPerson) for m in all_models_but_abstractperson: self.assertIn('_relation_tree', m._meta.__dict__)
import argparse import atexit import copy import os import shutil import subprocess import sys import tempfile import warnings
TMPDIR = tempfile.mkdtemp(prefix='django_') tempfile.tempdir = os.environ['TMPDIR'] = TMPDIR
atexit.register(shutil.rmtree, six.text_type(TMPDIR))
CONTRIB_TESTS_TO_APPS = { 'flatpages_tests': 'django.contrib.flatpages', 'redirects_tests': 'django.contrib.redirects', }
('gis_tests', os.path.join(RUNTESTS_DIR, 'gis_tests')),
def no_available_apps(self): raise Exception("Please define available_apps in TransactionTestCase " "and its subclasses.") TransactionTestCase.available_apps = property(no_available_apps) TestCase.available_apps = None
django.setup()
test_modules = get_test_modules()
test_labels_set = set() for label in test_labels: bits = label.split('.')[:1] test_labels_set.add('.'.join(bits))
if not test_labels: module_found_in_labels = True else: module_found_in_labels = any( module_label == label or module_label.startswith(label + '.') for label in test_labels_set)
for key, value in state.items(): setattr(settings, key, value)
if all(conn.features.can_clone_databases for conn in connections.all()): return default_test_processes() else: return 1
if not hasattr(settings, 'TEST_RUNNER'): settings.TEST_RUNNER = 'django.test.runner.DiscoverRunner' TestRunner = get_runner(settings)
for label in [bisection_label, 'model_inheritance_same_model_name']: try: test_labels.remove(label) except ValueError: pass
for label in [paired_test, 'model_inheritance_same_model_name']: try: test_labels.remove(label) except ValueError: pass
try:
options.modules = [os.path.normpath(labels) for labels in options.modules]
request.process_response_content = response.content request.process_response_reached = True return response
self.assertFalse(getattr(request, 'process_template_response_reached', False)) self.assertTrue(getattr(request, 'process_response_reached', False))
from __future__ import unicode_literals
self.assertIn(npath(filename), autoreload.gen_filenames()) self.assertIn(npath(filename), autoreload.gen_filenames())
self.assertNotIn(npath(filename), autoreload.gen_filenames()) self.assertNotIn(npath(filename), autoreload.gen_filenames())
self.assertIn(npath(filename), autoreload.gen_filenames(only_new=True)) self.assertNotIn(npath(filename), autoreload.gen_filenames(only_new=True))
self.clear_autoreload_caches() filenames = set(autoreload.gen_filenames(only_new=True)) filenames_reference = set(autoreload.gen_filenames()) self.assertEqual(filenames, filenames_reference)
filenames = set(autoreload.gen_filenames(only_new=True)) self.assertEqual(filenames, set())
with extend_sys_path(dirname): import_module('test_only_new_module') filenames = set(autoreload.gen_filenames(only_new=True)) self.assertEqual(filenames, {npath(filename)})
from __future__ import unicode_literals
return self.replace('<', '<<').replace('>', '>>')
ambiguous = datetime.datetime(2015, 10, 25, 2, 30)
non_existent = datetime.datetime(2015, 3, 29, 2, 30)
from __future__ import unicode_literals
exception = TypeError if six.PY3 else UnicodeError with self.assertRaises(exception): force_text(MyString())
('%&', '%&'), ('red&♥ros%#red', 'red&%E2%99%A5ros%#red'),
self.assertEqual(iri_to_uri(iri_to_uri(iri)), uri)
('/%E2%99%A5%E2%99%A5/', '/♥♥/'), ('/%E2%99%A5%E2%99%A5/?utf8=%E2%9C%93', '/♥♥/?utf8=✓'),
self.assertEqual(uri_to_iri(uri_to_iri(uri)), iri)
user = User.objects.create_user('johndoe', 'john@example.com', 'pass') x = SimpleLazyObject(lambda: user)
pickle.dumps(x)
pickle.dumps(x, 0) pickle.dumps(x, 1) pickle.dumps(x, 2)
cPickle.dumps(x)
dt = datetime(2015, 10, 25, 2, 30, 0)
dt = datetime(2009, 5, 16, microsecond=123) self.assertEqual(dateformat.format(dt, 'u'), '000123')
tz = get_fixed_timezone(-210) aware_dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=tz)
self.assertEqual(dateformat.format(aware_dt, 'O'), '-0330')
self.assertTrue(module_has_submodule(test_module, 'good_module')) mod = import_module('utils_tests.test_module.good_module') self.assertEqual(mod.content, 'Good Module')
self.assertTrue(module_has_submodule(test_module, 'bad_module')) with self.assertRaises(ImportError): import_module('utils_tests.test_module.bad_module')
self.assertFalse(module_has_submodule(test_module, 'no_such_module')) with self.assertRaises(ImportError): import_module('utils_tests.test_module.no_such_module')
self.assertFalse(module_has_submodule(test_module, 'django')) with self.assertRaises(ImportError): import_module('utils_tests.test_module.django')
self.assertFalse(module_has_submodule(test_no_submodule, 'anything')) with self.assertRaises(ImportError): import_module('utils_tests.test_no_submodule.anything')
self.assertTrue(module_has_submodule(egg_module, 'good_module')) mod = import_module('egg_module.good_module') self.assertEqual(mod.content, 'Good Module')
self.assertTrue(module_has_submodule(egg_module, 'bad_module')) with self.assertRaises(ImportError): import_module('egg_module.bad_module')
self.assertFalse(module_has_submodule(egg_module, 'no_such_module')) with self.assertRaises(ImportError): import_module('egg_module.no_such_module')
self.assertTrue(module_has_submodule(egg_module, 'bad_module')) with self.assertRaises(ImportError): import_module('egg_module.sub1.sub2.bad_module')
self.assertFalse(module_has_submodule(egg_module, 'no_such_module')) with self.assertRaises(ImportError): import_module('egg_module.sub1.sub2.no_such_module')
for needle_haystack in test_data[1:]: self.assertIn(self.lazy_wrap(needle), haystack) self.assertIn(self.lazy_wrap(needle), self.lazy_wrap(haystack))
l = [1, 2, 3]
l = [1, 2, 3]
foo = Foo()
foo = Foo()
l = [1, 2, 3]
l = [1, 2, 3]
foo = Foo()
foo = Foo()
def lazy_wrap(self, wrapped_object): return SimpleLazyObject(lambda: wrapped_object)
obj = self.lazy_wrap(42) six.assertRegex(self, repr(obj), '^<SimpleLazyObject:')
from __future__ import unicode_literals
self.assertEqual(A.value.__doc__, "Here is the docstring...")
self.assertEqual(a.value, a.value)
self.assertEqual(a.value[0], 1)
a2 = A() self.assertNotEqual(a.value, a2.value)
self.assertIsInstance(A.value, cached_property)
self.assertEqual(a.other, 1) self.assertTrue(callable(a.other_value))
self.assertEqual(timesince(self.t, self.t), '0\xa0minutes')
s = OrderedSet() self.assertFalse(s) s.add(1) self.assertTrue(s)
with self.assertRaisesMessage(AttributeError, 'ImmutableList object is immutable.'): d.sort()
with self.assertRaisesMessage(AttributeError, 'Object is immutable!'): d.__setitem__(1, 'test')
from __future__ import unicode_literals
('&gotcha&#;<>', '&gotcha&#;<>'),
return 'some html content'
return 'some html safe content'
return 'some html content'
return 'some html safe content'
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
os.chdir(TEST_DIR)
s = Site.objects.get_current() self.assertIsInstance(s, Site) s.delete() with self.assertRaises(ObjectDoesNotExist): Site.objects.get_current()
site.delete() with self.assertRaises(ObjectDoesNotExist): get_current_site(request)
request.META = {'HTTP_HOST': 'example.com'} site = get_current_site(request) self.assertEqual(site, s1)
request.META = {'HTTP_HOST': 'example.com:80'} site = get_current_site(request) self.assertEqual(site, s2)
request.META = {'HTTP_HOST': 'example.com:81'} site = get_current_site(request) self.assertEqual(site, s1)
request.META = {'HTTP_HOST': 'example.net'} with self.assertRaises(ObjectDoesNotExist): get_current_site(request)
Site.objects.all().delete()
pass
data = {'var': '\xf2'} response = self.client.get('/get_view/', data)
self.assertRedirects(response, '/get_view/')
self.assertRedirects(response, '/get_view/?var=value')
self.assertRedirects(response, '/get_view/', status_code=301)
self.assertRedirects(response, '/get_view/', status_code=302)
self.assertRedirects(response, '/permanent_redirect_view/', target_status_code=301)
self.assertContains(response, 'MAGIC', status_code=404)
self.assertContains(response, 'Select a valid choice.', 0)
self.assertEqual(response.status_code, 404)
self.assertEqual(response.request['PATH_INFO'], '/unknown_view/;some-parameter')
response = self.client.get('/login_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_view/')
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/login_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_view/')
self.client.force_login(self.u1)
response = self.client.get('/login_protected_method_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_method_view/')
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/login_protected_method_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_method_view/')
self.client.force_login(self.u1)
response = self.client.get('/login_protected_view_custom_redirect/') self.assertRedirects(response, '/accounts/login/?redirect_to=/login_protected_view_custom_redirect/')
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/login_protected_view_custom_redirect/') self.assertRedirects(response, '/accounts/login/?redirect_to=/login_protected_view_custom_redirect/')
self.client.force_login(self.u1)
response = self.client.get('/login_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_view/')
self.client.force_login(self.u2, backend='django.contrib.auth.backends.AllowAllUsersModelBackend')
self.client.login(username='testclient', password='password')
self.client.logout()
response = self.client.get('/login_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_view/')
self.client.force_login(self.u1)
self.client.logout()
response = self.client.get('/login_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_view/')
response = self.client.get('/login_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/login_protected_view/')
self.client.force_login(self.u1, backend='test_client.auth_backends.TestClientBackend')
response = self.client.get('/permission_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/permission_protected_view/')
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/permission_protected_view/') self.assertRedirects(response, '/accounts/login/?next=/permission_protected_view/')
response = self.client.get('/permission_protected_view_exception/') self.assertEqual(response.status_code, 403)
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/permission_protected_view_exception/') self.assertEqual(response.status_code, 403)
response = self.client.get('/permission_protected_method_view/') self.assertRedirects(response, '/accounts/login/?next=/permission_protected_method_view/')
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/permission_protected_method_view/') self.assertRedirects(response, '/accounts/login/?next=/permission_protected_method_view/')
try: self.client.session['tobacconist'] self.fail("Shouldn't have a session value") except KeyError: pass
self.assertEqual(self.client.session['tobacconist'], 'hovercraft')
try: self.client.get('/broken_view/') self.fail('Should raise an error') except KeyError: pass
self.assertContains(response, 'This is a test')
response = self.client.post('/post_view/', {}) self.assertEqual(response.status_code, 200)
response = csrf_client.post('/post_view/', {}) self.assertEqual(response.status_code, 403)
res = HttpResponse('abc') conditional_content_removal(req, res) self.assertEqual(res.content, b'abc')
req.method = 'HEAD'
return
with translation.override('pl'): result = management.call_command('leave_locale_alone_false', stdout=StringIO()) self.assertIsNone(result)
with translation.override('pl'): result = management.call_command('leave_locale_alone_true', stdout=StringIO()) self.assertEqual(result, "pl")
for app_label in app_labels: if app_label.startswith('--'): raise CommandError("Sorry, Dave, I can't let you do that.")
bestchild = models.ForeignKey("Child", models.SET_NULL, null=True, related_name="favored_by")
parent = models.ForeignKey("mutually_referential.Parent", models.CASCADE)
q = Parent(name='Elizabeth') q.save()
c = q.child_set.create(name='Charles') q.child_set.create(name='Edward')
q.bestchild = c q.save() q.delete()
self.assertQuerysetEqual(Reporter.objects.all(), [])
self.assertFalse(transaction.get_rollback()) transaction.set_rollback(True)
transaction.rollback() transaction.set_autocommit(True)
self.assertTrue(transaction.get_rollback()) transaction.set_rollback(False) self.assertEqual(Reporter.objects.count(), 3) transaction.set_rollback(True)
self.assertQuerysetEqual(Reporter.objects.all(), [])
self.assertTrue(transaction.get_rollback()) transaction.set_rollback(False) self.assertEqual(Reporter.objects.count(), 3) transaction.set_rollback(True)
self.assertEqual(connection.autocommit, autocommit)
with self.assertRaises(transaction.TransactionManagementError): r2.save(force_update=True)
transaction.set_rollback(False) r2.save(force_update=True)
with self.assertRaises(Error): Reporter.objects.create(first_name="Cuthbert", last_name="Calculus")
time.sleep(1) Reporter.objects.exclude(id=1).update(id=2)
connection.close()
with transaction.atomic(): with transaction.atomic(): Reporter.objects.create(id=1, first_name="Tintin")
transaction.atomic(Callable())
with self.assertRaises(Error):
with transaction.atomic():
with self.assertRaisesMessage(Exception, "Oops"):
with transaction.atomic(): sid = connection.savepoint_ids[-1] raise Exception("Oops")
connection.savepoint_rollback(sid)
perms = models.Permission.objects.filter(codename__in=('add_customuser', 'change_customuser')) self.user.user_permissions.add(*perms)
original_pk = request.user.pk request.user.pk = 1 super(CustomUserAdmin, self).log_change(request, object, message) request.user.pk = original_pk
from __future__ import unicode_literals
class Mocked(PasswordResetTokenGenerator): def __init__(self, today): self._today_val = today
with self.assertRaises(ValueError): p0._make_token_with_timestamp(user, 175455491841851871349)
known_user = 'knownuser' known_user2 = 'knownuser2'
response = self.client.get('/remote_user/', **{self.header: 'newuser'}) self.assertEqual(User.objects.count(), num_users + 1)
default_login = datetime(2000, 1, 1) if settings.USE_TZ: default_login = default_login.replace(tzinfo=timezone.utc) user.last_login = default_login user.save()
known_user = 'knownuser@example.com' known_user2 = 'knownuser2@example.com'
ContentType.objects.clear_cache()
self.user.set_password('test') self.user.save()
ContentType.objects.clear_cache()
self.assertEqual(self.user_login_failed, [{'password': '********************', 'username': 'test'}])
self.assertTrue(self.client.login( username=self.TEST_USERNAME, password=self.TEST_PASSWORD) )
request = HttpRequest() request.session = self.client.session
with self.settings(AUTHENTICATION_BACKENDS=[ 'django.contrib.auth.backends.ModelBackend']): user = get_user(request)
self.assertIsNotNone(user) self.assertTrue(user.is_anonymous)
request = HttpRequest() request.session = self.client.session
return render(request, 'context_processors/auth_attrs_test_access.html', {'session_accessed': request.session.accessed})
url(r'^admin/', admin.site.urls),
from .test_auth_backends import ImportedModelBackend
from __future__ import unicode_literals
@override_settings(AUTHENTICATION_BACKENDS=['django.contrib.auth.backends.AllowAllUsersModelBackend']) class AuthenticationFormTest(TestDataMixin, TestCase):
data = { 'username': 'inactive', 'password': 'password', }
MyUserForm({})
user = User.objects.get(username='testclient') form_for_data = UserChangeForm(instance=user) post_data = form_for_data.initial
post_data['password'] = 'new password' form = UserChangeForm(instance=user, data=post_data)
self.assertIn('$', form.cleaned_data['password'])
self.assertEqual(form.initial['password'], form['password'].value())
Site.objects.clear_cache()
self.assertTrue(form.is_valid()) form.save() self.assertEqual(len(mail.outbox), 0)
widget = ReadOnlyPasswordHashWidget() html = widget.render(name='password', value=None, attrs={}) self.assertIn(_("No password set."), html)
from __future__ import unicode_literals
if crypt.crypt('', '') is None: crypt = None
self.assertTrue(check_password('letmein', encoded, setter, 'bcrypt')) self.assertFalse(state['upgraded'])
hasher.rounds = old_rounds
self.assertTrue(check_password('letmein', encoded, setter, 'bcrypt')) self.assertTrue(state['upgraded'])
self.assertEqual(hasher.encode.call_count, 3)
self.assertNotEqual(encoded, make_password(None), "Random password collision?")
self.assertTrue(check_password('letmein', encoded, setter)) self.assertFalse(state['upgraded'])
hasher.iterations = old_iterations
self.assertTrue(check_password('letmein', encoded, setter)) self.assertTrue(state['upgraded'])
self.assertEqual(hasher.encode.call_count, 1)
self.assertTrue(check_password('letmein', encoded, setter)) self.assertFalse(state['upgraded'])
check_password('letmein', encoded) self.assertEqual(hasher.harden_runtime.call_count, 0)
check_password('wrong_password', encoded) self.assertEqual(hasher.harden_runtime.call_count, 1)
setattr(hasher, attr, new_value) encoded = make_password('letmein', hasher='argon2') attr_value = hasher.safe_summary(encoded)[summary_key] self.assertEqual(attr_value, new_value)
self.assertTrue(check_password('letmein', encoded, setter, 'argon2')) self.assertFalse(state['upgraded'])
setattr(hasher, attr, old_value)
self.assertTrue(check_password('letmein', encoded, setter, 'argon2')) self.assertTrue(state['upgraded'])
from django.contrib.auth.models import AbstractBaseUser, BaseUserManager from django.db import models
def get_group_permissions(self, obj=None): return set()
@property def is_staff(self): return self.is_admin
with RemoveGroupsAndPermissions(): class ExtensionUser(AbstractUser): date_of_birth = models.DateField()
from __future__ import unicode_literals
self.assertFalse(mail.outbox[0].message().is_multipart())
self.assertContains(response, "Please enter your new password")
path = path[:-5] + ("0" * 4) + path[-1]
response = self.client.get('/reset/123456/1-1/') self.assertContains(response, "The password reset link was invalid")
response = self.client.get('/reset/zzzzzzzzzzzzz/1-1/') self.assertContains(response, "The password reset link was invalid")
u = User.objects.get(email='staffmember@example.com') self.assertTrue(not u.check_password("anewpassword"))
u = User.objects.get(email='staffmember@example.com') self.assertTrue(u.check_password("anewpassword"))
response = self.client.get(path) self.assertContains(response, "The password reset link was invalid")
username = User.objects.get(email='staffmember@example.com').username self.assertContains(response, "Hello, %s." % username)
response = self.client.get('/reset/zzzzzzzzzzzzz/1-1/') self.assertContains(response, "Hello, .")
UUIDUser.objects.create_user( email=self.user_email, username='foo', password='foo', ) return super(UUIDUserPasswordResetTest, self)._test_confirm_start()
self.assertRedirects(response, '/password_change/done/')
self.client.post('/custom_requestauth_login/', { 'username': 'testclient', 'password': 'password', }, follow=True)
SessionMiddleware().process_request(req) CsrfViewMiddleware().process_view(req, login_view, (), {})
self.assertNotEqual(token1, token2)
self.login() self.assertEqual(original_session_key, self.client.session.session_key)
self.login() response = self.client.get('/logout/') self.assertIn('site', response.context)
self.login() response = self.client.get('/logout/next_page/') self.assertEqual(response.status_code, 302) self.assertURLEqual(response.url, '/somewhere/')
engine = import_module(settings.SESSION_ENGINE) session = engine.SessionStore() session[LANGUAGE_SESSION_KEY] = 'pl' session.save() self.client.cookies[settings.SESSION_COOKIE_NAME] = session.session_key
@override_settings(ROOT_URLCONF='auth_tests.urls_admin') class ChangelistTests(AuthViewsTestCase):
User.objects.filter(username='testclient').update(is_staff=True, is_superuser=True) self.login() self.admin = User.objects.get(pk=self.u1.pk)
row.delete()
assert isinstance(prompt, six.binary_type)
management.get_system_username = lambda: 'J\xfalia' self.assertEqual(management.get_default_username(), 'julia')
User.objects.create_user(username='J\xfalia', password='qwerty') call_command('changepassword', username='J\xfalia', stdout=self.stdout)
self.assertFalse(u.has_usable_password())
self.assertFalse(u.has_usable_password())
new_io = six.StringIO() with self.assertRaises(CommandError): call_command( "createsuperuser", interactive=False, username="joe@somewhere.org", stdout=new_io, stderr=new_io, )
def bad_then_good_password(index=[0]): index[0] += 1 if index[0] <= 2: return '1234567890' return 'password'
entered_passwords = ["password", "not password", "password2", "password2"]
entered_passwords = ["", "", "password2", "password2"]
self.assertEqual(Permission.objects.filter( content_type=permission_content_type, ).count(), 4)
self.assertEqual(Permission.objects.filter( content_type=permission_content_type, ).count(), 1)
self.assertIsNone(check_password({}, 'unknown', ''))
self.assertTrue(check_password({}, 'test', 'test'))
User.objects.filter(username='test').update(is_active=False) self.assertFalse(check_password({}, 'test', 'test'))
self.assertFalse(check_password({}, 'test', 'incorrect'))
self.assertIsNone(check_password({}, 'unknown', ''))
self.assertTrue(check_password({}, 'test@example.com', 'test'))
self.assertFalse(check_password({}, 'test@example.com', 'incorrect'))
self.assertEqual(groups_for_user({}, 'unknown'), [])
site = admin.AdminSite(name='auth_test_admin') site.register(User, UserAdmin) site.register(Group, GroupAdmin)
returned = UserManager.normalize_email(r'Abc\@DEF@EXAMPLE.com') self.assertEqual(returned, r'Abc\@DEF@example.com')
user.is_active = False user.save() user_fetched = UserModel._default_manager.get(pk=user.pk) self.assertEqual(user_fetched.is_active, True)
self.assertEqual(response.context['user'], user) self.assertEqual(user, response.context['user'])
from __future__ import unicode_literals
self.assertEqual(u.get_username(), 'testuser')
u2 = User.objects.create_user('testuser2', 'test2@example.com') self.assertFalse(u2.has_usable_password())
with warnings.catch_warnings(record=True) as warns:
self.assertEqual(len(self.login_failed), 1)
self.client.get('/logout/next_page/') self.assertEqual(len(self.logged_out), 1) self.assertEqual(self.logged_out[0], None)
from __future__ import unicode_literals
logging.captureWarnings(self._old_capture_state)
admin_email_handler = [ h for h in logger.handlers if h.__class__.__name__ == "AdminEmailHandler" ][0] return admin_email_handler
orig_filters = admin_email_handler.filters try: admin_email_handler.filters = []
admin_email_handler.filters = orig_filters
orig_mail_admins = mail.mail_admins orig_email_backend = admin_email_handler.email_backend mail.mail_admins = my_mail_admins admin_email_handler.email_backend = ( 'logging_tests.logconfig.MyEmailBackend')
mail.mail_admins = orig_mail_admins admin_email_handler.email_backend = orig_email_backend
admin_email_handler.include_html = False try: self.client.get('/', HTTP_HOST='evil.com') finally: admin_email_handler.include_html = old_include_html
admin_email_handler.include_html = True try: self.client.get('/', HTTP_HOST='evil.com') finally: admin_email_handler.include_html = old_include_html
out, err = self.run_manage(['check']) self.assertNoOutput(err) self.assertOutput(out, "System check identified no issues (0 silenced).")
parent = models.OneToOneField(Place, models.CASCADE, primary_key=True, parent_link=True) capacity = models.IntegerField()
primary_key = models.AutoField(primary_key=True) parent = models.OneToOneField(Place, models.CASCADE, parent_link=True)
parent = models.OneToOneField(Place, models.CASCADE, parent_link=True)
@python_2_unicode_compatible class Person(models.Model): name = models.CharField(max_length=100)
class SearchableLocation(models.Model): keywords = models.CharField(max_length=256)
class Politician(models.Model): politician_id = models.AutoField(primary_key=True) title = models.CharField(max_length=50)
place2 = Place(name='Main St', address='111 Main St') place2.save_base(raw=True) park = ParkingLot(parent=place2, capacity=100) park.save_base(raw=True)
places = list(Place.objects.all()) self.assertEqual(places, [place1, place2])
place1.name = "Guido's All New House of Pasta" place1.save_base(raw=True)
Restaurant.objects.all().delete()
self.assertEqual(ParkingLot3._meta.get_ancestor_link(Place).name, "parent")
QualityControl.objects.create( headline="Problems in Django", pub_date=datetime.datetime.now(), quality=10, assignee="adrian")
self.assertFalse(hasattr(p2, 'messybachelorparty_set'))
messy = MessyBachelorParty.objects.create( name='Bachelor party for Dave') messy.attendees.set([p4]) messy_parent = messy.bachelorparty_ptr
self.assertEqual( InternalCertificationAudit._meta.verbose_name_plural, 'Audits' )
from __future__ import unicode_literals
self.assertEqual(request.GET.urlencode(), '') self.assertEqual(request.POST.urlencode(), '')
self.assertEqual(request.FILES.getlist('foo'), [])
('Sat, 01-Jan-2028 04:05:06 GMT', 'Sat, 01-Jan-2028 04:05:07 GMT')
self.assertIn('; %s' % http_cookies.Morsel._reserved['httponly'], str(example_cookie)) self.assertTrue(example_cookie['httponly'])
self.assertEqual(request.read(13), b'--boundary\r\nC') self.assertEqual(request.POST, {'name': ['value']})
self.assertEqual(request.get_port(), '8080')
self.assertEqual(request.get_port(), '80')
request = HttpRequest() request.META = { 'HTTP_HOST': "invalid_hostname.com", } self.assertEqual(request.get_host(), "invalid_hostname.com")
request = self.factory.get('////absolute-uri') self.assertEqual( request.build_absolute_uri(), 'http://testserver//absolute-uri' )
request = self.factory.get('////absolute-uri') self.assertEqual( request.build_absolute_uri(location='/foo/bar/'), 'http://testserver/foo/bar/' )
publications = models.ManyToManyField(Publication, name='publications') tags = models.ManyToManyField(Tag, related_name='tags')
class AbstractArticle(models.Model): class Meta: abstract = True ordering = ('title',)
with self.assertRaisesMessage(TypeError, "'Publication' instance expected, got <Article"): with transaction.atomic(): a6.publications.add(a5)
self.assertQuerysetEqual( Article.objects.exclude(publications=self.p2), ['<Article: Django lets you build Web apps easily>'] )
self.a1.publications.set([self.p1, self.p2])
self.p1.article_set.set([self.a1, self.a2])
requires_unique_target = False
kwargs['related_name'] = '+' kwargs['unique'] = True super(CurrentTranslation, self).__init__(to, on_delete, from_fields, to_fields, **kwargs)
translation = CurrentTranslation(ArticleTranslation, models.CASCADE, ['id'], ['article'])
class IndexTogetherSingleList(models.Model): headline = models.CharField(max_length=100) pub_date = models.DateTimeField()
self.assertIn( connection.ops.quote_name( editor._create_index_name(Article, ['headline', 'pub_date'], suffix='_idx') ), index_sql[0] )
index_sql = connection.schema_editor()._model_indexes_sql(IndexTogetherSingleList) self.assertEqual(len(index_sql), 1)
self.assertIn('("slug" varchar_pattern_ops)', index_sql[4])
from __future__ import unicode_literals
self.patched_settings = self.settings(STATIC_ROOT=temp_dir) self.patched_settings.enable() self.run_collectstatic() self.addCleanup(shutil.rmtree, six.text_type(temp_dir))
cls.settings_override = override_settings(**TEST_SETTINGS) cls.settings_override.enable() super(LiveServerBase, cls).setUpClass()
cls.settings_override.disable() super(LiveServerBase, cls).tearDownClass()
address_predefined = 'DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ old_address = os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS')
old_STATIC_URL = TEST_SETTINGS['STATIC_URL'] TEST_SETTINGS['STATIC_URL'] = None cls.raises_exception('localhost:8081', ImproperlyConfigured) TEST_SETTINGS['STATIC_URL'] = old_STATIC_URL
if address_predefined: os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = old_address else: del os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS']
pass
pass
self.run_collectstatic()
warning_string = 'Found another file'
with self.settings(STATICFILES_DIRS=[static_dir]): output = self._collectstatic_output(clear=True) self.assertNotIn(self.warning_string, output)
storage.staticfiles_storage.hashed_files.clear()
self.run_collectstatic()
os.unlink(self._clear_filename) self.run_collectstatic(clear=True)
def run_collectstatic(self, **kwargs): pass
models.OriginalRasterField = models.RasterField
if HAS_GEOMETRY_COLUMNS: self.assertGeometryColumnsCount(2)
if self.has_spatial_indexes: self.assertSpatialIndexExists('gis_neighborhood', 'path')
if self.has_spatial_indexes: self.assertSpatialIndexExists('gis_neighborhood', 'heatmap')
if HAS_GEOMETRY_COLUMNS: self.assertGeometryColumnsCount(2)
if self.has_spatial_indexes: self.assertSpatialIndexExists('gis_neighborhood', 'path')
if self.has_spatial_indexes: self.assertSpatialIndexExists('gis_neighborhood', 'heatmap')
if HAS_GEOMETRY_COLUMNS: self.assertGeometryColumnsCount(0)
center2 = models.PointField(srid=2276, db_column='mycenter') border1 = models.PolygonField() border2 = models.PolygonField(srid=2276)
tol = 0
self.assertEqual(list(Parcel.objects.transform(srid, field_name='city__location__point')), [])
aggs = City.objects.aggregate(Extent('location__point'))
aggs = City.objects.aggregate(Union('location__point'))
ref_u1 = MultiPoint(p1, p2, p4, p5, p3, srid=4326) ref_u2 = MultiPoint(p2, p3, srid=4326)
list(DirectoryEntry.objects.all().select_related())
c1 = b1.centroid c2 = c1.transform(2276, clone=True) Parcel.objects.create(name='P2', city=pcity, center1=c1, center2=c2, border1=b1, border2=b1)
qs = Parcel.objects.filter(center1__within=F('border1')) self.assertEqual(1, len(qs)) self.assertEqual('P2', qs[0].name)
qs = Parcel.objects.filter(center2__within=F('border1')) self.assertEqual(1, len(qs)) self.assertEqual('P2', qs[0].name)
qs = Parcel.objects.filter(center1=F('city__location__point')) self.assertEqual(1, len(qs)) self.assertEqual('P1', qs[0].name)
qs = Parcel.objects.filter(border2__contains=F('city__location__point')) self.assertEqual(1, len(qs)) self.assertEqual('P1', qs[0].name)
loc = Location.objects.annotate(num_cities=Count('city')).get(id=dallas.location.id) self.assertEqual(2, loc.num_cities)
self.assertEqual(4, len(coll)) self.assertTrue(ref_geom.equals(coll))
str(qs.query)
city_dict = {name: coords for name, coords in city_data}
for name, line, exp_z in interstate_data: line_3d = GEOSGeometry(line, srid=4269) line_2d = LineString([l[:2] for l in line_3d.coords], srid=4269)
lm = LayerMapping(Point2D, vrt_file, point_mapping, transform=False) lm.save() self.assertEqual(3, Point2D.objects.count())
with self.assertRaises(LayerMapError): LayerMapping(Point3D, city_file, point_mapping, transform=False)
lm = LayerMapping(Point3D, vrt_file, point_mapping, transform=False) lm.save() self.assertEqual(3, Point3D.objects.count())
lm = LayerMapping(MultiPoint3D, vrt_file, mpoint_mapping, transform=False) lm.save() self.assertEqual(3, MultiPoint3D.objects.count())
ref_kml_regex = re.compile(r'^<Point><coordinates>-95.363\d+,29.763\d+,18</coordinates></Point>$') self.assertTrue(ref_kml_regex.match(h.kml))
ref_kml_regex = re.compile(r'^<Point><coordinates>-95.363\d+,29.763\d+,18</coordinates></Point>$') self.assertTrue(ref_kml_regex.match(h.kml))
TEST_DATA = os.path.join(os.path.dirname(upath(__file__)), 'data')
ext = kwargs.pop('ext', 'shp') self.ds = get_ds_file(name, ext) super(TestDS, self).__init__(**kwargs)
coords = kwargs.pop('coords', None) if coords: self.coords = tuplize(coords)
with open(os.path.join(TEST_DATA, 'geometries.json')) as f: geometries = json.load(f) return TestGeomSet(**strconvert(geometries))
result = rast.bands[0].data() if numpy: result = result.flatten().tolist()
self.assertEqual(result, list(range(256)))
rstfile = tempfile.NamedTemporaryFile(suffix='.tif')
rstfile = tempfile.NamedTemporaryFile(suffix='.tif') ndv = 99
target = source.transform(3086)
target = GDALRaster(target.name)
self.band = None self.assertTrue(os.path.isfile(pam_file))
self.band = None if os.path.isfile(pam_file): os.remove(pam_file)
rs = GDALRaster(self.rs_path, write=False) band = rs.bands[0]
with self.assertRaises(GDALException): setattr(band, 'nodata_value', 10)
bandmem.nodata_value = 99 self.assertEqual(bandmem.nodata_value, 99)
block = list(range(100, 104)) packed_block = struct.pack('<' + 'B B B B', *block)
ds = DataSource(source.ds)
self.assertEqual(1, len(ds))
self.assertEqual(source.ds, ds.name)
self.assertEqual(source.driver, str(ds.driver))
try: ds[len(ds)] except OGRIndexError: pass else: self.fail('Expected an IndexError!')
for layer in ds: self.assertEqual(len(layer), source.nfeat)
self.assertEqual(source.nfld, layer.num_fields) self.assertEqual(source.nfld, len(layer.fields))
flds = layer.fields for f in flds: self.assertIn(f, source.fields)
with self.assertRaises(OGRIndexError): layer.__getitem__(-1) with self.assertRaises(OGRIndexError): layer.__getitem__(50000)
for fld_name in fld_names: self.assertEqual(source.field_values[fld_name], layer.get_fields(fld_name))
source = ds_list[0] ds = DataSource(source.ds)
def get_layer(): ds = DataSource(source.ds) return ds[0]
lyr = get_layer() self.assertEqual(source.nfeat, len(lyr)) self.assertEqual(source.gtype, lyr.geom_type.num)
self.assertEqual(str(lyr[0]['str']), "1")
for layer in ds: for feat in layer: self.assertEqual(source.nfld, len(list(feat))) self.assertEqual(source.gtype, feat.geom_type)
for k, v in source.fields.items(): self.assertIsInstance(feat[k], v)
for fld in feat: self.assertIn(fld.name, source.fields.keys())
for layer in ds: for feat in layer: g = feat.geom
self.assertEqual(source.geom, g.geom_name) self.assertEqual(source.gtype, g.geom_type)
self.assertIsNone(lyr.spatial_filter)
with self.assertRaises(TypeError): lyr._set_spatial_filter('foo')
lyr.spatial_filter = None self.assertEqual(3, len(lyr))
ds = DataSource(os.path.join(TEST_DATA, 'texas.dbf')) feat = ds[0][0] self.assertEqual(676586997978, feat.get('ALAND10'))
OGRGeomType(1) OGRGeomType(7) OGRGeomType('point') OGRGeomType('GeometrycollectioN') OGRGeomType('LINearrING') OGRGeomType('Unknown')
with self.assertRaises(GDALException): OGRGeomType(23) with self.assertRaises(GDALException): OGRGeomType('fooD') with self.assertRaises(GDALException): OGRGeomType(9)
gt = OGRGeomType('Geometry') self.assertEqual(0, gt.num) self.assertEqual('Unknown', gt.name)
exp_gml = exp_gml.replace('GeometryCollection', 'MultiGeometry')
geom2 = OGRGeometry(g.hex) self.assertEqual(geom1, geom2)
geom2 = OGRGeometry(wkb) self.assertEqual(geom1, geom2)
bbox = (-180, -90, 180, 90) p = OGRGeometry.from_bbox(bbox) self.assertEqual(bbox, p.extent)
self.assertEqual(poly, OGRGeometry(p.wkt)) self.assertNotEqual(poly, prev)
sr = SpatialReference('WGS84') mpoly = OGRGeometry(mp.wkt, sr) self.assertEqual(sr.wkt, mpoly.srs.wkt)
klone = mpoly.clone() self.assertEqual(sr.wkt, klone.srs.wkt)
for poly in mpoly: self.assertEqual(sr.wkt, poly.srs.wkt) for ring in poly: self.assertEqual(sr.wkt, ring.srs.wkt)
ring.srid = 4322 self.assertEqual('WGS 72', ring.srs.name) self.assertEqual(4322, ring.srid)
mpoly = OGRGeometry(mp.wkt, srs=None) mpoly.srs = mpoly.srs mpoly.srid = mpoly.srid
k1 = orig.clone() k2 = k1.transform(trans.srid, clone=True) self.assertEqual(k1, orig) self.assertNotEqual(k1, k2)
self.assertEqual(2, ls_orig.coord_dim) self.assertAlmostEqual(ls_trans.x[0], ls_orig.x[0], prec) self.assertAlmostEqual(ls_trans.y[0], ls_orig.y[0], prec)
mp = OGRGeometry('MultiPolygon') pnt = OGRGeometry('POINT(5 23)') with self.assertRaises(GDALException): mp.add(pnt)
class CityBase(NamedModel): population = models.IntegerField() density = models.DecimalField(max_digits=7, decimal_places=1) point = models.PointField()
co_mapping = { 'name': 'Name', 'state': {'name': 'State'},
from __future__ import unicode_literals
NAMES = ['Bexar', 'Galveston', 'Harris', 'Honolulu', 'Pueblo']
bad1 = copy(city_mapping) bad1['foobar'] = 'FooField'
bad2 = copy(city_mapping) bad2['name'] = 'Nombre'
bad3 = copy(city_mapping) bad3['point'] = 'CURVE'
for bad_map in (bad1, bad2, bad3): with self.assertRaises(LayerMapError): LayerMapping(City, city_shp, bad_map)
with self.assertRaises(LookupError): LayerMapping(City, city_shp, city_mapping, encoding='foobar')
lm = LayerMapping(City, city_shp, city_mapping) lm.save()
self.assertEqual(3, City.objects.count())
pnt1, pnt2 = feat.geom, city.point self.assertAlmostEqual(pnt1.x, pnt2.x, 5) self.assertAlmostEqual(pnt1.y, pnt2.y, 5)
with self.assertRaises(InvalidDecimal): lm = LayerMapping(Interstate, inter_shp, inter_mapping) lm.save(silent=True, strict=True) Interstate.objects.all().delete()
lm = LayerMapping(Interstate, inter_shp, inter_mapping) lm.save(silent=True)
self.assertEqual(2, Interstate.objects.count())
ds = DataSource(inter_shp)
valid_feats = ds[0][:2] for feat in valid_feats: istate = Interstate.objects.get(name=feat['Name'].value)
self.assertAlmostEqual(feat.get('Length'), float(istate.length), 2)
c = County.objects.get(name=name) self.assertEqual(n, len(c.mpoly))
if county_feat: qs = CountyFeat.objects.filter(name=name) self.assertEqual(n, qs.count())
lm = LayerMapping(County, co_shp, co_mapping, transform=False)
lm = LayerMapping(County, co_shp, co_mapping, source_srs=4269) lm = LayerMapping(County, co_shp, co_mapping, source_srs='NAD83')
for arg in ('name', ('name', 'mpoly')): lm = LayerMapping(County, co_shp, co_mapping, transform=False, unique=arg)
if connection.features.supports_transform: with self.assertRaises(LayerMapError): LayerMapping(County, co_shp, co_mapping)
lm = LayerMapping(County, co_shp, co_mapping, transform=False, unique='name') with self.assertRaises(MissingForeignKey): lm.save(silent=True, strict=True)
State.objects.bulk_create([ State(name='Colorado'), State(name='Hawaii'), State(name='Texas') ])
lm = LayerMapping(CountyFeat, co_shp, cofeat_mapping, transform=False) lm.save(silent=True, strict=True)
self.county_helper()
def clear_counties(): County.objects.all().delete()
lm = LayerMapping(County, co_shp, co_mapping, transform=False, unique='name')
bad_ranges = (5.0, 'foo', co_shp) for bad in bad_ranges: with self.assertRaises(TypeError): lm.save(fid_range=bad)
qs = County.objects.all() self.assertEqual(1, qs.count()) self.assertEqual('Galveston', qs[0].name)
clear_counties()
for st in (4, 7, 1000): clear_counties() lm.save(step=st, strict=True) self.county_helper(county_feat=False)
lm1 = LayerMapping(ICity1, city_shp, icity_mapping) lm1.save()
lm2 = LayerMapping(ICity2, city_shp, icity_mapping) lm2.save()
from __future__ import unicode_literals
with self.assertRaises(GeoIP2Exception): cntry_g.city('tmc.edu') with self.assertRaises(GeoIP2Exception): cntry_g.coords('tmc.edu')
with self.assertRaises(TypeError): cntry_g.country_code(17) with self.assertRaises(TypeError): cntry_g.country_name(GeoIP2)
self.assertIn(d['country_name'], ('Curaçao', 'Curacao'))
ogr_db = get_ogr_db_string() if not ogr_db: self.skipTest("Unable to setup an OGR connection to your database")
model_def = ogrinspect(ogr_db, 'Measurement', layer_key=AllOGRFields._meta.db_table, decimal=['f_decimal'])
self.assertIsNotNone(re.search(r' geom = models.PolygonField\(([^\)])*\)', model_def))
try: Driver(drv_name) except GDALException: return None
if db['NAME'] == ":memory:": return None
params = [db_str % {'db_name': db['NAME']}]
if value: params.append(template % value)
class PennsylvaniaCity(City): county = models.CharField(max_length=30) founded = models.DateTimeField(null=True)
pnt = Point(0, 0) nullcity = City(name='NullCity', point=pnt) nullcity.save()
new = Point(5, 23) nullcity.point = new
self.assertEqual(4326, nullcity.point.srid) nullcity.save()
self.assertEqual(new, City.objects.get(name='NullCity').point)
ply = Polygon(shell, inner) nullstate = State(name='NullState', poly=ply)
sa_4326 = 'POINT (-98.493183 29.424170)'
sa = City.objects.create(name='San Antonio', point=nad_pnt)
m1 = MinusOneSRID(geom=Point(17, 23, srid=4326)) m1.save() self.assertEqual(-1, m1.geom.srid)
PennsylvaniaCity.objects.create(name='Mansfield', county='Tioga', point='POINT(-77.071445 41.823881)')
qs = PennsylvaniaCity.objects.transform(32128)
texas = Country.objects.get(name='Texas')
ks = State.objects.get(poly__contains=lawrence.point) self.assertEqual('Kansas', ks.name)
co_border = State.objects.get(name='Colorado').poly ks_border = State.objects.get(name='Kansas').poly
vic = City.objects.get(point__left=co_border) self.assertEqual('Victoria', vic.name)
State.objects.create(name='Puerto Rico')
nullqs = State.objects.filter(poly__isnull=True) validqs = State.objects.filter(poly__isnull=False)
self.assertEqual(1, len(nullqs)) self.assertEqual('Puerto Rico', nullqs[0].name)
nmi = State.objects.create(name='Northern Mariana Islands', poly=None) self.assertEqual(nmi.poly, None)
pnt1 = fromstr('POINT (649287.0363174 4177429.4494686)', srid=2847) pnt2 = fromstr('POINT(-98.4919715741052 29.4333344025053)', srid=4326)
with self.assertRaises(ValueError): Country.objects.filter(mpoly__relate=(23, 'foo'))
ks = State.objects.get(name='Kansas') self.assertEqual('Lawrence', City.objects.get(point__relate=(ks.poly, within_mask)).name)
if spatialite: qs = qs.exclude(name='Texas') else: qs = qs.intersection(geom)
pass
expected = (-96.8016128540039, 29.7633724212646, -95.3631439208984, 32.782058715820)
if not connection.ops.geojson: with self.assertRaises(NotImplementedError): Country.objects.all().geojson(field_name='mpoly') return
with self.assertRaises(TypeError): City.objects.geojson(precision='foo')
self.assertEqual(pueblo_json, City.objects.geojson().get(name='Pueblo').geojson)
self.assertEqual(houston_json, City.objects.geojson(crs=True, model_att='json').get(name='Houston').json)
self.assertEqual(victoria_json, City.objects.geojson(bbox=True).get(name='Victoria').geojson)
self.assertEqual( chicago_json, City.objects.geojson(bbox=True, crs=True, precision=5).get(name='Chicago').geojson )
qs = City.objects.all() with self.assertRaises(TypeError): qs.kml('name')
for c in Country.objects.num_geom(): self.assertEqual(2, c.num_geom)
self.assertEqual(1, c.num_geom)
for c in City.objects.num_points(): self.assertEqual(1, c.num_points)
tol = 0.00001
tol = 0.000000001
htown = fromstr('POINT(1947516.83115183 6322297.06040572)', srid=3084) ptown = fromstr('POINT(992363.390841912 481455.395105533)', srid=2774)
self.assertAlmostEqual(c1[0] + xfac, c2[0], 5) self.assertAlmostEqual(c1[1] + yfac, c2[1], 5)
for item in items: self.assertChildNodes(item, ['title', 'link', 'description', 'guid', 'georss:point'])
self.assertChildNodes(feed2, ['title', 'link', 'id', 'updated', 'entry', 'georss:box'])
for entry in entries: self.assertChildNodes(entry, ['title', 'link', 'id', 'summary', 'georss:point'])
from __future__ import unicode_literals
pueblo = City.objects.get(name='Pueblo') state = State.objects.filter(poly__contains=pueblo.point) cities_within_state = City.objects.filter(id__in=state)
self.assertEqual(cities_within_state.count(), 1)
self.assertIsInstance(val1, bool) self.assertIsInstance(val2, bool) self.assertEqual(val1, True) self.assertEqual(val2, False)
if not connection.ops.geojson: with self.assertRaises(NotImplementedError): list(Country.objects.annotate(json=functions.AsGeoJSON('mpoly'))) return
with self.assertRaises(TypeError): City.objects.annotate(geojson=functions.AsGeoJSON('point', precision='foo'))
self.assertEqual( pueblo_json, City.objects.annotate(geojson=functions.AsGeoJSON('point')).get(name='Pueblo').geojson )
self.assertEqual( houston_json, City.objects.annotate(json=functions.AsGeoJSON('point', crs=True)).get(name='Houston').json )
self.assertEqual( victoria_json, City.objects.annotate( geojson=functions.AsGeoJSON('point', bbox=True) ).get(name='Victoria').geojson )
self.assertEqual( chicago_json, City.objects.annotate( geojson=functions.AsGeoJSON('point', bbox=True, crs=True, precision=5) ).get(name='Chicago').geojson )
with self.assertRaises(TypeError): City.objects.annotate(kml=functions.AsKML('name'))
if spatialite or oracle: qs = qs.exclude(name='Texas')
expected = None
expected = ''
for c in Country.objects.annotate(num_geom=functions.NumGeometries('mpoly')): self.assertEqual(2, c.num_geom)
if mysql: self.assertIsNone(city.num_geom) else: self.assertEqual(1, city.num_geom)
return
for c in City.objects.annotate(num_points=functions.NumPoints('point')): self.assertEqual(1, c.num_points)
tol = 0.000000001
if oracle: qs = qs.exclude(name='Texas') for country in qs: self.assertTrue(country.mpoly.sym_difference(geom).equals(country.sym_difference))
ptown = fromstr('POINT(992363.390841912 481455.395105533)', srid=2774)
self.assertAlmostEqual(c1[0] + xfac, c2[0], 5) self.assertAlmostEqual(c1[1] + yfac, c2[1], 5)
if spatialite: qs = qs.exclude(name='Texas') else: qs = qs.annotate(intersection=functions.Intersection('mpoly', geom))
return
return item.point.x, item.point.y
return ((-123.30, -41.32), (174.78, 48.46))
class TestW3CGeo2(TestGeoRSS2): feed_type = feeds.W3CGeoFeed
urlset = doc.firstChild self.assertEqual(urlset.getAttribute('xmlns'), 'http://www.sitemaps.org/schemas/sitemap/0.9')
kml_url = url.getElementsByTagName('loc')[0].childNodes[0].data.split('http://example.com')[1]
from __future__ import unicode_literals
ErrClass = socket.error if six.PY2 else OSError try: socket.gethostbyname(domain) return True except ErrClass: return False
with self.assertRaises(GeoIPException): cntry_g.city('google.com') with self.assertRaises(GeoIPException): cntry_g.coords('yahoo.com')
with self.assertRaises(TypeError): cntry_g.country_code(17) with self.assertRaises(TypeError): cntry_g.country_name(GeoIP)
self.assertIn(d['country_name'], ('Curaçao', 'Curacao'))
from __future__ import unicode_literals
itemList = ['x'] * length for i, v in enumerate(items): itemList[i] = v
ul_longer = ul + [2] ul_longer._IndexError = TypeError ul._IndexError = TypeError self.assertNotEqual(ul_longer, pl) self.assertGreater(ul_longer, ul)
wkt_r = WKTReader() wkt = 'POINT (5 23)'
ref = GEOSGeometry(wkt) g1 = wkt_r.read(wkt.encode()) g2 = wkt_r.read(wkt)
with self.assertRaises(TypeError): wkt_r.read(1) with self.assertRaises(TypeError): wkt_r.read(memoryview(b'foo'))
wkt_w = WKTWriter() with self.assertRaises(TypeError): wkt_w._set_ptr(WKTReader.ptr_type())
wkb_r = WKBReader()
g1 = wkb_r.read(wkb) g2 = wkb_r.read(hex) for geom in (g1, g2): self.assertEqual(ref, geom)
for bad_byteorder in (-1, 2, 523, 'foo', None): with self.assertRaises(ValueError): wkb_w._set_byteorder(bad_byteorder)
wkb_w.byteorder = 0 self.assertEqual(hex2, wkb_w.write_hex(g)) self.assertEqual(wkb2, wkb_w.write(g))
wkb_w.byteorder = 1
g = GEOSGeometry('POINT (5 23 17)') g.srid = 4326
wkb_w.outdim = 3
wkb_w.srid = True self.assertEqual(hex3d_srid, wkb_w.write_hex(g)) self.assertEqual(wkb3d_srid, wkb_w.write(g))
class FakeGeom1(GEOSBase): pass
c_float_p = ctypes.POINTER(ctypes.c_float)
fg1 = FakeGeom1() fg2 = FakeGeom2()
fg1.ptr = ctypes.c_void_p() fg1.ptr = None fg2.ptr = c_float_p(ctypes.c_float(5.23)) fg2.ptr = None
for fg in (fg1, fg2): with self.assertRaises(GEOSException): fg._get_ptr()
ogc_hex = b'01010000000000000000000000000000000000F03F' ogc_hex_3d = b'01010000800000000000000000000000000000F03F0000000000000040' hexewkb_2d = b'0101000020E61000000000000000000000000000000000F03F' hexewkb_3d = b'01010000A0E61000000000000000000000000000000000F03F0000000000000040'
self.assertEqual(ogc_hex, pnt_2d.hex) self.assertEqual(ogc_hex_3d, pnt_3d.hex)
self.assertEqual(hexewkb_2d, pnt_2d.hexewkb) self.assertEqual(hexewkb_3d, pnt_3d.hexewkb) self.assertEqual(True, GEOSGeometry(hexewkb_3d).hasz)
self.assertEqual(six.memoryview(a2b_hex(hexewkb_2d)), pnt_2d.ewkb) self.assertEqual(six.memoryview(a2b_hex(hexewkb_3d)), pnt_3d.ewkb)
self.assertEqual(4326, GEOSGeometry(hexewkb_2d).srid)
for err in self.geometries.errors: with self.assertRaises((GEOSException, ValueError)): fromstr(err.wkt)
with self.assertRaises(GEOSException): GEOSGeometry(six.memoryview(b'0'))
with self.assertRaises(TypeError): GEOSGeometry(NotAGeometry()) with self.assertRaises(TypeError): GEOSGeometry(None)
for fh in (wkt_f, wkb_f): fh.seek(0) pnt = fromfile(fh) self.assertEqual(ref_pnt, pnt)
self.assertAlmostEqual(p.x, pnt.tuple[0], 9) self.assertAlmostEqual(p.y, pnt.tuple[1], 9)
self.assertEqual(p.centroid, pnt.centroid.tuple)
pnt.y = 3.14 pnt.x = 2.71 self.assertEqual(3.14, pnt.y) self.assertEqual(2.71, pnt.x)
pnt.tuple = set_tup1 self.assertEqual(set_tup1, pnt.tuple) pnt.coords = set_tup2 self.assertEqual(set_tup2, pnt.coords)
self.assertEqual(ls.wkt, LineString(*tuple(Point(tup) for tup in ls.tuple)).wkt) if numpy:
self.assertEqual(poly, fromstr(p.wkt))
for r in poly: self.assertEqual(r.geom_type, 'LinearRing') self.assertEqual(r.geom_typeid, 2)
with self.assertRaises(TypeError): Polygon(0, [1, 2, 3]) with self.assertRaises(TypeError): Polygon('foo')
rings = tuple(r for r in poly) self.assertEqual(poly, Polygon(rings[0], rings[1:]))
ring_tuples = tuple(r.tuple for r in poly) self.assertEqual(poly, Polygon(*ring_tuples))
poly = fromstr(self.geometries.polygons[1].wkt) ring1 = poly[0] ring2 = poly[1]
del ring1 del ring2 ring1 = poly[0] ring2 = poly[1]
del poly
str(ring1) str(ring2)
poly = fromstr(p.wkt) cs = poly.exterior_ring.coord_seq
for i in range(len(p.ext_ring_cs)):
if len(c1) == 2: tset = (5, 23) else: tset = (5, 23, 8) cs[i] = tset
for j in range(len(tset)): cs[i] = tset self.assertEqual(tset[j], cs[i][j])
exp_buf = fromstr(bg.buffer_wkt) quadsegs = bg.quadsegs width = bg.width
with self.assertRaises(ctypes.ArgumentError): g.buffer(width, float(quadsegs))
buf = g.buffer(width, quadsegs) self.assertEqual(exp_buf.num_coords, buf.num_coords) self.assertEqual(len(exp_buf), len(buf))
hex = '0101000020E610000000000000000014400000000000003740' p1 = fromstr(hex) self.assertEqual(4326, p1.srid)
pnt_wo_srid = Point(1, 1) pnt_wo_srid.srid = pnt_wo_srid.srid
for p in self.geometries.polygons: poly = fromstr(p.wkt)
with self.assertRaises(TypeError): poly.__setitem__(0, LineString((1, 1), (2, 2)))
poly.exterior_ring = new_shell
mpoly[i] = poly
pnt = Point(0, 0) self.assertEqual(0.0, pnt.distance(Point(0, 0)))
self.assertEqual(1.0, pnt.distance(Point(0, 1)))
self.assertAlmostEqual(1.41421356237, pnt.distance(Point(1, 1)), 11)
pnt = Point(0, 0) self.assertEqual(0.0, pnt.length)
ls = LineString((0, 0), (1, 1)) self.assertAlmostEqual(1.41421356237, ls.length, 11)
mpoly = MultiPolygon(poly.clone(), poly) self.assertEqual(8.0, mpoly.length)
if isinstance(g, Polygon):
gc1 = GEOSGeometry(gc_wkt)
gc2 = GeometryCollection(*tuple(g for g in gc1))
self.assertEqual(gc1, gc2)
k1 = orig.clone() k2 = k1.transform(trans.srid, clone=True) self.assertEqual(k1, orig) self.assertNotEqual(k1, k2)
from django.utils.six.moves import cPickle import pickle
del mpoly self.assertTrue(prep.covers(Point(5, 5)))
p._set_single(0, 100) self.assertEqual(p.coords, (100.0, 2.0, 3.0), 'Point _set_single')
p._set_list(2, (50, 3141)) self.assertEqual(p.coords, (50.0, 3141.0), 'Point _set_list')
rast = GDALRaster(json.loads(JSON_RASTER)) stx_pnt = GEOSGeometry('POINT (-95.370401017314293 29.704867409475465)', 4326) stx_pnt.transform(3086)
combos = [{x[0]: x[1]} for x in zip(combo_keys, combo_values)]
qs = RasterModel.objects.filter(**combo)
self.assertTrue(qs.count() in [0, 1])
qs = RasterModel.objects.filter(Q(**combos[0]) & Q(**combos[1])) self.assertTrue(qs.count() in [0, 1])
rast = GDALRaster(json.loads(JSON_RASTER)) stx_pnt = GEOSGeometry('POINT (-95.370401017314293 29.704867409475465)', 4326) stx_pnt.transform(3086)
qs = RasterModel.objects.filter(rastprojected__dwithin=(rast, D(km=1))) self.assertEqual(qs.count(), 1)
qs = RasterModel.objects.filter(rast__dwithin=(rast, 40)) self.assertEqual(qs.count(), 1)
qs = RasterModel.objects.filter(rast__dwithin=(stx_pnt, 500)) self.assertEqual(qs.count(), 1)
qs = RasterModel.objects.filter(geom__dwithin=(rast, 500)) self.assertEqual(qs.count(), 1)
qs = RasterRelatedModel.objects.filter(rastermodel__rast__dwithin=(rast, 40)) self.assertEqual(qs.count(), 1)
qs = RasterRelatedModel.objects.filter(rastermodel__rast__1__dwithin=(rast, 40)) self.assertEqual(qs.count(), 1)
qs = RasterModel.objects.filter(rastprojected__bbcontains=rast) self.assertEqual(qs.count(), 1)
'ellipsoid': (6378137.0, 6356752.31414, 298.257222101), 'eprec': (1, 5, 10),
auth_name, oracle_flag = sd['auth_name'] if postgis or (oracle and oracle_flag): self.assertTrue(srs.auth_name.startswith(auth_name))
if postgis: self.assertTrue(srs.wkt.startswith(sd['srtext'])) six.assertRegex(self, srs.proj4text, sd['proj4_re'])
self.assertTrue(sr.name.startswith(sd['name']))
ellps1 = sd['ellipsoid'] prec = sd['eprec']
srs = SpatialRefSys.objects.get(srid=sd['srid']) ellps2 = srs.ellipsoid
htown = City.objects.get(name='Houston') Zipcode.objects.distance(htown.point)
htown = City.objects.get(name='Houston') with self.assertRaises(ValueError): City.objects.get(point__exact=htown.point)
from django.contrib.gis.utils import LayerMapping
names = ['Bexar', 'Galveston', 'Harris', 'Honolulu', 'Pueblo']
def no_oracle(func): return no_backend(func, 'oracle')
gisfield_may_be_null = not mysql
tx_cities = ['Downtown Houston', 'Southside Place'] au_cities = ['Mittagong', 'Shellharbour', 'Thirroul', 'Wollongong']
for dist in au_dists: if isinstance(dist, D) and not oracle: type_error = True else: type_error = False
dist1 = SouthTexasCity.objects.distance(lagrange, field_name='point').order_by('id')
tol = 2 if oracle else 5
ls = LineString(((150.902, -34.4245), (150.87, -34.5789)))
z = SouthTexasZipcode.objects.get(name='77005')
dists_m = [3553.30384972258, 1243.18391525602, 2186.15439472242]
qs1 = SouthTexasCity.objects.filter(point__distance_gte=(self.stx_pnt, D(km=7))).filter( point__distance_lte=(self.stx_pnt, D(km=20)), )
with self.assertRaises(ValueError): len(AustraliaCity.objects.filter(point__distance_lte=('POINT(5 23)',)))
wollongong = AustraliaCity.objects.get(name='Wollongong')
len_m1 = 473504.769553813 len_m2 = 4617.668
with self.assertRaises(ValueError): Interstate.objects.length()
i10 = SouthTexasInterstate.objects.length().get(name='I-10') self.assertAlmostEqual(len_m2, i10.length.m, 2)
for i, c in enumerate(SouthTexasCity.objects.perimeter(model_att='perim')): self.assertEqual(0, c.perim.m)
tol = 2 if oracle else 5
ls = LineString(((150.902, -34.4245), (150.87, -34.5789)), srid=4326)
z = SouthTexasZipcode.objects.get(name='77005')
dists_m = [3553.30384972258, 1243.18391525602, 2186.15439472242]
len_m1 = 473504.769553813 len_m2 = 4617.668
qs = SouthTexasCity.objects.annotate(perim=Perimeter('point')) for city in qs: self.assertEqual(0, city.perim.m)
fld = forms.GeometryField() with self.assertRaisesMessage(forms.ValidationError, "No geometry value provided."): fld.clean(None)
fld = forms.GeometryField(required=False) self.assertIsNone(fld.clean(None))
@override_settings(USE_L10N=True, USE_THOUSAND_SEPARATOR=True) def test_pointfield(self): class PointForm(forms.Form): p = forms.PointField()
self.assertIn(escape(point.json), widget.render('p', point.json)) self.assertEqual(widget.deserialize_called, 1)
self.assertEqual(form.cleaned_data['p'].srid, 4326)
return name + '_valid'
return '%s/%s' % (random.randint(100, 999), filename)
from __future__ import unicode_literals
requires_pytz = unittest.skipIf(pytz is None, "this test requires pytz")
storage_class = FileSystemStorage
self.temp_dir2 = tempfile.mkdtemp(suffix='aBc')
self._test_file_time_getter_tz_handling_on(getter) self._test_file_time_getter_tz_handling_off(getter)
self.assertTrue(timezone.is_aware(dt)) self.assertEqual(now.tzname(), dt.tzname())
self.assertLess(abs(dt - now), timedelta(seconds=2))
with timezone.override(timezone.get_fixed_timezone(-300)): self.assertFalse(self.storage.exists('test.file.tz.off'))
self.assertTrue(timezone.is_naive(dt))
with self.assertRaises(OSError): self.storage.save('error/test.file', ContentFile('not saved'))
obj1 = Storage() self.assertEqual(obj1.normal.name, "") with self.assertRaises(ValueError): obj1.normal.size
names = [o.limited_length.name for o in objs] self.assertEqual(names[0], 'tests/%s' % filename) six.assertRegex(self, names[1], 'tests/fi_%s.ext' % FILE_SUFFIX_REGEX)
obj.delete() obj = Storage() self.assertEqual(obj.default.read(), b"default content") obj.default.close()
self.assertTrue(obj.custom_valid_name.name.endswith("/random_file_valid")) obj.custom_valid_name.close()
temp_storage.save('tests/example.txt', ContentFile('some content'))
output = six.StringIO() output.write('content') output.seek(0)
result_key = f.generate_filename(None, key) self.assertEqual(result_key, expected_key)
def upload_to(instance, filename): return folder + filename
result_key = f.generate_filename(None, key) self.assertEqual(result_key, expected_key)
membership = Membership.objects.create( membership_country_id=self.usa.id, person_id=self.bob.id, group_id=self.cia.id)
membership = Membership.objects.create( membership_country_id=self.usa.id, person_id=self.jane.id, group_id=self.cia.id)
Membership.objects.create( membership_country_id=self.usa.id, person_id=self.bob.id, group_id=self.cia.id)
Membership.objects.create( membership_country_id=self.soviet_union.id, person_id=self.bob.id, group_id=self.republican.id)
Membership.objects.create(membership_country_id=self.soviet_union.id, person_id=self.george.id, group_id=self.cia.id)
Membership.objects.create( membership_country_id=self.soviet_union.id, person_id=self.george.id, group_id=self.cia.id, date_joined=timemark + timedelta)
Membership.objects.create( membership_country_id=self.soviet_union.id, person_id=self.george.id, group_id=self.cia.id)
self.assertQuerysetEqual( self.cia.members.all(), [] )
self.assertQuerysetEqual( self.bob.groups.all(), [] )
self.assertQuerysetEqual( self.bob.groups.all(), [ 'CIA', 'Republican' ], attrgetter("name") )
self.assertQuerysetEqual( self.cia.members.all(), [] )
Membership.objects.create(membership_country=self.usa, person=self.jane, group=self.cia)
self.assertQuerysetEqual( self.cia.members.all(), [] )
self.assertQuerysetEqual( self.jane.groups.all(), [] )
Membership.objects.create(membership_country=self.usa, person=self.jane, group=self.cia)
self.assertQuerysetEqual( self.jane.groups.all(), [] )
Friendship.objects.create( from_friend_id=self.jane.id, to_friend_id=self.george.id, to_friend_country_id=self.jane.person_country_id, from_friend_country_id=self.george.person_country_id)
class ArticleForm(forms.ModelForm): class Meta: model = Article fields = '__all__'
from_fields=['company', 'customer_id'], to_fields=['company', 'customer_id'],
def __init__(self, alias, col, value): self.alias, self.col, self.value = alias, col, value
name = models.CharField(max_length=50)
name = models.CharField(max_length=128) person_country_id = models.IntegerField()
name = models.CharField(max_length=128) group_country = models.ForeignKey(Country, models.CASCADE) members = models.ManyToManyField(Person, related_name='groups', through='Membership')
from_friend_country = models.ForeignKey(Country, models.CASCADE, related_name="from_friend_country") from_friend_id = models.IntegerField() to_friend_country_id = models.IntegerField() to_friend_id = models.IntegerField()
from_friend = models.ForeignObject( Person, on_delete=models.CASCADE, from_fields=['from_friend_country', 'from_friend_id'], to_fields=['person_country_id', 'id'], related_name='from_friend')
self.assertQuerysetEqual(Article.objects.filter(id__iexact=str(self.a1.id)), ['<Article: Article 1>'])
self.assertIsInstance(Article.objects.iterator(), collections.Iterator)
self.assertQuerysetEqual( Article.objects.filter(headline__endswith='4').iterator(), ['Article 4'], transform=attrgetter('headline'))
self.assertEqual(Article.objects.filter(pub_date__exact='2005-07-27 00:00:00').count(), 3)
Article.objects.create(headline='Article_ with underscore', pub_date=datetime(2005, 11, 20))
msg = 'Related Field got invalid lookup: editor' with self.assertRaisesMessage(FieldError, msg): Article.objects.filter(author__editor__name='James')
from __future__ import unicode_literals
return HttpResponse(content="")
return HttpResponse(content="")
return HttpResponse(content="")
CsrfViewMiddleware().process_view(req, token_view, (), {}) resp = token_view(req)
CsrfViewMiddleware().process_view(req, token_view, (), {}) resp = token_view(req)
def test_inheritance(self): Event.objects.create() Screening.objects.create(movie=self.movie)
self.assertEqual(len(Event.objects.select_related('screening__movie')), 2)
self.assertEqual(len(Event.objects.values('screening__movie__pk', 'screening__movie__title')), 2)
def test_inheritance_null_FK(self): Event.objects.create() ScreeningNullFK.objects.create(movie=None) ScreeningNullFK.objects.create(movie=self.movie)
def test_explicit_ForeignKey(self): Package.objects.create() screening = Screening.objects.create(movie=self.movie) Package.objects.create(screening=screening)
self.assertEqual(len(Package.objects.values('screening__movie__pk', 'screening__movie__title')), 2)
from __future__ import unicode_literals
@skipUnlessDBFeature("__class__") def test_func(): raise ValueError
dom1 = parse_html('<p>foo') dom2 = parse_html('<p>foo</p>') self.assertIn(dom1, dom2) self.assertIn(dom2, dom1)
def __call__(self, result=None): with self.assertNumQueries(0): super(SkippingExtraTests, self).__call__(result)
with self.assertRaisesMessage(AssertionError, msg): with self.assertRaisesMessage(ValueError, "Expected message"): raise ValueError("Unexpected message")
def func(): raise ValueError("Unexpected message")
def func1(): raise ValueError("[.*x+]y?")
pass
raise cls.MyException()
self.assertFalse(self._in_atomic_block)
self.available_apps = None
self._fixture_teardown() call_command.assert_called_with( 'flush', interactive=False, allow_cascade=False, reset_sequences=False, inhibit_post_migrate=True, database='default', verbosity=0, )
self.assertLess(ordered_sigs.index('s3'), ordered_sigs.index('s1'))
self.assertLess(ordered_sigs.index('s3'), ordered_sigs.index('s1'))
raw = [ ('s1', ('s1_db', ['bravo', 'alpha'])) ]
self.assertTrue(other.features.supports_transactions, msg) self.assertTrue(connections_support_transactions(), msg)
self.assertIn('DocTestCase', [t.__class__.__name__ for t in suite._tests[2:]])
from .article import Article from .publication import Publication
class Category(models.Model): name = models.CharField(max_length=50)
class EpisodePermanent(Episode): pass
from __future__ import unicode_literals
@override_settings(DEBUG=True, ROOT_URLCONF='generic_inline_admin.urls') class GenericAdminViewTest(TestDataMixin, TestCase):
"generic_inline_admin-media-content_type-object_id-TOTAL_FORMS": "1", "generic_inline_admin-media-content_type-object_id-INITIAL_FORMS": "0", "generic_inline_admin-media-content_type-object_id-MAX_NUM_FORMS": "0",
inline_formset = generic_inlineformset_factory(Media, exclude=('url',))
e = Episode.objects.get(name='This Week in Django') formset = inline_formset(instance=e) self.assertTrue(formset.get_queryset().ordered)
formset = media_inline.get_formset(request) self.assertEqual(formset.max_num, DEFAULT_MAX_NUM) self.assertEqual(formset.can_order, False)
formset = media_inline.get_formset(request, max_num=100, can_order=True) self.assertEqual(formset.max_num, 100) self.assertEqual(formset.can_order, True)
class MediaForm(ModelForm): class Meta: model = Media fields = '__all__'
assert self.author.name is not None
alt_editor = models.ForeignKey(Editor, models.SET_NULL, blank=True, null=True) title = models.CharField(max_length=100)
lat = models.CharField(max_length=100) lon = models.CharField(max_length=100)
@python_2_unicode_compatible class Repository(models.Model): name = models.CharField(max_length=25)
class Person(models.Model): name = models.CharField(max_length=128)
class Team(models.Model): name = models.CharField(max_length=100)
@python_2_unicode_compatible class Poet(models.Model): name = models.CharField(max_length=100)
class UUIDPKParent(models.Model): uuid = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False) name = models.CharField(max_length=255)
self.assertEqual(formset.is_valid(), False) self.assertEqual(Poet.objects.count(), 1)
self.assertEqual(formset.is_valid(), False) self.assertEqual(Poet.objects.count(), 1)
data = { 'form-TOTAL_FORMS': '2', 'form-INITIAL_FORMS': '2', 'form-0-id': str(poem.pk), 'form-0-name': 'foo',
self.assertTrue(formset.is_valid())
qs = Author.objects.none()
Author.objects.create(name='Charles Baudelaire') qs = Author.objects.all()
author = super(PoetForm, self).save(commit=False) author.name = "Vladimir Mayakovsky" if commit: author.save() return author
self.assertQuerysetEqual(author.book_set.order_by('title'), [ '<Book: Les Fleurs du Mal>', '<Book: Les Paradis Artificiels>', ])
AuthorBooksFormSet = inlineformset_factory(Author, Book, can_delete=False, extra=2, fields="__all__") Author.objects.create(name='Charles Baudelaire')
self.maxDiff = 1024
poem = super(PoemForm, self).save(commit=False) poem.name = "Brooklyn Bridge" if commit: poem.save() return poem
poet.name = 'Lamartine' poet.save() poem = formset.save()[0] self.assertEqual(poem.name, 'Le Lac by Lamartine')
FormSet = modelformset_factory(Price, fields="__all__", extra=1, max_num=1) formset = FormSet(data) self.assertTrue(formset.is_valid())
team = Team.objects.create(name="Red Vipers") Player(name="Timmy").save() Player(name="Bobby", team=team).save()
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertIn("tamaño = models.IntegerField()", output)
from __future__ import unicode_literals
name = '"SOME%NAME"' quoted_name = connection.ops.quote_name(name) self.assertEqual(quoted_name % (), name)
from django.db.backends.oracle.base import convert_unicode
from django.db.backends.oracle.base import Database
connection.ensure_connection() self.assertEqual(connection.connection.encoding, "UTF-8") self.assertEqual(connection.connection.nencoding, "UTF-8")
class CursorMock(object): "Very simple mock of DB-API cursor" def execute(self, arg): pass
conn = OlderConnectionMock() self.assertEqual(pg_version.get_version(conn), 90300)
del new_connection.timezone_name
with self.settings(TIME_ZONE=new_tz): new_connection.set_autocommit(False) cursor = new_connection.cursor() new_connection.rollback()
cursor.execute("SHOW TIMEZONE") tz = cursor.fetchone()[0] self.assertEqual(new_tz, tz)
new_connection.cursor() self.assertFalse(new_connection.get_autocommit())
self.assertEqual(connection.connection.isolation_level, read_committed)
new_connection.set_autocommit(False) self.assertEqual(new_connection.connection.isolation_level, serializable)
models.Post.objects.create(id=10, name='1st post', text='hello world')
cursor = connection.cursor() commands = connections[DEFAULT_DB_ALIAS].ops.sequence_reset_sql(no_style(), [models.Post]) for sql in commands: cursor.execute(sql)
obj = models.Post.objects.create(name='New post', text='goodbye world') self.assertGreater(obj.pk, 10)
class ConnectionCreatedSignalTest(TransactionTestCase):
@skipUnlessDBFeature('test_db_allows_multiple_connections') def test_signal(self): data = {}
args = [] self.create_squares_with_executemany(args) self.assertEqual(models.Square.objects.count(), 0)
self.create_squares_with_executemany(args)
self.create_squares(args, 'pyformat', multiple=True)
pass
with self.assertRaises(connection.features.closed_cursor_error_class): cursor.execute("SELECT 1" + connection.features.bare_select_suffix)
with connection.cursor() as cursor: self.assertIsInstance(cursor, CursorWrapper) self.assertTrue(cursor.closed)
with new_connection.cursor(): pass new_connection.queries_log.clear()
self.r = models.Reporter.objects.create(first_name='John', last_name='Smith')
connections_dict = {} connection.cursor() connections_dict[id(connection)] = connection
from django.db import connections connection = connections[DEFAULT_DB_ALIAS] connection.allow_thread_sharing = True connection.cursor() connections_dict[id(connection)] = connection
connections_dict = {} for conn in connections.all(): connections_dict[id(conn)] = conn
conn.allow_thread_sharing = True connections_dict[id(conn)] = conn
for conn in connections_dict.values(): if conn is not connection: conn.close()
exceptions = [] do_thread() self.assertIsInstance(exceptions[0], DatabaseError)
connections['default'].allow_thread_sharing = False exceptions = [] do_thread() self.assertIsInstance(exceptions[0], DatabaseError)
connections['default'].allow_thread_sharing = True exceptions = [] do_thread() self.assertEqual(exceptions, [])
exceptions = set()
self.assertEqual(len(exceptions), 1)
exceptions = set()
connections['default'].allow_thread_sharing = True t2 = threading.Thread(target=runner2, args=[connections['default']]) t2.start() t2.join()
self.assertEqual(len(exceptions), 0)
test_connection = copy.copy(connections[DEFAULT_DB_ALIAS]) test_connection.settings_dict = copy.copy(connections[DEFAULT_DB_ALIAS].settings_dict) return test_connection
Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16))
dive = Book() dive.title = "Dive into Python" dive.published = datetime.date(2009, 5, 4) dive.save()
Book.objects.using('other').create(title="Pro Django", published=datetime.date(2008, 12, 16))
dive = Book() dive.title = "Dive into Python" dive.published = datetime.date(2009, 5, 4) dive.save(using='other')
pro = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16))
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
pro.authors.set([marty]) dive.authors.set([mark])
self.assertEqual(list(dive.authors.all().values_list('name', flat=True)), ['Mark Pilgrim'])
dive.authors.set([mark])
dive.authors.set([mark])
grease = Book.objects.using('other').create(title="Greasemonkey Hacks", published=datetime.date(2005, 11, 1))
pro = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16))
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
with self.assertRaises(ValueError): with transaction.atomic(using='default'): marty.edited.set([pro, dive])
with self.assertRaises(ValueError): with transaction.atomic(using='default'): marty.book_set.add(dive)
with self.assertRaises(ValueError): with transaction.atomic(using='default'): marty.book_set.set([pro, dive])
with self.assertRaises(ValueError): with transaction.atomic(using='other'): dive.authors.add(marty)
with self.assertRaises(ValueError): with transaction.atomic(using='other'): dive.authors.set([mark, marty])
dive.delete(using='other')
mark.delete(using='other')
pro = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16))
pro.editor = george pro.save()
self.assertEqual(list(chris.edited.values_list('title', flat=True)), ['Dive into Python'])
dive.editor = chris dive.save()
pro = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16)) marty = Person.objects.create(name="Marty Alchin")
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
with self.assertRaises(ValueError): dive.editor = marty
with self.assertRaises(ValueError): with transaction.atomic(using='default'): marty.edited.set([pro, dive])
with self.assertRaises(ValueError): with transaction.atomic(using='default'): marty.edited.add(dive)
mark.delete(using='other')
alice = User.objects.using('default').get(username="alice") self.assertEqual(alice.userprofile.flavor, "chocolate")
self.assertEqual(alice_profile.user.username, 'alice') self.assertEqual(bob_profile.user.username, 'bob')
alice = User.objects.db_manager('default').create_user('alice', 'alice@example.com')
bob = User.objects.db_manager('other').create_user('bob', 'bob@example.com')
alice_profile = UserProfile.objects.using('default').create(user=alice, flavor='chocolate') with self.assertRaises(ValueError): bob.userprofile = alice_profile
bob_profile = UserProfile.objects.using('other').create(user=bob, flavor='crunchy frog')
charlie = User(pk=51, username='charlie', email='charlie@example.com') charlie.set_unusable_password()
self.assertEqual(new_bob_profile._state.db, None) self.assertEqual(charlie._state.db, None)
new_bob_profile.user = bob charlie.userprofile = bob_profile self.assertEqual(new_bob_profile._state.db, 'other') self.assertEqual(charlie._state.db, 'other')
denise = User.objects.db_manager('other').create_user('denise', 'denise@example.com') denise_profile = UserProfile(flavor="tofu", user=denise)
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
dive = Book.objects.using('other').get(title="Dive into Python")
self.assertEqual(list(dive.reviews.all().values_list('source', flat=True)), ['Python Weekly'])
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
with self.assertRaises(ValueError): review1.content_object = dive
with self.assertRaises(ValueError): with transaction.atomic(using='other'): dive.reviews.add(review1)
review3 = Review(source="Python Daily") self.assertEqual(review3._state.db, None)
dive.delete(using='other')
book = Book.objects.using('other').select_related('editor').get(title="Dive into Python")
self.assertEqual(book.editor._state.db, 'other')
with self.assertRaises(ValueError): str(qs.query)
with self.assertRaises(ValueError): for obj in qs: pass
connection_router = ConnectionRouter([TestRouter(), WriteRouter()]) self.assertListEqual([r.__class__.__name__ for r in connection_router.routers], ['TestRouter', 'WriteRouter'])
@override_settings(DATABASE_ROUTERS=[TestRouter()]) class RouterTestCase(TestCase): multi_db = True
self.assertTrue(router.allow_migrate_model('default', User)) self.assertTrue(router.allow_migrate_model('default', Book))
self.assertFalse(router.allow_migrate_model('default', User)) self.assertTrue(router.allow_migrate_model('default', Book))
Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
Book.objects.filter(title='Pro Django').update(pages=200)
Book.objects.get(title='Pro Django')
pro = Book.objects.using('default').get(title='Pro Django')
self.assertEqual(pro.pages, 200)
book, created = Book.objects.get_or_create(title="Pro Django") self.assertFalse(created)
Book.objects.filter(pages__gt=150).delete()
pro = Book.objects.using('default').create(title="Pro Django", published=datetime.date(2008, 12, 16))
dive = Book.objects.using('other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
try: dive.editor = marty except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
dive.save() self.assertEqual(dive._state.db, 'default')
dive = Book.objects.using('other').get(title='Dive into Python') self.assertEqual(dive._state.db, 'other')
try: marty.edited.set([pro, dive], bulk=False) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
dive = Book.objects.using('other').get(title='Dive into Python') self.assertEqual(dive._state.db, 'other')
try: marty.edited.add(dive, bulk=False) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
dive = Book.objects.using('other').get(title='Dive into Python')
self.assertEqual(dive._state.db, 'other') chris.save() dive.editor = chris html5.editor = mark
water = Book(title="Dive into Water", published=datetime.date(2001, 1, 1), editor=mark) self.assertEqual(water._state.db, 'default')
mark.save(using='default')
mark.save(using='other') self.assertEqual(mark._state.db, 'other')
pro = Book.objects.using('other').create(pk=1, title="Pro Django", published=datetime.date(2008, 12, 16))
pro.save(using='default') marty.save(using='default') dive.save(using='other') mark.save(using='other')
try: marty.book_set.set([pro, dive]) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
Book.authors.through.objects.using('default').delete()
try: marty.book_set.add(dive) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
Book.authors.through.objects.using('default').delete()
try: dive.authors.set([mark, marty]) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
Book.authors.through.objects.using('default').delete()
try: dive.authors.add(marty) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
alice = dive.authors.create(name='Alice') self.assertEqual(alice._state.db, 'default')
alice, created = dive.authors.get_or_create(name='Alice') self.assertEqual(alice._state.db, 'default')
alice = User.objects.db_manager('default').create_user('alice', 'alice@example.com')
bob = User.objects.db_manager('other').create_user('bob', 'bob@example.com')
bob.save() self.assertEqual(bob._state.db, 'default')
pro = Book.objects.using( 'default').create(title="Pro Django", published=datetime.date(2008, 12, 16))
dive = Book.objects.using( 'other').create(title="Dive into Python", published=datetime.date(2009, 5, 4))
try: review1.content_object = dive except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
dive.save() self.assertEqual(review1._state.db, 'default') self.assertEqual(dive._state.db, 'default')
dive = Book.objects.using('other').get(title='Dive into Python') self.assertEqual(dive._state.db, 'other')
try: dive.reviews.add(review1) except ValueError: self.fail("Assignment across primary/replica databases with a common source should be ok")
dive.save() self.assertEqual(dive._state.db, 'default')
review3 = Review(source="Python Daily") self.assertEqual(review3._state.db, None)
review3.content_object = dive self.assertEqual(review3._state.db, 'default')
str(qs.query)
self.assertEqual(list(qs.values_list('title', flat=True)), ['Dive into Python'])
User.objects.create_user('alice', 'alice@example.com')
User.objects.db_manager('default').create_user('bob', 'bob@example.com')
alice = User.objects.using('other').get(username='alice')
bob = User.objects.using('default').get(username='bob')
self.assertEqual(command_output, "Installed 0 object(s) (of 2) from 1 fixture(s)")
receiver = DatabaseReceiver() signals.m2m_changed.connect(receiver=receiver)
b = Book.objects.create(title="Pro Django", published=datetime.date(2008, 12, 16)) p = Person.objects.create(name="Marty Alchin")
b.authors.add(p) b.authors.remove(p) b.authors.clear() b.authors.set([p]) b.delete()
person.delete()
return 'default'
def db_for_write(self, model, **hints): return 'writer'
from django.contrib import admin from django.db import models
class SongAdmin(admin.ModelAdmin): pass site.register(Song, SongAdmin)
self.check_ordering_of_field_choices([self.b2, self.b1])
self.check_ordering_of_field_choices([self.b2, self.b1])
self.check_ordering_of_field_choices([self.b1, self.b2])
with self.assertRaises(Http404): get_object_or_404(Article, title="Foo")
self.assertEqual( get_object_or_404(Article, title__contains="Run"), article )
self.assertEqual( get_object_or_404(a1.article_set, title__contains="Run"), article )
with self.assertRaises(Http404): get_object_or_404(a1.article_set, title__contains="Camelot")
self.assertEqual( get_object_or_404(Article.by_a_sir, title="Run away!"), article )
self.assertEqual( get_object_or_404(Article.objects.all(), title__contains="Run"), article )
with self.assertRaises(Http404): get_object_or_404(Article.objects.none(), title__contains="Run")
self.assertEqual( get_list_or_404(a1.article_set, title__icontains="Run"), [article] )
with self.assertRaises(Http404): get_list_or_404(a1.article_set, title__icontains="Shrubbery")
self.assertEqual( get_list_or_404(Article.by_a_sir, title__icontains="Run"), [article] )
self.assertEqual( get_list_or_404(Article.objects.all(), title__icontains="Run"), [article] )
p1.save_base(raw=True) self.assertEqual(data, [ (p1, True), (p1, False, True), ]) data[:] = []
class PostDeleteHandler(object): def __init__(self, data): self.data = data
b1.authors.set([a1]) self.assertEqual(data, []) b1.authors.set([]) self.assertEqual(data, [])
from django.contrib.contenttypes.fields import ( GenericForeignKey, GenericRelation, ) from django.contrib.contenttypes.models import ContentType from django.db import models from django.utils.encoding import python_2_unicode_compatible
id = models.IntegerField(primary_key=True)
class Alfa(models.Model): name = models.CharField(max_length=10, null=True)
qs = Publisher.objects.extra(select={ 'name_of_shortest_book': shortest_book_sql, }).annotate(total_books=Count('book')) list(qs)
self.assertEqual( Book.objects.aggregate(Sum("pages")), {"pages__sum": 3703}, )
self.assertEqual( len(Author.objects.annotate(Avg('friends__age')).values()), 9 )
with self.assertRaises(FieldError): Book.objects.all().aggregate(num_authors=Count('foo'))
self.assertEqual( Book.objects.annotate(num_authors=Count('authors')).count(), 6 )
vals = Book.objects.annotate(num_authors=Count('authors')).aggregate(Max('num_authors')) self.assertEqual( vals, {'num_authors__max': 3} )
self.assertEqual( Book.objects.filter(id__in=[]).count(), 0 )
with self.assertRaises(ValueError): Book.objects.all().annotate(Avg('authors__age'), authors__age__avg=Avg('authors__age'))
with self.assertRaises(ValueError): Author.objects.annotate(age=Avg('friends__age'))
with self.assertRaises(ValueError): Author.objects.annotate(friends=Count('friends'))
with self.assertRaises(ValueError): Author.objects.annotate(book_contact_set=Avg('friends__age'))
qs = Book.objects.annotate(num_authors=Count('authors')) pickle.dumps(qs)
publishers = Publisher.objects.filter(id__in=[1, 2]) self.assertEqual( sorted(p.name for p in publishers), [ "Apress", "Sams" ] )
self.assertEqual( HardbackBook.objects.aggregate(n_pages=Sum('book_ptr__pages')), {'n_pages': 2078} )
with self.assertRaises(FieldError): Book.objects.annotate(mean_age=Avg('authors__age')).annotate(Avg('mean_age'))
self.assertEqual( Author.objects.none().aggregate(Avg('age')), {'age__avg': None} )
self.assertEqual(len(qs.exclude(publisher=-1)), 6)
qs = Book.objects.annotate(avg_price=Avg('price')).aggregate( publisher_awards=Sum('publisher__num_awards') ) self.assertEqual(qs['publisher_awards'], 30)
qs = qs.all() self.assertQuerysetEqual( qs, [c], lambda x: x) self.assertEqual(qs[0].alfa, a)
field = models.ForeignKey("auth.Permission", models.CASCADE) name, path, args, kwargs = field.deconstruct()
field = models.ManyToManyField("auth.Permission") name, path, args, kwargs = field.deconstruct()
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
apps = Apps() verbose_name = 'úñí©óðé µóðéø' verbose_name_plural = 'úñí©óðé µóðéøß'
apps = Apps()
from __future__ import unicode_literals
from __future__ import unicode_literals
for db in connections: recorder = MigrationRecorder(connections[db]) recorder.migration_qs.filter(app='migrations').delete()
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
Publisher = apps.get_model('migrations', 'Publisher') Publisher.objects.create(name='Test Publisher') raise RuntimeError('Abort migration')
"migrations_project", "migrations_task", "migrations_project_tasks", "migrations_task_projects",
executor.loader.build_graph()
executor.loader.build_graph()
executor.loader.build_graph()
executor.migrate([("lookuperror_a", None)])
executor.loader.build_graph()
executor.migrate([("author_app", "0002_alter_id")])
executor.loader.build_graph()
self.assertIn( ("migrations", "0001_squashed_0002"), recorder.applied_migrations(), )
self.assertIn( ("migrations", "0001_squashed_0002"), recorder.applied_migrations(), )
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
@ignore_warnings(category=ImportWarning) def test_migration_path(self): test_apps = [ 'migrations.migrations_test_apps.normal', 'migrations.migrations_test_apps.with_package_model', 'migrations.migrations_test_apps.without_init_file', ]
class DeconstructibleInstances(object): def deconstruct(self): return ('DeconstructibleInstances', [], {})
self.assertEqual(food_no_managers_state.managers, [])
('default', base_mgr), ('food_mgr2', mgr2), (b'food_mgr1', mgr1),
apps = Apps(["migrations"])
ms = ModelState.from_model(Novel) with self.assertRaises(InvalidBasesError): ms.render(apps)
ModelState.from_model(Book).render(apps) ModelState.from_model(Novel).render(apps)
ms = ModelState.from_model(FooBar) with self.assertRaises(InvalidBasesError): ms.render(apps)
self.assertIs(model_a_old._meta.get_field('b').related_model, model_b_old) self.assertIs(model_b_old._meta.get_field('a_ptr').related_model, model_a_old)
project_state = ProjectState() project_state.add_model(ModelState.from_model(A)) project_state.add_model(ModelState.from_model(B)) old_state = project_state.clone()
operation.state_forwards("something", project_state)
project_state = ProjectState() project_state.add_model(ModelState.from_model(TestModel)) with self.assertRaises(ValueError): project_state.apps
from __future__ import unicode_literals
from __future__ import unicode_literals
Editor = apps.get_model('migrations', 'Editor') Editor.objects.create(name='Test Editor') raise RuntimeError('Abort migration')
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
import functools import re
migration_name = 'custom_name' changes = autodetector.arrange_for_graph(changes, graph, migration_name)
self.assertNumberMigrations(changes, "testapp", 0)
self.assertEqual(len(changes), 0)
default=models.IntegerField,
before = self.make_project_state([]) after = self.make_project_state([self.custom_user_no_inherit, self.aardvark]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes()
before = self.make_project_state([]) after = self.make_project_state([address, tenant]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes()
before = self.make_project_state([]) after = self.make_project_state([address, tenant]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes()
before = self.make_project_state([]) after = self.make_project_state([person]) autodetector = MigrationAutodetector(before, after) changes = autodetector._detect_changes()
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.unapply_operations("test_rmflmm", with_field_state, operations=operations) self.assertTableExists("test_rmflmm_pony_stables")
"DELETE FROM i_love_ponies WHERE special_thing LIKE '%Django%';" "DELETE FROM i_love_ponies WHERE special_thing LIKE '%%Ponies%%';" "DROP TABLE i_love_ponies",
self.assertTableNotExists("i_love_ponies") new_state = project_state.clone() with connection.schema_editor() as editor: operation.database_forwards("test_runsql", editor, project_state, new_state)
with connection.schema_editor() as editor: operation.database_backwards("test_runsql", editor, new_state, project_state) self.assertTableNotExists("i_love_ponies")
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertIn("basic", project_state.real_apps)
loader.build_graph() self.assertEqual(num_nodes(), 5)
recorder.record_applied("migrations", "1_auto") loader.build_graph() self.assertEqual(num_nodes(), 4)
recorder.record_applied("migrations", "3_auto") loader.build_graph() self.assertEqual(num_nodes(), 4)
recorder.record_applied("migrations", "5_auto") loader.build_graph() self.assertEqual(num_nodes(), 2)
loader.build_graph() self.assertEqual(num_nodes(), 5)
recorder.record_applied("migrations", "1_auto") loader.build_graph() self.assertEqual(num_nodes(), 4)
recorder.record_applied("migrations", "5_auto") loader.build_graph() self.assertEqual(num_nodes(), 2)
from __future__ import unicode_literals
call_command("migrate", "migrations", fake=True, verbosity=0) call_command("migrate", "migrations", "zero", verbosity=0)
call_command("migrate", "migrations", "zero", verbosity=0)
call_command("migrate", "migrations", verbosity=0)
call_command("migrate", "migrations", "zero", verbosity=0)
del apps._pending_operations[('migrations', 'tribble')]
call_command("migrate", "migrations", "zero", verbosity=0)
init_file = os.path.join(migration_dir, "__init__.py") self.assertTrue(os.path.exists(init_file))
initial_file = os.path.join(migration_dir, "0001_initial.py") self.assertTrue(os.path.exists(initial_file))
self.assertIn('\\xfa\\xf1\\xed\\xa9\\xf3\\xf0\\xe9 \\xb5\\xf3\\xf0\\xe9\\xf8', content) self.assertIn('\\xfa\\xf1\\xed\\xa9\\xf3\\xf0\\xe9 \\xb5\\xf3\\xf0\\xe9\\xf8\\xdf', content)
importlib.invalidate_caches()
apps.register_model('migrations', UnserializableModel)
initial_file = os.path.join(migration_dir, "0001_initial.py") self.assertTrue(os.path.exists(initial_file))
content = content.replace(' ', '') self.assertIn('dependencies=[\n]', content) self.assertIn('operations=[\n]', content)
self.assertIn("Add field silly_date to sillymodel", out.getvalue())
self.assertIn("- Add field silly_char to sillymodel", out.getvalue())
initial_file = os.path.join(migration_dir, "0001_initial.py") self.assertTrue(os.path.exists(initial_file))
self.assertIn(" - Create model SillyModel", out.getvalue())
self.assertTrue(os.path.exists(migration_file)) with codecs.open(migration_file, "r", encoding="utf-8") as fp: content = fp.read()
migration_name_0001 = "my_initial_migration" content = cmd("0001", migration_name_0001) self.assertIn("dependencies=[\n]", content)
if hasattr(importlib, 'invalidate_caches'): importlib.invalidate_caches()
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
def f(): return 42
__slots__ = 'a',
cache.set("key", "value") self.assertEqual(cache.get("key"), "value")
cache.set('somekey', 'value')
self.assertFalse(caches['prefix'].has_key('somekey'))
self.assertIsNone(cache.get("does_not_exist")) self.assertEqual(cache.get("does_not_exist", "bang!"), "bang!")
cache.set("hello2", "goodbye2") self.assertIn("hello2", cache) self.assertNotIn("goodbye2", cache)
from zlib import compress, decompress value = 'value_to_be_compressed' compressed_value = compress(value.encode())
cache.set("key1", "spam", 100.2) self.assertEqual(cache.get("key1"), "spam")
def func(key, *args): return key
with self.assertRaises(pickle.PickleError): cache.add('unpicklable', Unpicklable())
cache_add.return_value = False self.assertEqual(cache.get_or_set('key', 'default'), 'default')
LOCATION='test cache table'
super(DBCacheTests, self).setUp() self.create_table()
super(DBCacheTests, self).tearDown() self.drop_table()
LOCATION='createcachetable_dry_run_mode'
caches['prefix']._cache = cache._cache caches['prefix']._expire_info = cache._expire_info
memcached_params = {} for _cache_params in settings.CACHES.values(): if _cache_params['BACKEND'].startswith('django.core.cache.backends.memcached.'): memcached_params = _cache_params
cache.set('infinite_foo', 'bar') self.assertEqual(cache.get('infinite_foo'), 'bar')
cache.set('future_foo', 'bar') self.assertEqual(cache.get('future_foo'), 'bar')
pass
pass
value = cache.get('small_value') self.assertTrue(value is None or value == large_value)
for cache_params in settings.CACHES.values(): cache_params.update({'LOCATION': self.dirname}) setting_changed.send(self.__class__, setting='CACHES', enter=False)
shutil.rmtree(self.dirname)
cache.set('unpicklable', UnpicklableType())
key = 'some key with spaces' * 15 val = 'a value' cache.set(key, val) self.assertEqual(cache.get(key), val)
self.DEFAULT_TIMEOUT = caches[DEFAULT_CACHE_ALIAS].default_timeout
self.assertIsNone(get_cache_key(request)) learn_cache_key(request, response)
self.assertIsNone(get_cache_key(request)) learn_cache_key(request, response)
learn_cache_key(request, response)
(None, {'private': True}, {'private'}), ('', {'private': True}, {'private'}),
class CustomTzName(timezone.UTC): name = ''
request = self.factory.get(self.path, {'foo': 'bar', 'other': 'true'}) request._cache_update_cache = True
en_message = "Hello world!" es_message = "Hola mundo!"
content = ['Check for cache with streaming content.'] response = StreamingHttpResponse(content) UpdateCacheMiddleware().process_response(request, response)
middleware = CacheMiddleware()
self.assertEqual(middleware.cache_timeout, 30) self.assertEqual(middleware.key_prefix, 'middlewareprefix') self.assertEqual(middleware.cache_alias, 'other')
as_view_decorator = CacheMiddleware(cache_alias=None, key_prefix=None)
self.assertEqual(as_view_decorator.cache_alias, 'default')
as_view_decorator_with_custom = CacheMiddleware(cache_timeout=60, cache_alias='other', key_prefix='foo')
result = middleware.process_request(request) self.assertIsNone(result)
response = middleware.process_response(request, response)
result = middleware.process_request(request) self.assertIsNotNone(result) self.assertEqual(result.content, b'Hello World 1')
result = prefix_middleware.process_request(request) self.assertIsNone(result)
result = timeout_middleware.process_request(request) self.assertIsNotNone(result) self.assertEqual(result.content, b'Hello World 1')
default_view = cache_page(3)(hello_world_view) default_with_prefix_view = cache_page(3, key_prefix='prefix1')(hello_world_view)
response = default_view(request, '1') self.assertEqual(response.content, b'Hello World 1')
response = default_view(request, '2') self.assertEqual(response.content, b'Hello World 1')
response = explicit_default_view(request, '3') self.assertEqual(response.content, b'Hello World 1')
response = explicit_default_with_prefix_view(request, '4') self.assertEqual(response.content, b'Hello World 4')
response = explicit_default_with_prefix_view(request, '5') self.assertEqual(response.content, b'Hello World 4')
response = default_with_prefix_view(request, '6') self.assertEqual(response.content, b'Hello World 4')
response = other_view(request, '7') self.assertEqual(response.content, b'Hello World 7')
response = other_view(request, '8') self.assertEqual(response.content, b'Hello World 7')
response = other_with_prefix_view(request, '9') self.assertEqual(response.content, b'Hello World 9')
time.sleep(2)
caches['default'] response = default_view(request, '11') self.assertEqual(response.content, b'Hello World 1')
response = default_with_prefix_view(request, '12') self.assertEqual(response.content, b'Hello World 4')
response = explicit_default_view(request, '13') self.assertEqual(response.content, b'Hello World 1')
response = explicit_default_with_prefix_view(request, '14') self.assertEqual(response.content, b'Hello World 4')
response = other_view(request, '15') self.assertEqual(response.content, b'Hello World 15')
response = other_with_prefix_view(request, '16') self.assertEqual(response.content, b'Hello World 16')
self.assertIsNone(cache_middleware.process_request(request))
self.assertIsNone(get_cache_key(request)) learn_cache_key(request, response)
learn_cache_key(request, response, key_prefix=key_prefix) self.assertEqual( get_cache_key(request, key_prefix=key_prefix), 'views.decorators.cache.cache_page.localprefix.GET.' '58a0a05c8a5620f813686ff969c26853.d41d8cd98f00b204e9800998ecf8427e' )
self.assertNumQueries(2, self._collect, 0)
n.collect(objs)
class MockModelAdmin(object): def my_property(self): return "this if from property" my_property.short_description = 'property short description' test_from_property = property(my_property)
class MyForm(forms.Form): text = forms.CharField(label=mark_safe('<i>text</i>')) cb = forms.BooleanField(label=mark_safe('<i>cb</i>'))
class MyForm(forms.Form): text = forms.CharField(label='&text') cb = forms.BooleanField(label='&cb')
from __future__ import unicode_literals
log_entry.action_flag = 4 self.assertEqual(six.text_type(log_entry), 'LogEntry Object')
logentry.content_type = None logentry.save()
class Entity(models.Model): pass
Answer = None Post = None Question = None
a = self.Answer.objects.create(text="Number five", question=self.q1)
id_list = [o.pk for o in self.q1.answer_set.all()] x = id_list.pop() id_list.insert(-1, x)
self.assertNotEqual(list(a.question.get_answer_order()), id_list)
gc.collect() time.sleep(0.1)
gc.collect() gc.collect()
self.assertFalse(signal.has_listeners()) self.assertEqual(signal.receivers, [])
d_signal.disconnect(receiver_1_arg)
class Author(models.Model): name = models.CharField(max_length=150)
class SystemInfo(models.Model): system_name = models.CharField(max_length=32)
self.assertEqual(len(list(Article.objects.all())), 3)
self.assertEqual(len(list(Comment.objects.all())), 4)
with self.assertRaises(TypeError): class ProxyModel(SwappableModel):
resp = ProxyBug.objects.get(version__icontains='beta') self.assertEqual(repr(resp), '<ProxyBug: ProxyBug:fix this>')
resp = ProxyBug.objects.select_related().get(version__icontains='beta') self.assertEqual(repr(resp), '<ProxyBug: ProxyBug:fix this>')
resp = ProxyProxyBug.objects.select_related().get( version__icontains='beta' ) self.assertEqual(repr(resp), '<ProxyProxyBug: ProxyProxyBug:fix this>')
resp = ProxyImprovement.objects.select_related().get( reporter__name__icontains='butor' ) self.assertEqual( repr(resp), '<ProxyImprovement: ProxyImprovement:improve that>' )
resp = ProxyImprovement.objects.select_related().get( associated_bug__summary__icontains='fix' ) self.assertEqual( repr(resp), '<ProxyImprovement: ProxyImprovement:improve that>' )
@override_settings(DEBUG=True) class SelectForUpdateTests(TransactionTestCase):
self.person = Person.objects.create(name='Reinhardt')
self.new_connection = connection.copy()
self.new_connection.rollback() self.new_connection.set_autocommit(True)
for_update_sql = tested_connection.ops.for_update_sql(nowait) sql = tested_connection.queries[-1]['sql'] return bool(sql.find(for_update_sql) > -1)
with transaction.atomic(): people = list( Person.objects.all().select_for_update(nowait=nowait) ) people[0].name = 'Fred' people[0].save()
connection.close()
self.start_blocking_transaction()
status = [] thread = threading.Thread( target=self.run_select_for_update, args=(status,) )
p = Person.objects.get(pk=self.person.pk) self.assertEqual('Reinhardt', p.name)
self.end_blocking_transaction() thread.join(5.0)
self.assertFalse(thread.isAlive())
transaction.commit()
connection.close()
return None
class Student(models.Model): character = models.ForeignKey(Character, models.CASCADE) study = models.CharField(max_length=30)
class Photo(models.Model): title = models.CharField(max_length=30) image = models.FileField(storage=temp_storage, upload_to='tests')
def __init__(self, *args, **kwargs): super(Photo, self).__init__(*args, **kwargs) self._savecount = 0
class StrictAssignmentFieldSpecific(models.Model): title = models.CharField(max_length=30) _should_error = False
class Award(models.Model): name = models.CharField(max_length=30) character = models.ForeignKey(Character, models.SET_NULL, blank=False, null=True)
f1 = FormForTestingIsValid(data) self.assertTrue(f1.is_valid())
self.assertIn('no-field', e.args[0])
class ReplaceField(forms.ModelForm): url = forms.BooleanField()
class ReplaceField(forms.ModelForm): url = forms.BooleanField()
class PriceFormWithoutQuantity(forms.ModelForm): class Meta: model = Price exclude = ('quantity',)
self.assertEqual(form.instance.price, Decimal('6.00')) self.assertIsNone(form.instance.quantity) self.assertIsNone(form.instance.pk)
class BadForm(ArticleForm, BaseCategoryForm): pass
with self.assertRaises(ValueError): InvalidModelForm()
with self.assertRaises(ValueError): InvalidModelForm(instance=Category)
for c in categories: self.assertIn(c.pk, d['categories']) self.assertIsInstance(d['categories'], list)
for c in categories: self.assertIn(c.pk, d['categories']) self.assertIsInstance(d['categories'], list)
class PartialArticleFormWithSlug(forms.ModelForm): class Meta: model = Article fields = ('headline', 'slug', 'pub_date')
new_art.save() art_id_3 = new_art.id self.assertNotIn(art_id_3, (None, art_id_1, art_id_2))
new_art = Article.objects.get(id=art_id_3) self.assertQuerysetEqual(new_art.categories.all(), [])
f.save_m2m() self.assertQuerysetEqual(new_art.categories.order_by('name'), ["Entertainment", "It's a test"])
class ShortCategory(forms.ModelForm): name = forms.CharField(max_length=5) slug = forms.CharField(max_length=5) url = forms.CharField(max_length=3)
c4 = Category.objects.create(name='Fourth', url='4th') self.assertEqual(f.clean(c4.id).name, 'Fourth')
Category.objects.get(url='4th').delete() with self.assertRaises(ValidationError): f.clean(c4.id)
self.assertEqual(len(f.choices), 2)
self.assertIsNot(field1, ModelChoiceForm.base_fields['category']) self.assertIs(field1.widget.choices.field, field1)
Category.objects.get(url='6th').delete() with self.assertRaises(ValidationError): f.clean([c6.id])
model = BetterWriter fields = '__all__'
model = WriterProfile fields = '__all__'
new_author = Author.objects.get(pk=author.pk) self.assertEqual(new_author.publication, None)
instance.file.delete()
instance.file.delete() instance.delete()
instance.file.delete() instance.delete()
form = CFFForm(data={'f': None}) form.save()
data = {'title': 'Testing'} files = {"image": SimpleUploadedFile('test.png', img, 'image/png')}
self.assertEqual(p._savecount, 1)
p = Photo.objects.get() p.image.delete(save=False)
instance.image.delete(save=False)
instance.image.delete(save=False) instance.delete()
instance.image.delete(save=False) instance.delete()
if connection.features.interprets_empty_strings_as_nulls: expected_null_imagefield_repr = '' else: expected_null_imagefield_repr = None
instance.image.delete() instance.delete()
self.assertNotIn('created', ArticleForm().fields)
f = ArticleForm() with self.assertRaises(ValidationError): f.fields['status'].clean('42')
self.assertEqual(form.instance.left, 1)
Form = modelform_factory(Person, fields="__all__") self.assertNotEqual(Form.base_fields['name'].widget.__class__, forms.Textarea)
Form = modelform_factory(Person, fields="__all__", widgets={'name': widget}) self.assertEqual(Form.base_fields['name'].widget.__class__, forms.Textarea)
with self.assertRaises(TypeError): modelform_factory(Person, fields="__all__", formfield_callback='not a function or callable')
PASSWORD_HASHERS = [ 'django.contrib.auth.hashers.MD5PasswordHasher', ]
functions = list(reversed(functions))
require_http_methods(["GET"]), require_GET, require_POST, require_safe, condition(lambda r: None, lambda r: None),
vary_on_headers('Accept-language'), vary_on_cookie,
cache_page(60 * 15), cache_control(private=True), never_cache,
user_passes_test(lambda u: True), login_required, permission_required('change_world'),
staff_member_required,
keep_lazy(HttpResponse), keep_lazy_text, lazy,
def simple_dec(func): def wrapper(arg): return func("test:" + arg) return wraps(func)(wrapper)
def myattr_dec(func): def wrapper(*args, **kwargs): return func(*args, **kwargs) wrapper.myattr = True return wraps(func)(wrapper)
@myattr_dec @myattr2_dec def func(): pass
class TestPlain(object): @myattr_dec_m @myattr2_dec_m def method(self): "A method" pass
@method_decorator(myattr_dec_m, "method") class TestMethodAndClass(object): @method_decorator(myattr2_dec_m) def method(self): "A method" pass
decorators = (myattr_dec_m, myattr2_dec_m)
def test_argumented(self): class Test(object): @method_decorator(ClsDec(False)) def method(self): return True
decorators = (add_exclamation_mark, add_question_mark)
r = XFrameOptionsMiddleware().process_response(req, resp) self.assertEqual(r.get('X-Frame-Options', None), None)
@python_2_unicode_compatible class SelfRefer(models.Model): name = models.CharField(max_length=10) references = models.ManyToManyField('self') related = models.ManyToManyField('self')
@python_2_unicode_compatible class TagCollection(Tag): tags = models.ManyToManyField(Tag, related_name='tag_collections')
@python_2_unicode_compatible class Entry(models.Model): name = models.CharField(max_length=10) topics = models.ManyToManyField(Tag) related = models.ManyToManyField(Tag, related_name="similar")
class SelfReferChild(SelfRefer): pass
@python_2_unicode_compatible class Line(models.Model): name = models.CharField(max_length=100)
class User(models.Model): name = models.CharField(max_length=30) friends = models.ManyToManyField(auth.User)
class Post(models.Model): primary_lines = models.ManyToManyField(Line, related_name='+') secondary_lines = models.ManyToManyField(Line, related_name='+')
self.assertIs(t1.entry_set.__class__, t1.entry_set.__class__) self.assertIs(e1.topics.__class__, e1.topics.__class__)
self.assertIs(e1.topics.__class__, e2.topics.__class__) self.assertIs(t1.entry_set.__class__, t2.entry_set.__class__)
m1 = RegressionModelSplit(name='1') m1.save()
s1 = Person.objects.only('name').get(pk=s.pk) with self.assertNumQueries(1): s1.save()
with self.assertNumQueries(0): s.save(update_fields=[]) self.assertEqual(len(pre_save_data), 0) self.assertEqual(len(post_save_data), 0)
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(Article.objects.all()[0], self.a4)
self.assertEqual(str(connections.query).count(" JOIN "), 6)
troy = SpecialClient.objects.select_related('state').defer('value', 'state__name').get(name='Troy Buswell')
troy = SpecialClient.objects.select_related('state').only('name', 'state').get(name='Troy Buswell')
self.assertNotIn('LEFT OUTER', str(qs.query))
self.assertIn('LEFT OUTER', str(qs.query))
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(Foo.objects.get(friend__contains=b'\xc3\xa7'), fx)
self.assertEqual(Article.objects.filter(submitted_from__contains='32').count(), 0)
self.assertIsNone(Comment.objects.select_related('post').filter(post__isnull=True)[0].post)
self.assertEqual(Item.objects.get(q1), item) self.assertEqual(Item.objects.get(q2), item)
self.assertEqual(list(qs1), list(qs2)) self.assertEqual(list(qs3), list(qs4))
class Note(models.Model): content_type = models.ForeignKey(ContentType, models.CASCADE) object_id = models.PositiveIntegerField() content_object = GenericForeignKey() note = models.TextField()
class Developer(models.Model): name = models.CharField(max_length=15)
dev1 = Developer(name='Joe') note = Note(note='Deserves promotion', content_object=dev1) with self.assertRaises(IntegrityError): note.save()
g1 = Guild.objects.create(name='First guild') note = Note(note='Note for guild', content_object=g1) note.save()
b1 = Board.objects.create(name='') tag = Tag(label='VP', content_object=b1) tag.save()
with self.assertRaises(ProtectedError): related.delete()
default_manager = models.Manager() objects = HiddenInventoryManager()
from __future__ import unicode_literals
class MyModelAdmin(admin.ModelAdmin): pass for k in admin_overrides: setattr(MyModelAdmin, k, admin_overrides[k])
ma = MyModelAdmin(model, admin.site) ff = ma.formfield_for_dbfield(model._meta.get_field(fieldname), request=None)
if isinstance(ff.widget, widgets.RelatedFieldWidgetWrapper): widget = ff.widget.widget else: widget = ff.widget
return ff
response = self.client.post(reverse('admin:admin_widgets_event_add'), {"main_band": test_str})
big_honeycomb = models.Honeycomb.objects.create(location='Old tree') big_honeycomb.bee_set.create() rel = models.Bee._meta.get_field('honeycomb').remote_field
rel = models.Inventory._meta.get_field('parent').remote_field w = widgets.ForeignKeyRawIdWidget(rel, widget_admin_site)
consultor1 = models.Advisor.objects.create(name='Rockstar Techie')
w = widgets.RelatedFieldWidgetWrapper(w, rel, widget_admin_site) self.assertFalse(w.can_add_related)
self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))
self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))
self.selenium.find_element_by_id('id_birthdate_0').send_keys('2013-06-01')
self.selenium.find_element_by_id('calendarlink0').click()
calendar0 = self.selenium.find_element_by_id('calendarin0') tds = calendar0.find_elements_by_tag_name('td')
for td in tds[:6] + tds[-6:]: self.assertEqual(td.get_attribute('class'), 'nonday')
self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))
self.selenium.find_element_by_id('id_birthdate_0').send_keys('2013-06-01')
self.selenium.find_element_by_id('calendarlink0').click()
calendar0 = self.selenium.find_element_by_id('calendarin0') tds = calendar0.find_elements_by_tag_name('td')
selected = tds[6] self.assertEqual(selected.get_attribute('class'), 'selected')
self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_member_add'))
self.selenium.find_element_by_id('calendarlink0').click()
calendar0 = self.selenium.find_element_by_id('calendarin0') tds = calendar0.find_elements_by_tag_name('td')
selected = [td for td in tds if td.get_attribute('class') == 'selected']
member = models.Member.objects.create(name='Bob', birthdate=datetime(1984, 5, 15), gender='M')
may_translation = month_name expected_caption = '{0:s} {1:d}'.format(may_translation.upper(), 1984)
with override_settings(LANGUAGE_CODE=language_code, USE_L10N=True):
shortcuts = self.selenium.find_elements_by_css_selector('.field-birthdate .datetimeshortcuts')
self.selenium.find_elements_by_css_selector('.field-birthdate .timezonewarning')
self.selenium.find_element_by_tag_name('form').submit() self.wait_page_loaded()
member = models.Member.objects.get(name='test') self.assertGreater(member.birthdate, now - error_margin) self.assertLess(member.birthdate, now + error_margin)
@override_settings(TIME_ZONE='US/Eastern') class DateTimePickerAltTimezoneSeleniumTests(DateTimePickerShortcutsSeleniumTests): pass
from_lisa_select_option = self.get_select_option(from_box, str(self.lisa.id))
self.assertEqual(from_lisa_select_option.get_attribute('title'), from_lisa_select_option.get_attribute('text'))
to_lisa_select_option = self.get_select_option(to_box, str(self.lisa.id)) self.assertEqual(to_lisa_select_option.get_attribute('title'), to_lisa_select_option.get_attribute('text'))
self.get_select_option(from_box, str(self.peter.id)).click() self.get_select_option(from_box, str(self.lisa.id)).click()
self.get_select_option(from_box, str(self.peter.id)).click() self.get_select_option(from_box, str(self.lisa.id)).click()
self.get_select_option(to_box, str(self.jason.id)).click() self.get_select_option(to_box, str(self.john.id)).click()
self.get_select_option(to_box, str(self.jason.id)).click() self.get_select_option(to_box, str(self.john.id)).click()
self.assertEqual(self.selenium.current_url, original_url)
self.selenium.find_element_by_xpath('//input[@value="Save"]').click() self.wait_page_loaded()
self.selenium.find_element_by_xpath('//input[@value="Save"]').click() self.wait_page_loaded()
self.selenium.execute_script("location.reload()") self.wait_page_loaded()
self.assertEqual(self.selenium.find_element_by_id('id_main_band').get_attribute('value'), '')
self.selenium.switch_to.window(main_window) self.wait_for_value('#id_main_band', '42')
self.selenium.switch_to.window(main_window) self.wait_for_value('#id_main_band', '98')
self.assertEqual(self.selenium.find_element_by_id('id_supporting_bands').get_attribute('value'), '')
self.assertEqual( self.selenium.find_element_by_css_selector('.field-supporting_bands p.help').text, 'Supporting Bands.' )
self.selenium.switch_to.window(main_window) self.wait_for_value('#id_supporting_bands', '42')
self.selenium.switch_to.window(main_window) self.wait_for_value('#id_supporting_bands', '42,98')
self.selenium.find_element_by_css_selector('#id_user option[value=newuser]')
self.selenium.find_element_by_id('change_id_user').click() self.wait_for_popup() self.selenium.switch_to.window('id_user')
from __future__ import unicode_literals
d = Donut(name='Apple Fritter') d.baked_time = datetime.datetime(year=2007, month=4, day=20, hour=16, minute=19, second=59) d.save()
with self.assertRaises(ValueError): d.save()
self.assertIsInstance(b.baked_timestamp, datetime.datetime) self.assertIsInstance(b.baked_date, datetime.date) self.assertNotIsInstance(b.baked_date, datetime.datetime)
a.gender = '' self.assertEqual(a.get_gender_display(), '')
if kwargs['using'] == MIGRATE_DATABASE: self.call_counter = self.call_counter + 1 self.call_args = kwargs self.signal.disconnect(self, sender=APP_CONFIG)
self.assertFalse(args['plan'][0][1]) self.assertIsInstance(args['apps'], migrations.state.StateApps)
from __future__ import unicode_literals
super(Person, self).save(*args, **kwargs) self.data.append("After save")
super(Person, self).delete() self.data.append("After deletion")
content_type = models.ForeignKey(ContentType, models.CASCADE) object_pk = models.TextField() content_object = GenericForeignKey(ct_field="content_type", fk_field="object_pk")
return sorted(self.houses.all(), key=lambda house: -house.rooms.count())[0]
self.assertEqual(self.book1.authors.count(), 3)
self.assertEqual(self.author1.books.count(), 2)
self.traverse_qs( Person.objects.prefetch_related('houses__rooms', 'houses'), [['houses', 'rooms']] )
with self.assertNumQueries(2): lst1 = self.traverse_qs( Person.objects.prefetch_related('houses'), [['houses']] )
with self.assertNumQueries(2): lst1 = self.traverse_qs( House.objects.prefetch_related('occupants'), [['occupants']] )
with self.assertNumQueries(3): lst1 = self.traverse_qs( Room.objects.prefetch_related('house__occupants'), [['house', 'occupants']] )
with self.assertNumQueries(3): lst1 = self.traverse_qs( Person.objects.prefetch_related('houses', 'houses__rooms'), [['houses', 'rooms']] )
with self.assertNumQueries(3): qs = TaggedItem.objects.prefetch_related('content_object') list(qs)
with self.assertNumQueries(2): qs = Comment.objects.prefetch_related('content_object') [c.content_object for c in qs]
self.assertEqual(list(bookmark.tags.all()), list(bookmark.tags.all().all()))
self.assertIn('authorwithage', connection.queries[-1]['sql'].lower()) self.assertIn(' IN ', connection.queries[-1]['sql'])
qs = Person.objects.prefetch_related('houses__rooms', 'primary_house__occupants') [list(p.primary_house.occupants.all()) for p in qs]
with self.assertNumQueries(2, using='other'): authors = ", ".join(a.author.name for a in A.prefetch_related('author'))
house.main_room = self.rooms[-3] house.save()
r = self.a.reporter self.assertEqual(r.id, self.r.id)
self.r2.article_set.set([self.a2, self.a3])
def __init__(self, custom_optional_arg=None, model=None, query=None, using=None, hints=None): super(CustomInitQuerySet, self).__init__(model=model, query=query, using=using, hints=hints)
manager.public_method() with self.assertRaises(AttributeError): manager._private_method()
manager._optin_private_method() with self.assertRaises(AttributeError): manager.optout_public_method()
Car.cars.create(name="Corvette", mileage=21, top_speed=180) Car.cars.create(name="Neon", mileage=31, top_speed=100)
self.assertEqual(RestrictedModel.plain_manager.count(), 1)
self.assertEqual(len(RestrictedModel.plain_manager.all()), 0)
self.session.delete()
self.accessed = False self.modified = False
def test_default_expiry(self): self.assertEqual(self.session.get_expiry_age(), settings.SESSION_COOKIE_AGE)
self.session.set_expiry(0) self.assertEqual(self.session.get_expiry_age(), settings.SESSION_COOKIE_AGE)
original_now = timezone.now try: timezone.now = lambda: modification self.session.set_expiry(timedelta(seconds=10)) finally: timezone.now = original_now
with override_settings(SESSION_EXPIRE_AT_BROWSER_CLOSE=False): self.session.set_expiry(10) self.assertFalse(self.session.get_expire_at_browser_close())
self.assertEqual(len(calls), 1) self.assertIn('corrupted', calls[0])
with override_settings(SESSION_SERIALIZER='django.contrib.sessions.serializers.PickleSerializer'):
self.assertNotEqual(session.session_key, 'someunknownkey')
s1 = self.backend() s1['test_data'] = 'value1' s1.save(must_create=True)
s2 = self.backend(s1.session_key) s2.delete()
s1['test_data'] = 'value2' with self.assertRaises(UpdateError): s1.save()
self.session['y'] = 1 self.session.save()
self.session['foo'] = 'bar' self.session.set_expiry(3600) self.session.save()
other_session = self.backend() other_session['foo'] = 'bar' other_session.set_expiry(-3600) other_session.save()
self.session['_auth_user_id'] = 42 self.session.save()
s = self.model.objects.get(session_key=self.session.session_key) self.assertEqual(s.account_id, 42)
self.session.pop('_auth_user_id') self.session.save()
s = self.model.objects.get(session_key=self.session.session_key) self.assertEqual(s.account_id, None)
with self.assertRaises(InvalidCacheBackendError): self.backend()
class FileSessionTests(SessionTestsMixin, unittest.TestCase):
with self.assertRaises(ImproperlyConfigured): self.backend()
with self.assertRaises(InvalidSessionKey): self.backend()._key_to_file("a\\b\\c")
with self.assertRaises(InvalidSessionKey): self.backend()._key_to_file("a/b/c")
self.session['foo'] = 'bar' self.session.set_expiry(3600) self.session.save()
other_session = self.backend() other_session['foo'] = 'bar' other_session.set_expiry(-3600) other_session.save()
other_session2 = self.backend() other_session2['foo'] = 'bar' other_session2.save()
self.assertEqual(3, count_sessions()) management.call_command('clearsessions') self.assertEqual(1, count_sessions())
self.session = self.backend()
middleware.process_request(request) request.session['hello'] = 'world'
response = middleware.process_response(request, response) self.assertTrue( response.cookies[settings.SESSION_COOKIE_NAME]['secure'])
middleware.process_request(request) request.session['hello'] = 'world'
middleware.process_request(request) request.session['hello'] = 'world'
response = middleware.process_response(request, response) self.assertFalse(response.cookies[settings.SESSION_COOKIE_NAME]['httponly'])
middleware.process_request(request) request.session['hello'] = 'world'
response = middleware.process_response(request, response)
self.assertNotIn('hello', request.session.load())
response = middleware.process_response(request, response)
self.assertEqual(response.status_code, 302) self.assertEqual(response['Location'], path)
request.COOKIES[settings.SESSION_COOKIE_NAME] = 'abc'
middleware.process_request(request) request.session.flush()
response = middleware.process_response(request, response)
request.COOKIES[settings.SESSION_COOKIE_NAME] = 'abc'
middleware.process_request(request) request.session.flush()
response = middleware.process_response(request, response)
middleware.process_request(request) request.session.flush()
response = middleware.process_response(request, response)
self.assertEqual(response.cookies, {}) self.assertEqual(response['Vary'], 'Cookie')
class CookieSessionTests(SessionTestsMixin, unittest.TestCase):
super(CookieSessionTests, self).test_actual_expiry()
self.assertEqual(self.session.serializer, JSONSerializer) self.session.save()
for method in SimpleView.http_method_names: kwargs = dict(((method, "value"),)) with self.assertRaises(TypeError): SimpleView.as_view(**kwargs)
CustomizableView.as_view(parameter="value") with self.assertRaises(TypeError): CustomizableView.as_view(foobar="value")
response2 = self.client.get('/template/cached/bar/') self.assertEqual(response2.status_code, 200)
response = RedirectView.as_view(url='/bar/')(self.rf.request(PATH_INFO='/foo/')) self.assertEqual(response.status_code, 302)
self.assertIn('test_name', context) self.assertEqual(context['kwarg_test'], 'kwarg_value') self.assertEqual(context['custom_key'], 'custom_value')
context = test_view.get_context_data(test_name='test_value') self.assertEqual(context['test_name'], 'test_value')
test_view = views.CustomSingleObjectView() test_view.context_object_name = 'pony' context = test_view.get_context_data() self.assertEqual(context['pony'], test_view.object)
test_view = views.CustomSingleObjectView() context = test_view.get_context_data() self.assertEqual(context['object'], test_view.object)
context = test_view.get_context_data() self.assertEqual(context['object_list'], test_view.queryset)
context = test_view.get_context_data(object_list=queryset) self.assertEqual(context['object_list'], queryset)
from __future__ import unicode_literals
url(r'^edit/artists/create/$', views.ArtistCreate.as_view()), url(r'^edit/artists/(?P<pk>[0-9]+)/update/$', views.ArtistUpdate.as_view()),
url(r'^accounts/login/$', auth_views.login)
author = self.get_object() context = {'custom_' + self.get_context_object_name(author): author} return self.render_to_response(context)
self.object = None
from __future__ import unicode_literals
self.assertEqual(len(res.context['object_list']), 7)
self.assertEqual(len(res.context['object_list']), 7)
from __future__ import unicode_literals
with self.assertNumQueries(2): self.client.get('/dates/books/reverse/')
self.assertEqual(res.context['next_year'], None) self.assertEqual(res.context['previous_year'], datetime.date(2006, 1, 1))
with self.assertNumQueries(4): self.client.get('/dates/books/2008/reverse/')
self.assertEqual(res.context['next_month'], None) self.assertEqual(res.context['previous_month'], datetime.date(2006, 5, 1))
res = self.client.get('/dates/books/2000/jan/') self.assertEqual(res.status_code, 404)
res = self.client.get('/dates/books/%s/' % urlbit) self.assertEqual(res.status_code, 404)
self.assertEqual(res.context['next_month'], None) self.assertEqual(res.context['previous_month'], datetime.date(2008, 10, 1))
self.assertEqual(res.context['next_week'], None) self.assertEqual(res.context['previous_week'], datetime.date(2006, 4, 30))
res = self.client.get('/dates/books/2008/week/12/') self.assertEqual(res.status_code, 404)
self.assertEqual(res.context['next_week'], None) self.assertEqual(res.context['previous_week'], datetime.date(2008, 9, 28))
self.assertEqual(res.context['next_day'], None) self.assertEqual(res.context['previous_day'], datetime.date(2006, 5, 1))
res = self.client.get('/dates/books/2000/jan/1/') self.assertEqual(res.status_code, 404)
res = self.client.get('/dates/books/%s/' % urlbit) self.assertEqual(res.status_code, 404)
self.assertEqual(res.context['next_day'], None) self.assertEqual(res.context['previous_day'], datetime.date(2008, 10, 1))
with self.assertRaises(ImproperlyConfigured): self.client.post('/edit/author/%d/delete/naive/' % a.pk)
from __future__ import unicode_literals
cls.site1 = Site(pk=1, domain='example.com', name='example.com') cls.site1.save()
], ROOT_URLCONF='flatpages_tests.urls', TEMPLATES=FLATPAGES_TEMPLATES, SITE_ID=1,
], ROOT_URLCONF='flatpages_tests.urls', TEMPLATES=FLATPAGES_TEMPLATES, SITE_ID=1,
Site.objects.clear_cache()
raise ImportError("Oops")
@python_2_unicode_compatible class Person(models.Model): name = models.CharField(max_length=128)
inviter = models.ForeignKey(Person, models.CASCADE, related_name='invitations_sent') invitee = models.ForeignKey(Person, models.CASCADE, related_name='invitations')
self.assertQuerysetEqual( tony.friends.all(), ['Chris'], attrgetter("name") )
encoded_data = encoded_data[1:]
example_messages = ['test', 'me'] set_cookie_data(storage, example_messages) self.assertEqual(list(storage), example_messages)
storage = self.get_storage() response = self.get_response() storage.add(constants.INFO, 'test') for m in storage:
example_messages = ['test', 'me'] set_cookie_data(storage, example_messages, invalid=True) self.assertEqual(list(storage), [])
encoded_messages = json.loads(encoded_messages) for obj in encoded_messages: obj.pop(1) encoded_messages = json.dumps(encoded_messages, separators=(',', ':'))
decoded_messages = json.loads(encoded_messages, cls=MessageDecoder) self.assertEqual(messages, decoded_messages)
example_messages = ['test', 'me'] set_session_data(storage, example_messages) self.assertEqual(list(storage), example_messages)
example_messages = [str(i) for i in range(5)] set_cookie_data(cookie_storage, example_messages)
self.get_session_storage(storage)._get = None
self.assertEqual(list(storage), example_messages)
self.get_session_storage(storage)._get = None
self.assertEqual(list(storage), [])
example_messages = [str(i) for i in range(5)] set_cookie_data(cookie_storage, example_messages[:4] + [CookieStorage.not_finished]) set_session_data(session_storage, example_messages[4:])
self.assertEqual(list(storage), example_messages)
example_messages = [str(i) for i in range(5)] set_cookie_data(cookie_storage, [CookieStorage.not_finished], encode_empty=True) set_session_data(session_storage, example_messages)
self.assertEqual(list(storage), example_messages)
set_cookie_data(cookie_storage, ['cookie', CookieStorage.not_finished]) set_session_data(session_storage, ['session'])
response = self.get_response() list(storage) storage.update(response) session_storing = self.stored_session_messages_count(storage, response) self.assertEqual(session_storing, 0)
self.get_session_storage(storage)._store = None
self.old_level_tags = base.LEVEL_TAGS base.LEVEL_TAGS = utils.get_level_tags()
response = self.client.get(show_url) for msg in data['messages']: self.assertNotContains(response, msg)
data = list(storage) self.assertTrue(storage.used) self.assertEqual(data, list(storage))
request = self.get_request() self.assertEqual(get_level(request), constants.INFO)
storage = self.get_storage() request._messages = storage self.assertEqual(get_level(request), constants.INFO)
add_level_messages(storage) self.assertEqual(len(storage), 5)
article = Article.objects.annotate( headline=Coalesce('summary', 'text', output_field=TextField()), )
article = Article.objects.annotate( headline=Coalesce(Lower('summary'), Lower('text'), output_field=TextField()), )
self.assertEqual(len(list(pair.flatten())), 3)
self.assertEqual(str(qs.query), str(qs.all().query))
Author.objects.filter(alias__isnull=True).update( alias=Lower(Substr('name', 1, 5)), )
if tzinfo is not None: value = value.astimezone(tzinfo)
if isinstance(value, datetime): return value.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0) return value.replace(month=1, day=1)
value = timezone.make_aware(value.replace(tzinfo=None), tzinfo)
Site.objects.all().delete() management.call_command('loaddata', 'fixture1.json', verbosity=0)
management.call_command('loaddata', 'fixture4.json', verbosity=0) self.assertQuerysetEqual(Article.objects.all(), [ '<Article: Django pets kitten>', ])
management.call_command('loaddata', 'fixture1', verbosity=0)
self.assertQuerysetEqual(Article.objects.all(), [ '<Article: Time to reform copyright>', '<Article: Poker has no place on ESPN>', ])
from __future__ import unicode_literals
"Big data" * 68000, encode=base64.encodestring if PY2 else base64.encodebytes)
filenames = ['', 'C:\\Windows\\']
smallfile.write(b'a' * (2 ** 21)) smallfile.seek(0)
bigfile.write(b'a' * (10 * 2 ** 20)) bigfile.seek(0)
with self.assertRaises(AttributeError): self.client.post('/quota/broken/', {'f': file})
self.assertTrue(hasattr(request, '_files'))
self.assertFalse(hasattr(request, '_files'))
self.client.handler = POSTAccessingHandler()
self.assertEqual(err.__class__, uploadhandler.CustomUploadError)
MultiPartParser({ 'CONTENT_TYPE': 'multipart/form-data; boundary=_foo', 'CONTENT_LENGTH': '1' }, StringIO('x'), [], 'utf-8')
if os.path.dirname(form_data['file_field'].name) != '': return HttpResponseServerError() return HttpResponse('')
largefile = request.FILES['file_field2'] obj = FileModel() obj.testfile.save(largefile.name, largefile)
if not request.FILES['file_unicode'].name.endswith(UNICODE_FILENAME): return HttpResponseServerError()
obj.delete() os.unlink(full_name)
self.assertTemplateNotUsed(response, 'GET Template')
self.assertRedirects(response, '/some_view/')
self.assertRedirects(response, 'http://testserver/permanent_redirect_view/')
self.assertRedirects(response, 'http://testserver/permanent_redirect_view/', msg_prefix='abc')
self.assertRedirects(response, '/redirect_to_self/', status_code=302, target_status_code=302) self.assertEqual(len(response.redirect_chain), 2)
self.assertRedirects(response, '/circular_redirect_2/', status_code=302, target_status_code=302) self.assertEqual(len(response.redirect_chain), 4)
c = Client() login = c.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = c.get("/login_protected_redirect_view/")
self.assertRedirects(response, "/get_view/")
@override_settings(ROOT_URLCONF='test_client_regress.urls') class UrlconfSubstitutionTests(SimpleTestCase):
class zzUrlconfSubstitutionTests(SimpleTestCase):
response = self.client.get('/check_session/') self.assertEqual(response.status_code, 200) self.assertEqual(response.content, b'NO')
response = self.client.get('/set_session/') self.assertEqual(response.status_code, 200) self.assertEqual(response.content, b'set_session')
response = self.client.get('/check_session/') self.assertEqual(response.status_code, 200) self.assertEqual(response.content, b'YES')
login = self.client.login(username='testclient', password='password') self.assertTrue(login, 'Could not log in')
response = self.client.get('/check_session/') self.assertEqual(response.status_code, 200) self.assertEqual(response.content, b'YES')
self.assertNotEqual(response.content, b'request method: HEAD') self.assertEqual(response.content, b'')
response = method("/request_data/?foo=whiz") self.assertEqual(response.context['get-foo'], 'whiz')
response = method("/request_data/?foo=whiz", data={'foo': 'bang'}) self.assertEqual(response.context['get-foo'], 'bang')
self.client.get('/') self.common_test_that_should_always_pass()
self.common_test_that_should_always_pass()
request.special_path = request.path return render(request, 'request_context.html')
test_combined_expression = unittest.expectedFailure(test_combined_expression)
When(integer=1, then=Value(timedelta(1), output_field=models.DurationField())), When(integer=2, then=Value(timedelta(2), output_field=models.DurationField())),
When(integer=1, then=Value(time(1), output_field=models.TimeField())), When(integer=2, then=Value(time(2), output_field=models.TimeField())),
class SchoolManager(models.Manager): def get_queryset(self): return super(SchoolManager, self).get_queryset().filter(is_public=True)
w = self.r1.waiter_set.create(name='Joe') self.assertEqual(repr(w), '<Waiter: Joe the waiter at Demon Dogs the restaurant>')
o1 = ManualPrimaryKey(primary_key="abc123", name="primary") o1.save() o2 = RelatedModel(link=o1, name="secondary") o2.save()
with self.assertRaises(Restaurant.DoesNotExist): place.restaurant
self.p1.undergroundbar bar.place.name = 'foo' bar.place = None bar.save() self.p1.delete()
p.undergroundbar = None
p = Place.objects.get(name="Demon Dogs") r = p.restaurant
self.assertIs(p.restaurant, r)
del p._restaurant_cache self.assertIsNot(p.restaurant, r)
r2 = Restaurant.objects.get(pk=r.pk) p.restaurant = r2 self.assertIs(p.restaurant, r2)
ug_bar = UndergroundBar.objects.create(place=p, serves_cocktails=False) ug_bar.place = None self.assertIsNone(ug_bar.place)
setattr(p, 'restaurant', None)
with self.assertRaises(ValueError): setattr(p, 'restaurant', p)
p = Place.objects.get(name="Demon Dogs") r = Restaurant(place=p) self.assertIs(r.place, p)
p = Place() r = Restaurant(place=p) self.assertTrue(r.place is p)
r = Restaurant.objects.get(pk=self.r1.pk) p = r.place with self.assertNumQueries(0): self.assertEqual(p.restaurant, r)
p = Place.objects.get(pk=self.p1.pk) r = p.restaurant with self.assertNumQueries(0): self.assertEqual(r.place, p)
with self.assertNumQueries(0): with self.assertRaises(UndergroundBar.DoesNotExist): p.undergroundbar
with self.assertNumQueries(0): with self.assertRaises(UndergroundBar.DoesNotExist): p.undergroundbar
if connection.features.supports_nullable_unique_constraints: UndergroundBar.objects.create()
with self.assertNumQueries(0): with self.assertRaises(UndergroundBar.DoesNotExist): p.undergroundbar
p.undergroundbar = b
self.assertQuerysetEqual( School.objects.all(), ["<School: School object>"] )
self.assertQuerysetEqual( Director.objects.all(), ["<Director: Director object>"] )
self.assertEqual(private_director.school, private_school)
self.assertEqual(private_school.director, private_director)
self.assertFalse(hasattr(Director(), 'director')) self.assertFalse(hasattr(School(), 'school'))
managed = True
@python_2_unicode_compatible class C02(models.Model): mm_a = models.ManyToManyField(A02, through="Intermediate") f_a = models.CharField(max_length=10, db_index=True) f_b = models.IntegerField()
class Proxy1(models.Model): class Meta: db_table = "unmanaged_models_proxy1"
class Unmanaged2(models.Model): mm = models.ManyToManyField(Unmanaged1)
class Managed1(models.Model): mm = models.ManyToManyField(Unmanaged1)
a2 = A02.objects.all()[0] self.assertIsInstance(a2, A02) self.assertEqual(a2.f_a, "foo")
from functools import partial from os import path
url(r'^non_existing_url/', partial(defaults.page_not_found, exception=None)), url(r'^server_error/', defaults.server_error),
url(r'raises/$', views.raises),
url(r'^site_media/(?P<path>.*)$', static.serve, {'document_root': media_dir}),
from __future__ import unicode_literals
from __future__ import unicode_literals
url('^中文/target/$', views.index_page),
urlpatterns += [ url(r'^json/response/$', views.json_response_view), ]
def callable(): raise Exception try: raise Exception except Exception: return technical_500_response(request, *sys.exc_info())
try: raise Exception except Exception: return technical_500_response(request, *sys.exc_info())
try: return render(request, path) except TemplateDoesNotExist: return technical_500_response(request, *sys.exc_info())
return render(request, [], {})
'timestamp': datetime.datetime(2013, 5, 19, 20), 'value': decimal.Decimal('3.14'),
from __future__ import unicode_literals
response = self.client.get('/old_jsi18n_admin/?language=de') self.assertContains(response, '\\x04')
available_apps = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'view_tests', ]
from __future__ import unicode_literals
self.assertEqual(reverse('with_parameter', kwargs={'parameter': 'x'}), '/test-setlang/x/') lang_code = self._get_inactive_language_code()
response = self.client.post( '/i18n/setlang/', data={'language': 'en'}, follow=True, HTTP_REFERER='/nl/vertaald/' ) self.assertRedirects(response, '/en/translated/')
response = self.client.post( '/i18n/setlang/', data={'language': 'en'}, follow=True, HTTP_REFERER='/nl/vertaald/' ) self.assertRedirects(response, '/en/translated/')
response = self.client.get('/jsi18n_admin/?language=de') self.assertContains(response, '\\x04')
available_apps = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'view_tests', ]
self.assertEqual(len(second_chunk.strip()), 1449)
) self.assertIsInstance(response, HttpResponseNotModified)
from __future__ import unicode_literals
response = self.client.get('/raises400/') self.assertContains(response, '<div class="context" id="', status_code=400)
allow_database_queries = True
TEMPLATES=[{ 'BACKEND': 'django.template.backends.dummy.TemplateStrings', }],
response = self.client.get('/raises400/') self.assertContains(response, '<div class="context" id="', status_code=400)
self.assertContains(response, k, status_code=500) self.assertContains(response, v, status_code=500)
self.assertContains(response, k, status_code=500)
self.assertNotContains(response, 'sausage-value', status_code=500) self.assertNotContains(response, 'bacon-value', status_code=500)
self.assertContains(response, k, status_code=500) self.assertNotContains(response, v, status_code=500)
self.assertIn(k, body_plain) self.assertIn(v, body_plain) self.assertIn(k, body_html) self.assertIn(v, body_html)
self.assertIn(k, body_plain)
exc_type, exc_value, tb = sys.exc_info()
from __future__ import unicode_literals
from __future__ import unicode_literals
url(r'^admin/', admin.site.urls),
self.assertCalcEqual(True, [False, 'and', False, 'or', True])
self.assertCalcEqual(True, [True, 'or', False, 'and', False])
self.assertCalcEqual(True, [1, 'or', 1, '==', 2])
from __future__ import unicode_literals
self.verify_tag(custom.assignment_no_params, 'assignment_no_params')
class CustomURLConfMiddleware(MiddlewareMixin): def process_request(self, request): request.urlconf = 'template_tests.alternate_urls'
response = self._response() self.assertFalse(response.is_rendered) response.render() self.assertTrue(response.is_rendered)
response = self._response().render() self.assertEqual(response.content, b'foo')
template = engines['django'].from_string('bar{{ baz }}') response.template_name = template response.render() self.assertEqual(response.content, b'foo')
response.content = 'bar' self.assertEqual(response.content, b'bar')
response = self._response() self.assertFalse(response.is_rendered)
response = self._response().render() res = [x for x in response] self.assertEqual(res, [b'foo'])
response = self._response() self.assertFalse(response.is_rendered) with self.assertRaises(ContentNotRenderedError): response.content self.assertFalse(response.is_rendered)
response = self._response().render() self.assertEqual(response.content, b'foo')
response.render() self.assertEqual(response.content, b'First template\n') self.assertEqual(post, ['post1', 'post2'])
response.render() pickled_response = pickle.dumps(response) unpickled_response = pickle.loads(pickled_response)
template_attrs = ('template_name', 'context_data', '_post_render_callbacks') for attr in template_attrs: self.assertFalse(hasattr(unpickled_response, attr))
for attr in template_attrs: with self.assertRaises(AttributeError): getattr(unpickled_response, attr)
response.render() pickled_response = pickle.dumps(response) unpickled_response = pickle.loads(pickled_response)
template_attrs = ( 'template_name', 'context_data', '_post_render_callbacks', '_request', ) for attr in template_attrs: self.assertFalse(hasattr(unpickled_response, attr))
for attr in template_attrs: with self.assertRaises(AttributeError): getattr(unpickled_response, attr)
response2 = self.client.get('/template_response_view/') self.assertEqual(response2.status_code, 200)
response2 = self.client.get('/template_response_view/') self.assertEqual(response2.status_code, 200)
from __future__ import unicode_literals
from __future__ import unicode_literals
from django.http import HttpResponse from django.template.response import TemplateResponse
from django.template import RequestContext, TemplateSyntaxError from django.test import RequestFactory, SimpleTestCase, override_settings from django.urls import NoReverseMatch, resolve
@setup({'url-fail01': '{% url %}'}) def test_url_fail01(self): with self.assertRaises(TemplateSyntaxError): self.engine.get_template('url-fail01')
with self.assertRaises(TemplateSyntaxError): self.engine.render_to_string('if-tag-single-eq', {'foo': 1})
output = self.engine.render_to_string('template', {'foo': False}) self.assertEqual(output, 'yes')
@setup({'widthratio08': '{% widthratio %}'}) def test_widthratio08(self): with self.assertRaises(TemplateSyntaxError): self.engine.get_template('widthratio08')
from __future__ import unicode_literals
from __future__ import unicode_literals
@setup(inheritance_templates) def test_inheritance31(self): output = self.engine.render_to_string('inheritance31', {'optional': True}) self.assertEqual(output, '1two3')
@setup(inheritance_templates) def test_inheritance39(self): output = self.engine.render_to_string('inheritance39', {'optional': True}) self.assertEqual(output, '1new23')
warnings.filterwarnings( "ignore", "Using a non-integer number instead of an " "integer will result in an error in the future", DeprecationWarning )
@setup({'basic-syntax13': "{{ va>r }}"}) def test_basic_syntax13(self): with self.assertRaises(TemplateSyntaxError): self.engine.get_template('basic-syntax13')
with self.assertRaises(TemplateSyntaxError): FilterExpression("article._hidden|upper", p)
with self.assertRaises(TemplateSyntaxError): Variable("article._hidden")
with six.assertRaisesRegex(self, TypeError, "Variable must be a string or number, got <(class|type) 'dict'>"): Variable({})
'1|no_arguments', '1|one_argument:"1"', '1|one_opt_argument', '1|one_opt_argument:"1"', '1|two_one_opt_arg:"1"',
template = engine.get_template('other-recursive.html') output = template.render(Context({})) self.assertEqual(output.strip(), 'fs3/recursive fs2/recursive fs/recursive')
output = engine.render_to_string('index.html') self.assertEqual(output, 'base')
t = self.engine.from_string('{{ my_doodad.value }}') self.assertEqual(t.render(c), '')
self.assertEqual(my_doodad.num_calls, 1)
self.assertEqual(my_doodad.num_calls, 0)
self.assertEqual(my_doodad.num_calls, 0)
self.assertEqual(my_doodad.num_calls, 0)
from __future__ import unicode_literals
loader = self.engine.template_loaders[0] source, name = loader.load_template('index.html') self.assertEqual(template.origin.template_name, 'index.html')
with self.assertRaises(TemplateDoesNotExist): loader.find_template(template_name)
self.assertNotEqual(t1.render(Context({})), t2.render(Context({})))
check_sources(b'\xc3\x85ngstr\xc3\xb6m', ['/dir1/Ångström', '/dir2/Ångström']) check_sources('Ångström', ['/dir1/Ångström', '/dir2/Ångström'])
test_once = kwargs.get('test_once', False)
templates["inclusion.html"] = "{{ result }}"
@override_settings(TEMPLATES=None) @functools.wraps(func) def inner(self): libraries = getattr(self, 'libraries', {})
@python_2_unicode_compatible class UnsafeClass: def __str__(self): return 'you & me'
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(upper('\xeb'), '\xcb')
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
test_many_zeroes = expectedFailure(test_many_zeroes)
from __future__ import unicode_literals
self.assertEqual(lower('\xcb'), '\xeb')
from __future__ import unicode_literals
import warnings
test_context.push()
a.update({'a': 1}) self.assertNotEqual(a, b)
a.update({'c': 3}) b.update({'c': 3}) self.assertNotEqual(a, b)
b.update({'a': 1}) self.assertEqual(a, b)
self.assertEqual(len(ctx.dicts), 4)
test_data = {'x': 'y', 'v': 'z', 'd': {'o': object, 'a': 'b'}}
request = RequestFactory().get('/')
url(r'^template_response_view/$', views.template_response_view),
url(r'^snark/', views.snark, name='snark'),
from __future__ import unicode_literals
from __future__ import unicode_literals
class Scene(models.Model): scene = models.CharField(max_length=255) setting = models.CharField(max_length=255)
from __future__ import unicode_literals
from __future__ import unicode_literals
CreateExtension('uuid-ossp'), HStoreExtension(), TrigramExtension(), UnaccentExtension(),
from __future__ import unicode_literals
from __future__ import unicode_literals
from __future__ import unicode_literals
instance = IntegerArrayModel(field=['1']) instance.save() loaded = IntegerArrayModel.objects.get() self.assertEqual(loaded.field, [1])
self.assertSequenceEqual( CharArrayModel.objects.filter(field__contains=['text']), [] )
self.assertEqual(errors[0].id, 'postgres.E001') self.assertIn('max_length', errors[0].msg)
self.assertEqual(errors[0].id, 'postgres.E001') self.assertIn('max_length', errors[0].msg)
field.clean([1, None], None)
o2o_setnull = models.ForeignKey(R, models.SET_NULL, null=True, related_name="o2o_nullable_set")
class AvatarProxy(Avatar): class Meta: proxy = True
replacement_r = R.objects.create()
b.delete()
self.assertNumQueries(5, s.delete) self.assertFalse(S.objects.exists())
calls = []
calls = []
expected_num_queries = (ceil(TEST_SIZE // batch_size) + ceil(TEST_SIZE // GET_ITERATOR_CHUNK_SIZE) + 2)
with self.assertNumQueries(2): avatar.delete()
self.assertNumQueries(2, a.delete) self.assertFalse(User.objects.exists()) self.assertFalse(Avatar.objects.exists())
self.assertNumQueries(2, f.delete)
self.assertNumQueries(2, f.delete)
response = self.client.get(url) self.assertContains(response, 'Have debug')
with override_settings(DEBUG=False): response = self.client.get(url) self.assertNotContains(response, 'Have debug')
self.assertContains(response, 'Third query list: 2') self.assertContains(response, 'Fourth query list: 3')
test_gmbh = Company.objects.get(name="Test GmbH")
qs = Employee.objects.filter( company_ceo_set__num_chairs=F('company_ceo_set__num_employees')) self.assertEqual(str(qs.query).count('JOIN'), 1)
n_qs = Number.objects.filter(id=f) self.assertEqual(n_qs.get(), n) self.assertEqual(c_qs.get(), c)
Number.objects.filter(pk=self.n.pk).update( integer=F('integer') + 15, float=F('float') + 42.7 )
Number.objects.filter(pk=self.n.pk).update(integer=F('integer') - 15, float=F('float') - 42.7)
Number.objects.filter(pk=self.n.pk).update(integer=F('integer') * 15, float=F('float') * 42.7)
Number.objects.filter(pk=self.n.pk).update(integer=F('integer') / 2, float=F('float') / 42.7)
Number.objects.filter(pk=self.n.pk).update(integer=F('integer') % 20)
Number.objects.filter(pk=self.n.pk).update(integer=F('integer').bitand(56))
Number.objects.filter(pk=self.n.pk).update(integer=F('integer').bitor(48))
Number.objects.filter(pk=self.n.pk).update(integer=15 + F('integer'), float=42.7 + F('float'))
Number.objects.filter(pk=self.n.pk).update(integer=15 * F('integer'), float=42.7 * F('float'))
Number.objects.filter(pk=self.n.pk).update(integer=640 / F('integer'), float=42.7 / F('float'))
Number.objects.filter(pk=self.n.pk).update(integer=69 % F('integer'))
cls.deltas = [] cls.delays = [] cls.days_long = []
qs = Experiment.objects.filter(end__lt=F('start') + datetime.timedelta(hours=1)) qs2 = qs.all() list(qs) list(qs2)
environ['QUERY_STRING'] = str(raw_query_string, 'iso-8859-1')
self.assertListEqual(got, ['café', 'café', 'caf\ufffd', 'café'])
self.assertEqual(request.COOKIES['want'], force_str("café"))
self.assertIsInstance(request.COOKIES, dict)
self.assertEqual(response.status_code, 400)
script_name = get_script_name({'SCRIPT_URL': '/foobar/'}) self.assertEqual(script_name, '/foobar/')
return SelfRef.objects.get(selfref=self).pk
a.save() self.assertIsNotNone(a.id) self.assertEqual(Article.objects.all().count(), 1)
a = Article(None, 'Fourth article', pub_date=datetime(2005, 7, 31)) a.save() self.assertEqual(a.headline, 'Fourth article')
self.assertIn(a, Article.objects.all()) self.assertTrue(Article.objects.filter(id=a.id).exists())
with self.assertRaises(TypeError): EmptyQuerySet() self.assertIsInstance(Article.objects.none(), EmptyQuerySet) self.assertFalse(isinstance('', EmptyQuerySet))
Article.objects.create(headline='foo', pub_date=datetime.now())
self.assertEqual(hash(Article(id=1)), hash(1)) with self.assertRaises(TypeError): hash(Article())
self.a = Article( id=None, headline='Swallow programs in Python', pub_date=datetime(2005, 7, 28), ) self.a.save()
self.a.headline = 'Parrot programs in Python' self.a.save()
self.assertQuerysetEqual(Article.objects.all(), ['<Article: Parrot programs in Python>'])
self.assertEqual(Article.objects.get(pk=self.a.id), self.a)
self.assertQuerysetEqual(Article.objects.filter(pk__in=[self.a.id]), ["<Article: Swallow programs in Python>"])
a = Article.objects.get(pk=self.a.id) b = Article.objects.get(pk=self.a.id) self.assertEqual(a, b)
a = Article( id=None, headline='Swallow bites Python', pub_date=datetime(2005, 7, 28), ) a.save()
Article.objects.filter(pk=a.pk).delete()
called = False
with self.assertRaises(DatabaseError): asos.save(force_update=True) with self.assertRaises(DatabaseError): asos.save(update_fields=['pub_date'])
return val - timedelta(microseconds=val.microsecond)
self.assertFalse(hasattr(s3_copy.selfref, 'touched')) self.assertEqual(s3_copy.selfref, s2)
with self.assertNumQueries(0): mtv = ModelToValidate(number=10, name='Some Name') setattr(mtv, '_adding', True) mtv.full_clean()
with self.assertNumQueries(1): mtv = ModelToValidate(number=10, name='Some Name', id=123) setattr(mtv, '_adding', True) mtv.full_clean()
with self.assertNumQueries(0): mtv = ModelToValidate(number=10, name='Some Name') mtv.full_clean()
p = Post(title="Work on Django 1.1 begins", posted=datetime.date(2008, 9, 3)) p.full_clean()
p = Post(title="Django 1.0 is released", posted=datetime.datetime(2008, 9, 4)) p.full_clean()
from __future__ import unicode_literals
fake_app = object()
application = object()
lookup_name = 'testyear'
[a1, a2, a3, a4], lambda x: x)
baseqs.filter(age__mult3__gte=models.F('average_rating')), [a2, a3], lambda x: x)
self.assertQuerysetEqual( baseqs.filter(birthdate__testyear__lt=2012), [self.a1], lambda x: x)
lookup_name = 'testyear' call_order = []
return now.replace(tzinfo=tz) + tz.utcoffset(now)
with translation.override('ja'), self.settings(USE_L10N=True): self.humanize_tester([100], ['100'], 'intcomma')
self.assertNotEqual(naturalday_one, naturalday_two)
dt = datetime.datetime(2012, 3, 9, 1, 30, tzinfo=utc)
result_list_with_tz_support = result_list[:] assert result_list_with_tz_support[-4] == '2\xa0days, 6\xa0hours from now' result_list_with_tz_support[-4] == '2\xa0days, 5\xa0hours from now'
from __future__ import unicode_literals
parent_msg.attach(content=child_s, mimetype='message/rfc822') parent_s = parent_msg.message().as_string()
self.assertIn(str('Child Subject'), parent_s)
self.assertIn(str('Child Subject'), parent_s)
self.assertIn(str('Child Subject'), parent_s)
with self.assertRaises(BadHeaderError): send_mail('Subject\nMultiline', 'Content', 'from@example.com', ['to@example.com'])
pass
msg = None for i, m in enumerate(smtp_messages): if m[:4] == 'data': msg = smtp_messages[i + 1] break
msg = msg.replace('\r\n', '') self.assertNotIn('\r', msg) self.assertNotIn('\n', msg)
self.test_outbox.extend(email_messages) return len(email_messages)
from __future__ import unicode_literals
object_id = models.PositiveIntegerField() content_object = GenericForeignKey()
content_object = GenericForeignKey()
from __future__ import unicode_literals
from __future__ import unicode_literals
with self.assertNumQueries(1): ContentType.objects.get_for_model(ContentType)
ContentType.objects.clear_cache() with self.assertNumQueries(1): ContentType.objects.get_for_model(ContentType)
ct_fetched = ContentType.objects.get_for_id(ct.pk) self.assertIsNone(ct_fetched.model_class())
self.assertContains(response, 'Base view for admindocs views.')
response = self.client.get(reverse('django-admindocs-filters')) self.assertContains(response, '<title>Template filters</title>', html=True)
response = self.client.get(reverse('django-admindocs-tags')) self.assertContains(response, '<title>Template tags</title>', html=True)
self.assertContains(self.response, "<p>Get the full name of the person</p>")
self.assertContains(self.response, company_markup)
self.assertContains(self.response, "%s\n - place of work" % company_markup)
from __future__ import unicode_literals
self.assertQuerysetEqual( Article.objects.complex_filter({'pk': self.a1}), [ 'Hello' ], attrgetter("headline"), )
self.assertEqual( Article.objects.get(Q(headline__startswith='Hello'), Q(headline__contains='bye')).headline, 'Hello and goodbye' )
@skipUnlessDBFeature('test_db_allows_multiple_connections') class DeleteLockingTest(TransactionTestCase):
self.conn2 = connection.copy() self.conn2.set_autocommit(False)
self.conn2.rollback() self.conn2.close()
self.assertEqual(3, Book.objects.count())
with self.conn2.cursor() as cursor2: cursor2.execute("DELETE from delete_regress_book WHERE id = 1") self.conn2.commit()
Book.objects.filter(pagecount__lt=250).delete()
self.assertEqual(AwardNote.objects.count(), 0)
self.assertEqual(PlayedWithNote.objects.count(), 0)
test_image = Image() test_image.save() foo_image = FooImage(my_image=test_image) foo_image.save()
test_file = File.objects.get(pk=test_image.pk) foo_file = FooFile(my_file=test_file) foo_file.save()
self.assertEqual(len(Image.objects.all()), 0) self.assertEqual(len(File.objects.all()), 0)
self.assertEqual(len(FooImage.objects.all()), 0) self.assertEqual(len(FooFile.objects.all()), 0)
test_photo = Photo.objects.get(pk=test_image.pk) foo_photo = FooPhoto(my_photo=test_photo) foo_photo.save()
self.assertEqual(len(File.objects.all()), 0) self.assertEqual(len(Image.objects.all()), 0)
self.assertEqual(len(FooFile.objects.all()), 0) self.assertEqual(len(FooImage.objects.all()), 0)
image = Image.objects.create() as_file = File.objects.get(pk=image.pk) FooFileProxy.objects.create(my_file=as_file)
self.assertEqual(event.dt, dt.replace(microsecond=0))
self.assertEqual(event.dt.replace(tzinfo=EAT), dt)
self.assertEqual(event.dt.replace(tzinfo=EAT), dt)
self.assertEqual(event.dt.replace(tzinfo=EAT), dt)
self.assertEqual(event.dt.replace(tzinfo=EAT), dt)
self.assertEqual(event.dt, dt.replace(tzinfo=EAT))
self.assertEqual(event.dt, dt.replace(tzinfo=EAT))
self.assertEqual(event.dt, dt.replace(microsecond=0, tzinfo=EAT))
self.assertEqual(event.dt, dt.replace(microsecond=0))
e = MaybeEvent.objects.create() self.assertEqual(e.dt, None)
connection.timezone del connection.timezone connection.timezone_name del connection.timezone_name
connection.timezone del connection.timezone connection.timezone_name del connection.timezone_name
six.assertRegex(self, yaml, r"\n fields: {dt: !(!timestamp)? '%s'}" % re.escape(dt))
def t(*result): return '|'.join(datetimes[key].isoformat() for key in result)
self.assertTrue(tpl.render(ctx).startswith("2011"))
self.assertTrue(form.is_valid()) self.assertEqual(form.cleaned_data['dt'], datetime.datetime(2011, 3, 27, 2, 30, 0))
self.assertTrue(form.is_valid()) self.assertEqual(form.cleaned_data['dt'], datetime.datetime(2011, 10, 30, 2, 30, 0))
self.assertFalse(form.is_valid())
from __future__ import unicode_literals
verbose_name = '¿Chapter?'
class Color2(Color): class Meta: proxy = True
class Title(models.Model): pass
class FieldOverridePost(Post): class Meta: proxy = True
slug = models.SlugField(max_length=1000, db_index=False)
class ReferencedByParent(models.Model): name = models.CharField(max_length=20, unique=True)
class ReferencedByInline(models.Model): name = models.CharField(max_length=20, unique=True)
class Recipe(models.Model): rname = models.CharField(max_length=20, unique=True)
class NotReferenced(models.Model): pass
class ExplicitlyProvidedPK(models.Model): name = models.IntegerField(primary_key=True)
class ReferencedByGenRel(models.Model): content_type = models.ForeignKey(ContentType, on_delete=models.CASCADE) object_id = models.PositiveIntegerField() content_object = GenericForeignKey('content_type', 'object_id')
from __future__ import unicode_literals
"article_set-TOTAL_FORMS": "3", "article_set-INITIAL_FORMS": "0", "article_set-MAX_NUM_FORMS": "0",
self.assertEqual(Section.objects.latest('id').article_set.count(), 2)
response = self.client.get(reverse('admin:admin_views_language_changelist'), {'o': '-1'}) self.assertContentBefore(response, link1, link2)
self.assertContains(response, '<th scope="col"', count=5)
self.assertContentBefore(response, 'Name', 'Colored name')
self.assertContentBefore(response, link2, link1)
response = self.client.get(changelist_url, {'notarealfield__whatever': '5'}) self.assertRedirects(response, '%s?e=1' % changelist_url)
response = self.client.get(changelist_url, {'pub_date__gte': 'foo'}) self.assertRedirects(response, '%s?e=1' % changelist_url)
response = self.client.get("%s?age__gt=30" % reverse('admin:admin_views_person_changelist')) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('admin:admin_views_notreferenced_changelist'), {TO_FIELD_VAR: 'id'}) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('admin:admin_views_recipe_changelist'), {TO_FIELD_VAR: 'rname'}) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('admin:admin_views_ingredient_changelist'), {TO_FIELD_VAR: 'iname'}) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('admin:admin_views_referencedbyparent_changelist'), {TO_FIELD_VAR: 'name'}) self.assertEqual(response.status_code, 200)
response = self.client.get(reverse('admin:admin_views_referencedbyinline_changelist'), {TO_FIELD_VAR: 'name'}) self.assertEqual(response.status_code, 200)
url = "%s?leader__name=Palin&leader__age=27" % reverse('admin:admin_views_inquisition_changelist') response = self.client.get(url) self.assertEqual(response.status_code, 200)
self.assertContains(response, '<th class="field-__str__">UnchangeableObject object</th>', html=True)
response = self.client.get(reverse('admin:admin_views_customarticle_add')) self.assertTemplateUsed(response, 'custom_admin/add_form.html')
self.assertNotContains(response, 'value="test_value"') self.assertContains(response, 'value="overridden_value"')
opts = Article._meta
login = self.client.post(login_url, self.super_login) self.assertRedirects(login, self.index_url) self.assertFalse(login.context)
login = self.client.post(login_url, self.joepublic_login) self.assertEqual(login.status_code, 200) self.assertContains(login, ERROR_MESSAGE)
login = self.client.post(login_url, self.super_login) self.assertRedirects(login, self.index_url) self.assertFalse(login.context)
self.client.force_login(self.joepublicuser) self.client.force_login(self.superuser) self.assertEqual(self.client.session.test_cookie_worked(), False)
response = self.client.get(shortcut_url, follow=True) self.assertTemplateUsed(response, 'admin/login.html')
self.assertEqual(r.status_code, 200) self.client.get(reverse('admin:logout'))
data = { "form-TOTAL_FORMS": "3", "form-INITIAL_FORMS": "3", "form-MAX_NUM_FORMS": "0",
data = { "form-TOTAL_FORMS": "2", "form-INITIAL_FORMS": "2", "form-MAX_NUM_FORMS": "0",
data = { "form-TOTAL_FORMS": "1", "form-INITIAL_FORMS": "1", "form-MAX_NUM_FORMS": "0",
"form-1-id": str(fd2.id), "form-1-reference": "456", "form-1-driver": "bill", "form-1-restaurant": "thai",
"form-1-id": str(fd2.id), "form-1-reference": "456", "form-1-driver": "bill", "form-1-restaurant": "thai",
"form-2-id": str(fd3.id), "form-2-reference": "789", "form-2-driver": "bill", "form-2-restaurant": "thai",
data = { "form-TOTAL_FORMS": "1", "form-INITIAL_FORMS": "1", "form-MAX_NUM_FORMS": "0",
"_save": "Save",
data = { "form-TOTAL_FORMS": "1", "form-INITIAL_FORMS": "1", "form-MAX_NUM_FORMS": "0",
data = { "form-TOTAL_FORMS": "4", "form-INITIAL_FORMS": "4", "form-MAX_NUM_FORMS": "0",
"_save": "Save",
self.assertEqual(response.status_code, 302)
data = { "form-TOTAL_FORMS": "3", "form-INITIAL_FORMS": "3", "form-MAX_NUM_FORMS": "0",
data = { "form-TOTAL_FORMS": "3", "form-INITIAL_FORMS": "3", "form-MAX_NUM_FORMS": "0",
self.assertContains(response, "\n1 recommendation\n")
self.assertContains(response, "\n1 recommendation\n")
self.assertContains(response, "\n0 recommendations\n")
self.assertContains(response, "\n1 person\n") self.assertContains(response, "Guido")
self.assertContains(response, "\n0 persons\n") self.assertNotContains(response, "Guido")
self.assertContains(response, "\n1 pluggable search person\n") self.assertContains(response, "Bob")
self.assertContains(response, "\n1 pluggable search person\n") self.assertContains(response, "Amy")
self.assertEqual(len(names), len(set(names)))
self.assertEqual(response.status_code, 200) self.assertContains(response, "<li>Unchangeable object: UnchangeableObject object</li>", 1, html=True)
'action': ['external_mail', 'delete_selected'], 'index': 0
self.assertEqual(len(mail.outbox), 1) self.assertEqual(mail.outbox[0].subject, 'Greetings from a function action')
post_data = {"name": "First Gadget"} response = self.client.post(reverse('admin:admin_views_gadget_add'), post_data)
Person.objects.create(name='person1', gender=1) Person.objects.create(name='person2', gender=2) changelist_url = reverse('admin:admin_views_person_changelist')
response = self.client.get(collector_url) self.assertContains(response, 'name="widget_set-0-id"')
response = self.client.get(collector_url) self.assertContains(response, 'name="grommet_set-0-code"')
response = self.client.get(collector_url) self.assertContains(response, 'name="doohickey_set-0-code"')
response = self.client.get(collector_url) self.assertContains(response, 'name="whatsit_set-0-index"')
response = self.client.get(collector_url) self.assertContains(response, 'name="fancydoodad_set-0-doodad_ptr"')
self.post_data.update({ "name": "Frederick Clegg",
self.assertEqual(response.status_code, 302)
self.selenium.find_element_by_xpath('//input[@value="Save"]').click() self.wait_page_loaded()
self.assertContains(response, 'test<br /><br />test<br /><br />test<br /><br />test') self.assertContains(response, 'test<br />link')
pd.plot = None pd.save()
self.assertContains(response, threepwood.username, count=2) self.assertNotContains(response, marley.username)
m = re.search(br'<a href="([^"]*)"[^>]* id="lookup_id_inquisition"', response.content)
m = re.search(br'<a href="([^"]*)"[^>]* id="lookup_id_defendant0"', response.content)
m = re.search(br'<a href="([^"]*)"[^>]* id="lookup_id_defendant1"', response.content)
ContentType.objects.clear_cache()
ContentType.objects.clear_cache()
self.assertContains(response, '<td class="field-url">') self.assertContains(response, '<td class="field-posted">')
self.assertContains(response, "<h2>Built-in tags</h2>", count=2, html=True)
self.assertContains(response, '<h3 id="built_in-autoescape">autoescape</h3>', html=True) self.assertContains(response, '<li><a href="#built_in-autoescape">autoescape</a></li>', html=True)
self.assertContains(response, '<h3 id="flatpages-get_flatpages">get_flatpages</h3>', html=True) self.assertContains(response, '<li><a href="#flatpages-get_flatpages">get_flatpages</a></li>', html=True)
self.assertContains(response, "<h2>admin_list</h2>", count=2, html=True)
self.assertContains(response, '<h3 id="admin_list-admin_actions">admin_actions</h3>', html=True) self.assertContains(response, '<li><a href="#admin_list-admin_actions">admin_actions</a></li>', html=True)
self.assertContains(response, "<h2>Built-in filters</h2>", count=2, html=True)
self.assertContains(response, '<h3 id="built_in-add">add</h3>', html=True) self.assertContains(response, '<li><a href="#built_in-add">add</a></li>', html=True)
self.assertNotContains(response, 'release_date__day=') for date in DATES: self.assert_contains_month_link(response, date) self.assert_non_localized_year(response, 2000)
self.assertNotContains(response, 'release_date__day=') self.assertNotContains(response, 'release_date__month=') for date in DATES: self.assert_contains_year_link(response, date)
with self.assertRaises(AssertionError): self.assertURLEqual( 'http://testserver{}?_changelist_filters=is_staff__exact%3D0%26is_superuser__exact%3D0'.format( change_user_url ), 'http://testserver{}?_changelist_filters=is_staff__exact%3D1%26is_superuser__exact%3D1'.format( change_user_url ) )
self.assertURLEqual( 'http://testserver{}?_changelist_filters=is_staff__exact%3D0%26is_superuser__exact%3D0'.format( change_user_url ), '{}?_changelist_filters=is_staff__exact%3D0%26is_superuser__exact%3D0'.format(change_user_url) )
self.assertURLEqual( '{}?_changelist_filters=is_staff__exact%3D0%26is_superuser__exact%3D0'.format(change_user_url), '{}?_changelist_filters=is_superuser__exact%3D0%26is_staff__exact%3D0'.format(change_user_url) )
response = self.client.get(self.get_change_url()) self.assertEqual(response.status_code, 200)
response = self.client.get(self.get_add_url()) self.assertEqual(response.status_code, 200)
self.assertIn('some_required_info', response.context['adminform'].form.errors)
self.assertIn('some_required_info', response.context['adminform'].form.errors)
CityAdmin.view_on_site = True
self.assertEqual(len(apps), 2)
from __future__ import unicode_literals
return 'list-display-sketch'
return super(PersonAdmin, self).get_queryset(request).order_by('age')
return [ url(r'^extra/$', self.extra, name='cable_extra'), ]
urlpatterns = super(UnchangeableObjectAdmin, self).get_urls() return [p for p in urlpatterns if p.name and not p.name.endswith("_change")]
site.register(User, UserAdmin) site.register(Group, GroupAdmin)
def index(self, request, extra_context=None): return super(Admin2, self).index(request, {'foo': '*bar*'})
def get_queryset(self, request): qs = super(UserLimitedAdmin, self).get_queryset(request) return qs.filter(is_superuser=False)
Site.objects.clear_cache()
chan_elem = feed.getElementsByTagName('channel') self.assertEqual(len(chan_elem), 1) chan = chan_elem[0]
d = Entry.objects.latest('published').published last_build_date = rfc2822_date(timezone.make_aware(d, TZ))
self.assertChildNodeContent(chan, { 'title': 'My blog', 'link': 'http://example.com/blog/', })
self.assertEqual( chan.getElementsByTagName('atom:link')[0].getAttribute('href'), 'http://example.com/syndication/rss2/' )
d = Entry.objects.get(pk=1).published pub_date = rfc2822_date(timezone.make_aware(d, TZ))
self.assertIsNone(item.getElementsByTagName( 'guid')[0].attributes.get('isPermaLink'))
self.assertEqual( chan.getElementsByTagName('atom:link')[0].getAttribute('href'), 'http://example.com/syndication/rss091/' )
response = self.client.get('/syndication/atom/') feed = minidom.parseString(response.content).firstChild updated = feed.getElementsByTagName('updated')[0].firstChild.wholeText
response = self.client.get('/syndication/latest/') feed = minidom.parseString(response.content).firstChild updated = feed.getElementsByTagName('updated')[0].firstChild.wholeText
response = self.client.get('/syndication/naive-dates/') doc = minidom.parseString(response.content) updated = doc.getElementsByTagName('updated')[0].firstChild.wholeText
response = self.client.get('/syndication/no_pubdate/') self.assertFalse(response.has_header('Last-Modified'))
def item_title(self): return "Not in a template"
return item.published.replace(tzinfo=get_fixed_timezone(42))
thing = models.CharField(max_length=100, blank=True, choices=Things())
managed = False
managed = False
managed = False
management.call_command("check", stdout=six.StringIO())
date = models.DateField(db_column="event_date")
m.custom_method = custom_method m.list_display = ['id', 'name', 'parent', 'custom_method']
self.assertEqual(cl.result_count, 1)
self.assertEqual(cl.result_count, 1)
self.assertEqual(cl.result_count, 1)
self.assertEqual(cl.result_count, 1)
self.assertEqual(cl.result_count, 1)
Child.objects.create(parent=parent, name='Daniel') Child.objects.create(parent=parent, name='Daniel')
self.assertEqual(cl.queryset.count(), 1)
self.assertEqual(cl.queryset.count(), 1)
self.assertEqual(cl.queryset.count(), 1)
m = custom_site._registry[Child] request = self._mocked_authenticated_request('/child/', user_noparents) response = m.changelist_view(request) self.assertNotContains(response, 'Parent object')
m = DynamicListDisplayChildAdmin(Child, custom_site) request = self._mocked_authenticated_request('/child/', user_parents) response = m.changelist_view(request) self.assertContains(response, 'Parent object')
request = self.factory.get('/child/', data={ALL_VAR: ''})
check_results_order()
UnorderedObjectAdmin.ordering = ['bool'] check_results_order()
check_results_order(ascending=True)
OrderedObjectAdmin.ordering = ['bool'] check_results_order()
Group.objects.all().delete() for i in range(objects_count): Group.objects.create(name='test band')
cl.page_num = page_num cl.get_results(request) real_page_range = pagination(cl)['page_range']
self.assertNotIn('Add ', response.rendered_content)
self.assertEqual(template.render(context), '')
rows = self.selenium.find_elements_by_css_selector( '%s #result_list tbody tr' % form_id) self.assertEqual(len(rows), 1)
selection_indicator = self.selenium.find_element_by_css_selector( '%s .action-counter' % form_id) self.assertEqual(selection_indicator.text, "0 of 1 selected")
TestModel.objects.create(name='Test Object') self.i18n_model = I18nTestModel.objects.create(name='Test Object')
Site.objects.clear_cache()
from __future__ import unicode_literals
from __future__ import unicode_literals
with self.assertRaisesMessage(CommandError, "No fixture named 'unknown' found."): management.call_command("loaddata", "unknown.json", verbosity=0)
self.assertTrue(apps.ready) self.assertTrue(Apps().ready)
self.assertEqual(apps.get_model('admin', 'loGentrY'), LogEntry) with self.assertRaises(LookupError): apps.get_model('Admin', 'LogEntry')
self.assertSetEqual(set(apps._pending_operations) - initial_pending, {('apps', 'lazyb')})
apps.lazy_model_operation(test_func, ('apps', 'lazyb'))
self.assertSetEqual(set(apps._pending_operations) - initial_pending, {('apps', 'lazyc')})
self.assertListEqual(model_classes, [LazyA, LazyB, LazyB, LazyC, LazyA])
with extend_sys_path(self.base_location, self.other_location): with self.assertRaises(ImproperlyConfigured): with self.settings(INSTALLED_APPS=['nsapp']): pass
id2 = models.AutoField(primary_key=True) name2 = models.CharField(max_length=50)
self.assertIsNone(p2.image)
parent = models.OneToOneField(Place, models.CASCADE, primary_key=True, parent_link=True) main_site = models.ForeignKey(Place, models.CASCADE, related_name='lot')
self.assertEqual(Student._meta.ordering, [])
with self.assertRaises(AttributeError): CommonInfo.objects.all()
self.assertQuerysetEqual(Place.objects.filter(supplier__name="foo"), []) with self.assertRaises(FieldError): Restaurant.objects.filter(supplier__name="foo")
with self.assertRaises(AttributeError): getattr(post, "attached_%(class)s_set")
msg = "Cannot resolve keyword 'attached_comment_set' into field." with self.assertRaisesMessage(FieldError, msg): Post.objects.filter(attached_comment_set__is_spam=True)
self.assertEqual(s.titles.related_val, (s.id,)) self.assertQuerysetEqual(s.titles.all(), [])
self.assertNotEqual(Place(id=1), Restaurant(id=1)) self.assertNotEqual(Restaurant(id=1), Place(id=1))
p = Place.objects.get(name="Demon Dogs") self.assertIs(type(p), Place)
with self.assertRaises(ItalianRestaurant.DoesNotExist): Place.objects.get(name="Demon Dogs").restaurant.italianrestaurant
with self.assertRaises(Place.DoesNotExist): ItalianRestaurant.objects.get(name="The Noodle Void")
with self.assertRaises(Place.MultipleObjectsReturned): Restaurant.objects.get()
p = Place.objects.get(name="Joe's Chickens") with self.assertRaises(Restaurant.DoesNotExist): p.restaurant
with self.assertNumQueries(2): objs = list(qs.all()) self.assertTrue(objs[1].italianrestaurant.serves_gnocchi)
messages = _log_level_code(level, status_code) self.assertIn('GET A %d' % status_code, messages[0])
for wrong_level in level_status_codes.keys(): if wrong_level != level: messages = _log_level_code(wrong_level, status_code) self.assertEqual(len(messages), 0)
class UnclosableBytesIO(BytesIO): def close(self): pass
with patch_logger('django.server', 'info'): WSGIRequestHandler(request, '192.168.0.2', server)
address_predefined = 'DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ old_address = os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS')
cls.raises_exception('localhost', ImproperlyConfigured)
cls.raises_exception('blahblahblah:8081', socket.error)
if address_predefined: os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = old_address else: del os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS']
cls.live_server_url_test = [cls.live_server_url]
pass
return
self.assertNotEqual( self.live_server_url, TestCase.live_server_url, "Acquired duplicate server addresses for server threads: %s" % self.live_server_url )
class DecimalLessThanOne(models.Model): d = models.DecimalField(max_digits=3, decimal_places=3)
class FieldClassAttributeModel(models.Model): field_class = models.CharField
temp_storage_dir = tempfile.mkdtemp() temp_storage = FileSystemStorage(temp_storage_dir) temp_upload_to_dir = os.path.join(temp_storage.location, 'tests')
from __future__ import unicode_literals
b5 = BooleanModel.objects.all().extra(select={'string_col': 'string'})[0] self.assertNotIsInstance(b5.pk, bool)
self.assertEqual(1, true_cardinality_flags)
self.assertEqual(MANY_TO_MANY_CLASSES, {f.__class__ for f in m2m_type_fields})
for field in m2m_type_fields: reverse_field = field.remote_field self.assertTrue(reverse_field.is_relation) self.assertTrue(reverse_field.many_to_many) self.assertTrue(reverse_field.related_model)
self.assertEqual(ONE_TO_MANY_CLASSES, {f.__class__ for f in o2m_type_fields})
for field in o2m_type_fields: if field.concrete: reverse_field = field.remote_field self.assertTrue(reverse_field.is_relation and reverse_field.many_to_one)
self.assertEqual(MANY_TO_ONE_CLASSES, {f.__class__ for f in m2o_type_fields})
for obj in m2o_type_fields: if hasattr(obj, 'field'): reverse_field = obj.field self.assertTrue(reverse_field.is_relation and reverse_field.one_to_many)
self.assertEqual(ONE_TO_ONE_CLASSES, {f.__class__ for f in o2o_type_fields})
for obj in o2o_type_fields: if hasattr(obj, 'field'): reverse_field = obj.field self.assertTrue(reverse_field.is_relation and reverse_field.one_to_one)
self.assertFalse(AllFieldsModel._meta.get_field('m2m').null) self.assertTrue(AllFieldsModel._meta.get_field('reverse2').null)
class Person(): pass PersonWithHeight = PersonWithHeightAndWidth = PersonDimensionsFirst = Person PersonTwoImages = Person
PersonModel = PersonWithHeightAndWidth File = ImageFile
p = self.PersonModel.objects.get(name="Joan") self.assertEqual(p.mugshot.closed, True)
p.mugshot.size self.assertEqual(p.mugshot.closed, True)
self.assertIsInstance(p.mugshot, TestImageFieldFile) self.assertEqual(bool(p.mugshot), False)
p = self.PersonModel(name='Joe') p.mugshot = self.file1 self.check_dimensions(p, 4, 8)
p.mugshot = None self.check_dimensions(p, None, None)
p.mugshot.save("mug", self.file2) self.check_dimensions(p, 8, 4)
p.mugshot.delete(save=False) self.assertEqual(p.mugshot, None) self.check_dimensions(p, None, None)
p.mugshot.save("mug", self.file1) self.check_dimensions(p, 4, 8)
p.mugshot = self.file2 self.check_dimensions(p, 8, 4) self.assertEqual(p.mugshot.was_opened, True)
UUIDGrandchild().save()
available_apps = ['model_fields']
Foo.objects.filter(d__gte=100000000000)
DateTimeModel.objects.create(d=d, dt=dt2, t=t) self.assertEqual(m, DateTimeModel.objects.get(dt__date=d))
from __future__ import unicode_literals
class Rock(Mineral): tags = GenericRelation(TaggedItem)
class AllowsNullGFK(models.Model): content_type = models.ForeignKey(ContentType, models.SET_NULL, null=True) object_id = models.PositiveIntegerField(null=True) content_object = GenericForeignKey()
self.comp_func = lambda obj: ( obj.tag, obj.content_type.model_class(), obj.object_id )
self.assertEqual(tag.tag, 'stinky')
TaggedItem.objects.create(content_object=self.quartz, tag="shiny") TaggedItem.objects.create(content_object=self.quartz, tag="clearish")
with self.assertNumQueries(1): bacon.tags.add(t1, t2) self.assertEqual(t1.content_object, bacon) self.assertEqual(t2.content_object, bacon)
with self.assertRaises(FieldError): TaggedItem.objects.filter(vegetable__isnull=True)
tiger = Animal.objects.create(common_name="tiger") cheetah = Animal.objects.create(common_name="cheetah") bear = Animal.objects.create(common_name="bear")
Comparison.objects.create( first_obj=cheetah, other_obj=tiger, comparative="faster" ) Comparison.objects.create( first_obj=tiger, other_obj=cheetah, comparative="cooler" )
cheetah.delete() self.assertQuerysetEqual(Comparison.objects.all(), [ "<Comparison: tiger is stronger than None>" ])
tailless = Gecko.objects.create(has_tail=False) tag = TaggedItem.objects.create(content_object=tailless, tag="lizard") self.assertEqual(tag.content_object, tailless)
AllowsNullGFK(content_object=None) TaggedItem(content_object=None)
third = models.ForeignKey(OuterB, models.SET_NULL, null=True)
self.assertQuerysetEqual(Choice.objects.filter(choice__exact=None), [])
self.assertQuerysetEqual(Choice.objects.filter(choice__iexact=None), [])
with self.assertRaises(FieldError): Choice.objects.filter(foo__exact=None)
with self.assertRaises(ValueError): Choice.objects.filter(id__gt=None)
p2 = Poll(question="How?") self.assertEqual(repr(p2.choice_set.all()), '<QuerySet []>')
Restaurant.objects.bulk_create([ Restaurant(name='foo') for i in range(0, 2) ]) self.assertEqual(Restaurant.objects.count(), 2)
Restaurant.objects.bulk_create([ Restaurant() for i in range(0, 501) ])
self.lock = threading.Lock()
ordering = ('author__extra__note', 'author__name', 'rank')
assert False, 'type checking should happen without calling model __iter__'
class Individual(models.Model): alive = models.BooleanField()
cls.e2 = ExtraInfo.objects.create(info='e2', note=n2, value=41) e1 = ExtraInfo.objects.create(info='e1', note=cls.n1, value=42)
cls.rank1 = Ranking.objects.create(rank=2, author=cls.a2)
query = Item.objects.filter(tags=self.t2).query self.assertNotIn(LOUTER, [x.join_type for x in query.alias_map.values()])
query = Item.objects.exclude(creator__in=[self.a1, self.a2]).query self.assertNotIn(LOUTER, [x.join_type for x in query.alias_map.values()])
self.assertQuerysetEqual( Cover.objects.all(), ['<Cover: first>', '<Cover: second>'] )
self.assertTrue(repr(qs[0].note), '<Note: n2>') self.assertEqual(repr(qs[0].creator.extra.note), '<Note: n1>')
self.assertIn('note_id', ExtraInfo.objects.values()[0])
self.assertValueQuerysetEqual( ExtraInfo.objects.values('note_id'), [{'note_id': 1}, {'note_id': 2}] )
self.assertValueQuerysetEqual( ExtraInfo.objects.values('note'), [{'note': 1}, {'note': 2}] )
self.assertQuerysetEqual( Item.objects.datetimes('modified', 'day'), ['datetime.datetime(2007, 12, 19, 0, 0)'] )
pickle.dumps(Item.objects.all())
self.assertQuerysetEqual( Item.objects.filter(created__in=[self.time1, self.time2]), ['<Item: one>', '<Item: two>'] )
self.assertEqual( len(Note.objects.order_by('extrainfo__info').distinct()), 3 )
qs = Item.objects.datetimes('created', 'month') pickle.loads(pickle.dumps(qs))
self.assertEqual(Tag.objects.filter(name__in=()).update(name="foo"), 0)
def f(): return iter([]) n_obj = Note.objects.all()[0]
self.assertIsNone(subq._result_cache)
self.assertIsNone(subq._result_cache)
self.assertIsNone(subq._result_cache)
self.assertEqual(len(Tag.objects.order_by('parent__name')), 5)
q = Tag.objects.filter(parent__isnull=True)
q = Tag.objects.filter(parent__parent__isnull=False)
q1 = Tag.objects.filter(parent__isnull=True) q2 = Tag.objects.filter(parent__isnull=False)
qs = Author.objects.annotate(Count('item')) qs = qs.filter(~Q(extra__value=0))
count = Number.objects.count() qs = Number.objects.all()
self.assertQuerysetEqual(Valid.objects.all(), [])
with self.assertRaisesMessage(AssertionError, "'name' isn't a DateField or DateTimeField."): Item.objects.datetimes('name', 'month')
ManagedModel.objects.create(data='mm1', tag=self.t1, public=True) self.assertEqual(ManagedModel.objects.update(data='mm'), 1)
self.assertQuerysetEqual( Tag.objects.filter(id__in=Tag.objects.filter(id__in=[])), [] )
self.assertQuerysetEqual(X.objects.all(), []) self.assertQuerysetEqual(X.objects.select_related(), [])
self.assertEqual(Celebrity.objects.count(), num_celebs)
Plaything.objects.create(name="p1") self.assertQuerysetEqual( Plaything.objects.all(), ['<Plaything: p1>'] )
self.assertQuerysetEqual( Tag.objects.exclude(parent__annotation__name="a1"), ['<Tag: t1>', '<Tag: t4>', '<Tag: t5>'] )
self.assertQuerysetEqual( Annotation.objects.exclude(tag__children__name="t2"), ['<Annotation: a2>'] )
self.assertQuerysetEqual( Annotation.objects.filter(notes__in=Note.objects.filter(note="n1")), ['<Annotation: a1>'] )
q1 = Tag.objects.order_by('name') self.assertIsNot(q1, q1.all())
self.assertQuerysetEqual( Note.objects.filter(pk__in=(x for x in ())), [] )
list(n_list) self.assertEqual(ExtraInfo.objects.filter(note__in=n_list)[0].info, 'good')
self.assertQuerysetEqual( Number.objects.none().values('num').order_by('num'), [] )
q = Author.objects.none() self.assertQuerysetEqual(q.values(), []) self.assertQuerysetEqual(q.values_list(), [])
self.assertQuerysetEqual( self.get_ordered_articles()[5:], ["<Article: Article 6>", "<Article: Article 7>"] )
self.assertNumQueries(0, lambda: list(Number.objects.all()[1:1]))
with self.assertRaisesMessage(FieldError, 'Infinite loop caused by ordering.'):
self.assertEqual(len(Tag.objects.order_by('parent')), 5)
self.assertQuerysetEqual( LoopX.objects.all().order_by('y__x__y__x__id'), [] )
cls.o1 = Order.objects.create(pk=1) cls.o2 = Order.objects.create(pk=2) cls.o3 = Order.objects.create(pk=3)
self.assertIs(inner_qs._result_cache, None)
qs = Article.objects.order_by('invalid_column') with self.assertRaises(FieldError): list(qs) with self.assertRaises(FieldError): list(qs)
qs = qs.filter(a__f1='foo') self.assertEqual(str(qs.query).count('INNER JOIN'), 1)
self.assertEqual(list(children), [my2]) self.assertEqual(list(parents), [my1])
self.assertQuerysetEqual( Order.objects.filter(items__in=OrderItem.objects.values_list('status')), [o1.pk], lambda x: x.pk)
with self.assertRaisesMessage(ValueError, self.error % (self.wrong_type, ObjectA._meta.object_name)): ObjectB.objects.get(objecta=self.wrong_type)
with self.assertRaisesMessage(ValueError, self.error % (self.ob, ObjectA._meta.object_name)): ObjectB.objects.filter(objecta__in=[self.poa, self.ob])
self.assertQuerysetEqual(ObjectC.objects.exclude(childobjecta=self.oa), out_c)
with self.assertNumQueries(0): ObjectB.objects.filter(objecta__in=ObjectA.objects.all())
qs = Node.objects.values('parent__parent__id') self.assertIn(' LEFT OUTER JOIN ', str(qs.query))
for k in kwargs: assert k in [f.attname for f in self._meta.fields], \ "Author.__init__ got an unexpected parameter: %s" % k
self.assertNotEqual(author.last_name, None)
first_iterations = 0 for index, raw_author in enumerate(raw_authors): self.assertEqual(normal_authors[index], raw_author) first_iterations += 1
second_iterations = 0 for index, raw_author in enumerate(raw_authors): self.assertEqual(normal_authors[index], raw_author) second_iterations += 1
query = "SELECT * FROM raw_query_author ORDER BY id ASC" third_author = Author.objects.raw(query)[2] self.assertEqual(third_author.first_name, 'Bob')
with self.assertRaises(Article.DoesNotExist): Article.objects.earliest()
self.assertEqual(Article.objects.order_by('id').earliest(), a1)
with self.assertRaises(Article.DoesNotExist): Article.objects.latest()
self.assertEqual(Article.objects.order_by('id').latest(), a4)
IndexErrorArticle.objects.create( headline="Article 1", pub_date=datetime(2005, 7, 26), expire_date=datetime(2005, 9, 1) ) check()
data = BytesIO(data) for chunk in iter(lambda: data.read(MAX_SOCKET_CHUNK_SIZE), b''): self._write(chunk) self._flush()
return [b'x' * (MAX_SOCKET_CHUNK_SIZE + MAX_SOCKET_CHUNK_SIZE // 2)]
class SimpleTestCaseSubclass(SimpleTestCase): pass
self.assertEqual('override', settings.TEST) with self.assertRaises(AttributeError): getattr(settings, 'TEST2')
lazy_settings.APPEND_SLASH self.assertEqual(repr(lazy_settings), expected)
from __future__ import unicode_literals
return six.text_type(value)
from __future__ import unicode_literals
with self.assertRaises(SerializerDoesNotExist) as cm: serializers.get_serializer("nonsense") self.assertEqual(cm.exception.args, ("nonsense",))
for stream in (StringIO(), HttpResponse()): serializers.serialize(self.serializer_name, [obj], indent=2, stream=stream)
string_data = serializers.serialize(self.serializer_name, [obj], indent=2)
if isinstance(stream, StringIO): self.assertEqual(string_data, stream.getvalue()) else: self.assertEqual(string_data, stream.content.decode('utf-8'))
self.assertTrue(Article.objects.filter(headline=old_headline)) self.assertFalse(Article.objects.filter(headline=new_headline))
self.assertTrue(Article.objects.filter(headline=new_headline)) self.assertFalse(Article.objects.filter(headline=old_headline))
self.assertEqual(deserial_objs[0].object.__class__, Author)
a = Article.objects.create( author=self.jane, headline="Nobody remembers the early years", pub_date=datetime(1, 2, 3, 4, 5, 6))
with transaction.atomic(): objs = serializers.deserialize(self.serializer_name, self.fwd_ref_str) with connection.constraint_checks_disabled(): for obj in objs: obj.save()
adrian = NaturalKeyAnchor.objects.create(**book1) james = NaturalKeyAnchor.objects.create(**book2)
string_data = serializers.serialize( format, NaturalKeyAnchor.objects.all(), indent=2, use_natural_foreign_keys=True, use_natural_primary_keys=True, )
james.delete()
register_tests(NaturalKeySerializerTests, 'test_%s_natural_key_serializer', natural_key_serializer_test) register_tests(NaturalKeySerializerTests, 'test_%s_serializer_natural_keys', natural_key_test)
if connection.features.interprets_empty_strings_as_nulls: test_data = [data for data in test_data if not (data[0] == data_obj and data[2]._meta.get_field('data').empty_strings_allowed and data[3] is None)]
if connection.features.allows_auto_pk_0: test_data.extend([ (data_obj, 0, Anchor, "Anchor 0"), (fk_obj, 465, FKData, 0), ])
objects = [] instance_count = {} for (func, pk, klass, datum) in test_data: with connection.constraint_checks_disabled(): objects.extend(func[0](pk, klass, datum))
for klass in instance_count: instance_count[klass] = klass.objects.count()
objects.extend(Tag.objects.all())
serialized_data = serializers.serialize(format, objects, indent=2)
for (func, pk, klass, datum) in test_data: func[1](self, pk, klass, datum)
for klass, count in instance_count.items(): self.assertEqual(count, klass.objects.count())
data = models.OneToOneField(Anchor, models.CASCADE, primary_key=True)
from __future__ import unicode_literals
serializers._serializers = {}
serializers._serializers = {}
if isinstance(field_value, six.string_types): ret_list.append(field_value) else: ret_list.append(str(field_value))
from __future__ import unicode_literals
from django.db import models from django.utils.encoding import python_2_unicode_compatible
self.assertEqual(formset.is_valid(), False) self.assertEqual(Poem.objects.count(), 0)
self.assertEqual(formset.is_valid(), False) self.assertEqual(Poem.objects.count(), 1)
inlineformset_factory( Parent, Child, exclude=['school'], fk_name='mother' )
from __future__ import unicode_literals
from __future__ import unicode_literals
self.assertEqual(Employee.objects.get(employee_code=123), self.dan)
e = Employee.objects.get(pk=123) self.assertEqual(e.pk, 123) self.assertEqual(e.employee_code, 123)
Business.objects.create(name='jaźń')
new_bar = Bar.objects.create() new_foo = Foo.objects.create(bar=new_bar)
@python_2_unicode_compatible class Thing(models.Model): when = models.CharField(max_length=1, primary_key=True)
self.local_models = []
def get_default(self): return self.default
columns = self.column_classes(Note) self.assertFalse(columns['info'][1][6])
columns = self.column_classes(Note) self.assertFalse(columns['info'][1][6])
columns = self.column_classes(Note) self.assertFalse(columns['info'][1][6])
Author.objects.create(name='Foo') Author.objects.create(name='Bar')
old_field = IntegerPK._meta.get_field('j') new_field = IntegerField(primary_key=True) new_field.model = IntegerPK new_field.set_attributes_from_name('j')
IntegerUnique.objects.create(i=1, j=1) with self.assertRaises(IntegrityError): IntegerUnique.objects.create(i=1, j=2)
with connection.schema_editor() as editor: editor.alter_field(LocalAuthorWithM2M, new_field, new_field)
with connection.schema_editor() as editor: editor.remove_field(LocalAuthorWithM2M, new_field) with self.assertRaises(DatabaseError): self.column_classes(new_field.remote_field.through)
opts = LocalAuthorWithM2M._meta opts.local_many_to_many.remove(new_field) del new_apps.all_models['schema'][new_field.remote_field.through._meta.model_name] opts._expire_cache()
with connection.schema_editor() as editor: editor.create_model(AuthorWithEvenLongerName) editor.create_model(BookWithLongName) column_name = connection.ops.quote_name("author_foreign_key_with_really_long_field_name_id")
self.assertIn( column_name, self.get_indexes(BookWithLongName._meta.db_table), )
contribute_to_related_class = ManyToManyField.__dict__['contribute_to_related_class'] _get_m2m_attr = ManyToManyField.__dict__['_get_m2m_attr'] _get_m2m_reverse_attr = ManyToManyField.__dict__['_get_m2m_reverse_attr'] _get_m2m_db_table = ManyToManyField.__dict__['_get_m2m_db_table']
return self.headline
from __future__ import unicode_literals
self.assertEqual(str(a), b'Girl wins \xe2\x82\xac12.500 in lottery')
from __future__ import unicode_literals
(validate_integer, '42', None), (validate_integer, '-42', None), (validate_integer, -42, None),
try: validator(value) except expected: pass else: self.fail("%s not raised when validating '%s'" % ( expected.__name__, value))
Permission.objects.filter(content_type__app_label='swappable_models').delete() ContentType.objects.filter(app_label='swappable_models').delete()
new_io = StringIO() management.call_command('migrate', interactive=False, stdout=new_io)
self._push_count(start_pos)
while len(func_lines) > 0 and func_lines[-1] == '': func_lines = func_lines[:-1]
clean_lines = [] in_docstring = False for line in func_lines: line = line.strip() if in_docstring and _is_triplequote(line): in_docstring = False continue
results = self.proxy.search({ 'name': self.package_name, 'description': self.package_name }, 'or')[:15]
matches = [] for match in results: name = match['name'] if name not in matches: matches.append(name)
if len(matches) == 1: self.package_name = matches[0] return self.releases
seen.add(cand.__name__) q.insert(0,cand)
collect = [e for e in collect if e.undoc_names and len(e.undoc_names) != e.nsig_names] collect.sort(key=lambda x:x.path)
res[b][a] = group.sum()
files = [] state = dict(files=files) os.path.walk('.',file_filter,state)
env = os.environ del env['PYTHONPATH']
remote_files = set(srv.listdir(path='.'))
file_handler = logging.FileHandler("C:\Builds\logs\check_and_build.log") file_handler.setFormatter(logging.Formatter(fmt)) logger.addHandler(file_handler)
while(True):
return result_index, result
return result_index, result
elapsed += time.clock() - _s gc.enable() result[kind] = (elapsed / iterations) * 1000
return [thing.get(x,default) for x in fields]
pass
from pandas import *
import os
if BC_DEBUG: print("2to3 cache miss (will process) %s,%s" % (f,h)) to_process[h] = f
if "Generated by Cython" not in first_line.decode('utf-8'): hash.update(first_line) hash.update(f.read()) return hash.hexdigest()
setuptools_kwargs['use_2to3'] = True if BUILD_CACHE_DIR is None else False
setuptools_args['use_2to3'] = True if BUILD_CACHE_DIR is None else False
store = HDFStore('bugzilla.h5', mode='w')
PR_REMOTE_NAME = os.environ.get("PR_REMOTE_NAME", "upstream")
PUSH_REMOTE_NAME = os.environ.get("PUSH_REMOTE_NAME", "upstream")
BRANCH_PREFIX = "PR_TOOL"
if isinstance(cmd, six.string_types): cmd = cmd.split(' ')
merge_message_flags += [ "-m", "Closes #%s from %s and squashes the following commits:" % (pr_num, pr_repo_desc)] for c in commits: merge_message_flags += ["-m", c]
classes = [pd.Series, pd.DataFrame, pd.Panel, pd.Panel4D]
return ' ' + x
class_members = set() for cls in classes: class_members.update([cls.__name__ + '.' + x[0] for x in inspect.getmembers(cls)])
try: v1 = arg_val_dict[key] v2 = compat_args[key]
if (v1 is not None and v2 is None) or \ (v1 is None and v2 is not None): match = False else: match = (v1 == v2)
except: match = (arg_val_dict[key] is compat_args[key])
kwargs = dict(zip(compat_args, args)) _check_for_default_values(fname, kwargs, compat_args)
diff = set(kwargs) - set(compat_args)
_check_arg_length(fname, args + tuple(kwargs.values()), max_fname_arg_count, compat_args)
args_dict = dict(zip(compat_args, args))
if 'cygwin' in platform.system().lower():
xclipExists = call(['which', 'xclip'], stdout=PIPE, stderr=PIPE) == 0
import gtk gtkInstalled = True
qtBindingInstalled = True try: from PyQt4 import QtGui except ImportError: try: from PySide import QtGui except ImportError: qtBindingInstalled = False
if xclipExists:
clipboard_get = paste clipboard_set = copy
if not pkg_name: return "pandas"
pkg_name.reverse()
if pkg_name[0].endswith('.egg'): pkg_name.pop(0)
verbose = min(verbose, 3)
import doctest doctest.master = None
from pandas import __version__ from distutils.version import StrictVersion try: StrictVersion(__version__) raise_warnings = 'release' except ValueError: raise_warnings = 'develop'
_testing_mode_warnings = (DeprecationWarning, compat.ResourceWarning)
testing_mode = os.environ.get('PANDAS_TESTING_MODE', 'None') if 'deprecate' in testing_mode:
testing_mode = os.environ.get('PANDAS_TESTING_MODE', 'None') if 'deprecate' in testing_mode: warnings.simplefilter('ignore', _testing_mode_warnings)
pd.reset_option('^display.', silent=True)
def assertEquals(self, *args, **kwargs): return deprecate('assertEquals', self.assertEqual)(*args, **kwargs)
if len(os.path.dirname(filename)): raise ValueError("Can't pass a qualified name to ensure_clean()")
assertIsInstance(left, Index, '[index] ') assertIsInstance(right, Index, '[index] ')
_check_types(left, right, obj=obj)
if left.nlevels > 1: for level in range(left.nlevels): llevel = _get_ilevel_values(left, level) rlevel = _get_ilevel_values(right, level)
_check_types(left.levels[level], right.levels[level], obj=obj)
return x
return True
result = False
return assert_numpy_array_equal(seq, np.sort(np.array(seq)))
assert_class_equal(left, right, obj=obj) assertIsInstance(left, np.ndarray, '[ndarray] ') assertIsInstance(right, np.ndarray, '[ndarray] ')
if not array_equivalent(l, r, strict_nan=strict_nan): diff += 1
if not array_equivalent(left, right, strict_nan=strict_nan): _raise(left, right, err_msg)
def assert_series_equal(left, right, check_dtype=True, check_index_type='equiv', check_series_type=True, check_less_precise=False, check_names=True, check_exact=False, check_datetimelike_compat=False, check_categorical=True, obj='Series'):
assertIsInstance(left, Series, '[Series] ') assertIsInstance(right, Series, '[Series] ')
assert_index_equal(left.index, right.index, exact=check_index_type, check_names=check_names, check_less_precise=check_less_precise, check_exact=check_exact, check_categorical=check_categorical, obj='{0}.index'.format(obj))
if (is_datetimelike_v_numeric(left, right) or is_datetimelike_v_object(left, right) or needs_i8_conversion(left) or needs_i8_conversion(right)):
if check_names: assert_attr_equal('name', left, right, obj=obj)
assertIsInstance(left, DataFrame, '[DataFrame] ') assertIsInstance(right, DataFrame, '[DataFrame] ')
assert_index_equal(left.index, right.index, exact=check_index_type, check_names=check_names, check_less_precise=check_less_precise, check_exact=check_exact, check_categorical=check_categorical, obj='{0}.index'.format(obj))
assert_index_equal(left.columns, right.columns, exact=check_column_type, check_names=check_names, check_less_precise=check_less_precise, check_exact=check_exact, check_categorical=check_categorical, obj='{0}.columns'.format(obj))
assertIsInstance(left.sp_index, pd._sparse.SparseIndex, '[SparseIndex]') assertIsInstance(right.sp_index, pd._sparse.SparseIndex, '[SparseIndex]')
assert_sp_frame_equal(frame, right[item], exact_indices=exact_indices)
def makeStringIndex(k=10, name=None): return Index(rands_array(nchars=10, size=k), name=name)
def makeFloatSeries(name=None): index = makeStringIndex(N) return Series(randn(N), index=index, name=name)
def makeTimeDataFrame(nper=None, freq='B'): data = getTimeSeriesData(nper, freq) return DataFrame(data)
names = [prefix + str(i) for i in range(nlevels)]
names = None
if isinstance(names, compat.string_types) and nlevels == 1: names = [names]
if nentries == 1: index = Index(tuples[0], name=names[0]) else: index = MultiIndex.from_tuples(tuples, names=names) return index
>> makeCustomDataframe(5,3)
>> mkdf(5,3,data_gen_f=lambda r,c:randint(1,100))
>> a=makeCustomDataframe(5,3,r_idx_nlevels=2,r_ndupe_l=[2])
>> a=makeCustomDataframe(5,3,c_idx_names=False,r_idx_names=False, r_idx_type="dt",c_idx_type="u")
>> a=makeCustomDataframe(5,3,r_idx_nlevels=4, r_idx_names=["FEE","FI","FO","FAM"], c_idx_nlevels=2)
if data_gen_f is None: data_gen_f = lambda r, c: "R%dC%d" % (r, c)
size = int(np.round((1 - density) * nrows * ncols)) min_rows = 5 fac = 1.02 extra_size = min(size + min_rows, fac * size)
_network_errno_vals = (
_network_error_classes = (IOError, httplib.HTTPException)
attrs[k] = v
if _callable is not None: with manager: _callable(*args, **kwargs) else: return manager
return False
if not is_list_like(clear): clear = [clear] for m in clear: try: m.__warningregistry__.clear() except: pass
if current_os == 'Linux' or \ current_os == 'Darwin' or \ current_os.startswith('CYGWIN'): tuple_xy = _get_terminal_size_linux() if tuple_xy is None:
figsize = self.cell_width * hcells, self.cell_height * vcells
df = self._insert_index(df) tb = plotting.table(ax, df, loc=9) tb.set_fontsize(self.font_size)
if callable(fail_condition): fail_val = fail_condition else: fail_val = lambda: fail_condition
import nose
if values_passed and not values_multi and not table.empty: table = table[values[0]]
if hasattr(table, 'columns'): for level in table.columns.names[1:]: if margins_name in table.columns.get_level_values(level): raise ValueError(exception_msg)
return table.append(Series({key: grand_margin[margins_name]}))
for k in margin_keys: if isinstance(k, compat.string_types): row_margin[k] = grand_margin[k] else: row_margin[k] = grand_margin[k[0]]
result.index = result.index._to_safe_for_reshape() result = result.append(margin_dummy)
table_pieces = [] margin_keys = []
piece = piece.copy() try: piece[all_key] = margin[key] except TypeError:
piece.set_axis(cat_axis, piece._get_axis( cat_axis)._to_safe_for_reshape()) piece[all_key] = margin[key]
new_order = [len(cols)] + lrange(len(cols)) row_margin.index = row_margin.index.reorder_levels(new_order)
margin_keys = []
col_0 d e f row_0 a 1 0 0 b 0 1 0 c 0 0 0
if normalize is not False: table = _normalize(table, normalize=normalize, margins=margins)
table = _normalize(table, normalize=normalize, margins=False)
if normalize == 'columns': column_margin = column_margin / column_margin.sum() table = concat([table, column_margin], axis=1) table = table.fillna(0)
return Index(values, name=arg.name)
from __future__ import division
return (matplotlib.__version__ >= LooseVersion('1.3.1') or matplotlib.__version__[0] == '0')
import cycler colors = mpl_stylesheet.pop('axes.color_cycle') mpl_stylesheet['axes.prop_cycle'] = cycler.cycler('color', colors)
_ALIASES = {'x_compat': 'xaxis.compat'} _DEFAULT_KEYS = ['xaxis.compat']
fig.subplots_adjust(wspace=0, hspace=0)
kwds.setdefault('c', plt.rcParams['patch.facecolor'])
if diagonal == 'hist': ax.hist(values, **hist_kwds)
locs = locs.astype(int)
coeffs = np.delete(np.copy(amplitudes), 0) coeffs.resize(int((coeffs.size + 1) / 2), 2)
harmonics = np.arange(0, coeffs.shape[0]) + 1 trig_args = np.outer(harmonics, t)
data = list(series.values) samplings = [random.sample(data, size) for _ in range(samples)]
kwds.setdefault('c', plt.rcParams['patch.facecolor'])
self.sharex = False
self._rot_set = True
self.kwds['color'] = [self.kwds['color']]
return self._get_ax_layer(ax)
return ax.right_ax
return ax
return self._get_ax_layer(self.axes[0], primary=False)
if is_empty: raise TypeError('Empty {0!r}: no numeric data to ' 'plot'.format(numeric_data.__class__.__name__))
if style is not None: args = (x, y, style) else: args = (x, y) return ax.plot(*args, **kwds)
if isinstance(err, DataFrame):
elif isinstance(err, dict): pass
err = np.atleast_2d(err)
if len(err) == 1: err = np.tile(err, (self.nseries, 1))
points = ax.get_position().get_points() x_set.add(points[0][0]) y_set.add(points[0][1])
s = 20
cb = self.kwds.pop('colorbar', self.colormap or c_is_column)
cmap = self.colormap or 'BuGn' cmap = self.plt.cm.get_cmap(cmap) cb = self.kwds.pop('colorbar', True)
return not self.x_compat and self.use_index and self._use_dynamic_x()
freq, data = _maybe_resample(data, ax, kwds)
format_dateaxis(ax, ax.freq) return lines
cls._initialize_stacker(ax, stacking_id, len(values))
if not self._rot_set: self.rot = 30 format_date_labels(ax, rot=self.rot)
self.kwds.setdefault('alpha', 0.5)
xdata, y_values = lines[0].get_data(orig=False)
MPLPlot.__init__(self, data, **kwargs)
values = (self.data._convert(datetime=True)._get_numeric_data()) values = np.ravel(values) values = values[~com.isnull(values)]
n, bins, patches = ax.hist(y, bins=bins, bottom=bottom, **kwds) cls._update_stacker(ax, stacking_id, n) return patches
kwds['bottom'] = self.bottom kwds['bins'] = self.bins return kwds
leglabels = labels if labels is not None else idx for p, l in zip(patches, leglabels): self._add_legend_handle(p, l)
BP = namedtuple("Boxplot", ['ax', 'lines'])
if return_type not in self._valid_return_types: raise ValueError( "return_type must be {None, 'axes', 'dict', 'both'}")
if self.orientation == 'vertical': self.sharex = False else: self.sharey = False
y = [v if v.size > 0 else np.array([np.nan]) for v in y]
colors = _get_standard_colors(num_colors=3, colormap=self.colormap, color=None) self._boxes_c = colors[0] self._whiskers_c = colors[0] self._medians_c = colors[2]
boxes = self.color or self._boxes_c whiskers = self.color or self._whiskers_c medians = self.color or self._medians_c caps = self.color or self._caps_c
data = data[y].copy() data.index.name = y
if return_type not in BoxPlot._valid_return_types: raise ValueError("return_type must be {None, 'axes', 'dict', 'both'}")
if return_type == 'dict': return bp elif return_type == 'both': return BoxPlot.BP(ax=ax, lines=bp) else: return ax
kwargs.setdefault('c', plt.rcParams['patch.facecolor'])
warnings.warn("figsize='default' is deprecated. Specify figure" "size by tuple instead", FutureWarning, stacklevel=4) figsize = None
if return_type is None: result = axes
f, ax = plt.subplots() ax.plot(x, y) ax.set_title('Simple plot')
axarr = np.empty(nplots, dtype=object)
ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)
if nplots == 1: axes = axarr[0] else: axes = axarr.reshape(nrows, ncols).squeeze()
axes = axarr.reshape(nrows, ncols)
layout = np.zeros((nrows + 1, ncols + 1), dtype=np.bool) for ax in axarr: layout[ax.rowNum, ax.colNum] = ax.get_visible()
if not layout[ax.rowNum + 1, ax.colNum]: continue if sharex or len(ax.get_shared_x_axes() .get_siblings(ax)) > 1: _remove_labels_from_axis(ax.xaxis)
for ax in axarr: if ax.is_last_row(): continue if sharex or len(ax.get_shared_x_axes() .get_siblings(ax)) > 1: _remove_labels_from_axis(ax.xaxis)
if ax.is_first_col(): continue if sharey or len(ax.get_shared_y_axes().get_siblings(ax)) > 1: _remove_labels_from_axis(ax.yaxis)
pivot_table(self.data, values='D', index=index)
rtable = self.data.pivot_table(columns=['AA', 'BB'], margins=True, aggfunc=np.mean) tm.assertIsInstance(rtable, Series)
aggs = {'D': 'sum', 'E': 'mean'}
self.assertRaises(ValueError, cut, [], 2)
result = cut(np.arange(11.), 2)
path = os.path.join(curpath(), 'cut_data.csv')
appended = self.mixed_frame[:5].append(self.mixed_frame[5:]) assert_frame_equal(appended, self.mixed_frame)
mixed_appended = self.mixed_frame[:5].append(self.frame[5:]) mixed_appended2 = self.frame[:5].append(self.mixed_frame[5:])
empty = DataFrame({})
self.assertRaises(ValueError, self.frame.append, self.frame, verify_integrity=True)
joined = df_list[0].join(df_list[1:], how='outer') _check_diff_index(df_list, joined, df.index)
result = concat([df, df2, df3], axis=1, copy=True) for b in result._data.blocks: self.assertIsNone(b.values.base)
df = DataFrame(np.random.randn(3, 4)) df2 = DataFrame(np.random.randn(4, 4))
df = DataFrame(np.random.randn(4, 3)) df2 = DataFrame(np.random.randn(4, 4))
foo = Series([1, 2], name='foo') bar = Series([1, 2]) baz = Series([4, 5])
result = df.iloc[0:8, :].append(df.iloc[8:]) assert_frame_equal(result, df)
concat([df1, df2])
panel = tm.makePanel() self.assertRaises(ValueError, lambda: concat([panel, s1], axis=1))
result = p1.join(p2) expected = p1.copy() expected['ItemC'] = p2['ItemC'] tm.assert_panel_equal(result, expected)
result = p1.join(p2, how='inner') expected = panel.ix[:, 5:10, 2:3] tm.assert_panel_equal(result, expected)
p1 = panel.ix[:2, :, :2] p2 = panel.ix[:, :, 2:] p1['ItemC'] = 'baz'
def make_panel(): index = 5 cols = 3
concat([panel1, panel3], axis=1, verify_integrity=True)
p1 = p4d.ix[:, :2, :, :2] p2 = p4d.ix[:, :, :, 2:] p1['L5'] = 'baz'
s = Series(randn(5), name='A') s2 = Series(randn(5), name='B')
df0 = DataFrame([[10, 20, 30], [10, 20, 30], [10, 20, 30]])
left = concat([ts1, ts2], join='outer', axis=1) right = concat([ts2, ts1], join='outer', axis=1)
concat(DataFrame(np.random.rand(5, 5)) for _ in range(3))
self.df = self.df[self.df['key2'] > 1]
self.source = DataFrame({'MergedA': data['A'], 'MergedD': data['D']}, index=data['C'])
self.assertRaises(KeyError, target.join, source, on='E')
source_copy = source.copy() source_copy['A'] = 0 self.assertRaises(ValueError, target.join, source_copy, on='A')
wrongly_typed = [Series([0, 1]), 2, 'str', None, np.array([0, 1])] df = DataFrame({'a': [1, 1]})
joined = df.join(df2, on=['key']) expected = df.join(df2, on='key')
df1 = DataFrame(index=np.arange(10)) df1['bool'] = True df1['string'] = 'foo'
x = DataFrame() x.join(DataFrame([3], index=[0], columns=['A']), how='outer')
a.join(d) d.join(a)
joined = left.join(right, on='key', sort=False) self.assert_index_equal(joined.index, pd.Index(lrange(4)))
self.assertEqual(df['key'].dtype, 'object')
mn.join(cn, rsuffix='_right')
exp_in.index = exp_in.index.astype(object)
exp_in.index = exp_in.index.astype(object)
df = DataFrame({'date': [pd.Timestamp('20130101').tz_localize('UTC'), pd.NaT]})
import datetime as dt from pandas import NaT
df2.columns = ['key1', 'foo', 'foo'] self.assertRaises(ValueError, merge, df, df2)
df3 = pd.concat([df2.A.to_frame(), df2.B.to_frame()], axis=1) assert_frame_equal(df2, df3)
first = pd.DataFrame([[datetime(2016, 1, 1)]]) first[0] = first[0].dt.tz_localize('UTC')
first = pd.DataFrame([[datetime(2016, 1, 1)]]) first[0] = first[0].dt.tz_localize('Europe/London')
first = pd.DataFrame([[datetime(2016, 1, 1)], [datetime(2016, 1, 2)]]) first[0] = first[0].dt.tz_localize('Europe/London')
first = pd.DataFrame([[datetime(2016, 1, 1)]]) first[0] = first[0].dt.tz_localize('Europe/London')
exp = pd.Series([1, 2, 3]) tm.assert_series_equal(res, exp)
assert_frame_equal(df1, df1_copy) assert_frame_equal(df2, df2_copy)
df_result_custom_name = df_result df_result_custom_name = df_result_custom_name.rename( columns={'_merge': 'custom_name'})
for i in ['_right_indicator', '_left_indicator', '_merge']: df_badcolumn = DataFrame({'col1': [1, 2], i: [2, 2]})
df_badcolumn = DataFrame( {'col1': [1, 2], 'custom_column_name': [2, 2]})
df3 = DataFrame({'col1': [0, 1], 'col2': ['a', 'b']})
for sort in [False, True]: merged1 = self.data.merge(self.to_join, left_on=['key1', 'key2'], right_index=True, how='left', sort=sort)
key1 = tm.rands_array(10, 10000) key1 = np.tile(key1, 2) key2 = key1[::-1]
merge(df, df2, how='outer')
result = merge(df1, df2, how='outer') self.assertTrue(len(result) == 2000)
out = merge(left, right, how='left', sort=False) assert_frame_equal(left, out[left.columns.tolist()])
shape = left.apply(Series.nunique).values self.assertTrue(_int64_overflow_possible(shape))
left = concat([left, left], ignore_index=True)
i = np.random.choice(len(left), n) right = concat([right, right, left.iloc[i]], ignore_index=True)
i = np.random.permutation(len(left)) left = left.iloc[i].copy() left.index = np.arange(len(left))
ldict, rdict = defaultdict(list), defaultdict(list)
assert_frame_equal(frame, align(res), check_dtype=how not in ('right', 'outer'))
household.index.name = 'foo'
for c in join_col: assert(result[c].notnull().all())
group = group.ix[:, found]
group = group.rename(columns=lambda x: x.replace(suffix, ''))
group = group.ix[:, columns]
assert len(tm.get_locales()) > 0
res = to_numeric(s, errors='coerce') expected = pd.Series([1., 0., np.nan]) tm.assert_series_equal(res, expected)
s = [True, False, True, True] res = to_numeric(s) tm.assert_numpy_array_equal(res, np.array(s))
if not np.iterable(bins): if lib.isscalar(bins) and bins < 1: raise ValueError("`bins` should be a positive integer.")
else: rng = (nanops.nanmin(x), nanops.nanmax(x)) mn, mx = [mi + 0.0 for mi in rng]
if '.' not in val: if x < 0: return '%d' % (-whole - 1) else: return '%d' % (whole + 1)
suffixes=suffixes, fill_method=fill_method)
try: if k in merged: merged[k] = key except: pass
(self.left_join_keys, self.right_join_keys, self.join_names) = self._get_merge_keys()
mask = left_indexer == -1 if mask.all(): key_col = rvals else: key_col = Index(lvals).where(~mask, rvals)
if len(left) > 0: right_drop.append(rk) else: left_drop.append(lk)
if self.on is None and self.left_on is None and self.right_on is None:
fkeys = partial(_factorize_keys, sort=sort)
llab, rlab, shape = map(list, zip(* map(fkeys, left_keys, right_keys)))
lkey, rkey = _get_join_keys(llab, rlab, shape, sort)
lkey, rkey, count = fkeys(lkey, rkey)
kwargs = {'sort': sort} if how == 'left' else {} join_func = _join_functions[how] return join_func(lkey, rkey, count, **kwargs)
ldata, rdata = self.left._data, self.right._data lsuf, rsuf = self.suffixes
fkeys = partial(_factorize_keys, sort=sort)
lkey, rkey = _get_join_keys(llab, rlab, shape, sort)
lkey, rkey, count = fkeys(lkey, rkey)
join_index = left_ax.take(left_indexer) return join_index, left_indexer, right_indexer
return left_ax, None, right_indexer
lmask = llab == -1 lany = lmask.any() rmask = rlab == -1 rany = rmask.any()
uniques = Index(uniques).values
pred = lambda i: not _int64_overflow_possible(shape[:i]) nlev = next(filter(pred, range(len(shape), 0, -1)))
lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)
ndims = set() for obj in objs: if not isinstance(obj, NDFrame): raise TypeError("cannot concatenate a non-NDFrame object")
obj.consolidate(inplace=True) ndims.add(obj.ndim)
non_empties = [obj for obj in objs if sum(obj.shape) > 0 or isinstance(obj, Series)]
self._is_frame = isinstance(sample, DataFrame) if self._is_frame: axis = 1 if axis == 0 else 0
if len(ndims) > 1: current_column = 0 max_ndim = sample.ndim self.objs, objs = [], self.objs for obj in objs:
if self._is_frame and axis == 1: name = 0 obj = sample._constructor({name: obj})
self.axis = axis self.join_axes = join_axes self.keys = keys self.names = names self.levels = levels
if self._is_series:
else: data = dict(zip(range(len(self.objs)), self.objs)) cons = _concat._get_series_result_type(data)
indices = lrange(ndim) indices.remove(self.axis)
if not len(set([idx.nlevels for idx in indexes])) == 1: raise AssertionError("Cannot concat indices that do" " not have the same number of levels")
names = names + _get_consensus_names(indexes)
new_names = list(names) new_levels = list(levels)
new_labels = []
from pandas.sparse.api import SparseSeries return SparseSeries
def is_nonempty(x): try: return x.shape[axis] > 0 except Exception: return True
if 'datetime' in typs or 'datetimetz' in typs or 'timedelta' in typs: return _concat_datetime(to_concat, axis=axis, typs=typs)
typs = get_dtype_kinds(to_concat) if len(typs) != 1:
pass
to_concat = [x.astype('object') for x in to_concat]
if com.is_categorical_dtype(x.dtype): return x.get_values() return x.ravel()
return _concat_compat([np.array(x, copy=False, dtype=object) for x in to_concat], axis=0)
categoricals = [x for x in to_concat if com.is_categorical_dtype(x.dtype)]
categories = categoricals[0] rawcats = categories.categories for x in categoricals[1:]: if not categories.is_dtype_equal(x): raise ValueError("incompatible categories in categorical concat")
if len(typs) == 1:
to_concat = [convert_to_pydatetime(x, axis) for x in to_concat] return np.concatenate(to_concat, axis=axis)
if isinstance(x, SparseArray): x = x.get_values() x = x.ravel() if axis > 0: x = np.atleast_2d(x) return x
sparses = [c for c in to_concat if com.is_sparse(c)] fill_values = [c.fill_value for c in sparses] sp_indexes = [c.sp_index for c in sparses]
to_concat = [convert_sparse(x, axis) for x in to_concat] result = np.concatenate(to_concat, axis=axis)
result = SparseArray(result.ravel(), fill_value=fill_values[0], kind=sp_indexes[0])
result = result.astype('object')
return hash(str(self))
return object.__new__(cls)
return "datetime64[{unit}, {tz}]".format(unit=self.unit, tz=self.tz)
return hash(str(self))
def create_pandas_abc_type(name, attr, comp): @classmethod def _check(cls, inst): return getattr(inst, attr, '_typ') in comp
_hacked_nodes = frozenset(['Assign', 'Module', 'Expr'])
_unsupported_nodes = ((_stmt_nodes | _mod_nodes | _handler_nodes | _arguments_nodes | _keyword_nodes | _alias_nodes | _expr_context_nodes | _unsupported_expr_nodes) - _hacked_nodes)
_base_supported_nodes = (_all_node_names - _unsupported_nodes) | _hacked_nodes _msg = 'cannot both support and not support {0}'.format(_unsupported_nodes & _base_supported_nodes) assert not _unsupported_nodes & _base_supported_nodes, _msg
op_instance = node.op op_type = type(op_instance)
if is_term(left) and is_term(right) and op_type in self.rewrite_map:
if left_list or right_list or left_str or right_str: op_instance = self.rewrite_map[op_type]()
if right_str: name = self.env.add_tmp([right.value]) right = self.term_type(name, self.env)
name = self.env.add_tmp(np.float32(left.value)) left = self.term_type(name, self.env)
name = self.env.add_tmp(np.float32(right.value)) right = self.term_type(name, self.env)
return self._possibly_eval(res, self.binary_ops)
return self._possibly_eval(res, eval_in_python)
return self._possibly_eval(res, eval_in_python + maybe_eval_in_python)
v = value.value[result]
lhs = pd.eval(value, local_dict=self.env, engine=self.engine, parser=self.parser) v = lhs[result]
try: res = FuncNode(node.func.id) except ValueError: raise
if len(comps) == 1: op = self.translate_In(ops[0]) binop = ast.BinOp(op=op, left=node.left, right=comps[0]) return self.visit(binop)
if compat.PY35: BaseExprVisitor.visit_Call = BaseExprVisitor.visit_Call_35 else: BaseExprVisitor.visit_Call = BaseExprVisitor.visit_Call_legacy
try: return self.env.resolve(self.name, is_local=False) except UndefinedVariableError: return self.name
return TermValue(v, stringify(v), u('string'))
if self.op in ['==', '!='] and len(values) > self._max_selectors:
if self.op in ['==', '!=']:
raise NotImplementedError("cannot use an invert condition when " "passing to numexpr")
if not self.is_in_table: return None
if self.op in ['==', '!=']:
else: return None
resolved = self.visit(value)
try: resolved = resolved.value except (AttributeError): pass
if isinstance(value, ast.Name) and value.id == attr: return resolved
where = self.parse_back_compat(where, op, value)
local_dict = DeepChainMap()
return any(op in s for op in ops)
level += 1 env = _ensure_scope(level, global_dict=global_dict, local_dict=local_dict, resolvers=resolvers, target=target)
eng = _engines[engine] eng_inst = eng(parsed_expr) ret = eng_inst.evaluate()
if not inplace and first_expr: target = env.target.copy() else: target = env.target
for resolver in resolvers: if parsed_expr.assigner in resolver: resolver[parsed_expr.assigner] = ret break else: resolvers += ({parsed_expr.assigner: ret},)
return reduce(np.result_type, arrays_and_dtypes)
if isinstance(key, string_types): self.env.swapkey(self.local_name, key, new_value=value)
return self._value.values.dtype
return self._value.dtype
return type(self._value)
if self.op in (_cmp_ops_syms + _bool_ops_syms): return np.bool_ return _result_type_many(*(term.type for term in com.flatten(self)))
keys = list(_binary_ops_dict.keys()) raise ValueError('Invalid binary operator {0!r}, valid' ' operators are {1}'.format(op, keys))
if self.op == '/' and env.scope['truediv']: self.func = op.truediv
left = self.lhs(env) right = self.rhs(env)
left = self.lhs.evaluate(env, engine=engine, parser=parser, term_type=term_type, eval_in_python=eval_in_python) right = self.rhs.evaluate(env, engine=engine, parser=parser, term_type=term_type, eval_in_python=eval_in_python)
if self.op in eval_in_python: res = self.func(left.value, right.value) else: res = pd.eval(self, local_dict=env, engine=engine, parser=parser)
acceptable_dtypes = [np.float32, np.float_] _cast_inplace(com.flatten(self), acceptable_dtypes, np.float_)
res = self._evaluate() return _reconstruct_object(self.result_type, res, self.aligned_axes, self.expr.terms.return_type)
s = self.convert()
try: msg = e.message except AttributeError: msg = compat.text_type(e) raise UndefinedVariableError(msg)
if len(terms) == 1: return _align_core_single_unary_op(terms[0])
if not _any_pandas_objects(terms): return _result_type_many(*term_values), None
biggest = terms[ndims.idxmax()].value typ = biggest._constructor axes = biggest.axes naxes = len(axes) gt_than_one_axis = naxes > 1
terms = list(com.flatten(terms))
if all(term.isscalar for term in terms): return _result_type_many(*(term.value for term in terms)).type, None
typ, axes = _align_core(terms) return typ, axes
_MIN_ELEMENTS = 10000
global _USE_NUMEXPR if _NUMEXPR_INSTALLED: _USE_NUMEXPR = v
if _NUMEXPR_INSTALLED and _USE_NUMEXPR: if n is None: n = ne.detect_number_of_cores() ne.set_num_threads(n)
if np.prod(a.shape) > _MIN_ELEMENTS:
if not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes: return True
if reversed: a, b = b, a
set_use_numexpr(True)
for lhs, rhs in product(self.lhses, self.rhses): self.check_pow(lhs, '**', rhs)
#local_dict={'lhs': lhs, 'rhs': rhs}, #import ipdb; ipdb.set_trace()
pass
expected = self.ne.evaluate('nlhs {0} ghs'.format(op)) tm.assert_numpy_array_equal(result.values, expected)
for engine in self.current_engines: tm.skip_if_no_ne(engine) ev = pd.eval(ex, engine=self.engine, parser=self.parser) tm.assert_almost_equal(ev, result)
expr = self.ex('~')
lhs = Series(rand(5) > 0.5) expect = ~lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_series_equal(expect, result)
lhs = DataFrame(randn(5, 2)) expect = -lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_frame_equal(expect, result)
lhs = Series(randn(5)) expect = -lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_series_equal(expect, result)
lhs = Series(randint(5, size=5)) expect = -lhs result = pd.eval(expr, engine=self.engine, parser=self.parser) assert_series_equal(expect, result)
pass
self.assertRaises(SyntaxError, df.eval, 'd c = a + b')
def f(): df = orig_df.copy()
df = orig_df.copy() df.eval('c = a + b', inplace=True) self.assertRaises(SyntaxError, df.eval, 'c = a = b')
tm.skip_if_no_ne('numexpr') df = DataFrame(np.random.randn(5, 2), columns=list('ab'))
with tm.assert_produces_warnings(FutureWarning): df.eval('c = a + b')
with tm.assert_produces_warnings(None): df.eval('a + b')
raise nose.SkipTest("unreliable tests on complex128")
self.check_result_type(np.complex128, np.complex128)
try: hexin = ord(x) except TypeError: hexin = x
packed = struct.pack('@P', id(obj)) return ''.join(map(_replacer, packed))
self.scope = DeepChainMap(_DEFAULT_GLOBALS.copy()) self.target = target
if isinstance(local_dict, Scope): resolvers += tuple(local_dict.resolvers.maps) self.resolvers = DeepChainMap(*resolvers) self.temps = {}
if is_local: return self.scope[key]
if self.has_resolvers: return self.resolvers[key]
assert not is_local and not self.has_resolvers return self.scope[key]
return self.temps[key]
del frame
stack = inspect.stack()
assert name not in self.temps self.temps[name] = value assert name in self.temps
return name
result = str(thing).decode('utf-8', "replace")
if x.count(":"): props.append(x.split(":")) else: props.append(['', ''])
col_formatter = _maybe_wrap_formatter(col_formatter) col_num = self.data.columns.get_indexer_for([col])[0]
locs = product(*(row_locs, col_locs)) for i, j in locs: formatter = _maybe_wrap_formatter(formatter) self._display_funcs[(i, j)] = formatter
result = func(self.data.loc[subset], **kwargs)
from __future__ import print_function from distutils.version import LooseVersion
if footer: footer += '\n' footer += level_info
if com.is_categorical_dtype(self.tr_series.dtype): level_info = self.tr_series._values._repr_categories_info() if footer: footer += "\n" footer += level_info
dot_str = self.adj.justify([dot_str], width, mode='center')[0] fmt_values.insert(row_num + n_header_rows, dot_str) fmt_index.insert(row_num + 1, '')
def _get_pad(t): return max_len - self.len(t) + len(t)
self.tr_size_col = -1
max_cols = self.max_cols max_rows = self.max_rows
if max_cols == 0 and len(self.frame.columns) > w: max_cols = w if max_rows == 0 and len(self.frame) > h: max_rows = h
truncate_h = self.truncate_h truncate_v = self.truncate_v
text = self.adj.adjoin(1, *strcols)
size_tr_col = len(headers[self.tr_size_col])
self._chk_truncate() strcols = self._to_str_columns() text = self.adj.adjoin(1, *strcols)
restrict_formatting = any([l.is_floating for l in columns.levels]) need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))
return str_columns
index = frame.index columns = frame.columns
if show_col_names: col_header = ['%s' % x for x in self._get_column_name_list()] else: col_header = [''] * columns.nlevels
return indent
sentinel = com.sentinel_factory()
sentinel = com.sentinel_factory() levels = frame.index.format(sparsify=sentinel, adjoin=False, names=False)
quotechar = None
if not self.obj.columns.is_unique and engine == 'python': raise NotImplementedError("columns.is_unique == False not " "supported with engine='python'")
if self.has_mi_columns: if cols is not None: raise TypeError("cannot specify cols with a MultiIndex on the " "columns")
cols = self.obj.columns if isinstance(cols, Index): cols = cols.to_native_types(na_rep=na_rep, float_format=float_format, date_format=date_format, quoting=self.quoting) else: cols = list(cols)
self.cols = cols
self.blocks = self.obj._data.blocks ncols = sum(b.shape[0] for b in self.blocks) self.data = [None] * ncols
def _helper_csv(self, writer, na_rep=None, cols=None, header=True, index=True, index_label=None, float_format=None, date_format=None): if cols is None: cols = self.columns
self._helper_csv(self.writer, na_rep=self.na_rep, float_format=self.float_format, cols=self.cols, header=self.header, index=self.index, index_label=self.index_label, date_format=self.date_format)
if has_mi_columns: columns = obj.columns
for i in range(columns.nlevels):
col_line = [] if self.index:
col_line.append(columns.names[i])
encoded_labels.extend([''] * len(columns))
writer.writerow(encoded_labels)
chunksize = self.chunksize chunks = int(nrows / chunksize) + 1
self.data[col_loc] = col
for lnum in range(len(level_lengths)): name = columns.names[lnum] yield ExcelCell(lnum, coloffset, name, header_style)
index_values = self.df.index if isinstance(self.df.index, PeriodIndex): index_values = self.df.index.to_timestamp()
if (self.index_label and isinstance(self.index_label, (list, tuple, np.ndarray, Index))): index_labels = self.index_label
if isinstance(self.columns, MultiIndex) and self.merge_cells: self.rowcounter += 1
if (any(x is not None for x in index_labels) and self.header is not False):
level_strs = self.df.index.format(sparsify=True, adjoin=False, names=False) level_lengths = _get_level_lengths(level_strs)
return '%s' % formatter(x)
if self.float_format is not None and self.formatter is None: if callable(self.float_format): self.formatter = self.float_format self.float_format = None
if float_format is None: float_format = self.float_format
def format_values_with(float_format): formatter = self._value_formatter(float_format, threshold)
values = self.values mask = isnull(values)
if self.float_format is None and self.fixed_width: float_format = '%% .%df' % self.digits else: float_format = self.float_format
has_large_values = (abs_vals > 1e6).any() has_small_values = ((abs_vals < 10**(-self.digits)) & (abs_vals > 0)).any()
if self.formatter is not None: return [self.formatter(x) for x in self.values]
values = Index(self.values, dtype='object').to_native_types()
values = DatetimeIndex(values) if values.tz is not None: return False
return [x + "0" if x.endswith('.') and x != na_rep else x for x in trimmed]
if not encoding or 'ascii' in encoding.lower(): try: encoding = locale.getpreferredencoding() except Exception: pass
if not encoding or 'ascii' in encoding.lower(): encoding = sys.getdefaultencoding()
if not _initial_defencoding: _initial_defencoding = sys.getdefaultencoding()
display_height = get_option('display.height', silent=True)
from pandas.core.config import get_default_val terminal_width = get_default_val('display.width') terminal_height = get_default_val('display.height')
terminal_width, terminal_height = get_terminal_size()
return (display_width or terminal_width, display_height or terminal_height)
' 1.0M'
'-1.00E-06'
return cls._simple_new(subarr, sparse_index, fill_value)
data = data.astype(float)
raise ValueError('sp_index must be a SparseIndex')
__iadd__ = disable __isub__ = disable __imul__ = disable __itruediv__ = disable __ifloordiv__ = disable __ipow__ = disable
if not compat.PY3: __idiv__ = disable
return self.view(np.ndarray)
return self[indices]
raise TypeError( "SparseArray does not support item assignment via setitem")
raise TypeError("SparseArray does not support item assignment via " "slices")
if lib.isscalar(arr): arr = [arr]
if isinstance(arr, np.ndarray): pass
indices = mask.sp_index.indices
if not v.index.equals(index): v = v.reindex(index)
return dict(_typ=self._typ, _subtyp=self._subtyp, _data=self._data, _default_fill_value=self._default_fill_value, _default_kind=self._default_kind)
if (value is not None and value == value and method is None and limit is None): self._default_fill_value = value
else: clean = sp_maker(value, self.index)
return clean
new = values.take(indexer) if need_mask: new = new.values np.putmask(new, mask, fill_value)
axes = [_ensure_index(columns), _ensure_index(index)]
minor_labels = np.repeat(np.arange(len(frame.columns)), lengths)
ops.add_flex_arithmetic_methods(SparseDataFrame, use_numexpr=False, **ops.frame_flex_funcs) ops.add_special_arithmetic_methods(SparseDataFrame, use_numexpr=False, **ops.frame_special_funcs)
from pandas.sparse.array import SparseArray from pandas.sparse.list import SparseList from pandas.sparse.series import SparseSeries, SparseTimeSeries from pandas.sparse.frame import SparseDataFrame from pandas.sparse.panel import SparsePanel
warnings.warn("SparsePanel is deprecated and will be removed in a " "future version", FutureWarning, stacklevel=2)
if items is None: items = Index(sorted(frames.keys())) items = _ensure_index(items)
for item in items: if item not in clean_frames: raise ValueError('column %r not found in data' % item)
pass
return np.array([self._frames[item].values for item in self.items])
major_axis = SparsePanelAxis('_major_axis', 'index')
minor_axis = SparsePanelAxis('_minor_axis', 'columns')
if com.is_list_like(key): return self.reindex(**{self._get_axis_name(axis): key})
indexer = minor * N + major
mask = counts == I
values = np.column_stack([d_values[item][mask.take(d_indexer[item])] for item in self.items])
major_labels = inds % N minor_labels = inds // N
new_default_fill = func(self.default_fill_value, other.default_fill_value)
SparsePanel._add_aggregate_operations(use_numexpr=False) ops.add_special_arithmetic_methods(SparsePanel, use_numexpr=False, ** ops.panel_special_funcs) SparseWidePanel = SparsePanel
minor_labels = np.repeat(np.arange(len(frame.columns)), lengths)
inds = index.to_int_index().indices + total_length
self.assertTrue(cp.index.identical(self.frame.index))
sdf = SparseDataFrame(columns=np.arange(10), index=np.arange(10)) for col, series in compat.iteritems(sdf): tm.assertIsInstance(series, SparseSeries)
data = {} for c, s in compat.iteritems(self.frame): data[c] = s.to_dict()
self.assertRaises(TypeError, self.frame.reindex, idx, level=0)
sp = SparseDataFrame(self.frame.values)
self.assertRaises(TypeError, self.frame.reindex, columns=['A'], level=1)
with tm.assertRaisesRegexp(ValueError, "^Index length"): SparseDataFrame(self.frame.values, index=self.frame.index[:-1])
def test_constructor_empty(self): sp = SparseDataFrame() self.assertEqual(len(sp.index), 0) self.assertEqual(len(sp.columns), 0)
series = [frame.xs(fidx[0]), frame.xs(fidx[3]), frame.xs(fidx[5]), frame.xs(fidx[7]), frame.xs(fidx[5])[:2]]
sdf = SparseDataFrame(index=[0, 1, 2], columns=['a', 'b', 'c'])
result = self.frame.iloc[:, 0] self.assertTrue(isinstance(result, SparseSeries)) tm.assert_sp_series_equal(result, self.frame['A'])
sliced = self.frame.ix[-2:, :] expected = self.frame.reindex(index=self.frame.index[-2:]) tm.assert_sp_frame_equal(sliced, expected)
sliced = self.frame.ix[:, -2:] expected = self.frame.reindex(columns=self.frame.columns[-2:]) tm.assert_sp_frame_equal(sliced, expected)
sl = self.frame[:20] tm.assert_sp_frame_equal(sl, self.frame.reindex(self.frame.index[:20]))
d = self.frame.index[5] indexer = self.frame.index > d
frame['H'] = np.random.randn(N) tm.assertIsInstance(frame['H'], SparseSeries)
self.assertRaises(Exception, frame.__setitem__, 'foo', np.random.randn(N - 1))
broadcasted = self.frame.apply(np.sum, broadcast=True) tm.assertIsInstance(broadcasted, SparseDataFrame)
df = df_orig.T.to_sparse()
result = self.frame.applymap(lambda x: x * 2) tm.assertIsInstance(result, SparseDataFrame)
tm.assert_series_equal(result.to_dense(), expected)
tm.assert_almost_equal(sparse_result.default_fill_value, frame.default_fill_value) tm.assert_almost_equal(sparse_result['A'].fill_value, frame['A'].fill_value)
reindexed = self.frame.reindex(self.frame.index, copy=False) reindexed['F'] = reindexed['A'] self.assertIn('F', self.frame)
self.assertRaises(Exception, _check, self.zframe) self.assertRaises(Exception, _check, self.fill_frame)
tm.assert_frame_equal(shifted.to_dense(), exp, check_dtype=False) shifted = frame.shift(1) exp = orig.shift(1) tm.assert_frame_equal(shifted, exp)
tm.assert_series_equal(result, dense_result, check_dtype=False)
df = SparseDataFrame({'A': [1.1, 3.3], 'B': [2.5, -3.9]})
df = SparseDataFrame({'A': [nan, 0, 1]})
result = 1**df
nan_colname = DataFrame(Series(1.0, index=[0]), columns=[nan]) nan_colname_sparse = nan_colname.to_sparse() self.assertTrue(np.isnan(nan_colname_sparse.columns[0]))
_check_case([0], [5], [], [], [], []) _check_case([], [], [], [], [], [])
self.assertRaises(Exception, BlockIndex, 10, [5], [10])
self.assertRaises(Exception, BlockIndex, 10, [2, 5], [5, 3])
xseries = Series(x, xdindex.indices) xseries = xseries.reindex(np.arange(TEST_LENGTH)).fillna(xfill)
import numpy as np import pandas as pd import pandas.util.testing as tm
sparse_grouped = self.sparse.groupby('A') dense_grouped = self.dense.groupby('A')
sparse = self.dense1.to_sparse() sparse2 = self.dense2.to_sparse()
sparse = self.dense1.to_sparse(fill_value=0) sparse2 = self.dense2.to_sparse(fill_value=0)
sparse = self.dense1.to_sparse() sparse2 = self.dense2.to_sparse(fill_value=0)
sparse = self.dense1.to_sparse() sparse3 = self.dense3.to_sparse()
sparse = self.dense1.to_sparse(fill_value=0) sparse3 = self.dense3.to_sparse(fill_value=0)
sparse = self.dense1.to_sparse() sparse2 = self.dense2.to_sparse()
sparse = self.dense1.to_sparse(fill_value=0) sparse2 = self.dense2.to_sparse(fill_value=0)
sparse = self.dense1.to_sparse() sparse3 = self.dense3.to_sparse()
sparse = self.dense1.to_sparse(fill_value=0) sparse3 = self.dense3.to_sparse(fill_value=0)
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): result = op(panel, 1) tm.assert_sp_frame_equal(result['ItemA'], op(panel['ItemA'], 1))
def test_deprecation(self): with tm.assert_produces_warning(FutureWarning): SparsePanel()
from __future__ import print_function
data = np.array([False, False, True, True, False, False]) arr = SparseArray(data, fill_value=False, dtype=bool)
data = np.array([1., np.nan, 3], dtype=np.float32) arr = SparseArray(data, dtype=np.float32)
result = self.arr[-12:] exp = SparseArray(self.arr) tm.assert_sp_array_equal(result, exp)
dense[4:, :]
arr = np.arange(20, dtype=float) index = np.arange(20) arr[:2] = nan arr[5:10] = nan arr[-3:] = nan
arr = np.arange(15, dtype=float) index = np.arange(15) arr[7:12] = nan arr[-1:] = nan return arr, index
arr, index = _test_data1() arr[np.isnan(arr)] = 0 return arr, index
arr, index = _test_data2() arr[np.isnan(arr)] = 0 return arr, index
with tm.assert_produces_warning(FutureWarning): pd.SparseTimeSeries(1, index=pd.date_range('20130101', periods=3))
df = DataFrame({'col': self.bseries})
df.iloc[:1] df['col'] df.dtypes str(df)
expected = Series({'col': 'float64:sparse'}) result = df.ftypes tm.assert_series_equal(expected, result)
ts = Series(np.random.randn(10)) ts[2:-2] = nan sts = ts.to_sparse()
result = SparseSeries(sparse, name='x') tm.assert_sp_series_equal(result, sparse, check_names=False) self.assertEqual(result.name, 'x')
date_index = bdate_range('1/1/2000', periods=len(self.bseries)) s5 = SparseSeries(self.bseries, index=date_index) tm.assertIsInstance(s5, SparseSeries)
bseries2 = SparseSeries(self.bseries.to_dense()) tm.assert_numpy_array_equal(self.bseries.sp_values, bseries2.sp_values)
values = np.ones(self.bseries.npoints) sp = SparseSeries(values, sparse_index=self.bseries.sp_index) sp.sp_values[:5] = 97 self.assertEqual(values[0], 97)
sp = SparseSeries(values, sparse_index=self.bseries.sp_index, copy=True) sp.sp_values[:5] = 100 self.assertEqual(values[0], 97)
def test_constructor_empty(self): sp = SparseSeries() self.assertEqual(len(sp.index), 0) self.assertEqual(sp.shape, (0, ))
cop[:5] = 97 self.assertEqual(cop.sp_values[0], 97) self.assertNotEqual(self.bseries.sp_values[0], 97)
zbcop = self.zbseries.copy() zicop = self.ziseries.copy()
view = self.bseries.copy(deep=False) view.sp_values[:5] = 5 self.assertTrue((self.bseries.sp_values[:5] == 5).all())
self.assertRaises(Exception, self.bseries.__getitem__, len(self.bseries) + 1)
self.assertRaises(Exception, self.btseries.__getitem__, self.btseries.index[-1] + BDay())
res = self.bseries[:-3] tm.assert_sp_series_equal(res, self.bseries.reindex(idx[:-3]))
check(self.bseries, 5)
check(self.zbseries, self.zbseries * 2) check(self.zbseries, self.zbseries2) check(self.ziseries, self.ziseries2)
result = self.bseries + self.bseries.to_dense() tm.assert_sp_series_equal(result, self.bseries + self.bseries)
import nose raise nose.SkipTest("skipping sparse binary operators test")
same_index = self.bseries.reindex(self.bseries.index) tm.assert_sp_series_equal(self.bseries, same_index) self.assertIsNot(same_index, self.bseries)
s = SparseSeries([0, 1, np.nan, 3, 4, 5], index=np.arange(6))
orig = pd.Series([np.nan, 2, np.nan, 4, 0, np.nan, 0]) sparse = orig.to_sparse()
orig = pd.Series([1, 2, 3, 4], dtype=np.int64) sparse = orig.to_sparse()
orig = pd.Series([1, 0, 0, 4], dtype=np.int64) sparse = orig.to_sparse(fill_value=0)
tm._skip_if_no_scipy() import scipy.sparse
tm.assert_numpy_array_equal(A.todense(), A_result.todense()) self.assertEqual(il, il_result) self.assertEqual(jl, jl_result)
result = sparse[orig % 2 == 1] exp = orig[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse[sparse % 2 == 1] exp = orig[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse[orig % 2 == 1] exp = orig[orig % 2 == 1].to_sparse(fill_value=0) tm.assert_sp_series_equal(result, exp)
result = sparse[sparse % 2 == 1] exp = orig[orig % 2 == 1].to_sparse(fill_value=0) tm.assert_sp_series_equal(result, exp)
s = pd.SparseSeries([1, np.nan, 2, 0, np.nan]) tm.assert_sp_series_equal(s[...], s)
result = sparse.loc[orig % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[sparse % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[orig % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[sparse % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[orig % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse(fill_value=0) tm.assert_sp_series_equal(result, exp)
result = sparse.loc[sparse % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse(fill_value=0) tm.assert_sp_series_equal(result, exp)
orig = pd.Series([0., 0., 0., 0., 0.], index=list('ABCDE')) sparse = orig.to_sparse(fill_value=0)
result = sparse[orig % 2 == 1] exp = orig[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse[sparse % 2 == 1] exp = orig[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
orig = self.orig sparse = self.sparse
result = sparse.loc[[1, 3, 4, 5]] exp = orig.loc[[1, 3, 4, 5]].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[orig % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[sparse % 2 == 1] exp = orig.loc[orig % 2 == 1].to_sparse() tm.assert_sp_series_equal(result, exp)
result = sparse.loc[[1, 3, 4, 5]] exp = orig.loc[[1, 3, 4, 5]].to_sparse() tm.assert_sp_frame_equal(result, exp)
result = sparse.loc[orig.x % 2 == 1] exp = orig.loc[orig.x % 2 == 1].to_sparse() tm.assert_sp_frame_equal(result, exp)
result = sparse.loc[sparse.x % 2 == 1] exp = orig.loc[orig.x % 2 == 1].to_sparse() tm.assert_sp_frame_equal(result, exp)
result = sparse.loc[orig.x % 2 == 1] exp = orig.loc[orig.x % 2 == 1].to_sparse() tm.assert_sp_frame_equal(result, exp)
result = sparse.loc[sparse.x % 2 == 1] exp = orig.loc[orig.x % 2 == 1].to_sparse() tm.assert_sp_frame_equal(result, exp)
name = name[2:-2]
if fastpath:
data = data._data
if sparse_index is None: data, sparse_index = make_sparse(data, kind=kind, fill_value=fill_value) else: assert (len(data) == sparse_index.npoints)
if isinstance(data, SingleBlockManager):
if not isinstance(data, SparseArray): data = SparseArray(data, sparse_index=sparse_index, fill_value=fill_value, dtype=dtype, copy=copy)
series_rep = Series.__unicode__(self) rep = '%s\n%s' % (series_rep, repr(self.sp_index)) return rep
return dict(_typ=self._typ, _subtyp=self._subtyp, _data=self._data, fill_value=self.fill_value, name=self.name)
data = np.empty(nd_state[1], dtype=nd_state[2]) np.ndarray.__setstate__(data, nd_state)
if not isinstance(data, SparseArray): data = SparseArray(data, sparse_index=sp_index, fill_value=fill_value, copy=False)
data = SingleBlockManager(data, index, fastpath=True) generic.NDFrame.__init__(self, data)
pass
key = self.index.get_loc(key)
if not isnull(self.fill_value): shifted = self.to_dense().shift(periods, freq=freq, axis=axis) return shifted.to_sparse(fill_value=self.fill_value, kind=self.kind)
class SparseTimeSeries(SparseSeries): def __init__(self, *args, **kwargs): warnings.warn("SparseTimeSeries is deprecated. Please use " "SparseSeries", FutureWarning, stacklevel=2)
_check_is_partition([row_levels, column_levels], range(ss.index.nlevels))
values = ss._data.internal_values()._valid_sp_values
try: return index.get_level_values(index.names[i]) except KeyError: return index.get_level_values(i)
row_levels = [ss.index._get_level_number(x) for x in row_levels] column_levels = [ss.index._get_level_number(x) for x in column_levels]
i = range(A.shape[0]) j = range(A.shape[1]) ind = MultiIndex.from_product([i, j]) s = s.reindex_axis(ind)
hard_dependencies = ("numpy", "pytz", "dateutil") missing_dependencies = []
from pandas.compat.numpy import *
import pandas.core.config_init
import pandas.util.testing from pandas.util.nosetester import NoseTester test = NoseTester().test del NoseTester
from ._version import get_versions v = get_versions() __version__ = v.get('closest-tag',v['version']) del get_versions, v
from cStringIO import StringIO as cStringIO from StringIO import StringIO BytesIO = StringIO import cPickle import httplib
range = range map = map zip = zip filter = filter reduce = functools.reduce long = int unichr = chr
def lrange(*args, **kwargs): return list(range(*args, **kwargs))
import re _name_re = re.compile(r"[a-zA-Z_][a-zA-Z0-9_]*$")
range = xrange zip = itertools.izip filter = itertools.ifilter map = itertools.imap reduce = reduce long = long unichr = unichr
lrange = builtins.range lzip = builtins.zip lmap = builtins.map lfilter = builtins.filter
if not PY3: setattr(cls, name, types.MethodType(func, None, cls)) else: setattr(cls, name, func)
_EAW_MAP = {'Na': 1, 'N': 1, 'W': 2, 'F': 2, 'H': 1}
return len(data)
callable = callable
raise Exception('dateutil 2.0 incompatible with Python 2.x, you must ' 'install version 1.5 or 2.1+!')
wrapper.__module__ = getattr(user_function, '__module__') wrapper.__doc__ = getattr(user_function, '__doc__') wrapper.__name__ = getattr(user_function, '__name__') return wrapper
return mapping[key]
return self.__missing__(key)
return len(set().union(*self.maps))
np.seterr(all='ignore')
if hasattr(arr, '__iter__') and not \ isinstance(arr, string_and_binary_types): arr = [tz_replacer(s) for s in arr] else: arr = tz_replacer(arr)
SQUEEZE_DEFAULTS = dict(axis=None) validate_squeeze = CompatValidator(SQUEEZE_DEFAULTS, fname='squeeze', method='kwargs')
if "the 'axes' parameter is not supported" in msg: msg += " for {klass} instances".format(klass=klass)
if issubclass(cls, Index): obj = object.__new__(cls) else: obj = cls.__new__(cls, *args)
def load_newobj_ex(self): kwargs = self.stack.pop() args = self.stack.pop() cls = self.stack.pop()
if issubclass(cls, Index): obj = object.__new__(cls) else: obj = cls.__new__(cls, *args, **kwargs) self.append(obj)
def new_child(self, m=None): if m is None: m = {} return self.__class__(m, *self.maps)
if categories is None: categories = self.categories if ordered is None: ordered = self.ordered return super(CategoricalIndex, self)._shallow_copy(values=values, categories=categories, ordered=ordered, **kwargs)
return self._engine_type(lambda: self.codes.astype('i8'), len(self))
if len(missing): cats = self.categories.get_indexer(target)
result = Index(np.array(self), name=self.name) new_target, indexer, _ = result._reindex_non_unique( np.array(target))
new_target = self._shallow_copy(new_target)
return self._maybe_cast_indexer(label)
data = cls._coerce_to_ndarray(data)
return self.values.view('i8')
if kind != 'iloc': key = self._maybe_cast_indexer(key) return (super(Int64Index, self) ._convert_scalar_indexer(key, kind=kind))
return False
if dtype.kind in ['i', 'O', 'f']: dtype = np.float64
if subarr.dtype != np.float64: subarr = subarr.astype(np.float64)
if not isinstance(key, slice): return key
return self.slice_indexer(key.start, key.stop, key.step, kind=kind)
return np.isnan(other) and self.hasnans
return nan_idxs
import datetime import warnings from functools import partial from sys import getsizeof
_typ = 'multiindex' _names = FrozenList() _levels = FrozenList() _labels = FrozenList() _comparables = ['names'] rename = Index.set_names
result._set_levels(levels, copy=copy, validate=False) result._set_labels(labels, copy=copy, validate=False)
result._set_names(names)
__set_levels = deprecate("setting `levels` directly", partial(set_levels, inplace=True, verify_integrity=True), alt_name="set_levels") levels = property(fget=_get_levels, fset=__set_levels)
__set_labels = deprecate("setting labels directly", partial(set_labels, inplace=True, verify_integrity=True), alt_name="set_labels") labels = property(fget=_get_labels, fset=__set_labels)
kwargs.pop('freq', None) return MultiIndex.from_tuples(values, **kwargs)
return None
for l, name in zip(level, names): self.levels[l].rename(name, inplace=True)
return sum(name == n for n in self.names) > 1
mi = MultiIndex(levels=new_levels, labels=new_labels, names=self.names, sortorder=self.sortorder, verify_integrity=False)
elif level >= self.nlevels: raise IndexError('Too many levels: Index has only %d levels, ' 'not %d' % (self.nlevels, level + 1))
@property def _is_v1(self): return False
return True
raise NotImplementedError('isnull is not defined for MultiIndex')
from pandas.core.indexing import maybe_droplevels
s = _values_from_object(series) k = _values_from_object(key)
if is_iterator(key): raise InvalidIndexError(key) else: raise e1
mask = lab == -1 if mask.any(): formatted = np.array(formatted, dtype=object) formatted[mask] = na formatted = formatted.tolist()
if sparsify not in [True, 1]: sentinel = sparsify result_levels = _sparsify(result_levels, start=int(names), sentinel=sentinel)
labels = [x.reshape(n_shuffle, -1).ravel(order='F') for x in labels] names = self.names return MultiIndex(levels=levels, labels=labels, names=names)
raise TypeError('Cannot infer number of levels from empty list')
try: self.get_loc(key) return True except LookupError: return False
sortorder = None
try: return MultiIndex.from_tuples(new_tuples, names=self.names) except: return Index(new_tuples)
mask = new_labels[0] == -1 result = new_levels[0].take(new_labels[0]) if mask.any(): result = result.putmask(mask, np.nan)
if isinstance(ascending, list): if not len(level) == len(ascending): raise ValueError("level must have same length as ascending")
else:
preserve_names = not hasattr(target, 'names')
target = ibase._ensure_has_len(target) if len(target) == 0 and not isinstance(target, Index): idx = self.levels[level] attrs = idx._get_attributes_dict()
target = MultiIndex.from_tuples(target)
return super(MultiIndex, self).slice_locs(start, end, step, kind=kind)
loc = lev.searchsorted(lab, side=side) if side == 'right' and loc >= 0: loc -= 1 return start + section.searchsorted(loc, side=side)
orig_index = new_index = self[indexer] levels = [self._get_level_number(i) for i in levels] for i in sorted(levels, reverse=True): try: new_index = new_index.droplevel(i) except:
return orig_index
if isinstance(key, list): key = tuple(key)
if (not isinstance(indexer, slice) or indexer.stop - indexer.start != 1): return partial_selection(key, indexer)
if k.start == 0 and k.stop == len(self): k = slice(None, None)
start = stop = level_index.slice_indexer(key.start, key.stop, key.step, kind='loc') step = start.step
return convert_indexer(start.start, stop.stop, step)
return convert_indexer(start, stop + 1, step)
i = labels.searchsorted(start, side='left') j = labels.searchsorted(stop, side='right') return slice(i, j, step)
i = labels.searchsorted(loc, side='left') j = labels.searchsorted(loc, side='right') return slice(i, j)
n = len(self) indexer = None
k = np.asarray(k) indexer = _update_indexer(_convert_to_indexer(k), indexer=indexer)
continue
return Int64Index([])._values
indexer = _update_indexer(None, indexer=indexer)
indexer = _update_indexer(_convert_to_indexer( self._get_level_indexer(k, level=i, indexer=indexer)), indexer=indexer)
indexer = _update_indexer(_convert_to_indexer( self.get_loc_level(k, level=i, drop_level=False)[0]), indexer=indexer)
if indexer is None: return Int64Index([])._values return indexer._values
lev_loc = len(level) level = level.insert(lev_loc, k)
if isinstance(start, RangeIndex): if name is None: name = start.name return cls._simple_new(name=name, **dict(start._get_data_as_items()))
return None
gcd, s, t = self._extended_gcd(self._step, other._step)
if (self._start - other._start) % gcd: return RangeIndex()
new_index._start = new_index._min_fitting_element(int_low) return new_index
return self._int64index.join(other, how, level, return_indexers)
step = 1 if key.step is None else key.step if key.start is None: start = l - 1 if step < 0 else 0 else: start = key.start
if (start != int(start) or stop != int(stop) or step != int(step)): return super_getitem(key)
start = self._start + self._step * start stop = self._start + self._step * stop step = self._step * step
return super_getitem(key)
if step: rstep = step(self._step, other)
if not com.is_integer(rstep) or not rstep: raise ValueError
if not all([com.is_integer(x) for x in [rstart, rstop, rstep]]): result = result.astype('float64')
if isinstance(self, RangeIndex): self = self.values if isinstance(other, RangeIndex): other = other.values
default_pprint = lambda x, max_seq_items=None: \ pprint_thing(x, escape_chars=('\t', '\r', '\n'), quote_strings=True, max_seq_items=max_seq_items)
_join_precedence = 1
_groupby = _algos.groupby_object _arrmap = _algos.arrmap_object _left_indexer_unique = _algos.left_join_indexer_unique_object _left_indexer = _algos.left_join_indexer_object _inner_indexer = _algos.inner_join_indexer_object _outer_indexer = _algos.outer_join_indexer_object _box_scalars = False
_infer_as_myclass = False
if is_categorical_dtype(data) or is_categorical_dtype(dtype): from .category import CategoricalIndex return CategoricalIndex(data, copy=copy, name=name, **kwargs)
elif isinstance(data, (np.ndarray, Index, ABCSeries)):
return Float64Index(data, copy=copy, dtype=dtype, name=name)
if copy: subarr = subarr.copy()
pass
from pandas.tseries.index import DatetimeIndex return DatetimeIndex(subarr, copy=copy, name=name, **kwargs)
raise TypeError("Index can't be updated inplace")
return self._id is getattr( other, '_id', Ellipsis) and self._id is not None
if not isinstance(data, (ABCSeries, list, tuple)): data = list(data) data = np.asarray(data)
if data is None: data = ''
return " "
is_justify = not (self.inferred_type in ('string', 'unicode') or (self.inferred_type == 'categorical' and is_object_dtype(self.categories)))
is_truncated = n > max_seq_items
adj = _get_adjustment()
if is_justify:
summary += line.rstrip() + space2 + '...' line = space2
summary, line = _extend_line(summary, line, tail[-1], display_width - 2, space2) summary += line summary += '],'
summary = '[' + summary[len(space2):]
return False
return self.values
if self.inferred_type not in ['floating', 'mixed-integer-float', 'string', 'unicode', 'mixed']: return self._invalid_indexer('label', key)
if not isinstance(key, slice): return key
start, stop, step = key.start, key.stop, key.step
def is_int(v): return v is None or is_integer(v)
indexer[indexer < 0] = len(self) from pandas.core.indexing import maybe_convert_indices return maybe_convert_indices(indexer, len(self))
return self._engine_type(lambda: self.values, len(self))
try: return key in self._engine except TypeError: return False
getitem = self._data.__getitem__ promote = self._shallow_copy
return promote(getitem(key))
values = np.empty(len(self), dtype=np.bool_) values.fill(False) return values
return self.astype(object).putmask(mask, value)
mask = isnull(values) if mask.any(): result = np.array(result) result[mask] = na_rep result = result.tolist()
result = list(self.values)
value_set = set(self.values) result.extend([x for x in other._values if x not in value_set])
return self._wrap_union_result(other, result)
indexer = Index(self.values).get_indexer_non_unique( other._values)[0].unique() indexer = indexer[indexer != -1]
s = getattr(series, '_values', None) if isinstance(s, Index) and lib.isscalar(key): try: return s[key] except (IndexError, ValueError):
pass
if is_iterator(key): raise InvalidIndexError(key) else: raise e1
self._validate_index_level(level) return self
return tolerance
if not self.is_unique and len(indexer): raise ValueError("cannot reindex from a duplicate axis")
preserve_names = not hasattr(target, 'name')
if target.is_unique:
new_indexer = np.arange(len(indexer)) new_indexer[cur_indexer] = np.arange(len(cur_labels)) new_indexer[missing_indexer] = -1
else:
indexer = indexer._values indexer[~check] = 0
new_indexer = np.arange(len(self.take(indexer))) new_indexer[~check] = -1
if level is None and (self_is_mi or other_is_mi):
if self.names == other.names: pass else: return self._join_multi(other, how=how, return_indexers=return_indexers)
if level is not None and (self_is_mi or other_is_mi): return self._join_level(other, level, how=how, return_indexers=return_indexers)
if not (self_is_mi and other_is_mi):
how = {'right': 'left', 'left': 'right'}.get(how, how)
raise NotImplementedError("merging with both multi-indexes is not " "implemented")
tic = labels[0][:-1] != labels[0][1:] for lab in labels[1:-1]: tic |= lab[:-1] != lab[1:]
left_indexer = left_indexer[counts[0]:] new_labels = [lab[left_indexer] for lab in new_labels]
if not mask_all: left_indexer = mask.nonzero()[0][left_indexer]
elif is_integer(label): self._invalid_indexer('slice', label)
pos = self[::-1].searchsorted(label, side='right' if side == 'left' else 'right') return len(self) - pos
label = self._maybe_cast_slice_bound(label, side, kind)
try: slc = self.get_loc(label) except KeyError as err: try: return self._searchsorted_monotonic(label, side) except ValueError: raise err
start, end = end, start
if end_slice == -1: end_slice -= len(self) if start_slice == -1: start_slice -= len(self)
return Index(result, name=self.name)
if needs_i8_conversion(self) and needs_i8_conversion(other): return self._evaluate_compare(other, op)
if is_bool_dtype(result): return result try: return Index(result) except TypeError: return result
if not self._is_numeric_dtype: raise TypeError("cannot evaluate a numeric op {opstr} " "for type: {typ}".format( opstr=opstr, typ=type(self)) )
pass
pass
values = self.values if reversed: values, other = other, values
return result.dtype.type(result.item())
if isinstance(index_like, list): if type(index_like) != list: index_like = list(index_like) converted, all_arrays = lib.clean_index_list(index_like)
if func_kw is None: func_kw = [] kwds = {} for k in func_kw: value = kwargs.pop(k, None) if value is not None: kwds[k] = value
how = kwargs.pop('how', None) if how is not None: kwds['how'] = how
_WINDOW_TYPES.update((v, v) for k, v in list(_WINDOW_TYPES.items())) _ADDITIONAL_CLUSTER_TYPES = set(("entity", "time"))
result = ols(y=y, x=x)
result = ols(y=y, x=x, window_type='rolling', window=10) print(result.beta)
y = A x = {'B' : B, 'C' : C}
result = ols(y=y, x=x)
result = ols(y=y, x=x, cluster='entity', window_type='expanding', window=10)
return newey_west(m, new_max_lags, nobs, df)
pass
return self._intercept
return self.sm_ols.fittedvalues
return self._time_has_obs.astype(int)
cum_xx = self._cum_xx(x) cum_xy = self._cum_xy(x, y)
if self._nw_lags is None: F = self._r2_raw / (self._r2_raw - self._r2_adj_raw)
result = starmap(get_result_simple, zip(F, df_resid))
pass
dummy = DataFrame(index=self._y.index) dummy['y'] = 1
window = len(self._index)
return self._nobs_raw >= max(self._min_periods, len(self._x.columns) + 1)
data = {}
values = np.asarray(values)
shifter = 10 ** max(xbins, ybins) _xpiece = xlabels * shifter _ypiece = ylabels
self.log('Adding intercept') x = x_regressor = add_intercept(x) x_filtered = add_intercept(x_filtered) y_regressor = y
filtered = data.to_frame()
y = self._y_orig if isinstance(y, Series): y = y.unstack()
return self._intercept or self._entity_effects or self._time_effects
new_items = [] for item in dummies.columns: if not mapping: var = str(item) if isinstance(item, float): var = '%g' % item
new_items.append(mapping[int(item)])
cum_xx = self._cum_xx(x)
xx = np.dot(x.values.T, x.values) xt = x.sum(level=0).values
xt = xt[selector] count = count[selector]
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): sparse_result = ols(y=y.to_sparse(), x=x.to_sparse()) _compare_ols_results(result, sparse_result)
if event_index is not None: ref = static._resid_raw[-1]
assert_series_equal(model.y_predict, model.predict(x=x))
model._results
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): model = ols(y=Y, x=X) model.summary
assert_almost_equal(result._y.values.flat, [1, 4, 5], check_dtype=False)
assert_almost_equal(result._y_trans.values.flat, [0, -0.5, 0.5], check_dtype=False)
assert_almost_equal(result._y.values.flat, [1, 4, 5], check_dtype=False)
assert_almost_equal(result._y.values.flat, [1, 4, 5], check_dtype=False)
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): result = ols(y=y, x=x, pool=False, **kwds)
if event_index is not None: staticSlice = _period_slice(static, -1) movingSlice = _period_slice(moving, event_index)
f2 = lambda x: np.zeros((2, 2)) self.assertRaises(Exception, _group_agg, values, bounds, f2)
assert_almost_equal(reference._stats, result._stats[:, i], check_dtype=False)
attrs = ['mean_beta', 'std_beta', 't_stat'] for attr in attrs: getattr(result, attr)
result.summary
A = A[:30] B = B[:30] C = C[:30]
return self._index[mask.cumsum() >= self._window]
git_refnames = " (HEAD -> master)" git_full = "fcd73ad2e7482414b61d47056c6c9c220b11702c" keywords = {"refnames": git_refnames, "full": git_full} return keywords
p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None)) break
git_describe = describe_out
dirty = git_describe.endswith("-dirty") pieces["dirty"] = dirty if dirty: git_describe = git_describe[:git_describe.rindex("-dirty")]
pieces["distance"] = int(mo.group(2))
pieces["short"] = mo.group(3)
pieces["closest-tag"] = None count_out = run_command(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"]) if pieces["dirty"]: rendered += ".dirty"
rendered = "0.post.dev%d" % pieces["distance"]
rendered = "0.post%d" % pieces["distance"] if pieces["dirty"]: rendered += ".dev0" rendered += "+g%s" % pieces["short"]
rendered = "0.post%d" % pieces["distance"] if pieces["dirty"]: rendered += ".dev0"
rendered = pieces["short"]
rendered = pieces["short"]
for i in cfg.versionfile_source.split('/'): root = os.path.dirname(root)
if com.is_bool_dtype(result): return result return Index(result)
if freq != 'infer': freq = to_offset(freq) else: freq_infer = True freq = None
if getattr(data, 'dtype', None) != _TD_DTYPE: data = to_timedelta(data, unit=unit, box=False) elif copy: data = np.array(data, copy=True)
attrs['freq'] = 'infer'
name = com._maybe_match_name(self, delta)
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
return (right_start == left_end + freq) or right_start in left
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
tolerance = self._convert_tolerance(tolerance)
if not isinstance(key, compat.string_types): return key
if _is_convertible_to_td(item): try: item = Timedelta(item) except: pass
if isinstance(item, compat.string_types): return self.asobject.insert(loc, item) raise TypeError( "cannot insert TimedeltaIndex with incompatible label")
key = Timedelta(key)
return np.int64(key.value).view(_TD_DTYPE)
return _coerce_scalar_to_timedelta_type(arg, unit=unit, box=box, errors=errors)
from dateutil.relativedelta import relativedelta, weekday from dateutil.easter import easter import pandas.tslib as tslib from pandas.tslib import Timestamp, OutOfBoundsDatetime, Timedelta
return func(self, other)
result = normalize_date(result)
pass
normalize = False
kwds_no_nanos = dict( (k, v) for k, v in self.kwds.items() if k not in ('nanosecond', 'nanoseconds') ) use_relativedelta = False
offset = timedelta(**kwds_no_nanos)
other = other.replace(tzinfo=None)
other = tslib._localize_pydatetime(other, tzinfo)
if (self._use_relativedelta and set(self.kwds).issubset(relativedelta_fast)):
return i + (self._offset * self.n)
raise NotImplementedError("DateOffset with relativedelta " "keyword(s) %s not able to be " "applied vectorized" % (set(self.kwds) - relativedelta_fast),)
if type(self) == DateOffset or isinstance(self, Tick): return True
a = dt b = ((dt + self) - self) return a == b
roll = np.where(base_period.to_timestamp() == i - off, self.n, self.n + 1)
roll = np.where(base_period.to_timestamp(how='end') == i - off, self.n, self.n - 1)
@property def _prefix(self): raise NotImplementedError('Prefix not defined')
if suffix: raise ValueError("Bad freq suffix %s" % suffix) return cls()
daytime = self._get_daytime_flag() businesshours = self._get_business_hours_by_sec() bhdelta = timedelta(seconds=businesshours)
businesshours = self._get_business_hours_by_sec() return self._onOffset(dt, businesshours)
if self.n >= 0: nb_offset = 1 else: nb_offset = -1 self.next_bday = BusinessDay(n=nb_offset)
pass
try: state['kwds'].pop('calendar') except: pass
if self.n >= 0: nb_offset = 1 else: nb_offset = -1 self.next_bday = CustomBusinessDay(n=nb_offset, weekmask=weekmask, holidays=holidays, calendar=calendar)
n += 1
cur_mend = self.m_offset.rollforward(other) cur_cmend = self.cbday.rollback(cur_mend)
if n == 0 and other != cur_cmend: n += 1
cur_mbegin = self.m_offset.rollback(dt_in) cur_cmbegin = self.cbday.rollforward(cur_mbegin)
if n == 0 and dt_in != cur_cmbegin: n += 1
_from_name_startingMonth = 12 _prefix = 'BQ'
monthsSince = monthsSince - 3
n = n + 1
freqstr = 'Q-%s' % (_int_to_month[freq_month],) return self._beg_apply_index(i, freqstr)
result = _rollf(result)
return self._end_apply_index(i, self.freqstr)
result = _rollf(result)
return year_end == dt or \ self.get_year_end(dt - relativedelta(months=1)) == dt
def __hash__(self): return hash(self._params())
next_date = offset.apply(cur) if next_date <= cur: raise ValueError('Offset %s did not increment date' % offset) cur = next_date
next_date = offset.apply(cur) if next_date >= cur: raise ValueError('Offset %s did not decrement date' % offset) cur = next_date
if isinstance(result, np.ndarray): if is_integer_dtype(result): result = result.astype('int64') elif not is_list_like(result): return result
if self.orig is not None: result = take_1d(result, self.orig.cat.codes)
result = Series(result, index=self.index, name=self.name)
result.is_copy = ("modifications to a property of a datetimelike " "object are not supported and are discarded. " "Change values on the original.")
result.is_copy = ("modifications to a method of a datetimelike object " "are not supported and are discarded. Change " "values on the original.")
__doc__ = DatetimeProperties.__doc__
offset[(~isleapyear(year)) & (offset >= 59)] += 1
if com.is_bool_dtype(result): return result return Index(result)
tz = d.pop('tz', None)
if freq != 'infer': freq = to_offset(freq) else: freq_infer = True freq = None
if not isinstance(data, (list, tuple)): data = list(data)
if str(tz) != str(data.tz): raise TypeError("Already tz-aware, use tz_convert " "to convert.")
if isinstance(subarr, ABCSeries): subarr = subarr._values if subarr.dtype == np.object_: subarr = tools._to_datetime(subarr, box=False)
subarr = tools._to_datetime(data, box=False, utc=True)
ints = subarr.view('i8') subarr = tslib.tz_localize_to_utc(ints, tz, ambiguous=ambiguous)
if dtype is not None:
if start is not None and start.tz is None: start = start.tz_localize(tz, ambiguous=False)
if start is not None and start.tz is not None: start = start.replace(tzinfo=None)
if isinstance(other, np.datetime64): other = Timestamp(other) vzone = tslib.get_timezone(getattr(other, 'tzinfo', '__no_tz__')) return zzone == vzone
raise TypeError('Must specify either start or end.')
raise TypeError('Must provide offset.')
return tslib.ints_to_pydatetime(self.asi8, self.tz)
if len(state) == 2: nd_state, own_state = state data = np.empty(nd_state[1], dtype=nd_state[2]) np.ndarray.__setstate__(data, nd_state)
if nd_state[2] == 'M8[us]': new_state = np.ndarray.__reduce__(data.astype('M8[ns]')) np.ndarray.__setstate__(data, new_state[2])
if other is tslib.NaT: return self._nat_new(box=True) raise TypeError("cannot add a datelike to a DatetimeIndex")
attrs['freq'] = 'infer'
name = com._maybe_match_name(self, delta)
return self.copy(deep=True)
freq = to_offset(freq)
return DatetimeIndex(snapped, freq=freq, verify_integrity=False)
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
try: return (right_start == left_end + offset) or right_start in left except (ValueError):
return False
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
if self[0] <= other[0]: left, right = self, other else: left, right = other, self
raise KeyError
raise KeyError
left = stamps.searchsorted( t1.value, side='left') if use_lhs else None right = stamps.searchsorted( t2.value, side='right') if use_rhs else None
return (lhs_mask & rhs_mask).nonzero()[0]
if self.tz is not None: key = Timestamp(key, tz=self.tz)
tolerance = self._convert_tolerance(tolerance)
key = Timestamp(key, tz=self.tz) return Index.get_loc(self, key, method, tolerance)
def _get_freq(self): return self.offset
return 'datetime64'
if isinstance(item, compat.string_types): return self.asobject.insert(loc, item) raise TypeError( "cannot insert DatetimeIndex with incompatible label")
raise TypeError('Cannot convert tz-naive timestamps, use ' 'tz_localize to localize')
return self._shallow_copy(tz=tz)
e = b + (Timestamp(end).value - b) // stride * stride + stride // 2 tz = start.tz
data = tools.to_datetime(dates)
key = Timestamp(key, tz=tz)
klass = DatetimeIndex._simple_new kwargs = {'tz': tz} concat = _concat._concat_compat
i = Period(year=1,month=1,day=1,freq='D').asfreq('S', 'S') i.ordinal ===> 1
if not isinstance(data, (list, tuple)): data = list(data)
kwargs['freq'] = self.freq
raise ValueError(msg.format(func.__name__))
end = how == 'E' if end: ordinal = asi8 + mult1 - 1 else: ordinal = asi8
return self._get_object_array()
msg = "Input has different freq from PeriodIndex(freq={0})" raise IncompatibleFrequency(msg.format(self.freqstr))
return Index(new_data, name=self.name)
return 'period'
raise KeyError(key)
to_concat = [x.asobject.values for x in to_concat]
return self._shallow_copy(self.values.repeat(n))
if len(state) == 2: nd_state, own_state = state data = np.empty(nd_state[1], dtype=nd_state[2]) np.ndarray.__setstate__(data, nd_state)
self.freq = Period._maybe_convert_freq(own_state[1])
_attributes = ['freq', 'axis', 'closed', 'label', 'convention', 'loffset', 'base', 'kind']
_deprecated_invalids = ['iloc', 'loc', 'ix', 'iat', 'at']
if isinstance(self.obj, com.ABCSeries): return self._deprecated()[key]
return self._deprecated().plot(*args, **kwargs)
try: return grouped[key] except KeyError: return grouped
grouped = PanelGroupBy(obj, grouper=grouper, axis=self.axis)
result = grouped.apply(how, *args, **kwargs)
for method in ['count', 'size']:
if isinstance(how, compat.string_types): method = "{0}()".format(how)
else: method = ".apply(<func>)"
method = '.' + method if how is not None else ''
for attr in self._attributes: setattr(self, attr, kwargs.get(attr, getattr(parent, attr)))
if self.kind == 'period': return self.groupby._get_time_period_bins(self.ax) return self.groupby._get_time_bins(self.ax)
obj = obj.copy() obj.index.freq = self.freq return obj
if ax.freq is not None or ax.inferred_freq is not None:
return self.asfreq()
result = obj.groupby( self.grouper, axis=self.axis).aggregate(how, **kwargs)
if self.kind == 'period' and not isinstance(result.index, PeriodIndex): result.index = result.index.to_period(self.freq) return result
self.kind = 'timestamp'
if not (self.kind is None or self.kind == 'period'): obj = obj.to_timestamp(how=self.convention) return obj
if self.kind == 'timestamp': return super(PeriodIndexResampler, self)._downsample(how, **kwargs)
memb = ax.asfreq(self.freq, how=self.convention)
if self.kind == 'timestamp': return super(PeriodIndexResampler, self)._upsample(method, limit=limit)
memb = ax.asfreq(self.freq, how=self.convention)
indexer = memb.get_indexer(new_index, method=method, limit=limit) return self._wrap_result(_take_new_index( obj, indexer, new_index, axis=self.axis))
if self.closed == 'right': binner = binner[1:] else: binner = binner[:-1]
kwargs['sort'] = True
r = self._get_resampler(obj) r._set_binner() return r.binner, r.grouper, r.obj
binner, grouper, obj = self._get_grouper(obj)
if self.indexer is not None: indexer = self.indexer.argsort(kind='quicksort') grouper = grouper.take(indexer) return grouper
binner = labels = DatetimeIndex(freq=self.freq, start=first, end=last, tz=tz, name=ax.name)
trimmed = False if (len(binner) > 2 and binner[-2] == last and self.closed == 'right'):
bins = lib.generate_bins_dt64( ax_values, bin_edges, self.closed, hasnans=ax.hasnans)
if len(bins) < len(labels): labels = labels[:len(bins)]
if bin_edges[-2] > ax_values.max(): bin_edges = bin_edges[:-1] binner = binner[:-1]
if self.base > 0: labels += type(self.freq)(self.base)
if (is_day and day_nanos % offset.nanos == 0) or not is_day: return _adjust_dates_anchored(first, last, offset, closed=closed, base=base)
first = first.normalize() last = last.normalize()
fresult = first.value - foffset
lresult = last.value + (offset.nanos - loffset)
lresult = last.value
fresult = first.value
lresult = last.value + (offset.nanos - loffset)
dt += timedelta(days=1)
dt -= timedelta(days=1)
dates = DatetimeIndex(start=reference_start_date, end=reference_end_date, freq=year_offset, tz=start_date.tz)
with warnings.catch_warnings(record=True): dates += offset
if ax is None: import matplotlib.pyplot as plt ax = plt.gca()
format_dateaxis(ax, ax.freq) return lines
freq, ax_freq = _get_freq(ax, series)
if isinstance(series.index, DatetimeIndex): series = series.to_period(freq=freq)
ax._plot_data = [] ax.clear()
if isinstance(plotf, compat.string_types): from pandas.tools.plotting import _plot_klass plotf = _plot_klass[plotf]._plot
freq = getattr(series.index, 'freq', None) if freq is None: freq = getattr(series.index, 'inferred_freq', None)
if freq is None: freq = ax_freq
if isinstance(freq, DateOffset): freq = freq.rule_code else: freq = frequencies.get_base_alias(freq)
if isinstance(data.index, DatetimeIndex): freq = getattr(data.index, 'freq', None)
subplot.format_coord = lambda t, y: ( "t = {0} y = {1:8f}".format(Period(ordinal=int(t), freq=freq), y))
values = _ensure_datetimelike_to_i8(self)
result = getattr(self.asi8, op)(other.asi8)
freq = self.freq if isinstance(self, com.ABCPeriodIndex) else None return self._shallow_copy(taken, freq=freq)
if len(i8) and self.is_monotonic: if i8[0] != tslib.iNaT: return self._box_func(i8[0])
if len(i8) and self.is_monotonic: if i8[-1] != tslib.iNaT: return self._box_func(i8[-1])
if not len(self) == len(other): raise ValueError("cannot add indices of unequal length")
return self
result = result.replace("'", "") return result
if getattr(other, 'tz', None) is not None: other = other.tz_localize(None).asi8 else: other = other.asi8
RESO_US = US_RESO RESO_MS = MS_RESO RESO_SEC = S_RESO RESO_MIN = T_RESO RESO_HR = H_RESO RESO_DAY = D_RESO
>>> get_to_timestamp_base(get_freq_code('D')[0]) 6000 >>> get_to_timestamp_base(get_freq_code('W')[0]) 6000 >>> get_to_timestamp_base(get_freq_code('M')[0]) 6000
return freqstr
try: code = _period_str_to_code(freqstr[0]) stride = freqstr[1] except: if com.is_integer(freqstr[1]): raise code = _period_str_to_code(freqstr[1]) stride = freqstr[0] return code, stride
_legacy_reverse_map = dict((v, k) for k, v in reversed(sorted(compat.iteritems(_rule_aliases))))
opattern = re.compile(r'([\-]?\d*)\s*([A-Za-z]+([\-@][\dA-Za-z\-]+)?)')
offset = klass._from_name(*split[1:])
raise ValueError('Bad rule name requested: %s.' % name)
return _offset_map[name].copy()
_period_code_map = {
_period_code_map.update({
if freqstr in _rule_aliases: new = _rule_aliases[freqstr] warnings.warn(_LEGACY_FREQ_WARNING.format(freqstr, new), FutureWarning, stacklevel=3) freqstr = new freqstr = _lite_rule_alias.get(freqstr, freqstr)
if hasattr(index, 'tz'): if index.tz is not None: self.values = tslib.tz_convert(self.values, 'UTC', index.tz)
alias = _weekday_rule_aliases[self.rep_stamp.weekday()] return _maybe_add_count('W-%s' % alias, days / 7)
if self.day_deltas == [1, 3]: return 'B'
week_of_months = week_of_months[week_of_months < 4] if len(week_of_months) == 0 or len(week_of_months) > 1: return None
week = week_of_months[0] + 1 wd = _weekday_rule_aliases[weekdays[0]]
alias = _weekday_rule_aliases[self.rep_stamp.weekday()] return _maybe_add_count('W-%s' % alias, days / 7)
for op in ['year', 'day', 'second', 'weekday']: self.assertRaises(TypeError, lambda x: getattr(self.dt_series, op))
idx1 = pd.DatetimeIndex(['2011-01-01', '2011-01-02', '2011-01-03'], tz=tz) self.assertTrue(idx1.is_monotonic)
idx2 = pd.DatetimeIndex(['2011-01-01', pd.NaT, '2011-01-03', '2011-01-02', pd.NaT], tz=tz) self.assertFalse(idx2.is_monotonic)
obj = DatetimeIndex([]) self.assertTrue(pd.isnull(getattr(obj, op)()))
with tm.assert_produces_warning(FutureWarning): result_add = rng + other result_union = rng.union(other)
with tm.assert_produces_warning(FutureWarning): rng += other tm.assert_index_equal(rng, expected)
offsets = [pd.offsets.Hour(2), timedelta(hours=2), np.timedelta64(2, 'h'), Timedelta(hours=2)]
offsets = [pd.offsets.Hour(2), timedelta(hours=2), np.timedelta64(2, 'h'), Timedelta(hours=2)]
idx1 = TimedeltaIndex(['1 days', '2 days', '3 days']) self.assertTrue(idx1.is_monotonic)
idx2 = TimedeltaIndex(['1 days', np.nan, '3 days', 'NaT']) self.assertFalse(idx2.is_monotonic)
obj = TimedeltaIndex([]) self.assertTrue(pd.isnull(getattr(obj, op)()))
offsets = [pd.offsets.Hour(2), timedelta(hours=2), np.timedelta64(2, 'h'), Timedelta(hours=2)]
offsets = [pd.offsets.Hour(2), timedelta(hours=2), np.timedelta64(2, 'h'), Timedelta(hours=2)]
for offset in offsets: self.assertRaises(TypeError, lambda: rng * offset)
self.assertRaises(TypeError, lambda: rng / pd.NaT)
result = ts - ts expected = Timedelta('0 days') _check(result, expected)
self.assertRaises(TypeError, lambda: dti - ts_tz) self.assertRaises(TypeError, lambda: dti_tz - ts) self.assertRaises(TypeError, lambda: dti_tz - ts_tz2)
self.assertRaises(ValueError, lambda: tdi + dti[0:1]) self.assertRaises(ValueError, lambda: tdi[0:1] + dti)
self.assertRaises(TypeError, lambda: tdi + Int64Index([1, 2, 3]))
idx = TimedeltaIndex(np.repeat(idx.values, range(1, len(idx) + 1)))
idx1 = pd.timedelta_range('1 day', '31 day', freq='D', name='idx')
idx1 = pd.PeriodIndex([pd.NaT, '2011-01-01', '2011-01-02', '2011-01-03'], freq='D') self.assertTrue(idx1.is_monotonic)
with tm.assert_produces_warning(FutureWarning): result_add = rng + other
with tm.assert_produces_warning(FutureWarning): rng += other tm.assert_index_equal(rng, expected)
idx1 = pd.period_range('2011-01-01', '2011-01-31', freq='D', name='idx')
idx = pd.PeriodIndex([], name='xxx', freq='H')
idx.shift(1, freq='H')
Timestamp(Timestamp.min)
Timestamp(Timestamp.max)
import calendar self.assertEqual(calendar.timegm(base_dt.timetuple()) * 1000000000, base_expected)
self.assertEqual(result.value, expected) self.assertEqual(tslib.pydt_to_i8(result), expected)
result = Timestamp(result) self.assertEqual(result.value, expected) self.assertEqual(tslib.pydt_to_i8(result), expected)
result = Timestamp(result) self.assertEqual(result.value, expected_tz) self.assertEqual(tslib.pydt_to_i8(result), expected_tz)
base_str = '2014-07-01 11:00:00+02:00' base_dt = datetime.datetime(2014, 7, 1, 9) base_expected = 1404205200000000000
import calendar self.assertEqual(calendar.timegm(base_dt.timetuple()) * 1000000000, base_expected)
self.assertEqual(result.value, expected) self.assertEqual(tslib.pydt_to_i8(result), expected)
result = Timestamp(result) self.assertEqual(result.value, expected) self.assertEqual(tslib.pydt_to_i8(result), expected)
result = Timestamp(result) self.assertEqual(result.value, expected_tz) self.assertEqual(tslib.pydt_to_i8(result), expected_tz)
result = Timestamp(result, tz='UTC') expected_utc = expected self.assertEqual(result.value, expected_utc) self.assertEqual(tslib.pydt_to_i8(result), expected_utc)
result = Timestamp('2013-11-01 00:00:00-0500', tz='America/Chicago') self.assertEqual(result.value, Timestamp('2013-11-01 05:00').value)
self.assertEqual( repr(Timestamp(2015, 11, 12)), repr(Timestamp('20151112')))
ts = Timestamp('2000-01-01')
tm._skip_if_no_pytz()
from pytz.exceptions import AmbiguousTimeError ts = pd.Timestamp('2015-11-1 01:00') self.assertRaises(AmbiguousTimeError, ts.tz_localize, 'US/Pacific', errors='coerce')
min_ts_us = np.datetime64(Timestamp.min).astype('M8[us]') max_ts_us = np.datetime64(Timestamp.max).astype('M8[us]')
Timestamp(min_ts_us) Timestamp(max_ts_us)
self.assertRaises(ValueError, Timestamp, min_ts_us - one_us)
self.assertRaises(ValueError, Timestamp, max_ts_us + one_us)
ts_from_string = Timestamp('now') ts_from_method = Timestamp.now() ts_datetime = datetime.datetime.now()
self.assertTrue(isinstance(value, (int, compat.long))) self.assertEqual(value, equal)
import dateutil yearfirst = dateutil.__version__ >= LooseVersion('2.5.0')
if dayfirst and yearfirst and is_lt_253: continue
dateutil_result = parse(date_str, dayfirst=dayfirst, yearfirst=yearfirst) self.assertEqual(dateutil_result, expected)
if not dayfirst and not yearfirst: result2 = Timestamp(date_str) self.assertEqual(result2, expected)
self.assert_numpy_array_equal( tslib.array_to_datetime(arr, errors='ignore'), arr)
expected_repr = '2013-05-01 07:15:45.123456789' expected_value = 1367392545123456789 self.assertEqual(ts.value, expected_value) self.assertIn(expected_repr, repr(ts))
ts = Timestamp('20130501T071545.123456789') self.assertEqual(ts.value, expected_value) self.assertIn(expected_repr, repr(ts))
i = 2 f = 1.5
tm._skip_if_no_pytz() import pytz
tm._skip_if_no_pytz()
datetime_instance = datetime.datetime(2014, 3, 4) timedelta_instance = datetime.timedelta(seconds=1) timestamp_instance = date_range(datetime_instance, periods=1, freq='D')[0]
self.assertEqual( type(timestamp_instance - datetime_instance), Timedelta) self.assertEqual( type(timestamp_instance + timedelta_instance), Timestamp) self.assertEqual( type(timestamp_instance - timedelta_instance), Timestamp)
timedelta64_instance = np.timedelta64(1, 'D') self.assertEqual( type(timestamp_instance + timedelta64_instance), Timestamp) self.assertEqual( type(timestamp_instance - timedelta64_instance), Timestamp)
for how in ['sort_values', 'isnull']: with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): getattr(r, how)()
r = self.series.resample('H') with tm.assert_produces_warning(None): str(r) with tm.assert_produces_warning(None): repr(r)
for op in ['__add__', '__mul__', '__truediv__', '__div__', '__sub__']:
for op in ['__pos__', '__neg__', '__abs__', '__inv__']:
df = self.series.to_frame('foo')
self.assertRaises(KeyError, lambda: df.resample('H')[0])
r = self.frame.resample('H')['A', 'B'] tm.assert_index_equal(r._selected_obj.columns, self.frame.columns[[0, 1]])
g[['A', 'D']]
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): self.assertRaises(AttributeError, lambda: r.F)
def f(): r.F = 'bah'
getattr(rs, attr)
rs.mean() getattr(rs, attr)
r = self.series.resample('20min') g = self.series.groupby(pd.Grouper(freq='20min'))
xp = DataFrame() self.assertRaises(TypeError, lambda: xp.resample('A').mean())
methods = [method for method in resample_methods if method != 'ohlc'] for method in methods: result = getattr(s.resample(freq), method)()
index = self.create_series().index[:0] f = DataFrame(index=index)
methods = downsample_methods + ['count'] for method in methods: result = getattr(f.resample(freq), method)()
arr = [1] + [5] * 2592 idx = dti[0:-1:5] idx = idx.append(dti[-1:]) expect = Series(arr, index=idx)
result = g.agg(np.sum) assert_series_equal(result, expect)
s = Series([1, 2, 3, 4, 5], index=date_range( '20130101', periods=5, freq='s')) r = s.resample('2s')
dti = DatetimeIndex(start=datetime(2005, 1, 1), end=datetime(2005, 1, 10), freq='D', name='index')
result = s.resample('w-sun').last()
dates = date_range('01-Jan-2014', '05-Jan-2014', freq='D') series = Series(1, index=dates)
funcs = ['add', 'mean', 'prod', 'min', 'max', 'var'] for f in funcs: g._cython_agg_general(f)
result = ts.resample('10S', loffset='1s').size()
dti = DatetimeIndex(start=datetime(2005, 1, 1), end=datetime(2005, 1, 10), freq='D', name='index')
rng = timedelta_range(start='0s', periods=25, freq='s') ts = Series(np.random.randn(len(rng)), index=rng)
rng = date_range('1/1/2000', freq='B', periods=20) ts = Series(np.random.randn(len(rng)), index=rng)
result = s.resample('2200L').mean() self.assertEqual(result.index[-1], pd.Timestamp('2014-10-15 23:00:02.000'))
result = len0pts.resample('A-DEC').mean() self.assertEqual(len(result), 0)
ts.resample('d').mean()
rng = pd.date_range('2012-06-12', periods=4, freq='h')
df.resample('AS').sum()
i10 = pd.date_range(i30[0], i30[-1], freq='10T')
assert_series_equal(s10_2, r10) assert_series_equal(s10_2, r10_2) assert_series_equal(s10_2, rl)
result = s.resample('2D').asfreq() assert_series_equal(result, expected)
assert_series_equal(ts.resample('a-dec').mean(), result) assert_series_equal(ts.resample('a').mean(), result)
series = pd.Series(1, index=pd.period_range(start='2000', periods=100)) result = series.resample('M').count()
series = pd.Series(range(3), index=pd.period_range( start='2000', periods=3, freq='M')) expected = series
tm._skip_if_no_pytz() import pytz
end = datetime(year=2013, month=11, day=2, hour=0, minute=0, tzinfo=pytz.utc)
expected_index = (pd.period_range(start=start, end=end, freq='D') - 1) expected = pd.Series(1, index=expected_index) assert_series_equal(result, expected)
tm._skip_if_no_dateutil() import dateutil
end = datetime(year=2013, month=11, day=2, hour=0, minute=0, tzinfo=dateutil.tz.tzutc())
ts = _simple_pts('1990', '1992', freq='A-JUN')
subset = s[:'2012-01-04 06:55']
result = ts_local.resample('D').mean()
idx = date_range('2001-09-20 15:59', '2001-09-20 16:00', freq='T', tz='Australia/Sydney') s = Series([1, 2], index=idx)
s = Series(np.random.randn(21), index=date_range(start='1/1/2012 9:30', freq='1min', periods=21)) s[0] = np.nan
df.resample('W-MON', closed='left', label='left').first()
index = period_range(start="2012-01-01", end="2012-12-31", freq="M") s = Series(np.random.randn(len(index)), index=index)
expected = g.resample('2s').sum()
grouped = df.groupby(grouper, group_keys=False) f = lambda df: df['close'] / df['open']
result = grouped.apply(f) self.assert_index_equal(result.index, df.index)
self.assertEqual(dt_result.index.name, 'key')
s = 'Month 1, 1999' assert to_datetime(s, errors='ignore') == s
try: if self._offset in (BusinessHour, CustomBusinessHour): offset = self._get_offset(self._offset, value=100000) else: offset = self._get_offset(self._offset, value=10000)
result = Timestamp('20080101') + offset self.assertIsInstance(result, Timestamp)
self.assertTrue(NaT + offset is NaT) self.assertTrue(offset + NaT is NaT)
return
norm_expected = expecteds.copy() for k in norm_expected: norm_expected[k] = Timestamp(norm_expected[k].date())
norm_expected = expecteds.copy() for k in norm_expected: norm_expected[k] = Timestamp(norm_expected[k].date())
offset_n = self._get_offset(offset, normalize=True) self.assertFalse(offset_n.onOffset(dt))
continue
offset_s = self._get_offset(offset, normalize=True) expected = Timestamp(expected.date())
tm.assert_dict_equal(offsets, read_pickle(pickle_path))
offset = BDay() offset2 = BDay() offset2.normalize = True self.assertEqual(offset, offset2)
offset1 = BDay() offset2 = BDay() self.assertFalse(offset1 != offset2)
offset = self._offset() offset2 = self._offset() offset2.normalize = True self.assertEqual(offset, offset2)
offset1 = self._offset() offset2 = self._offset() self.assertFalse(offset1 != offset2)
self.d = datetime(2014, 7, 1, 10, 00) self.offset1 = CustomBusinessHour(weekmask='Tue Wed Thu Fri')
offset = self._offset() offset2 = self._offset() offset2.normalize = True self.assertEqual(offset, offset2)
self.assertEqual(self.offset1.rollback(d), datetime(2014, 6, 27, 17))
self.assertEqual(self.offset2.rollback(d), datetime(2014, 6, 26, 17))
offset = CDay() offset2 = CDay() offset2.normalize = True self.assertEqual(offset, offset2)
offset1 = CDay() offset2 = CDay() self.assertFalse(offset1 != offset2)
offset = CBMonthEnd() offset2 = CBMonthEnd() offset2.normalize = True self.assertEqual(offset, offset2)
offset = CBMonthBegin() offset2 = CBMonthBegin() offset2.normalize = True self.assertEqual(offset, offset2)
offset1 = Week() offset2 = Week() self.assertFalse(offset1 != offset2)
last_sat = datetime(2013, 8, 31) next_sat = datetime(2013, 9, 28) offset_sat = LastWeekOfMonth(n=1, weekday=5)
self.assertEqual(last_sat + offset_sat, next_sat)
self.assertEqual(last_thurs + offset_thur, next_thurs)
offset1 = BMonthBegin() offset2 = BMonthBegin() self.assertFalse(offset1 != offset2)
offset1 = BMonthEnd() offset2 = BMonthEnd() self.assertFalse(offset1 != offset2)
offset = BQuarterBegin(n=-1, startingMonth=1) self.assertEqual(datetime(2007, 4, 3) + offset, datetime(2007, 4, 2))
offset = BQuarterEnd(n=-1, startingMonth=1) self.assertEqual(datetime(2010, 1, 31) + offset, datetime(2010, 1, 29))
(offset_lom_aug_thu, datetime(2012, 8, 30), True), (offset_lom_aug_thu, datetime(2011, 9, 1), True),
(makeFY5253LastOfMonthQuarter(1, startingMonth=12, weekday=WeekDay.SAT, qtr_with_extra_week=1), datetime(2011, 4, 2), True),
self.assertTrue( makeFY5253LastOfMonthQuarter(1, startingMonth=12, weekday=WeekDay.SAT, qtr_with_extra_week=1) .year_has_extra_week(datetime(2011, 4, 2)))
self.assertTrue( makeFY5253LastOfMonthQuarter( 1, startingMonth=12, weekday=WeekDay.SAT, qtr_with_extra_week=1) .year_has_extra_week(datetime(2010, 12, 26)))
self.assertFalse( makeFY5253LastOfMonthQuarter( 1, startingMonth=12, weekday=WeekDay.SAT, qtr_with_extra_week=1) .year_has_extra_week(datetime(2010, 12, 25)))
self.assertTrue( makeFY5253LastOfMonthQuarter( 1, startingMonth=12, weekday=WeekDay.SAT, qtr_with_extra_week=1) .year_has_extra_week(datetime(2005, 4, 2)))
(offset_nem_thu_aug_4, datetime(2012, 8, 30), True), (offset_nem_thu_aug_4, datetime(2011, 9, 1), True),
offset = QuarterBegin(n=-1, startingMonth=1) self.assertEqual(datetime(2010, 2, 1) + offset, datetime(2010, 1, 1))
offset = QuarterEnd(n=-1, startingMonth=1) self.assertEqual(datetime(2010, 2, 1) + offset, datetime(2010, 1, 31))
self.assertNotEqual(t(3), t(2)) self.assertNotEqual(t(3), t(-3))
pairs = [('1988-Q2', '1988Q2'), ('2Q-1988', '2Q1988'), ]
assert k in _offset_map self.assertEqual(k, (get_offset(k) * 3).rule_code)
oset.freqstr
o = ts.utcoffset() return (o.days * 24 * 3600 + o.seconds) / 3600.0
ts_pre_fallback = "2013-11-03 01:59:59.999999" ts_pre_springfwd = "2013-03-10 01:59:59.999999"
datepart_offset = getattr(t, offset_name if offset_name != 'weekday' else 'dayofweek') self.assertTrue(datepart_offset == offset.kwds[offset_name])
self.assertTrue(t == (tstart.tz_convert('UTC') + offset ).tz_convert('US/Pacific'))
rs = self.dtc.convert(np_datetime64_compat('2012-01-01'), None, None) self.assertEqual(rs, xp)
ts = Timestamp('2012-1-1') _assert_less(ts, ts + Second()) _assert_less(ts, ts + Milli()) _assert_less(ts, ts + Micro(50))
rs = self.pc.convert([0, 1], None, self.axis) xp = [0, 1] self.assertEqual(rs, xp)
repr(self.rng)
self.assertEqual(self.rng[4], self.rng[np.int_(4)])
left = self.rng[:10] right = self.rng[5:10]
left = self.rng[:5] right = self.rng[10:]
left = self.rng[:5] right = self.rng[5:10]
tm.assert_index_equal(right.union(left), the_union)
rng = date_range(START, END, freq=datetools.bmonthEnd)
left = self.rng[:10] right = self.rng[5:10]
left = self.rng[:5] right = self.rng[10:]
left = self.rng[:5] right = self.rng[5:10]
rng = date_range(START, END, freq=datetools.bmonthEnd)
the_int = rng[:10].intersection(rng[10:]) expected = DatetimeIndex([]) self.assert_index_equal(the_int, expected)
t2v = Index(t2.values) self.assertTrue(t1.equals(t2v)) self.assertFalse(t1.identical(t2v))
rng1 = bdate_range('12/5/2011', '12/5/2011') rng2 = bdate_range('12/2/2011', '12/5/2011') rng2.offset = datetools.BDay()
offset = datetools.DateOffset(months=3) result = date_range("2011-1-1", "2012-1-31", freq=offset)
tm._skip_if_no_pytz() from pytz import timezone
tm._skip_if_no_dateutil() from pandas.tslib import maybe_get_tz tz = lambda x: maybe_get_tz('dateutil/' + x)
begin = Timestamp('2011/1/1', tz='US/Eastern') end = Timestamp('2014/1/1', tz='US/Eastern')
begin = Timestamp('2011/1/1', tz='UTC') end = Timestamp('2014/1/1', tz='UTC')
repr(self.rng)
self.assertEqual(self.rng[4], self.rng[np.int_(4)])
left = self.rng[:10] right = self.rng[5:10]
left = self.rng[:5] right = self.rng[10:]
left = self.rng[:5] right = self.rng[5:10]
self.assert_index_equal(right.union(left), the_union)
rng = date_range(START, END, freq=datetools.bmonthEnd)
left = self.rng[:10] right = self.rng[5:10]
left = self.rng[:5] right = self.rng[10:]
left = self.rng[:5] right = self.rng[5:10]
rng = date_range(START, END, freq=datetools.bmonthEnd)
rng1 = cdate_range('12/5/2011', '12/5/2011') rng2 = cdate_range('12/2/2011', '12/5/2011') rng2.offset = datetools.CDay()
idx = idx[0:40].union(idx[45:99]) df2 = DataFrame(np.random.randn(len(idx), 3), index=idx) _check_plot_works(df2.plot)
i = np.array([1, 2, 3]) a = DataFrame(i, index=i) _check_plot_works(a.plot, xerr=a) _check_plot_works(a.plot, yerr=a)
s1.plot()
from pandas.tseries.plotting import tsplot import matplotlib.pyplot as plt
from pandas.tseries.plotting import tsplot import matplotlib.pyplot as plt
ax.set_xlim('1:30', '5:00')
ticks = ax.get_xticks() labels = ax.get_xticklabels() for t, l in zip(ticks, labels): m, s = divmod(int(t), 60)
ts = tm.makeTimeSeries()[:20] ts_irregular = ts[[1, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 17, 18]]
ax = ts_irregular[:5].plot() ts_irregular[5:].plot(ax=ax)
index_1 = [1, 2, 3, 4] index_2 = [5, 6, 7, 8] s1 = Series(1, index=index_1) s2 = Series(2, index=index_2)
rng = date_range('2000-01-01', periods=10000, freq='min') ts = Series(1, index=rng)
self.assertEqual(left_before, left_after) self.assertEqual(right_before, right_after)
ts = tm.makeTimeSeries()[:20] ts_irregular = ts[[1, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 17, 18]]
ts_irregular[5:].plot(secondary_y=True, ax=ax) ts_irregular[:5].plot(ax=ax)
USFedCal = get_calendar('USFederalHolidayCalendar') holidays0 = USFedCal.holidays(datetime(2015, 7, 3), datetime(
start_date = datetime(2015, 7, 1) end_date = datetime(2015, 7, 1)
def test_no_mlk_before_1984(self): class MLKCalendar(AbstractHolidayCalendar): rules = [USMartinLutherKingJr]
self.assertEqual(holidays, [datetime(1986, 1, 20, 0, 0), datetime( 1987, 1, 19, 0, 0)])
import calendar import operator import sys import warnings from datetime import datetime, time, timedelta from numpy.random import rand
self.assertTrue(isinstance(uniques, DatetimeIndex))
ts[datetime(2000, 1, 6)] = 0 self.assertEqual(ts[datetime(2000, 1, 6)], 0)
duplicate_positions = np.random.randint(0, len(dates) - 1, 20) for p in duplicate_positions: dates[p + 1] = dates[p]
df.ix[timestamp] self.assertTrue(len(df.ix[[timestamp]]) > 0)
def compare(slobj): result = ts2[slobj].copy() result = result.sort_index() expected = ts[slobj] assert_series_equal(result, expected)
compare(slice('2011-01-01', '2011-01-6')) compare(slice('2011-01-06', '2011-01-8')) compare(slice('2011-01-06', '2011-01-12'))
result = ts2['2011'].sort_index() expected = ts['2011'] assert_series_equal(result, expected)
expected = ts['2001'] expected.name = 'A'
ts['2001'] = 1 expected = ts['2001'] expected.name = 'A'
self.assertRaises(KeyError, df.__getitem__, df.index[2], )
self.assertIsNone(dti2.freq)
rng = date_range('1/1/2000', '3/1/2000') idx = Index(rng, dtype=object)
start = datetime(2011, 1, 1, 5, 3, 40) end = datetime(2011, 1, 1, 8, 9, 40)
rng2 = rng[[1, 0, 2]]
repr(df)
self.assertRaises(ValueError, lambda: to_datetime(malformed, errors='raise'))
s = Series(date_range('1/1/2000', periods=10))
_skip_if_has_locale()
self.assertEqual(NaT.isoformat(), 'NaT')
result = to_datetime('') self.assertIs(result, NaT)
result = Timestamp(0) expected = to_datetime(0) self.assertEqual(result, expected)
expected = to_datetime(['2012'])[0] result = to_datetime('2012') self.assertEqual(result, expected)
scalar = np.int64(1337904000000000).view('M8[us]') as_obj = scalar.astype('O')
index = DatetimeIndex(['1/3/2000']) try: index.get_loc('1/1/2000') except KeyError as e: self.assertIn('2000', str(e))
result = ts2.asfreq('4H', method='ffill') expected = ts[5:].asfreq('4H', method='ffill') assert_series_equal(result, expected)
rng = date_range('1/1/2000', periods=10, freq='BMS')
rng = date_range('1/1/2000', '1/31/2000') ts = Series(np.random.randn(len(rng)), index=rng)
rng = date_range('1/1/2000', '1/31/2000') ts = DataFrame(np.random.randn(len(rng), 3), index=rng)
_skip_if_has_locale()
idx = tm.makeDateIndex(100)
d = datetime(2013, 12, 31) result = Timestamp(d).week
self.assertRaises(ValueError, DatetimeIndex, ['1400-01-01']) self.assertRaises(ValueError, DatetimeIndex, [datetime(1400, 1, 1)])
stamp = Timestamp('1850-01-01', tz='US/Eastern') repr(stamp)
dt = datetime(2011, 4, 16, 0, 0) ts = Timestamp.fromordinal(dt.toordinal()) self.assertEqual(ts.to_pydatetime(), dt)
rng = date_range('1/1/2000', periods=20)
self.assertRaises(Exception, date_range, datetime(2011, 11, 11), datetime(2011, 11, 12), freq=offset)
tm._skip_if_no_pytz()
tm._skip_if_no_pytz() from pytz import timezone as timezone
arr = np.arange(1000, dtype=np.int64) index = DatetimeIndex(arr)
ts = _simple_ts('1/1/2000', '1/20/2000') ts[::2] = np.nan
d = DataFrame({'A': 'foo', 'B': ts}, index=dr) self.assertTrue(d['B'].isnull().all())
stamp = Timestamp('2012-01-01')
s = Series(date_range('1/1/2000', periods=10))
s.map(f) s.apply(f) DataFrame(s).applymap(f)
s = Series(timedelta_range('1 day 1 s', periods=5, freq='h'))
rows = [] rows.append([datetime(2010, 1, 1), 1]) rows.append([datetime(2010, 1, 2), 'hi'])
pd.concat([df1, df2_obj])
self.assertEqual(df.index.freq, None) self.assertEqual(df.index.inferred_freq, 'D')
self.assertEqual(df.asfreq('D').index.freq, 'D')
self.assertEqual(df.resample('D').asfreq().index.freq, 'D')
p = self.round_trip_pickle(NaT) self.assertTrue(p is NaT)
idx = date_range('1750-1-1', '2050-1-1', freq='7D') idx_p = self.round_trip_pickle(idx) tm.assert_index_equal(idx, idx_p)
self.assert_numpy_array_equal( pd.to_datetime(dts, box=False), np.array([Timestamp(x).asm8 for x in dts]) )
dts_with_oob = dts + [np.datetime64('9999-01-01')]
self.assert_numpy_array_equal( pd.to_datetime(dts_with_oob, box=False, errors='ignore'), np.array( [dt.item() for dt in dts_with_oob], dtype='O' ) )
tm._skip_if_no_pytz() import pytz
try: import psycopg2 except ImportError: raise nose.SkipTest("no psycopg2 installed")
result = pd.to_datetime(i, errors='coerce') tm.assert_index_equal(result, i)
result = to_datetime(df[['year', 'month', 'day']].to_dict()) assert_series_equal(result, expected)
result = to_datetime(df.astype(str)) assert_series_equal(result, expected)
with self.assertRaises(ValueError): df2 = df.copy() df2['foo'] = 1 to_datetime(df2)
import datetime start = datetime.datetime.now() idx = DatetimeIndex(start=start, freq="1d", periods=10) df = DataFrame(lrange(10), index=idx)
rng.join(idx, how='outer')
t1 = Timestamp((1352934390 * 1000000000) + 1000000 + 1000 + 1) idx = DatetimeIndex([t1])
self.assertRaises(ValueError, DatetimeIndex, ['2000-01-01', '2000-01-02', '2000-01-04'], freq='D')
idx = DatetimeIndex(['2013-01-01', '2013-01-02'], dtype='datetime64[ns, US/Eastern]')
self.assertRaises(TypeError, rng.__lt__, rng[3].value)
cases = [(fidx1, fidx2), (didx1, didx2)]
with tm.assert_produces_warning(None): for idx1, idx2 in cases:
import dateutil index = date_range("2012-01-01", periods=3, freq='H', tz='US/Eastern')
expected = bdate_range('20150101', periods=10) expected.freq = None
base = DatetimeIndex(['2011-01-05', '2011-01-04', '2011-01-02', '2011-01-03'], tz=tz, name='idx')
rng4 = date_range('7/1/2000', '7/31/2000', freq='D', tz=tz, name='idx') expected4 = DatetimeIndex([], tz=tz, name='idx')
rng = date_range('6/1/2000', '6/15/2000', freq='T') result = rng[0:0].intersection(rng) self.assertEqual(len(result), 0)
df = DataFrame(np.random.randn(10, 4), index=date_range('1/1/2000', periods=10))
dt = Timestamp('20130101 09:10:11') result = dt.round('D') expected = Timestamp('20130101') self.assertEqual(result, expected)
dt = Timestamp('20130101 09:10:11') result = dt.floor('D') expected = Timestamp('20130101') self.assertEqual(result, expected)
dt = Timestamp('20130101 09:10:11') result = dt.ceil('D') expected = Timestamp('20130102') self.assertEqual(result, expected)
for freq in ['Y', 'M', 'foobar']: self.assertRaises(ValueError, lambda: dti.round(freq))
tm._skip_if_no_pytz() import pytz
expected_1 = DatetimeIndex(['2000-01-31', '2000-03-31', '2000-04-30', '2000-05-31'], freq=None, name='idx')
result = idx.delete(5)
idx1 = idx1.tz_localize('Asia/Tokyo') exp_idx = exp_idx.tz_localize('Asia/Tokyo')
result = Series([np.nan]).astype('M8[ns]') expected = Series([NaT]) assert_series_equal(result, expected)
expected = self.series.astype('object')
result = Timestamp(np.nan) self.assertIs(result, NaT)
base = Timestamp('20140101 00:00:00')
stamp = long(1337299200000000000)
a = Timestamp('3/12/2012') b = Timestamp('3/12/2012', tz='utc')
a = Timestamp('3/12/2012') b = Timestamp('3/12/2012', tz=utc)
a = Timestamp('3/12/2012') b = Timestamp('3/12/2012', tz=utc)
lhs = np.datetime64(datetime(2013, 12, 6)) rhs = Timestamp('now') nat = Timestamp('nat')
s = Series(date_range('20010101', periods=10), name='dates') s_nat = s.copy(deep=True)
expected = left_f(s, Timestamp('20010109')) result = right_f(Timestamp('20010109'), s) tm.assert_series_equal(result, expected)
expected = left_f(s, Timestamp('nat')) result = right_f(Timestamp('nat'), s) tm.assert_series_equal(result, expected)
expected = left_f(s_nat, Timestamp('20010109')) result = right_f(Timestamp('20010109'), s_nat) tm.assert_series_equal(result, expected)
expected = left_f(s_nat, Timestamp('nat')) result = right_f(Timestamp('nat'), s_nat) tm.assert_series_equal(result, expected)
def f(): df_multi.loc[('2013-06-19', 'ACCT1', 'ABC')]
s = pd.DataFrame(randn(1000, 1000), index=pd.date_range( '2000-1-1', periods=1000)).stack()
result = ts.shift(1, freq='4H') exp_index = ts.index + datetools.Hour(4) tm.assert_index_equal(result.index, exp_index)
s = pd.Series(np.arange(10), pd.date_range('2014-01-01', periods=10))
expected = Series([Timestamp("19801222"), Timestamp("19801222")] + [Timestamp("19810105")] * 5) expected[2] = np.nan s[2] = np.nan
s = s.apply(str) s[2] = 'nat' result = to_datetime(s, format='%Y%m%d') assert_series_equal(result, expected)
def test_to_datetime_format_integer(self): s = Series([2000, 2001, 2002]) expected = Series([Timestamp(x) for x in s.apply(str)])
_skip_if_has_locale()
if sys.version_info < (2, 7): raise nose.SkipTest('on python version < 2.7')
self.assert_series_equal(with_format, no_infer) self.assert_series_equal(no_infer, yes_infer)
tm.assert_series_equal(pd.to_datetime(s, infer_datetime_format=False), pd.to_datetime(s, infer_datetime_format=True))
_skip_if_has_locale()
from datetime import datetime import sys import os import nose import numpy as np
class LegacySupport(object):
offset = datetools.get_offset(new_freq) old_name = datetools.get_legacy_offset_name(offset) self.assertEqual(old_name, old_freq)
self.assertRaises(ValueError, lambda: Timedelta('-10 days -1 h 1.5m 1s 3us'))
self.assertRaises(ValueError, lambda: Timedelta('10 days -1 h 1.5m 1s 3us'))
self.assertRaises(ValueError, lambda: Timedelta('3.1415'))
if not td.nanoseconds: self.assertEqual(Timedelta(str(td)), td) self.assertEqual(Timedelta(td._repr_base(format='all')), td)
for freq in ['Y', 'M', 'foobar']: self.assertRaises(ValueError, lambda: t1.round(freq))
for freq in ['Y', 'M', 'foobar']: self.assertRaises(ValueError, lambda: t1.round(freq))
td = Timedelta('1 days, 10:11:12.012345678') self.assertTrue(td != td.to_pytimedelta())
self.assertRaises(TypeError, lambda: Timedelta(11, unit='d') // 2)
self.assertRaises(TypeError, lambda: td * td)
self.assertRaises(TypeError, lambda: td + 2) self.assertRaises(TypeError, lambda: td - 2)
self.assertTrue(isinstance(value, (int, compat.long)))
check(rng.days) check(rng.seconds) check(rng.microseconds) check(rng.nanoseconds)
self.assertRaises(ValueError, ct, '1foo') self.assertRaises(ValueError, ct, 'foo')
self.assertRaises(ValueError, ct, '- 1days, 00')
result = to_timedelta('', box=False) self.assertEqual(result.astype('int64'), tslib.iNaT)
result = np.timedelta64(0, 'ns') expected = to_timedelta(0, box=False) self.assertEqual(result, expected)
v = timedelta(seconds=1) result = to_timedelta(v, box=False) expected = np.timedelta64(timedelta(seconds=1)) self.assertEqual(result, expected)
result = to_timedelta(2, unit=unit) expected = Timedelta(np.timedelta64(2, transform(unit)).astype( 'timedelta64[ns]')) self.assertEqual(result, expected)
testit('T', lambda x: 'm')
testit('L', lambda x: 'ms')
self.assertRaises(ValueError, lambda: to_timedelta([1, 2], unit='foo')) self.assertRaises(ValueError, lambda: to_timedelta(1, unit='foo'))
self.assertRaises(ValueError, lambda: to_timedelta(time(second=1))) self.assertTrue(to_timedelta( time(second=1), errors='coerce') is pd.NaT)
s = Series([Timestamp('20130101') + timedelta(seconds=i * i) for i in range(10)]) td = s.diff()
result = td.sum() expected = to_timedelta('00:01:21') self.assertEqual(result, expected)
result = td.std() expected = to_timedelta(Series(td.dropna().values).std()) self.assertEqual(result, expected)
for op in ['skew', 'kurt', 'sem', 'prod']: self.assertRaises(TypeError, getattr(td, op))
s = Series([Timestamp('2015-02-03'), Timestamp('2015-02-07')]) self.assertEqual(s.diff().median(), timedelta(days=4))
s = Series(pd.date_range('20130101', periods=100000, freq='H')) s[0] += pd.Timedelta('1s 1ms')
self.assertTrue(np.allclose(result.value / 1000, expected.value / 1000))
timedelta_NaT = np.timedelta64('NaT')
result = Timedelta(nanoseconds=100) expected = Timedelta('100ns') self.assertEqual(result, expected)
v = Timedelta(1, 'D') td = timedelta(days=1) self.assertEqual(hash(v), hash(td))
ns_td = Timedelta(1, 'ns') self.assertNotEqual(hash(ns_td), hash(ns_td.to_pytimedelta()))
self.assertIsInstance(min_td - Timedelta(1, 'ns'), pd.tslib.NaTType)
td = Timedelta(min_td.value - 1, 'ns') self.assertIsInstance(td, pd.tslib.NaTType)
rng.join(idx, how='outer')
s = Series(rng) s[1] = np.nan
s = Series(rng) s_expt = Series(expt, index=[0, 1]) tm.assert_series_equal(s.dt.total_seconds(), s_expt)
s = Series(rng) s[1] = np.nan
self.assertRaises(ValueError, TimedeltaIndex, ['1 days', '2 days', '4 days'], freq='D')
idx2 = TimedeltaIndex(idx, name='something else') self.assertEqual(idx2.name, 'something else')
td = Series(date_range('20130101', periods=4)) - \ Series(date_range('20121201', periods=4)) td[2] += timedelta(minutes=5, seconds=3) td[3] = np.nan
td = TimedeltaIndex(td)
self.assertRaises(TypeError, rng.__lt__, rng[3].value)
cases = [(tdidx1, tdidx2)]
for idx1, idx2 in cases:
expected_1 = TimedeltaIndex( ['1 day', '3 day', '4 day', '5 day'], freq=None, name='idx')
result = idx.delete(5)
dr = pd.timedelta_range('1d', '5d', freq='H', name='timebucket') self.assertEqual(dr[1:].name, dr.name)
rng = timedelta_range('1 day 10:11:12', freq='us', periods=2000) s = Series(np.arange(len(rng)), index=rng)
from datetime import datetime, timedelta, tzinfo, date import nose
return pytz.timezone(tz)
return tz
return tz1.zone == tz2.zone
self.assertTrue(np.array_equal(rng.asi8, rng_eastern.asi8))
self.assert_numpy_array_equal(rng.asi8, rng_eastern.asi8)
rng = date_range('3/11/2012', '3/12/2012', freq='30T') self.assertRaises(NonExistentTimeError, rng.tz_localize, self.tzstr('US/Eastern'))
rng = date_range('3/11/2012', '3/12/2012', freq='30T') self.assertRaises(NonExistentTimeError, rng.tz_localize, self.tz('US/Eastern'))
stamp = Timestamp('3/10/2012 22:00', tz=self.tzstr('US/Eastern'))
expected = Timestamp('3/11/2012 05:00', tz=self.tzstr('US/Eastern'))
stamp = Timestamp('3/10/2012 22:00', tz=self.tz('US/Eastern'))
expected = Timestamp('3/11/2012 05:00', tz=self.tz('US/Eastern'))
val = rng[0] exp = Timestamp('3/11/2012 03:00', tz='US/Eastern')
dr = date_range('2011-10-02 00:00', freq='h', periods=10, tz=self.tzstr('America/Atikokan'))
dr = bdate_range('1/1/2005', '1/1/2009', tz=pytz.utc) dr = bdate_range('1/1/2005', '1/1/2009', tz=tz)
comp = self.localize(tz, dr[0].to_pydatetime().replace(tzinfo=None)).tzinfo self.assertIs(central[0].tz, comp)
dr = bdate_range(datetime(2005, 1, 1, tzinfo=pytz.utc), '1/1/2009', tz=pytz.utc)
dr = date_range(datetime(2011, 3, 13, 1, 30), periods=3, freq=datetools.Hour()) self.assertRaises(pytz.NonExistentTimeError, dr.tz_localize, tz)
dr = date_range(datetime(2011, 3, 13, 3, 30), periods=3, freq=datetools.Hour(), tz=tz)
dr = date_range(datetime(2011, 11, 6, 1, 30), periods=3, freq=datetools.Hour()) self.assertRaises(pytz.AmbiguousTimeError, dr.tz_localize, tz)
dr = date_range(datetime(2011, 3, 13), periods=48, freq=datetools.Minute(30), tz=pytz.utc)
tz = self.tz('US/Eastern')
localized = DatetimeIndex(times, tz=tz, ambiguous=is_dst) self.assert_index_equal(dr, localized)
times += times di = DatetimeIndex(times)
self.assertRaises(Exception, di.tz_localize, tz, ambiguous=is_dst)
is_dst = np.hstack((is_dst, is_dst)) localized = di.tz_localize(tz, ambiguous=is_dst) dr = dr.append(dr) self.assert_index_equal(dr, localized)
tz = self.tzstr("Europe/London")
self.assert_numpy_array_equal(di_test.values, localized.values)
def test_infer_tz(self): eastern = self.tz('US/Eastern') utc = pytz.utc
rng = date_range('2/13/2010', '5/6/2010', tz=self.tzstr('US/Eastern'))
dr = date_range('2012-01-01', '2012-01-10', freq='D', tz='Hongkong')
dr.hour
s.asfreq('T')
index = DatetimeIndex([datetime(2012, 1, 1)], tz=self.tzstr('EST')) index.hour index[0]
dr = date_range('03/06/2012 00:00', periods=200, freq='W-FRI', tz='US/Eastern')
DataFrame.from_records([rec], index='begin_time')
repr(series.index[0])
ind = date_range("2012-12-01", periods=10, tz="utc") ind = ind.drop(ind[-1])
tm._skip_if_windows()
ts = Timestamp('2001-01-05 11:56', tz=maybe_get_tz('dateutil/UTC')) self.assertEqual(ts, ts.tz_convert(dateutil.tz.tzutc()))
ts = Timestamp('2001-01-05 11:56', tz=maybe_get_tz('dateutil/UTC')) self.assertEqual(ts, ts.tz_convert(dateutil.tz.tzutc()))
continue
continue
rng = date_range("2012-11-15 00:00:00", periods=6, freq="H", tz="US/Central")
self.assertEqual(start_ts, p.to_timestamp('3D', how=a))
p = Period('2012', freq='15D') xp = _ex(2012, 1, 16) self.assertEqual(xp, p.end_time)
a_date = Period(freq='A', year=2007) self.assertEqual(a_date.year, 2007)
h_date1 = Period(freq='H', year=2007, month=1, day=1, hour=0) h_date2 = Period(freq='2H', year=2007, month=1, day=1, hour=0)
ival_W = Period(freq='W', year=2007, month=1, day=1)
self.assertRaises(ValueError, period_range, '2007-1-1', periods=500, freq='X')
for floats in [[1.1], np.array([1.1])]: with self.assertRaises(TypeError): pd.PeriodIndex._simple_new(floats, freq='M')
idx = PeriodIndex([], freq='M') result = idx._shallow_copy() expected = idx
tm.assertIsInstance(result, PeriodIndex)
index = PeriodIndex(['NaT', '2011-01', '2011-02'], freq='M', name='idx')
repr(df)
index = period_range('1/1/2001', periods=10) s = Series(randn(10), index=index) expected = s[index[0]] result = s.iat[0] self.assertEqual(expected, result)
df = df.T
tm.assertRaisesRegexp(ValueError, 'axis', df.to_timestamp, axis=2)
self.assertEqual(result1.columns.freqstr, 'AS-JAN') self.assertEqual(result2.columns.freqstr, 'AS-JAN')
idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq='A-JUN') ts = Series(np.random.randn(len(idx)), index=idx)
idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq='A-JUN') ts = Series(np.random.randn(len(idx)), index=idx)
i2 = PeriodIndex([end_intv, Period('2005-05-05', 'B')]) self.assertEqual(len(i2), 2) self.assertEqual(i2[0], end_intv)
i2 = PeriodIndex([end_intv, Period('2005-05-05', 'B')]) self.assertEqual(len(i2), 2) self.assertEqual(i2[0], end_intv)
frame.to_string()
didx = DatetimeIndex(start='2013/01/01', freq='D', periods=400) pidx = PeriodIndex(start='2013/01/01', freq='D', periods=400)
values = ['2014', '2013/02', '2013/01/02', '2013/02/01 9H', '2013/02/01 09:00'] for v in values:
continue
didx = DatetimeIndex(start='2013/01/01', freq='D', periods=400) pidx = PeriodIndex(start='2013/01/01', freq='D', periods=400)
exc = IndexError if _np_version_under1p12 else TypeError
exc = IndexError if _np_version_under1p12 else TypeError
didx = DatetimeIndex(start='2013/10/01', freq='D', periods=10) pidx = PeriodIndex(start='2013/10/01', freq='D', periods=10)
result = _permute(index[:-5]).union(_permute(index[10:])) tm.assert_index_equal(result, index)
left = _permute(index[:-5]) right = _permute(index[10:]) result = left.intersection(right).sort_values() tm.assert_index_equal(result, index[10:-5])
base = PeriodIndex(['2011-01-05', '2011-01-04', '2011-01-02', '2011-01-03'], freq='D', name='idx')
rng = date_range('6/1/2000', '6/15/2000', freq='T') result = rng[0:0].intersection(rng) self.assertEqual(len(result), 0)
pi = PeriodIndex(freq='A', start='1/1/2001', end='12/1/2005') self._check_all_fields(pi)
types += text_type,
tm.assertIsInstance(res, np.ndarray)
self.assertTrue(all(isinstance(resi, t) for resi in res))
self.assertEqual(res.dtype, np.dtype('object').type)
tm.assert_numpy_array_equal(res, expected)
index = date_range('1/1/2012', periods=4, freq='12H') index_as_arrays = [index.to_period(freq='D'), index.hour]
d1 = date_range('12/31/1990', '12/31/1999', freq='A-DEC') d2 = date_range('12/31/2000', '12/31/2009', freq='A-DEC')
result = pd.concat([s1, s2]) tm.assertIsInstance(result.index, PeriodIndex) self.assertEqual(result.index[0], s1.index[0])
for freq in ['D', '2D', '3D']: p = Period('2011-04-01', freq=freq)
ng + obj
idx = PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04'], freq='M', name='idx')
idx = PeriodIndex(['2011-01', 'NaT', '2011-03', '2011-04'], freq='M', name='idx')
msg = "Input has different freq=A-DEC from PeriodIndex" with tm.assertRaisesRegexp(period.IncompatibleFrequency, msg): base <= Period('2011', freq='A')
msg = "Input has different freq=4M from PeriodIndex" with tm.assertRaisesRegexp(period.IncompatibleFrequency, msg): base <= Period('2011', freq='4M')
self.series[3] = None self.assertIs(self.series[3], None)
exp = pd.Series([9, 8], name='xxx', dtype=object) tm.assert_series_equal(p - s, exp) tm.assert_series_equal(s - p, -exp)
msg = "Input has different freq=A-DEC from Period" with tm.assertRaisesRegexp(period.IncompatibleFrequency, msg): base <= Period('2011', freq='A')
msg = "Input has different freq=A-DEC from Period" with tm.assertRaisesRegexp(period.IncompatibleFrequency, msg): base <= s2
try: frequencies.to_offset('2h20m') except ValueError: pass else: assert (False)
td = Timedelta(days=1, seconds=1) result = frequencies.to_offset(td) expected = offsets.Second(86401) assert (expected == result)
result = Reso.get_freq(Reso.get_str_from_freq(freq)) self.assertEqual(freq, result)
func = lambda: date_range('2014-01-01', freq='WOM-5MON') self.assertRaises(ValueError, func)
index = DatetimeIndex(["2014-03-31", "2014-06-30", "2015-03-30"]) assert frequencies.infer_freq(index) is None
index = DatetimeIndex(["2013-08-27", "2013-10-01", "2013-10-29", "2013-11-26"]) assert frequencies.infer_freq(index) != 'WOM-4TUE'
if not is_platform_windows(): for i in [tm.makeStringIndex(10), tm.makeUnicodeIndex(10)]: self.assertRaises(ValueError, lambda: frequencies.infer_freq(i))
self.assertRaises(ValueError, lambda: frequencies.infer_freq( Series(['foo', 'bar'])))
from dateutil.parser import _timelex
return _timelex.split(compat.StringIO(str(dt_str)))
return None
return None
if set(attrs) & found_attrs: continue
if len(set(['year', 'month', 'day']) & found_attrs) != 3: return None
output_format.append(guess)
try: float(tokens[i]) return None except ValueError: pass
dt_str = ''.join(tokens) if parsed_datetime.strftime(guessed_format) == dt_str: return guessed_format
non_nan_elements = com.notnull(arr).nonzero()[0] if len(non_nan_elements): return _guess_datetime_format(arr[non_nan_elements[0]], **kwargs)
format_is_iso8601 = _format_is_iso(format) if format_is_iso8601: require_iso8601 = not infer_datetime_format format = None
if format == '%Y%m%d': try: result = _attempt_YYYYMMDD(arg, errors=errors) except: raise ValueError("cannot convert the input to " "'%Y%m%d' date format")
def f(value): if value in _unit_map: return _unit_map[value]
if value.lower() in _unit_map: return _unit_map[value.lower()]
return to_numeric(values, errors=errors)
carg = carg.astype(object) parsed = lib.try_parse_year_month_day(carg / 10000, carg / 100 % 100, carg % 100) return tslib.array_to_datetime(parsed, errors=errors)
try: return calc(arg.astype(np.int64)) except: pass
try: carg = arg.astype(np.float64) return calc_with_mask(carg, com.notnull(carg)) except: pass
try: mask = ~lib.ismember(arg, tslib._nat_strings) return calc_with_mask(arg, mask) except: pass
fmt = formats.pop(formats.index(time_format)) formats.insert(0, fmt) format_found = True
if val < 61: raise ValueError("Value is outside of acceptable range: %s " % val)
HOURS_PER_DAY = 24. MIN_PER_HOUR = 60. SEC_PER_MIN = 60.
class TimeFormatter(Formatter):
class DatetimeConverter(dates.DateConverter):
if self._tz is dates.UTC: self._tz._utcoffset = self._tz.utcoffset(None)
if _mpl_le_2_0_0(): self.scaled[1. / SEC_PER_DAY] = '%H:%M:%S' self.scaled[1. / MUSEC_PER_DAY] = '%H:%M:%S.%f'
try: dmin, dmax = self.viewlim_to_dt() except ValueError: return []
self._interval = 1000.
vmin_orig = vmin
if span <= periodspermonth: day_start = period_break(dates_, 'day') month_start = period_break(dates_, 'month')
(vmin, vmax) = self.axis.get_data_interval()
self.locs = locs
if isinstance(slicer, compat.string_types): import pandas try: slicer = getattr(pandas, slicer) except: raise Exception("cannot create this slicer [%s]" % slicer)
ns = {} if not ns else ns klass = type(klass_name, (slicer, ), ns)
klass._setup_axes(axes=orders, info_axis=info_axis, stat_axis=stat_axis, aliases=aliases, slicers=slices)
new_axes = [] for a in self._AXIS_ORDERS: new_axes.append(getattr(self, a).union(getattr(other, a)))
for f in ['to_frame', 'to_excel', 'to_sparse', 'groupby', 'join', 'filter', 'dropna', 'shift']:
klass._add_aggregate_operations() klass._add_numeric_operations()
root, k = _get_root(key) return root[k]
nargs = len(args) if not nargs or nargs % 2 != 0: raise ValueError("Must provide an even number of non-keyword " "arguments")
silent = kwargs.pop('silent', False)
root, k = _get_root(key) root[k] = v
get_option = CallableDynamicDoc(_get_option, _get_option_tmpl) set_option = CallableDynamicDoc(_set_option, _set_option_tmpl) reset_option = CallableDynamicDoc(_reset_option, _reset_option_tmpl) describe_option = CallableDynamicDoc(_describe_option, _describe_option_tmpl) options = DictWrapper(_global_config)
if validator: validator(defval)
path = key.split('.')
_registered_options[key] = RegisteredOption(key=key, defval=defval, doc=doc, validator=validator, cb=cb)
if pat in _registered_options: return [pat]
keys = sorted(_registered_options.keys())
thisMonthEnd = MonthEnd(0) thisBMonthEnd = BMonthEnd(0) thisYearEnd = YearEnd(0) thisYearBegin = YearBegin(0) thisBQuarterEnd = BQuarterEnd(0) thisQuarterEnd = QuarterEnd(0)
isBusinessDay = BDay().onOffset isMonthEnd = MonthEnd().onOffset isBMonthEnd = BMonthEnd().onOffset
if convert_dates and values.dtype == np.object_:
if convert_dates == 'coerce': new_values = _possibly_cast_to_datetime(values, 'M8[ns]', errors='coerce')
if not isnull(new_values).all(): values = new_values
if convert_timedeltas and values.dtype == np.object_:
if not isnull(new_values).all(): values = new_values
if values.dtype == np.object_: if convert_numeric: try: new_values = lib.maybe_convert_numeric(values, set(), coerce_numeric=True)
if not isnull(new_values).all(): values = new_values
values = lib.maybe_convert_objects(values)
values = np.array(values, dtype=np.object_)
values = values.copy() if copy else values return values
if datetime: values = lib.maybe_convert_objects(values, convert_datetime=datetime)
values = lib.maybe_convert_objects(values, convert_timedelta=timedelta)
values = converted if not isnull(converted).all() else values values = values.copy() if copy else values
if isinstance(data, mrecords.MaskedRecords): mgr = _masked_rec_array_to_mgr(data, index, columns, dtype, copy)
if index is None: extract_index(list(data.values()))
data = dict((k, v) for k, v in compat.iteritems(data) if k in columns)
if dtype is not None and issubclass(dtype.type, np.integer): continue
v = np.empty(len(index), dtype=object)
if not len(values) and columns is not None and len(columns): values = np.empty((0, 1), dtype=object)
def _get_axes(N, K, index=index, columns=columns):
if (is_categorical_dtype(getattr(values, 'dtype', None)) or is_categorical_dtype(dtype)):
values = _prep_ndarray(values, copy=copy)
if dtype is None and is_object_dtype(values): values = _possibly_infer_to_datetimelike(values)
if ((max_columns and nb_columns > max_columns) or ((not ignore_width) and width and nb_columns > (width // 2))): return False
if ignore_width or not com.in_interactive_session(): return True
max_rows = 1
buf = StringIO()
d = self
d = d.iloc[:min(max_rows, len(d))]
arrays.extend(self.iloc[:, k] for k in range(len(self.columns)))
return zip(*arrays)
if columns is not None: columns = _ensure_index(columns)
ix_vals = lmap(np.array, zip(*self.index.values))
if self.index.lexsort_depth < 2: selfsorted = self.sortlevel(0) else: selfsorted = self
major_axis = major_axis.copy() major_axis.name = self.index.names[0]
new_axes = [selfsorted.columns, major_axis, minor_axis]
new_mgr = selfsorted._data.reshape_nd(axes=new_axes, labels=[major_labels, minor_labels], shape=shape, ref_items=selfsorted.columns)
if longtable is None: longtable = get_option("display.latex.longtable") if escape is None: escape = get_option("display.latex.escape")
if max_cols is None: max_cols = get_option('display.max_info_columns', len(self.columns) + 1)
(vals, idx, cols), object_state = state
self.loc[index, col] = value self._item_cache.pop(col, None)
result = self.take(i, axis=axis) copy = True
copy = (isinstance(new_values, np.ndarray) and new_values.base is None) result = self._constructor_sliced(new_values, index=self.columns, name=self.index[i], dtype=new_values.dtype)
lab_slice = slice(label[0], label[-1]) return self.ix[:, lab_slice]
values = self._data.iget(i)
result._set_as_cached(label, self)
is_mi_columns = isinstance(self.columns, MultiIndex) try: if key in self.columns and not is_mi_columns: return self._getitem_column(key) except: pass
indexer = convert_to_index_sliceable(self, key) if indexer is not None: return self._getitem_slice(indexer)
return self._getitem_array(key)
if self.columns.is_unique: return self._get_item_cache(key)
result = self._constructor(self._data.get(key)) if result.columns.is_unique: result = result[key]
new_data = self[res]
include, exclude = map( lambda x: frozenset(map(com._get_dtype_from_object, x)), selection) for dtypes in (include, exclude): com._invalidate_string_dtypes(dtypes)
if not include.isdisjoint(exclude): raise ValueError('include and exclude overlap on %s' % (include & exclude))
indexer = convert_to_index_sliceable(self, key) if indexer is not None: return self._setitem_slice(indexer, value)
self._set_item(key, value)
if key.values.size and not com.is_bool_dtype(key.values): raise TypeError('Must pass DataFrame with boolean values only')
if len(self): self._check_setitem_copy()
results = {} for k, v in kwargs.items(): results[k] = com._apply_if_callable(v, data)
for k, v in sorted(results.items()): data[k] = v
try: value = value.reindex(self.index)._values except Exception as e:
if not value.index.is_unique: raise e
raise TypeError('incompatible index of inserted column ' 'with frame index')
if is_object_dtype(value.dtype): value = _possibly_infer_to_datetimelike(value)
dtype, value = _infer_dtype_from_scalar(value) value = np.repeat(value, len(self.index)).astype(dtype) value = com._possibly_cast_to_datetime(value, dtype)
if is_extension_type(value): return value
for n in range(col.nlevels - 1): arrays.append(col.get_level_values(n))
index._cleanup()
level_values = _maybe_casted_values(lev, lab) if level is None or i in level: new_obj.insert(0, col_name, level_values)
if level is not None:
if not labels.is_lexsorted(): labels = MultiIndex.from_tuples(labels.values)
if ((ascending and labels.is_monotonic_increasing) or (not ascending and labels.is_monotonic_decreasing)): if inplace: return else: return self.copy()
mask = left_mask ^ right_mask left[left_mask & mask] = fill_value right[right_mask & mask] = fill_value
if this.columns.is_unique:
else:
return self._constructor(data=self._series, index=self.index, columns=self.columns)
if self.columns.is_unique:
else:
new_columns = this.columns.union(other.columns) do_fill = fill_value is not None
if not overwrite and other_mask.all(): result[col] = this[col].copy() continue
new_dtype = this_dtype if this_dtype != other_dtype: new_dtype = com._lcd_dtypes(this_dtype, other_dtype) series = series.astype(new_dtype) otherSeries = otherSeries.astype(new_dtype)
needs_i8_conversion = com.needs_i8_conversion(new_dtype) if needs_i8_conversion: this_dtype = new_dtype arr = func(series, otherSeries, True) else: arr = func(series, otherSeries)
return self._constructor(result, index=new_index, columns=new_columns)._convert(datetime=True, copy=False)
if mask.all(): continue
if (reduce and axis == 1 and self._is_mixed_type and self._is_datelike_mixed_type): reduce = False
if reduce: values = self.values
if len(successes) < len(res_index): res_index = res_index.take(successes)
if i is not None: k = res_index[i] e.args = e.args + ('occurred at index %s' % pprint_thing(k), )
def infer(x): return lib.map_infer(x.asobject, func)
return self._join_compat(other, on=on, how=how, lsuffix=lsuffix, rsuffix=rsuffix, sort=sort)
if how == 'left': how = 'outer' join_axes = [self.index] else: join_axes = None
new_cols = [_series_round(v, decimals) for _, v in self.iteritems()]
left = left + right * 0 right = right + left * 0
ldem = left - left.mean() rdem = right - right.mean()
mask = notnull(frame).values
mask = notnull(frame.values)
mask = mask.T
return result.T
if axis == 1 and self._is_mixed_type and self._is_datelike_mixed_type: numeric_only = True
if filter_type is None and axis == 0: try:
result = self.apply(f, reduce=False) if result.ndim == self.ndim: result = result.iloc[0] return result
if axis == 0: result = com._coerce_to_dtypes(result, self.dtypes)
if index is None: index = extract_index(arrays) else: index = _ensure_index(index)
arrays = _homogenize(arrays, index, dtype)
axes = [_ensure_index(columns), _ensure_index(index)]
values = np.asarray(values) if copy: values = values.copy()
data = lmap(tuple, data) return _list_to_arrays(data, columns, coerce_float=coerce_float, dtype=dtype)
arrays, arr_columns = _reorder_arrays(new_arrays, arr_columns, columns) if columns is None: columns = arr_columns
content = list(lib.to_object_array(data).T)
data = [(type(d) is dict) and d or dict(d) for d in data]
raise AssertionError('%d columns passed, passed data had %s ' 'columns' % (len(columns), len(content)))
v = v.reindex(index, copy=False)
from pandas.core.series import Series result = Series(result.ravel()).replace(-1, na_sentinel).values.\ reshape(result.shape)
is_datetimetz = com.is_datetimetz(values) if is_datetimetz: values = DatetimeIndex(values) vals = values.tz_localize(None)
t = hash_klass(len(uniques)) t.map_locations(com._ensure_object(uniques))
uniques = DatetimeIndex(uniques.astype('M8[ns]')).tz_localize( values.tz)
result = Series(values).values.value_counts(dropna=dropna) result.name = name counts = result.values
keys, counts = _value_counts_arraylike(values, dropna=dropna)
keys = keys.astype(dtype)
from pandas.core.series import Series
return narr - 1 - inds
if com.is_categorical(arr): return arr.take_nd(indexer, fill_value=fill_value, allow_fill=allow_fill) elif com.is_datetimetz(arr): return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)
if out is None: out_shape = len(row_idx), len(col_idx) out = np.empty(out_shape, dtype=dtype)
if is_timedelta: res = arr[res_indexer] lag = arr[lag_indexer]
shape = values.shape
result = values.view('i8') == tslib.iNaT
if isinstance(obj, gt.ABCSeries): from pandas import Series result = Series(result, index=obj.index, name=obj.name, copy=False)
shape = values.shape
result = values.view('i8') == tslib.iNaT
if isinstance(obj, gt.ABCSeries): from pandas import Series result = Series(result, index=obj.index, name=obj.name, copy=False)
if left.shape != right.shape: return False
if is_string_dtype(left) or is_string_dtype(right):
return lib.array_equivalent_object(_ensure_object(left.ravel()), _ensure_object(right.ravel()))
if is_float_dtype(left) or is_complex_dtype(left): return ((left == right) | (np.isnan(left) & np.isnan(right))).all()
elif is_datetimelike_v_numeric(left, right): return False
elif needs_i8_conversion(left) and needs_i8_conversion(right): if not is_dtype_equal(left.dtype, right.dtype): return False
return np.array_equal(left, right)
if is_integer(r) and r not in [0, 1]: return int(r) r = bool(r)
if isinstance(val, np.ndarray): if val.ndim != 0: raise ValueError( "invalid ndarray passed to _infer_dtype_from_scalar")
if isinstance(fill_value, np.ndarray): if issubclass(fill_value.dtype.type, (np.datetime64, np.timedelta64)): fill_value = tslib.iNaT else:
if fill_value.dtype == np.object_: dtype = np.dtype(np.object_) fill_value = np.nan
if is_categorical_dtype(dtype): pass elif is_datetimetz(dtype): pass elif issubclass(np.dtype(dtype).type, compat.string_types): dtype = np.object_
r, _ = _maybe_upcast(result, fill_value=other, copy=True) np.place(r, mask, other)
if (lib.isscalar(other) or (isinstance(other, np.ndarray) and other.ndim < 1)): if isnull(other): return changeit()
else:
elif inferred_type == 'floating': dtype = 'int64' if issubclass(result.dtype.type, np.number):
if dtype.kind == result.dtype.kind: if (result.dtype.itemsize <= dtype.itemsize and np.prod(result.shape)): return result
if not np.prod(result.shape): return trans(result).astype(dtype)
r = result.ravel() arr = np.array([r[0]])
if isnull(arr).any() or not np.allclose(arr, trans(arr).astype(dtype)): return result
elif not isinstance(r[0], (np.integer, np.floating, np.bool, int, float, bool)): return result
if (new_result == result).all(): return new_result
pass
pass
if isinstance(values, np.ndarray): return obj[indexer.get_loc(key)]
return values
kind = arr.dtype.kind if kind == 'M' or kind == 'm': return arr.dtype in _DATELIKE_DTYPES
if lib.isscalar(value) and isnull(value): value = [value]
if value.ndim == 0: value = tslib.iNaT
raise TypeError('Cannot cast datetime64 to %s' % dtype)
if is_array and value.dtype.kind in ['M', 'm']: dtype = value.dtype
elif not (is_array and not (issubclass(value.dtype.type, np.integer) or value.dtype == np.object_)): value = _possibly_infer_to_datetimelike(value)
try: v = tslib.array_to_datetime(v, errors='raise') except ValueError:
try: from pandas import to_datetime return to_datetime(v) except: pass
from pandas.tseries.timedeltas import to_timedelta try: return to_timedelta(v)._values.reshape(shape) except: return v
sample = v[:min(3, len(v))] inferred_type = lib.infer_dtype(sample)
elif inferred_type in ['mixed']:
seq_it = iter(seq) seq_it_next = iter(seq) next(seq_it_next)
def __iter__(self): return iter(dict.items(self))
try: result = np.empty(len(values), dtype=object) result[:] = values except ValueError: result[:] = [tuple(x) for x in values]
return hasattr(obj, 'next') or hasattr(obj, '__next__')
return False
if not hasattr(a, 'dtype'): a = np.asarray(a) if not hasattr(b, 'dtype'): b = np.asarray(b)
if not hasattr(a, 'dtype'): a = np.asarray(a) if not hasattr(b, 'dtype'): b = np.asarray(b)
dtype = _get_dtype(arr_or_dtype) return dtype.kind in ('S', 'U')
return False
return lib.astype_unicode(arr.ravel()).reshape(arr.shape)
elif ((compat.PY3 and dtype not in [_INT64_DTYPE, _TD_DTYPE]) or (not compat.PY3 and dtype != _TD_DTYPE)):
if dtype.kind == 'm': mask = isnull(arr) result = arr.astype(dtype).astype(np.float64) result[mask] = np.nan return result
return lib.astype_intsafe(arr.ravel(), dtype).reshape(arr.shape)
_plotting_methods = frozenset(['plot', 'boxplot', 'hist'])
if isinstance(ax, MultiIndex): level = ax._get_level_number(level) ax = Index(ax.get_level_values( level), name=ax.names[level])
validate_kwargs('group', kwargs, {})
return [self.indices[name] for name in names]
msg = ("must supply a a same-length tuple to get_group" " with multiple grouping keys") raise ValueError(msg)
self._set_selection_from_grouper()
kwargs_with_axis = kwargs.copy() if 'axis' not in kwargs_with_axis or \ kwargs_with_axis['axis'] is None: kwargs_with_axis['axis'] = self.axis
curried.__name__ = curried_with_axis.__name__ = name
if name in _plotting_methods: return self.apply(curried)
try: return self._aggregate_item_by_item(name, *args, **kwargs) except (AttributeError): raise ValueError
if args or kwargs: if is_callable(func):
with option_context('mode.chained_assignment', None): return self._python_apply_general(f)
values = result if is_numeric_dtype(values.dtype): values = com.ensure_float(values)
for v in values: if v is not None: ax = v._get_axis(self.axis) ax._reset_identity() return values
group_keys = keys group_levels = self.grouper.levels group_names = self.grouper.names
keys = list(range(len(values))) result = concat(values, axis=self.axis, keys=keys)
mask = np.tile(mask, list(self._selected_obj.shape[1:]) + [1]).T
warnings.warn("irow(i) is deprecated. Please use .nth(i)", FutureWarning, stacklevel=2) return self.nth(i)
raise NotImplementedError
max_len = n if n >= 0 else - 1 - n dropped = self.obj.dropna(how=dropna, axis=self.axis)
if self.keys is None and self.level is None:
axis = self.grouper.axis grouper = axis[axis.isin(dropped.index)]
grouper, _, _ = _get_grouper(dropped, key=self.keys, axis=self.axis, level=self.level, sort=self.sort, mutated=self.mutated)
if len(result) and mask.any(): result.loc[mask] = np.nan
indexer = np.zeros_like(labels) _algos.group_shift_indexer(indexer, labels, ngroups, periods)
mapper = _KeyMapper(comp_ids, ngroups, self.labels, self.levels) return [mapper.get_key(i) for i in range(ngroups)]
group_axes = _get_axes(group) res = f(group) if not _is_indexed_like(res, group_axes): mutated = True result_values.append(res)
if self.indices: return max(len(v) for v in self.indices.values()) else: return 0
return Index(self.group_info[0]).is_monotonic
f = getattr(_algos, fname, None) if f is not None and is_numeric: return f
for dt in [dtype_str, 'object']: f = getattr(_algos, "%s_%s" % (fname, dtype_str), None) if f is not None: return f
f = ftype.get('f') if f is not None:
func = wrapper
accum = np.empty(out_shape, dtype=out_dtype) result = self._transform( result, accum, values, labels, func, is_numeric)
raise NotImplementedError("number of dimensions is currently " "limited to 3")
raise NotImplementedError("number of dimensions is currently " "limited to 3")
if values[0] < binner[0]: raise ValueError("Values falls before first bin")
for i in range(0, lenbin - 1): r_bin = binner[i + 1]
while j < lenidx and (values[j] < r_bin or (closed == 'right' and values[j] == r_bin)): j += 1
result = {} for key, value in zip(self.binlabels, self.bins): if key is not tslib.NaT: result[key] = value return result
if isinstance(grouper, (Series, Index)) and name is None: self.name = grouper.name
self._should_compress = True
labels, uniques = algos.factorize(inds, sort=True)
mask = inds != -1 ok_labels, uniques = algos.factorize(inds[mask], sort=True)
elif is_categorical_dtype(self.grouper):
if self.sort: if not self.grouper.ordered:
else: cat = self.grouper.unique() self.grouper = self.grouper.reorder_categories( cat.categories)
self._labels = self.grouper.codes
elif isinstance(self.grouper, Grouper):
if isinstance(self.grouper, Grouping): self.grouper = self.grouper.grouper
elif isinstance(key, BaseGrouper): return key, [], obj
def is_in_axis(key): if not _is_label_like(key): try: obj._data.items.get_loc(key) except Exception: return False
def is_in_obj(gpr): try: return id(gpr) == id(obj[gpr.name]) except Exception: return False
ping = Grouping(group_axis, gpr, obj=obj, name=name, level=level, sort=sort, in_axis=in_axis) \ if not isinstance(gpr, Grouping) else gpr
grouper = BaseGrouper(group_axis, groupings, sort=sort, mutated=mutated)
_apply_whitelist = _series_apply_whitelist for _def_str in _whitelist_method_generator(Series, _series_apply_whitelist): exec(_def_str)
if not _level and isinstance(ret, dict): from pandas import concat ret = concat(ret, axis=1) return ret
columns = lzip(*arg)[0]
if name in self._selected_obj: obj = copy.copy(obj) obj._reset_cache() obj._selection = name results[name] = obj.aggregate(func)
if _level: return results return list(compat.itervalues(results))[0]
return Series([], name=self.name, index=keys)
index = _get_index() result = DataFrame(values, index=index).stack() result.name = self.name return result
return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
return Series(values, index=_get_index(), name=self.name)
dtype = self._selected_obj.dtype result = self._selected_obj.values.copy()
try: common_type = np.common_type(np.array(res), result) if common_type != result.dtype: result = result.astype(common_type) except: pass
def true_and_notnull(x, *args, **kwargs): b = wrapper(x, *args, **kwargs) return b and notnull(b)
idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]] inc = np.r_[1, val[1:] != val[:-1]]
mask = isnull(val) if dropna: inc[idx] = 1 inc[mask] = 0 else: inc[mask & np.r_[False, mask[:-1]]] = 0 inc[idx] = 1
if len(res) != len(ri): res, out = np.zeros(len(ri), dtype=out.dtype), res res[ids] = out
return self.apply(Series.value_counts, normalize=normalize, sort=sort, ascending=ascending, bins=bins)
mask = ids != -1 ids, val = ids[mask], val[mask]
lab, lev, dropna = cat.codes, bins[:-1], False
idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]
inc = np.r_[True, lab[1:] != lab[:-1]]
rep = partial(np.repeat, repeats=np.add.reduceat(inc, idx))
diff = np.zeros(len(out), dtype='bool') for lab in labels[:-1]: diff |= np.r_[True, lab[1:] != lab[:-1]]
labels = list(map(lambda lab: np.repeat(lab[diff], nbin), labels[:-1])) labels.append(left[-1])
if self._selection is None: slice_axis = self.obj.columns else: slice_axis = self._selection_list slicer = lambda x: self.obj[x]
if self.axis == 0: new_axes[0], new_axes[1] = new_axes[1], self.grouper.result_index else: new_axes[self.axis] = self.grouper.result_index
assert new_axes[0].equals(items) new_axes[0] = items
result = block._try_coerce_and_cast_result(result)
if self.axis == 0: obj = obj.swapaxes(0, 1) return obj
if self.grouper.nkeys > 1: return self._python_agg_general(arg, *args, **kwargs) else:
if not len(result_columns) and errors is not None: raise errors
def first_non_None_value(values): try: v = next(v for v in values if v is not None) except StopIteration: return None return v
return DataFrame()
values = [values[i] for i in indexer]
if not self.as_index: key_index = None
if self.squeeze:
if singular_series: values[0].name = keys[0]
return self._concat_objects( keys, values, not_indexed_same=not_indexed_same )
elif all_indexed_same: from pandas.tools.merge import concat return concat(values)
return self._concat_objects( keys, values, not_indexed_same=True, )
return Series(values, index=key_index, name=self.name)
coerce = True if any([isinstance(x, Timestamp) for x in values]) else False return (Series(values, index=key_index, name=self.name) ._convert(datetime=True, coerce=coerce))
return self._concat_objects(keys, values, not_indexed_same=not_indexed_same)
if isinstance(res, Series): if res.index.is_(obj.index): group.T.values[:] = res else: group.values[:] = res
if not isinstance(result, DataFrame): return self._transform_general(func, *args, **kwargs)
if not result.columns.equals(obj.columns): return self._transform_general(func, *args, **kwargs)
cast = (self.size().fillna(0) > 0).any()
try: res_fast = fast_path(group)
output = {} inds = [] for i, col in enumerate(obj): try: output[col] = self[col].transform(wrapper) inds.append(i) except Exception: pass
for _def_str in _whitelist_method_generator(DataFrame, _apply_whitelist): exec(_def_str)
izip = zip(* map(reversed, ( self.grouper.names, self.grouper.get_group_levels(), [grp.in_axis for grp in self.grouper.groupings])))
if self._selection is None: slice_axis = self._selected_obj.items else: slice_axis = self._selection_list slicer = lambda x: self._selected_obj[x]
return algos.take_nd(self.labels, self.sort_idx, allow_fill=False)
return _get_group_index_sorter(self.labels, self.ngroups)
try: starts, ends = lib.generate_slices(self.slabels, self.ngroups) except: return [], True
data = self.data._data
sorted_axis = data.axes[self.axis].take(self.sort_idx) sorted_data = data.reindex_axis(sorted_axis, axis=self.axis)
nlev = _int64_cut_off(shape)
stride = np.prod(shape[1:nlev], dtype='i8') out = stride * labels[0].astype('i8', subok=False, copy=False)
comp_ids, obs_ids = _compress_group_index(out, sort=sort)
if _int64_overflow_possible(shape): raise ValueError('cannot deconstruct factorized group indices!')
out = decons_group_index(obs_ids, shape) return out if xnull or not lift.any() \ else [x - y for x, y in zip(out, lift)]
if is_categorical_dtype(key): c = key
else: c = Categorical(key, ordered=True)
if is_categorical_dtype(items): return items.argsort(ascending=ascending)
comp_ids, obs_group_ids = table.get_labels_groupby(group_index)
sorter = uniques.argsort()
reverse_indexer = np.empty(len(sorter), dtype=np.int64) reverse_indexer.put(sorter, np.arange(len(sorter)))
labels = algos.take_nd(reverse_indexer, labels, allow_fill=False) np.putmask(labels, mask, -1)
uniques = algos.take_nd(uniques, sorter, allow_fill=False)
from pandas.indexes.api import * from pandas.indexes.multi import _sparsify
if is_object_dtype(args[0]): raise TypeError(e) raise
if is_timedelta64_dtype(values): return lib.Timedelta(0) return 0
if _has_infs(result): result = alt(values, axis=axis, skipna=skipna, **kwds)
if (not is_object_dtype(dt) and not is_datetime_or_timedelta_dtype(dt)):
if name == 'nansum': if dt.itemsize < 8: return False
return False
return _int64_max
fill_value = _get_fill_value(dtype, fill_value=fill_value, fill_value_typ=fill_value_typ)
else: values, changed = _maybe_upcast_putmask(values, mask, fill_value)
dtype_max = dtype if is_integer_dtype(dtype) or is_bool_dtype(dtype): dtype_max = np.int64 elif is_float_dtype(dtype): dtype_max = np.float64
if np.fabs(result) > _int64_max: raise ValueError("overflow in timedelta operation")
if values.ndim > 1: if notempty: return _wrap_results( np.apply_along_axis(get_median, axis, values), dtype)
return _wrap_results(get_median(values) if notempty else np.nan, dtype)
if is_float_dtype(dtype): result = result.astype(dtype) return _wrap_results(result, values.dtype)
m2 = _zero_out_fperr(m2) m3 = _zero_out_fperr(m3)
numer = _zero_out_fperr(numer) denom = _zero_out_fperr(denom)
if count < 4: return np.nan if denom == 0: return 0
if use_numexpr: op = lambda x: x else: op = lambda x: None if special:
self._update_inplace(result.reindex_like(self, copy=False)._data, verify_is_copy=False)
if isinstance(left, ABCSeries) and isinstance(right, ABCSeries): left, right = left.align(right, copy=False)
elif ((self.is_timedelta_lhs and (self.is_timedelta_rhs or self.is_offset_rhs)) or (self.is_timedelta_rhs and (self.is_timedelta_lhs or self.is_offset_lhs))):
elif (self.is_datetime_lhs and (self.is_timedelta_rhs or self.is_offset_rhs)):
elif self.is_datetime_lhs and self.is_datetime_rhs:
values = to_timedelta(values, errors='coerce', box=False)
if self.is_datetime_lhs or self.is_datetime_rhs:
self.na_op = lambda x, y: getattr(x, self.name)(y) return lvalues, rvalues
if self.is_datetime64tz_lhs: lvalues = lvalues.tz_localize(None) if self.is_datetime64tz_rhs: rvalues = rvalues.tz_localize(None)
else:
if self.is_offset_lhs: lvalues = to_timedelta(lvalues, box=False) if self.is_offset_rhs: rvalues = to_timedelta(rvalues, box=False)
if mask.any():
try: x = np.array(x, dtype=self.dtype) except TypeError: x = np.array(x, dtype='datetime64[ns]')
is_timedelta_lhs = is_timedelta64_dtype(left) is_datetime_lhs = (is_datetime64_dtype(left) or is_datetime64tz_dtype(left))
return _algos.arrmap_object(rvalues, lambda x: op(lvalues, x))
if (hasattr(lvalues, 'values') and not isinstance(lvalues, pd.DatetimeIndex)): lvalues = lvalues.values
if is_categorical_dtype(x): return op(x, y) elif is_categorical_dtype(y) and not isscalar(y): return op(y, x)
if is_datetimelike_v_numeric(x, y): raise TypeError("invalid type comparison")
mask = None if (needs_i8_conversion(x) or (not isscalar(y) and needs_i8_conversion(y))):
if axis is not None: self._get_axis_number(axis)
res = op(self.values, other)
res = _values_from_object(res)
if not isnull(y): y = bool(y) result = lib.scalar_binop(x, y, op)
try: output = radd(left, right) except TypeError: raise
casted = pd.Series(other, index=self.columns)
casted = pd.Series(other, index=self.index)
casted = pd.Series(other, index=self.columns)
casted = pd.DataFrame(other, index=self.index, columns=self.columns) return self._combine_frame(casted, na_op, fill_value, level)
res = self._combine_const(other, func, raise_on_error=False) return res.fillna(True).astype(bool)
def na_op(x, y): try: result = expressions.evaluate(op, str_rep, x, y, raise_on_error=True, **eval_kwargs) except TypeError:
if axis is not None: axis = self._get_axis_number(axis)
def get_indexers_list():
_NS = slice(None, None)
class _IndexSlice(object): def __getitem__(self, arg): return arg
new_self = self.__class__(self.obj, self.name)
try: return self.obj._xs(label, axis=axis) except: return self.obj[label]
if 'cannot do' in str(e): raise raise IndexingError(key)
ax = self.obj._get_axis(min(axis, self.ndim - 1)) return ax._convert_scalar_indexer(key, kind=self.name)
ax = self.obj._get_axis(min(axis, self.ndim - 1)) return ax._convert_slice_indexer(key, kind=self.name)
pass
pass
take_split_path = self.obj._is_mixed_type
if not take_split_path and self.obj._data.blocks: blk, = self.obj._data.blocks
if (isinstance(ax, MultiIndex) and not (is_integer(i) or is_null_slice(i))): take_split_path = True break
key, _ = convert_missing_indexer(idx)
if self.ndim > 1 and i == self.obj._info_axis_number:
self.obj[key] = _infer_fill_value(value)
if self.ndim == 1: index = self.obj.index new_index = index.insert(len(index), indexer)
if index.is_unique: new_indexer = index.get_indexer([new_index[-1]]) if (new_indexer != -1).any(): return self._setitem_with_indexer(new_indexer, value)
if not len(self.obj.columns): raise ValueError("cannot set a frame with no defined " "columns")
if isinstance(value, Series):
else:
if is_list_like_indexer(value): if len(value) != len(self.obj.columns): raise ValueError("cannot set a row with " "mismatched columns")
elif self.ndim >= 3: return self.obj.__setitem__(indexer, value)
item_labels = self.obj._get_axis(info_axis)
if take_split_path:
if (len(labels) == 1 and isinstance(self.obj[labels[0]].axes[0], MultiIndex)): item = labels[0] obj = self.obj[item] index = obj.index idx = indexer[:info_axis][0]
if is_list_like_indexer(value) and np.iterable( value) and lplane_indexer != len(value):
value = getattr(value, 'values', value).ravel()
self.obj[item] = s
if len(index) == l: return True elif lplane_indexer == l: return True
if is_list_like_indexer(value) and getattr(value, 'ndim', 1) > 0:
if isinstance(value, ABCDataFrame) and value.ndim > 1: sub_indexer = list(indexer) multiindex_indexer = isinstance(labels, MultiIndex)
elif np.array(value).ndim == 2:
setter(item, value[:, i].tolist())
elif can_do_equal_len(): setter(labels[0], value)
else:
for item in labels: setter(item, value)
self.obj._check_is_chained_assignment_possible()
self.obj._consolidate_inplace() self.obj._data = self.obj._data.setitem(indexer=indexer, value=value) self.obj._maybe_update_cacher(clear=True)
ravel = lambda i: i.ravel() if isinstance(i, np.ndarray) else i indexer = tuple(map(ravel, indexer))
if is_frame: single_aligner = single_aligner and aligners[0]
elif is_panel: single_aligner = (single_aligner and (aligners[1] or aligners[2]))
if (sum_aligners == self.ndim and all([com.is_sequence(_) for _ in indexer])): ser = ser.reindex(obj.axes[0][indexer[0]], copy=True)._values
if len(indexer) > 1 and not multiindex_indexer: l = len(indexer[1]) ser = np.tile(ser, l).reshape(l, -1).T
elif single_aligner and is_frame:
elif single_aligner:
if len(labels & ser.index): ser = ser.reindex(labels) else: broadcast.append((n, len(labels)))
if is_panel:
if len(sindexers) == 1 and sindexers[0] != 0: df = df.T
if idx.equals(df.index) and cols.equals(df.columns): return df.copy()._values
self._has_valid_tuple(tup)
if self._multi_take_opportunity(tup): return self._multi_take(tup)
retval = self.obj for i, key in enumerate(tup): if i >= self.obj.ndim: raise IndexingError('Too many indexers')
if not isinstance(self.obj, NDFrame): return False
keyarr = key
keyarr = _asarray_tuplesafe(key)
return self._get_label(tup, axis=0)
pass
ax0 = self.obj._get_axis(0) if not ax0.is_lexsorted_for_tuple(tup): raise e1
if self.axis is not None: axis = self.obj._get_axis_number(self.axis) return self._getitem_axis(tup, axis=axis)
if self._is_nested_tuple_indexer(tup): return self._getitem_nested_tuple(tup)
ax0 = self.obj._get_axis(0) if isinstance(ax0, MultiIndex): result = self._handle_lowerdim_multi_index_axis0(tup) if result is not None: return result
for i, key in enumerate(tup): if is_label_like(key) or isinstance(key, tuple): section = self._getitem_axis(key, axis=i)
if not is_list_like_indexer(section): return section
new_key = tup[:i] + (_NS,) + tup[i + 1:]
if (isinstance(section, ABCDataFrame) and i > 0 and len(new_key) == 2): a, b = new_key new_key = b, a
return getattr(section, self.name)[new_key]
if len(tup) > self.ndim: result = self._handle_lowerdim_multi_index_axis0(tup) if result is not None: return result
return self._getitem_axis(tup, axis=0)
obj = self.obj axis = 0 for i, key in enumerate(tup):
if lib.isscalar(obj) or not hasattr(obj, 'ndim'): break
if obj.ndim < current_ndim:
if i >= 1 and current_ndim == 3 and obj.ndim == 2: obj = obj.T
key = labels._maybe_cast_indexer(key)
if not labels.is_floating() and not labels.is_integer(): return self._get_loc(key, axis=axis)
keyarr = key
keyarr = _asarray_tuplesafe(key)
indexer = labels._convert_list_indexer(keyarr, kind=self.name) if indexer is not None: return self.obj.take(indexer, axis=axis)
if (isinstance(labels, MultiIndex) and len(keyarr) and not isinstance(keyarr[0], tuple)): level = 0 else: level = None
if labels.is_unique and Index(keyarr).is_unique:
if axis != 0: raise AssertionError('axis must be 0') return self.obj.reindex(keyarr, level=level)
else:
if axis + 1 > self.obj.ndim: raise AssertionError("invalid indexing error with " "non-unique index")
try: obj = self._convert_scalar_indexer(obj, axis) except TypeError:
if is_setter: pass
is_int_index = labels.is_integer() is_int_positional = is_integer(obj) and not is_int_index
if is_int_positional:
if is_setter:
if self.name == 'loc': return {'key': obj}
if (obj >= self.obj.shape[axis] and not isinstance(labels, MultiIndex)): raise ValueError("cannot set by positional indexing with " "enlargement")
objarr = obj
indexer = labels._convert_list_indexer(objarr, kind=self.name) if indexer is not None: return indexer
if (isinstance(labels, MultiIndex) and not isinstance(objarr[0], tuple)): level = 0 _, indexer = labels.reindex(objarr, level=level)
if indexer is None: indexer = np.arange(len(labels))
if labels.is_unique: indexer = check = labels.get_indexer(objarr)
else: (indexer, missing) = labels.get_indexer_non_unique(objarr) check = indexer
if not is_list_like_indexer(obj) and is_setter: return {'key': obj} raise
key = com._apply_if_callable(key, self.obj)
if isinstance(key, tuple) and isinstance(ax, MultiIndex): return True
key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))
if not (isinstance(key, tuple) and isinstance(labels, MultiIndex)):
self._has_valid_type(key, axis) return self._get_label(key, axis=axis)
if retval.ndim < self.ndim: axis -= 1
axis += 1
else:
self._is_valid_list_like(key, axis)
key = list(key)
self._is_valid_integer(key, axis)
if isinstance(obj, slice): return self._convert_slice_indexer(obj, axis)
if not is_list_like_indexer(key): key = tuple([key]) else: raise ValueError('Invalid call for scalar access (getting)!')
key = com._apply_if_callable(key, self.obj)
if is_setter: return list(key)
_eps = np.finfo('f4').eps
if key in obj._data.items: return None
if idx.is_all_dates: try: return idx._get_string_slice(key) except (KeyError, ValueError, NotImplementedError): return None
result = np.asarray(result, dtype=bool)
indexer = indexer['key']
return np.empty(0, dtype=np.int_)
if not isinstance(tup, tuple): return False
for i, k in enumerate(tup):
return is_list_like(key) and not (isinstance(key, tuple) and type(key) is not tuple)
return not isinstance(key, slice) and not is_list_like_indexer(key)
kinds = tuple(list(compat.string_types) + [ABCSeries, np.ndarray, Index, list]) if isinstance(slice_, kinds): slice_ = IndexSlice[:, slice_]
return isinstance(part, slice) or com.is_list_like(part)
slice_ = [[slice_]]
import pandas.core.datetools as datetools
from __future__ import division
raxes = [self._extract_axis(self, data, axis=i) if a is None else a for i, a in enumerate(axes)] raxes_sm = self._extract_axes_for_slice(self, raxes)
nargs = len(args) nreq = self._AXIS_LEN + 1
if made_bigger: com._possibly_cast_item(result, args[0], likely_dtype)
if result._get_axis(0).is_unique: result = result[key]
this = self.reindex(items=items, major=major, minor=minor) other = other.reindex(items=items, major=major, minor=minor)
if axis == 0: values = self._data.iget(i) return self._box_item_values(key, values)
self._consolidate_inplace() new_data = self._data.xs(i, axis=axis, copy=True, takeable=True) return self._construct_return_type(new_data)
mask = com.notnull(self.values).all(axis=0) selector = mask.ravel()
selector = slice(None, None)
if isinstance(axis, (tuple, list)) and len(axis) == 2: return self._apply_2d(f, axis=axis)
return self._apply_1d(f, axis=axis)
points = cartesian_product(planes)
pts = tuple([p[i] for p in points]) indexer.put(indlist, slice_indexer)
slice_indexer[-1] += 1 n = -1 while (slice_indexer[n] >= shape[n]) and (n > (1 - ndim)): slice_indexer[n - 1] += 1 slice_indexer[n] = 0 n -= 1
if not len(results): return self._constructor(**self._construct_axes_dict())
indexer_axis = list(range(ndim)) for a in axis: indexer_axis.remove(a) indexer_axis = indexer_axis[0]
if ndim is None: if isinstance(result, dict): ndim = getattr(list(compat.itervalues(result))[0], 'ndim', 0)
if ndim != 0: ndim += 1
if ndim == 0: return Series(result)
elif self.ndim == ndim: if axes is None: return self._constructor(result) return self._constructor(result, **self._construct_axes_dict())
elif self.ndim == ndim + 1: if axes is None: return self._constructor_sliced(result) return self._constructor_sliced( result, **self._extract_axes_for_slice(self, axes))
if (len(args) == 1 and hasattr(args[0], '__iter__') and not com.is_string_like(args[0])): axes = args[0] else: axes = args
if issubclass(values.dtype.type, compat.string_types): values = np.array(values, dtype=object, copy=True)
if isinstance(frames, OrderedDict): result = OrderedDict()
result = missing.fill_zeros(result, x, y, name, fill_zeros) return result
ops.add_flex_arithmetic_methods( cls, _panel_arith_method, use_numexpr=use_numexpr, flex_comp_method=ops._comp_method_PANEL)
class WidePanel(Panel): def __init__(self, *args, **kwargs): warnings.warn("WidePanel is deprecated. Please use Panel", FutureWarning, stacklevel=2)
warnings.warn("LongPanel is deprecated. Please use DataFrame", FutureWarning, stacklevel=2)
from pandas.compat import range, zip from pandas import compat import itertools
self.lift = 1 if -1 in self.index.labels[self.level] else 0
remaining_labels = self.sorted_labels[:-1] level_sizes = [len(x) for x in new_levels]
length, width = self.full_shape stride = values.shape[1] result_width = width * stride result_shape = (length, result_width)
level_num = frame.columns._get_level_number(level)
level_to_sort = _convert_level_number(0, this.columns) this = this.sortlevel(level_to_sort, axis=1)
if dropna: result = result.dropna(axis=0, how='all')
frame.columns = frame.columns.get_level_values(col_level)
mdata[col] = np.asanyarray(frame.columns.get_level_values(i)).repeat(N)
def check_len(item, name): length_msg = ("Length of '{0}' ({1}) did not match the length of " "the columns being encoded ({2}).")
if isinstance(prefix_sep, compat.string_types): prefix_sep = cycle([prefix_sep]) elif isinstance(prefix_sep, dict): prefix_sep = [prefix_sep[col] for col in columns_to_encode]
cat = Categorical.from_array(Series(data), ordered=True) levels = cat.categories
if not dummy_na and len(levels) == 0: return get_empty_Frame(data, sparse)
if drop_first and len(levels) == 1: return get_empty_Frame(data, sparse)
continue
sp_indices = sp_indices[1:] dummy_cols = dummy_cols[1:]
dummy_mat[codes == -1] = 0
dummy_mat = dummy_mat[:, 1:] dummy_cols = dummy_cols[1:]
result = arg(self)
return cfunc(com._ensure_float64(arg), window, minp, **kwargs)
if center: offset = _offset(window, center) additional_nans = np.array([np.NaN] * offset)
pairwise = True if pairwise is None else pairwise
pairwise = True if pairwise is None else pairwise
pairwise = True if pairwise is None else pairwise
pairwise = True if pairwise is None else pairwise
for i, col in enumerate(arg1.columns): results[i] = f(arg1.iloc[:, i], arg2.iloc[:, i]) return dataframe_from_int_dict(results, arg1)
results[i][j] = results[j][i]
X = arg1 + 0 * arg2 Y = arg2 + 0 * arg1
raise TypeError("invalid type {0} for astype".format(dtype))
name = pprint_thing(self.__class__.__name__) if self._is_single_block:
return self.to_object_block(mgr=mgr).fillna(original_value, limit=limit, inplace=inplace, downcast=False)
if dtypes is False: return self
if self._is_single_block:
if dtypes is None: dtypes = 'infer'
if dtypes is None: return self
blocks = [] for i, rl in enumerate(self.mgr_locs):
if self.is_categorical_astype(dtype): return self.make_block(Categorical(self.values, **kwargs))
dtype = np.dtype(dtype) if self.dtype == dtype: if copy: return self.copy() return self
if values is None:
if self.is_datelike: values = self.to_native_types()
else: values = self.values
values = com._astype_nansafe(values.ravel(), dtype, copy=True) values = values.reshape(self.shape)
return _possibly_downcast_to_dtype(result, dtype)
if value is None: if self.is_numeric: value = np.nan
values, _, value, _ = self._try_coerce_args(self.values, value) arr_value = np.array(value)
if not self._can_hold_element(value): dtype, _ = com._maybe_promote(arr_value.dtype) values = values.astype(dtype)
elif isinstance(indexer, slice):
if _is_empty_indexer(indexer): pass
elif _is_scalar_indexer(indexer): values[indexer] = value
else: values[indexer] = value
if block.is_object and not self.is_object: block = block.convert(numeric=False)
try: block = self.make_block(transf(values.astype(value.dtype))) return block.setitem(indexer=indexer, value=value, mgr=mgr)
if not is_list_like(new) and isnull(new) and not self.is_object: new = self.fill_value
elif mask.any(): if transpose: mask = mask.T if isinstance(new, np.ndarray): new = new.T axis = new_values.ndim - axis - 1
new_blocks = [] if self.ndim > 1: for i, ref_loc in enumerate(self.mgr_locs): m = mask[i] v = new_values[i]
if m.any(): if isinstance(new, np.ndarray): n = np.squeeze(new[i % new.shape[0]]) else: n = np.array(new)
dtype, _ = com._maybe_promote(n.dtype)
n = n.astype(dtype)
block = self.make_block(values=nv[np.newaxis], placement=[ref_loc], fastpath=True)
if (self.is_bool or self.is_integer) and not self.is_timedelta: if inplace: return self else: return self.copy()
try: m = missing.clean_fill_method(method) except: m = None
try: m = missing.clean_interp_method(method, **kwargs) except: m = None
if coerce: if not self._can_hold_na: if inplace: return [self] else: return [self.copy()]
if not self.is_float: if not self.is_integer: return self data = data.astype(np.float64)
return missing.interpolate_1d(index, x, method=method, limit=limit, limit_direction=limit_direction, fill_value=fill_value, bounds_error=False, **kwargs)
interp_values = np.apply_along_axis(func, axis, data)
f_ordered = new_values.flags.f_contiguous if f_ordered: new_values = new_values.T axis = new_values.ndim - axis - 1
if f_ordered: new_values = new_values.T
values, values_mask, other, other_mask = self._try_coerce_args( transf(values), other)
def get_result(other):
if other is None: result = not func.__name__ == 'eq'
elif is_numeric_v_string_like(values, other): result = False
def handle_error():
result = np.empty(values.shape, dtype='O') result.fill(np.nan) return result
try: result = get_result(other)
except ValueError as detail: raise except Exception as detail: result = handle_error()
if not isinstance(result, np.ndarray): if not isinstance(result, np.ndarray):
if isinstance(values, np.ndarray) and is_list_like(other): raise ValueError('Invalid broadcasting comparison [%s] ' 'with block values' % repr(other))
result = transf(result)
if try_cast: result = self._try_cast_result(result)
def func(cond, values, other): if cond.ravel().all(): return values
result = np.empty(values.shape, dtype='float64') result.fill(np.nan) return result
result = func(cond, values, other) if self._can_hold_na or self.ndim == 1:
if try_cast: result = self._try_cast_result(result)
result = np.repeat(np.array([self._na_value] * len(qs)), len(values)).reshape(len(values), len(qs))
result = [_quantile(values, q * 100, axis=axis, **kw) for q in qs]
self.mgr_locs = placement
if ndim is None: if len(self.mgr_locs) != 1: ndim = 1 else: ndim = 2 self.ndim = ndim
return (issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype)
other = Timedelta(other) other_mask = isnull(other) other = other.value
rvalues.flat[imask] = np.array([Timedelta(val)._repr_base(format='all') for val in values.ravel()[imask]], dtype=object) return rvalues
blocks = [] if by_item and not self._is_single_block:
if check: try: if (self.values[locs] == values).all(): return except: pass try: self.values[locs] = values except (ValueError):
return _extend_blocks([b.convert(datetime=True, numeric=False) for b in blocks])
to_rep_re = regex and com.is_re_compilable(to_replace)
regex_re = com.is_re_compilable(regex)
if to_rep_re and regex_re: raise AssertionError('only one of to_replace and regex can be ' 'regex compilable')
if regex_re: to_replace = regex
try: pattern = to_replace.pattern except AttributeError: pattern = to_replace
if regex and pattern: rx = re.compile(to_replace) else: return super(ObjectBlock, self).replace(to_replace, value, inplace=inplace, filter=filter, regex=regex, mgr=mgr)
if isnull(value) or not isinstance(value, compat.string_types):
def re_replacer(s): try: return rx.sub(value, s) except TypeError: return s
block = self.make_block(new_values) if convert: block = block.convert(by_item=True, numeric=False)
super(CategoricalBlock, self).__init__(maybe_to_categorical(values), fastpath=True, placement=placement, **kwargs)
return self.values._slice(slicer)
if ((not com.is_categorical_dtype(result)) and isinstance(result, np.ndarray)): result = _block_shape(result, ndim=self.ndim)
if limit is not None: raise NotImplementedError("specifying a limit for 'fillna' has " "not been implemented yet")
new_values = self.values.take_nd(indexer, fill_value=fill_value)
if self.ndim == 1: new_mgr_locs = [0] else: if new_mgr_locs is None: new_mgr_locs = self.mgr_locs
values = values[slicer]
return values.reshape(1, len(values))
if com.is_datetime64tz_dtype(dtype): dtype = DatetimeTZDtype(dtype)
return super(DatetimeBlock, self)._astype(dtype=dtype, **kwargs)
raise TypeError
values = tslib.cast_to_nanoseconds(values)
if result.ndim > 1: result = result.reshape(len(result)) result = self._holder(result).tz_localize(self.values.tz)
new_values = self.values.tz_localize(None).asi8.take(indexer)
return self.values.fill_value
if issubclass(self.dtype.type, np.floating): v = float(v) self.values.fill_value = v
self.values = SparseArray(v, sparse_index=self.sp_index, kind=self.kind, dtype=v.dtype, fill_value=self.values.fill_value, copy=False)
return self.make_block(np.empty(values.shape, dtype=dtype), placement, fastpath=True)
if fill_value is None: fill_value = self.fill_value return self.make_block_same_class(self.values.take(indexer), fill_value=fill_value, placement=self.mgr_locs)
if self.ndim == 1: blocks = np.array([], dtype=self.array_dtype) else: blocks = [] return self.__class__(blocks, axes)
__bool__ = __nonzero__
def _get_items(self): return self.axes[0]
return axes_array, block_values, block_items, extra_state
if values.dtype == 'M8[us]': values = values.astype('M8[ns]') return make_block(values, placement=mgr_locs)
ax_arrays, bvalues, bitems = state[:3]
ndim = set([b.ndim for b in blocks])
if len(blocks) > 1: new_axes[1] = axes[0]
for b, sb in zip(blocks, self.blocks): b.mgr_locs = sb.mgr_locs
values = _concat._concat_compat([b.values for b in blocks])
if len(self.blocks) > 1:
values = self.as_matrix()
self._consolidate_inplace() return len(self.blocks) > 1
self._consolidate_inplace() return all([block.is_numeric for block in self.blocks])
self._consolidate_inplace() return any([block.is_datelike for block in self.blocks])
indexer = np.sort(np.concatenate([b.mgr_locs.as_array for b in blocks])) inv_indexer = lib.get_reverse_indexer(indexer, self.shape[0])
if takeable: loc = key else: loc = self.axes[axis].get_loc(key)
if isinstance(loc, (slice, np.ndarray)): new_axes[axis] = new_axes[axis][loc] else: new_axes.pop(axis)
for blk in self.blocks: newb = make_block(values=blk.values[slicer], klass=blk.__class__, fastpath=True, placement=blk.mgr_locs) new_blocks.append(newb)
if not items.is_unique: result = self._interleave() if self.ndim == 2: result = result.T return result[loc]
if not lib.isscalar(indexer): if len(indexer) == 1: loc = indexer.item() else: raise ValueError("cannot label index with a null key")
return SingleBlockManager( [block.make_block_same_class(values, placement=slice(0, len(values)), ndim=1, fastpath=True)], self.axes[1])
if values.ndim == 1: return values[full_loc[1]]
self.axes[0] = self.items[~is_deleted] self.blocks = tuple(b for blkno, b in enumerate(self.blocks) if not is_blk_deleted[blkno]) self._shape = None self._rebuild_blknos_and_blklocs()
if value_is_extension_type:
self.insert(len(self.items), item, value) return
is_deleted = np.zeros(self.nblocks, dtype=np.bool_) is_deleted[removed_blknos] = True
new_blocks.extend( make_block(values=value.copy(), ndim=self.ndim, placement=slice(mgr_loc, mgr_loc + 1)) for mgr_loc in unfit_mgr_locs)
unfit_val_items = unfit_val_locs[0].append(unfit_val_locs[1:])
self._known_consolidated = False
raise ValueError('cannot insert %s, already exists' % item)
new_axis = self.items.insert(loc, item)
self._blklocs = np.append(self._blklocs, 0) self._blknos = np.append(self._blknos, len(self.blocks))
if not allow_dups: self.axes[axis]._can_reindex(indexer)
if not blk._can_consolidate: for mgr_loc in mgr_locs: newblk = blk.copy(deep=True) newblk.mgr_locs = slice(mgr_loc, mgr_loc + 1) blocks.append(newblk)
def canonicalize(block): return (block.dtype.name, block.mgr_locs.as_array.tolist())
if fastpath: self.axes = [axis] if isinstance(block, list):
if isinstance(block, list):
if len(block) > 1: dtype = _interleaved_dtype(block) block = [b.astype(dtype) for b in block] block = _consolidate(block)
if self.index.equals(new_axis): if copy: return self.copy(deep=True) else: return self
if method is not None or limit is not None: new_values = missing.interpolate_2d(new_values, method=method, limit=limit, fill_value=fill_value)
float_items = [] complex_items = [] int_items = [] bool_items = [] object_items = [] sparse_items = [] datetime_items = [] datetime_tz_items = [] cat_items = [] extra_locs = []
if (v > 2**63 - 1).any(): object_items.append((i, k, v)) continue
block_values = np.empty(shape, dtype=object) block_values.fill(np.nan)
grouper = itertools.groupby(tuples, lambda x: x[2].dtype)
def _asarray_compat(x): if isinstance(x, ABCSeries): return x._values else: return np.asarray(x)
lcd = _lcd_dtype(counts[IntBlock]) kinds = set([i.dtype.kind for i in counts[IntBlock]]) if len(kinds) == 1: return lcd
if lcd.kind == 'u': return np.dtype('int%s' % (lcd.itemsize * 8 * 2)) return lcd
gkey = lambda x: x._consolidate_key grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)
new_mgr_locs = np.concatenate([b.mgr_locs.as_array for b in blocks]) new_values = _vstack([b.values for b in blocks], dtype)
return blocks
if dtype == _NS_DTYPE or dtype == _TD_DTYPE: new_values = np.vstack([x.view('i8') for x in to_stack]) return new_values.view(dtype)
if is_datetimelike_v_numeric(a, b): result = False
elif is_numeric_v_string_like(a, b): result = False
selector = _factor_indexer(shape[1:], labels) mask = np.zeros(np.prod(shape), dtype=bool) mask.put(selector, True)
for blkno, indexer in lib.get_blkno_indexers(blknos, group): yield blkno, BlockPlacement(indexer)
if not is_list_like(n): n = np.array([n] * len(m))
try: nn = n[m]
if not _is_na_compat(v, nn[0]): raise ValueError
if unit.is_null: null_upcast_classes[upcast_cls].append(dtype) else: upcast_classes[upcast_cls].append(dtype)
raise AssertionError("Concatenating join units along axis0")
concat_values = to_concat[0] if copy and concat_values.base is not None: concat_values = concat_values.copy()
mgr_shape = list(mgr.shape) for ax, indexer in indexers.items(): mgr_shape[ax] = len(indexer) mgr_shape = tuple(mgr_shape)
((ax0_indexer is None and blk.mgr_locs.is_slice_like and blk.mgr_locs.as_slice.step == 1) or (np.diff(ax0_blk_indexer) == 1).all()))
if unit_no_ax0_reindexing: join_unit_indexers.pop(0, None) else: join_unit_indexers[0] = ax0_blk_indexer
next_items[i] = (plc[min_len:], trim_join_unit(unit, min_len))
if indexers is None: indexers = {} self.block = block self.indexers = indexers self.shape = shape
if (indexer == -1).any(): return True
fill_value = self.block.fill_value values = self.block.get_values()
values = self.block.values.ravel(order='K') if len(values) and values[0] is None: fill_value = None
return self.block.values
values = self.block.astype(np.object_).values
values = self.block.get_values()
values = values.view()
return np.empty((0, 2), dtype=arr.dtype)
return object.__repr__(self)
return super(PandasObject, self).__sizeof__()
if overwrite or not hasattr(cls, name): setattr(cls, name, f)
return self.accessor_cls
if isinstance(v, dict): is_nested_renamer = True
keys = list(compat.iterkeys(arg)) result = compat.OrderedDict()
if is_nested_renamer: result = list(_agg(arg, _agg_1dim).values())
elif self._selection is not None:
if len(sl) == 1:
elif not len(sl - set(compat.iterkeys(arg))):
else:
else:
result = _agg(arg, _agg_2dim)
return result, True
if obj.ndim == 1: for a in arg: try: colg = self._gotitem(obj.name, ndim=1, subset=obj) results.append(colg.aggregate(a))
name = com._get_callable_name(a) or a keys.append(name)
if subset is None: subset = self.obj
def __getslice__(self, i, j): return self.__class__(super(FrozenList, self).__getslice__(i, j))
if isinstance(n, slice): return self.__class__(super(FrozenList, self).__getitem__(n)) return super(FrozenList, self).__getitem__(n)
__array_priority__ = 1000
raise ValueError('can only convert an array of size 1 to a ' 'Python scalar')
return self.values.searchsorted(key, side=side, sorter=sorter)
if com.is_numeric_v_string_like(arr, x): mask = False else: mask = arr == x
if lib.isscalar(mask): mask = np.zeros(arr.shape, dtype=bool)
if com.is_numeric_v_string_like(arr, x): mask |= False else: mask |= arr == x
if method in [None, 'asfreq']: return None
result = np.empty_like(np.asarray(xvalues), dtype=np.float64) result.fill(np.nan) return result
raise ValueError('time-weighted interpolation only works ' 'on Series or DataFrames with a ' 'DatetimeIndex')
alt_methods = { 'barycentric': interpolate.barycentric_interpolate, 'krogh': interpolate.krogh_interpolate, 'from_derivatives': _from_derivatives, 'piecewise_polynomial': _from_derivatives, }
x, new_x = x._values.astype('i8'), new_x.astype('i8')
if not order: raise ValueError("order needs to be specified and greater than 0") terp = interpolate.UnivariateSpline(x, y, k=order, **kwargs) new_y = terp(new_x)
method = interpolate.BPoly.from_derivatives m = method(xi, yi.reshape(-1, 1), orders=order, extrapolate=extrapolate)
P = interpolate.Akima1DInterpolator(xi, yi)
ndim = values.ndim if values.ndim == 1:
if ndim == 1: values = values[0]
pass
pass
mask = ((y == 0) & ~np.isnan(result)).ravel()
_register_xlsx('openpyxl', 'xlsxwriter')
if fastpath:
if not isinstance(data, SingleBlockManager): data = SingleBlockManager(data, index, fastpath=True) if copy: data = data.copy() if index is None: index = data.index
if name is None: name = data.name
data = _dict_compat(data) data = lib.fast_multiget(data, index.astype('O'), default=np.nan)
if ((dtype is not None) and not is_categorical_dtype(dtype)): raise ValueError("cannot specify a dtype with a " "Categorical unless " "dtype='category'")
if isinstance(data, ABCSparseArray): data = data.to_dense()
if isinstance(arr, ABCSparseArray): from pandas.sparse.series import SparseSeries cls = SparseSeries
@property def _can_hold_na(self): return self._data._can_hold_na
return self.index.is_all_dates
if fastpath: self._data.set_axis(axis, labels)
return generic.NDFrame._update_inplace(self, result, **kwargs)
@property def real(self): return self.values.real
__float__ = _coerce_method(float) __long__ = _coerce_method(int) __int__ = _coerce_method(int)
data = np.empty(nd_state[1], dtype=nd_state[2]) np.ndarray.__setstate__(data, nd_state)
index, name = own_state[0], None if len(own_state) > 1: name = own_state[1]
self._data = SingleBlockManager(data, index, fastpath=True) self._index = index self.name = name
values = self._values if isinstance(values, np.ndarray): return _index.get_value_at(values, i) else: return values[i]
if not self.index.is_unique: result = self._constructor( result, index=[key] * len(result), dtype=self.dtype).__finalize__(self)
pass
new_key = self.index._convert_scalar_indexer(key, kind='getitem') if type(new_key) != type(key): return self.__getitem__(new_key) raise
if not isinstance(key, (list, np.ndarray, Series, Index)): key = list(key)
if isinstance(key, (list, tuple)): return self.ix[key]
if isinstance(key[0], slice): return self._get_values(key) raise
if any(k is None for k in key): return self._get_values(key)
indexer, new_index = self.index.get_loc_level(key) return self._constructor(self._values[indexer], index=new_index).__finalize__(self)
if isnull(value): value = tslib.iNaT
cacher_needs_updating = self._check_is_chained_assignment_possible() setitem(key, value) if cacher_needs_updating: self._maybe_update_cacher()
nv.validate_reshape(tuple(), kwargs) return self
self.loc[label] = value return self
self.name = name or self.name
argmin = idxmin argmax = idxmax
return result
mask = this_mask ^ other_mask this_vals[this_mask & mask] = fill_value other_vals[other_mask & mask] = fill_value
result.name = None
if inplace and self._is_cached: raise ValueError("This Series is a view of some other array, to " "sort in-place you must create a copy")
try: return arr.argsort(kind=kind) except TypeError: return arr.argsort(kind='quicksort')
self._get_axis_number(axis) if numeric_only: raise NotImplementedError('Series.{0} does not implement ' 'numeric_only.'.format(name)) return op(delegate, skipna=skipna, **kwds)
new_values = algos.take_1d(self.get_values(), indexer) return self._constructor(new_values, index=new_index)
if convert: indices = maybe_convert_indices(indices, len(self._get_axis(axis)))
result = df.to_csv(path, index=index, sep=sep, na_rep=na_rep, float_format=float_format, header=header, index_label=index_label, mode=mode, nanRep=nanRep, encoding=encoding, date_format=date_format, decimal=decimal) if path is None: return result
pass
if data.dtype.kind in ['M', 'm']: data = _sanitize_array(data, index, copy=copy)
if take_fast_path: if _possibly_castable(arr) and not copy and dtype is None: return arr
if isinstance(data, (np.ndarray, Index, Series)):
subarr = _sanitize_index(data, index, copy=True)
if subarr.ndim == 0:
if dtype is None: dtype, value = _infer_dtype_from_scalar(value) else: value = _possibly_cast_to_datetime(value, dtype)
elif subarr.ndim == 1: if index is not None:
if len(subarr) != len(index) and len(subarr) == 1: subarr = create_from_value(subarr[0], index, subarr.dtype)
if issubclass(subarr.dtype.type, compat.string_types): subarr = np.array(data, dtype=object, copy=copy)
class TimeSeries(Series): def __init__(self, *args, **kwargs): warnings.warn("TimeSeries is deprecated. Please use Series", FutureWarning, stacklevel=2)
ops.add_flex_arithmetic_methods(Series, **ops.series_flex_funcs) ops.add_special_arithmetic_methods(Series, **ops.series_special_funcs)
if op in ['__eq__', '__ne__']: return getattr(np.array(self), op)(np.array(other))
__array_priority__ = 1000 _typ = 'categorical'
self._codes = _coerce_indexer_dtype(values, categories) self._categories = self._validate_categories( categories, fastpath=isinstance(categories, ABCIndexClass)) self._ordered = ordered return
if is_categorical_dtype(values):
if isinstance(values, (ABCSeries, ABCCategoricalIndex)): values = values._values
raise TypeError("'values' is not ordered, please " "explicitly specify the categories order " "by passing in a categories argument.")
raise NotImplementedError("> 1 ndim Categorical are not " "supported at this time")
categories = self._validate_categories(categories) codes = _get_codes_for_values(values, categories)
msg = ('\nSetting NaNs in `categories` is deprecated and ' 'will be removed in a future version of pandas.') warn(msg, FutureWarning, stacklevel=3)
return self._categories
self._codes[self._codes >= len(new_categories)] = -1
if any(isnull(removals)): not_included = [x for x in not_included if notnull(x)] new_categories = [x for x in new_categories if notnull(x)]
if '_ordered' not in state:
if 'ordered' in state: state['_ordered'] = state.pop('ordered') else: state['_ordered'] = False
if com.is_datetimelike(self.categories): return self.categories.take(self._codes, fill_value=np.nan) return np.array(self)
if method is not None:
assert isnull(fill_value)
if isinstance(slicer, tuple) and len(slicer) == 2: if not is_null_slice(slicer[0]): raise AssertionError("invalid slicing for a 1-ndim " "categorical") slicer = slicer[1]
category_strs = [x.strip() for x in category_strs] return category_strs
max_width = 0
return levheader + "[" + levstring.replace(" < ... < ", " ... ") + "]"
if isinstance(value, Categorical): if not value.categories.equals(self.categories): raise ValueError("Cannot set a Categorical with another, " "without identical categories")
if len(to_add) and not isnull(to_add).all(): raise ValueError("Cannot setitem on a Categorical with a new " "category, set the categories first")
if isinstance(key, (int, np.integer)): pass
elif isinstance(key, slice): pass
return [list_like]
import warnings import operator import weakref import gc
if dtype.kind == 'V': raise NotImplementedError("compound dtypes are not implemented" "in the {0} constructor" .format(self.__class__.__name__))
prepr = '[%s]' % ','.join(map(pprint_thing, self)) return '%s(%s)' % (self.__class__.__name__, prepr)
setattr(cls, '_typ', cls.__name__.lower())
cls._ix = None
if build_axes:
if isinstance(ns, dict): for k, v in ns.items(): setattr(cls, k, v)
args = list(args) for a in self._AXIS_ORDERS:
if a not in kwargs: try: kwargs[a] = args.pop(0) except IndexError: if require_all: raise TypeError("not enough/duplicate arguments " "specified!")
axis_index = getattr(self, axis) d = dict() prefix = axis[0]
key = '{prefix}level_{i}'.format(prefix=prefix, i=i) level = i
if isinstance(axis_index, MultiIndex): dindex = axis_index else: dindex = axis_index.to_series()
return [self._get_axis(a) for a in self._AXIS_ORDERS]
if len(axes) != len(set(axes)): raise ValueError('Must specify %s unique axes' % self._AXIS_LEN)
def _get_rename_function(mapper): if isinstance(mapper, (dict, ABCSeries)):
for axis in lrange(self._AXIS_LEN): v = axes.get(self._AXIS_NAMES[axis]) if v is None: continue f = _get_rename_function(v)
if not np.prod(self.shape): return self
return self
meta = set(self._internal_names + self._metadata) for k in list(meta): if k in state: v = state[k] object.__setattr__(self, k, v)
self._unpickle_matrix_compat(state)
coords = [(a, self._get_axis(a)) for a in self._AXIS_ORDERS] return xarray.DataArray(self, coords=coords, )
cls._internal_names_set.add(iname)
res.is_copy = self.is_copy
if ref is None: del self._cacher else: try: ref._maybe_cache_changed(cacher[0], self) except: pass
is_copy = axis != 0 or result._is_view result._set_is_copy(self, copy=is_copy) return result
df.iloc[0:5]['group'] = 'a'
try: gc.collect(2) if not gc.get_referents(self.is_copy()): self.is_copy = None return except: pass
try: if self.is_copy().shape == self.shape: self.is_copy = None return except: pass
if isinstance(self.is_copy, string_types): t = self.is_copy
self._data.delete(key)
try: del self._item_cache[key] except KeyError: pass
if is_copy: if not result._get_axis(axis).equals(self._get_axis(axis)): result._set_is_copy(self)
if isinstance(loc, slice): lev_num = labels._get_level_number(level) if labels.levels[lev_num].inferred_type == 'integer': loc = labels[loc]
indexer = [slice(None)] * self.ndim indexer[axis] = loc indexer = tuple(indexer)
if not is_list_like(new_values) or self.ndim == 1: return _maybe_box_datetimelike(new_values)
result._set_is_copy(self, copy=not result._is_view) return result
if self._needs_reindex_multi(axes, method, level): try: return self._reindex_multi(axes, copy, fill_value) except: pass
return self._reindex_axes(axes, level, limit, tolerance, method, fill_value, copy).__finalize__(self)
new_data = self._data for axis in sorted(reindexers.keys()): index, indexer = reindexers[axis] baxis = self._get_block_manager_axis(axis)
rs = com._random_state(random_state)
if weights is not None:
if isinstance(weights, pd.Series): weights = weights.reindex(self.axes[axis])
weights = weights.fillna(0)
if n < 0: raise ValueError("A negative number of rows requested. Please " "provide positive value.")
try: if np.isnan(value): return True except: pass
return self.as_matrix()
combined = self._data.combine(blocks, copy=copy) result[dtype] = self._constructor(combined).__finalize__(self)
if axis is None: axis = 0 axis = self._get_axis_number(axis) method = missing.clean_fill_method(method)
if self.ndim > 3: raise NotImplementedError('Cannot fillna with a method for > ' '3dims')
elif self.ndim == 3:
method = missing.clean_fill_method(method) new_data = self._data.interpolate(method=method, axis=axis, limit=limit, inplace=inplace, coerce=True, downcast=downcast)
if not is_dictlike(to_replace) and not is_dictlike(regex): to_replace = [to_replace]
to_rep_dict = {} value_dict = {}
for a in self._AXIS_ORDERS: if not len(self._get_axis(a)): return self
if method == 'linear': index = np.arange(len(_maybe_transposed_self._get_axis(alt_ax))) else: index = _maybe_transposed_self._get_axis(alt_ax)
if not offset.isAnchored() and hasattr(offset, '_inc'): if end_date in self.index: end = self.index.searchsorted(end_date, side='left')
if numeric_only is None: try: return ranker(self) except TypeError: numeric_only = True
join_index, join_columns = None, None ilidx, iridx = None, None clidx, cridx = None, None
right = other._reindex_with_indexers({0: [join_index, iridx], 1: [join_columns, cridx]}, copy=copy, fill_value=fill_value, allow_dups=True)
if isinstance(self, ABCSeries): if axis: raise ValueError('cannot align series to a series other than ' 'axis 0')
try_quick = True if hasattr(other, 'align'):
if other.ndim <= self.ndim:
if (axis is None and not all([other._get_axis(i).equals(ax) for i, ax in enumerate(self.axes)])): raise InvalidIndexError
else: raise NotImplemented("cannot align with a higher dimensional " "NDFrame")
try: new_other = np.array(other, dtype=self.dtype) except ValueError: new_other = np.array(other)
is_i8 = com.needs_i8_conversion(self.dtype) if is_i8: matches = False else: matches = (new_other == np.array(other))
try_quick = False
if len(other) == 1: other = np.array(other[0])
elif len(cond[icond]) == len(other):
if try_quick:
if not try_quick:
else: other = self._constructor(other, **self._construct_axes_dict())
if ax.is_all_dates: from pandas.tseries.tools import to_datetime before = to_datetime(before) after = to_datetime(after)
self._check_percentile(percentiles)
if 0.5 not in percentiles: percentiles.append(0.5) percentiles = np.asarray(percentiles)
unique_pcts = np.unique(percentiles) if len(unique_pcts) < len(percentiles): raise ValueError("percentiles cannot contain duplicates") percentiles = unique_pcts
data = self.select_dtypes(include=[np.number])
names = [] ldesc_indexes = sorted([x.index for x in ldesc], key=len) for idxnames in ldesc_indexes: for name in idxnames: if name not in names: names.append(name)
for _name, _indexer in indexing.get_indexers_list(): NDFrame._create_indexer(_name, _indexer)
return _map(f, arr, na_mask=True, na_value=na_result, dtype=dtype)
warnings.warn("In future versions of pandas, match will change to" " always return a bool indexer.", FutureWarning, stacklevel=3)
dtype = bool f = lambda x: bool(regex.match(x))
if regex.groups == 0: raise ValueError("pattern contains no capture groups")
f = lambda x: x.decode(encoding, errors)
f = lambda x: x.encode(encoding, errors)
self._orig = data self._freeze()
expand = False if result.ndim == 1 else True
if name is None: name = getattr(result, 'name', None) if name is None: name = self._orig.name
if isinstance(self._orig, Index): if is_bool_dtype(result): return result
cons = self._orig._constructor return cons(result, name=name, index=index)
def _make_str_accessor(self): from pandas.core.index import Index
from datetime cimport *
PyDateTime_IMPORT
import_array() import_ufunc()
take_1d_%(name)s_%(dest)s_memview(values, indexer, out, fill_value=fill_value) return
take_2d_axis0_%(name)s_%(dest)s_memview(values, indexer, out, fill_value=fill_value) return
take_2d_axis1_%(name)s_%(dest)s_memview(values, indexer, out, fill_value=fill_value) return
if N == 0: return
if N == 0: return
if N == 0: return
if N == 0: return
return False, False
if val == val and val != %(nan_val)s: nobs[lab, j] += 1 resx[lab, j] = val
if val == val and val != %(nan_val)s: nobs[lab, j] += 1 if nobs[lab, j] == rank: resx[lab, j] = val
if val == val: nobs[lab, j] += 1 sumx[lab, j] += val
if val == val: nobs[lab, 0] += 1 sumx[lab, 0] += val
if val == val: nobs[lab, j] += 1 prodx[lab, j] *= val
if val == val: nobs[lab, 0] += 1 prodx[lab, 0] *= val
if val == val and val != %(nan_val)s: nobs[lab, j] += 1 if val > maxx[lab, j]: maxx[lab, j] = val
if val == val and val != %(nan_val)s: nobs[lab, 0] += 1 if val > maxx[lab, 0]: maxx[lab, 0] = val
if val == val and val != %(nan_val)s:
if val == val and val != %(nan_val)s: nobs[lab, 0] += 1 if val < minx[lab, 0]: minx[lab, 0] = val
if val == val: nobs[lab, j] += 1 sumx[lab, j] += val
if val == val: nobs[lab, 0] += 1 sumx[lab, 0] += val
break
break
break
break
break
break
break
break
#('object', 'OBJECT', 'object_'),
ptr += _counts[0] for j in range(ngroups): size = _counts[j + 1] out[j, i] = _median_linear(ptr, size) ptr += size
label_indexer = np.zeros((ngroups, periods), dtype=np.int64) with nogil: for i in range(N):
directory = os.path.dirname(os.path.realpath(__file__)) filename = 'generated.pyx' path = os.path.join(directory, filename)
load = unpack loads = unpackb
from __future__ import print_function
tm.assertRaisesRegexp(TypeError, self.mutable_regex, *args, **kwargs)
self.check_result(r, self.lst)
self.assertIsInstance(self.container._shallow_copy(), FrozenNDArray)
def testit(container): container[0] = 16
delegate = self.Delegate(self.Delegator()) sys.getsizeof(delegate)
return False
if filter is not None: filt = o.index if isinstance(o, Series) else o if not filter(filt): continue
if not ignore_failures: for o in self.not_valid_objs:
expected_str = ' '.join([operand2, op, operand1]) self.assertTrue(expected_str in getattr(klass, 'r' + op_name).__doc__)
for o in self.is_valid_objs: if isinstance(o, Series):
try: self.assertIsNotNone(o.data) except ValueError: pass
expected = expected.astype('M8[ns]').astype('int64') self.assertEqual(result.value, expected)
for op in ['max', 'min']: for klass in [Index, Series]:
self.assertEqual(getattr(obj, op)(), datetime(2011, 11, 1))
self.assertEqual(getattr(obj, op)(), datetime(2011, 11, 1))
if isinstance(o, PeriodIndex):
expected_index = pd.Index(o[::-1]) expected_index.name = None
o = o.repeat(range(1, len(o) + 1)) o.name = 'a'
expected_index = pd.Index(o[::-1]) expected_index.name = None
o = o.repeat(range(1, len(o) + 1)) o.name = 'a'
if isinstance(o, PeriodIndex):
result = o.unique() self.assert_numpy_array_equal(result[1:], values[2:])
hist = s.value_counts(ascending=True) expected = Series([1, 2, 3, 4], index=list('cdab')) tm.assert_series_equal(hist, expected)
self.assertRaises(TypeError, lambda bins: s.value_counts(bins=bins), 1)
self.assert_numpy_array_equal(s.unique(), np.array([]), check_dtype=False) self.assertEqual(s.nunique(), 0)
s = df['dt'].copy() s = klass([v for v in s.values] + [pd.NaT])
td = df.dt - df.dt + timedelta(1) td = klass(td, name='dt')
self.assert_index_equal(uniques, exp_uniques, check_names=False)
if isinstance(o, Index) and o.is_boolean(): continue
for original in self.objs: if isinstance(original, Index):
if original.is_boolean(): result = original.drop_duplicates() expected = Index([False, True], name='a') tm.assert_index_equal(result, expected) continue
self.assertFalse(original.has_duplicates)
def get_fill_value(obj): if isinstance(obj, pd.tseries.base.DatetimeIndexOpsMixin): return obj.asobject.values[0] else: return obj.values[0]
fill_value = get_fill_value(o)
if o.values.dtype == 'datetime64[ns]' or isinstance( o, PeriodIndex): values[0:2] = pd.tslib.iNaT else: values[0:2] = null_obj
expected = [fill_value.ordinal] * 2 + list(values[2:]) expected = klass(ordinal=expected, freq=o.freq) o = klass(ordinal=values, freq=o.freq)
self.assertFalse(o is result)
self.assertTrue(res_deep > res)
diff = res_deep - sys.getsizeof(o) self.assertTrue(abs(diff) < 100)
for o in self.objs: index = np.searchsorted(o, max(o)) self.assertTrue(0 <= index <= len(o))
self.assertIs(getattr(t, "__frozen"), True)
exit=False)
raise nose.SkipTest("skipping for now")
none_coerced = block._try_coerce_args(block.values, None)[2] self.assertTrue(pd.Timestamp(none_coerced) is pd.NaT)
self.assertRaises(AssertionError, BlockManager, blocks, axes)
self.assertTrue(hasattr(mgr2, "_is_consolidated")) self.assertTrue(hasattr(mgr2, "_known_consolidated"))
self.assertFalse(mgr2._is_consolidated) self.assertFalse(mgr2._known_consolidated)
self.assertTrue(cp_blk.equals(blk)) self.assertTrue(cp_blk.values.base is blk.values.base)
self.assertEqual(mgr.as_matrix().dtype, np.float64)
for b in old_blocks: found = False for nb in new_blocks: if (b.values == nb.values).all(): found = True break self.assertTrue(found)
mgr = create_mgr('f: i8; g: f8') new_mgr = mgr.convert() _compare(mgr, new_mgr)
mgr = create_mgr('a: f8; b: i8; c: f8; d: i8; e: f8;' 'f: bool; g: f8-2')
create_single_mgr('sparse_na', N),
mat = mgr.as_matrix()
yield assert_slice_ok, mgr, ax, [] yield assert_slice_ok, mgr, ax, lrange(mgr.shape[ax])
yield assert_take_ok, mgr, ax, [] yield assert_take_ok, mgr, ax, [0, 0, 0] yield assert_take_ok, mgr, ax, lrange(mgr.shape[ax])
import nose
def test_basic_drop_first(self): s_list = list('abc') s_series = Series(s_list) s_series_index = Series(s_list, list('ABC'))
s_list = list('aaa') s_series = Series(s_list) s_series_index = Series(s_list, list('ABC'))
from itertools import product
def f(): np.dtype(self.dtype)
self.assertTrue(is_categorical_dtype(s.dtype)) self.assertTrue(is_categorical_dtype(s)) self.assertFalse(is_categorical_dtype(np.dtype('float64')))
self.assertTrue(is_dtype_equal(np.dtype("M8[ns]"), "datetime64[ns]"))
import nose import numpy as np
from __future__ import print_function import nose
agged = grouped.aggregate([np.mean, np.std]) agged = grouped.aggregate({'one': np.mean, 'two': np.std})
self.assertRaises(Exception, grouped.aggregate, lambda x: x * 2)
g[['A', 'C']]
grouped['B'].first() grouped['B'].last() grouped['B'].nth(0)
v = s[g == 1].iloc[0] self.assertEqual(expected.iloc[0], v) self.assertEqual(expected2.iloc[0], v)
grouped = self.three_group.groupby(['A', 'B']) result = grouped.nth(0) expected = grouped.first() assert_frame_equal(result, expected)
self.assertRaises(ValueError, self.df.groupby, [])
df = self.df_mixed_floats.copy() df['value'] = lrange(len(df))
def f(grp): if grp.name == 'Pony': return None return grp.iloc[0]
def f(grp): if grp.name == 'Pony': return None return grp.iloc[0].loc['C']
grouped = self.ts.groupby(self.ts * np.nan) self.assertEqual(self.ts.dtype, np.float64)
result = grouped['A'].agg('std') expected = grouped['A'].std() assert_series_equal(result, expected)
result = grouped.aggregate('var') expected = grouped.var() assert_frame_equal(result, expected)
T = [1.0 * x for x in lrange(1, 10) * 10][:1095] result = Series(T, lrange(0, len(T)))
df = tm.makeTimeDataFrame() g = df.groupby(pd.TimeGrouper('M')) g.transform(lambda x: x - 1)
grouped = values.groupby(labels) agged = grouped.agg(len) expected = Series([4, 2], index=['bar', 'foo'])
def f(x): return float(len(x))
result1 = df.groupby('a').apply(f1) result2 = df2.groupby('a').apply(f1) assert_frame_equal(result1, result2)
self.assertRaises(AssertionError, df.groupby('a').apply, f2) self.assertRaises(AssertionError, df2.groupby('a').apply, f2)
self.assertRaises(AssertionError, df.groupby('a').apply, f3) self.assertRaises(AssertionError, df2.groupby('a').apply, f3)
result = grouped.dtype expected = grouped.agg(lambda x: x.dtype)
self.assertRaises(AttributeError, getattr, grouped, 'foo')
aggregated = grouped.aggregate(np.mean) self.assertEqual(len(aggregated), 5) self.assertEqual(len(aggregated.columns), 4)
for weekday, group in grouped: self.assertEqual(group.index[0].weekday(), weekday)
groups = grouped.groups indices = grouped.indices
grouped = self.tsframe.groupby([lambda x: x.weekday(), lambda x: x.year ])
for g in grouped.grouper.groupings[0]: pass
aggregated = grouped.aggregate(np.mean) self.assertEqual(len(aggregated), len(self.tsframe)) self.assertEqual(len(aggregated.columns), 2)
for k, v in grouped: self.assertEqual(len(v.columns), 2)
df.groupby(1, as_index=False)[2].agg({'Q': np.mean})
self.assertRaises(Exception, grouped['C'].__getitem__, 'D')
assert_frame_equal(df.loc[[0, 2]], g_not_as.head(1)) assert_frame_equal(df.loc[[1, 2]], g_not_as.tail(1))
df_as = df
df = self.df.copy() df['bad'] = np.nan agged = df.groupby(['A', 'B']).mean()
return result
result.index.name = 'stat_%d' % len(group)
return result
df = DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})
self.assertRaises(ValueError, self.df.groupby, level=1)
frame = self.mframe
a = Series(data=np.arange(4) * (1 + 2j), index=[0, 0, 1, 1]) expected = Series((1 + 2j, 5 + 10j))
df = DataFrame({'key': [1, 1, 1, 2, 2, 2, 3, 3, 3], 'value': range(9)})
df = DataFrame({'key': [1, 1, 1, 2, 2, 2, 3, 3, 3], 'value': lrange(9)})
f = lambda x: x[:2]
grouped = df.groupby(['name', 'name2']) grouped.apply(lambda x: x.sort_values('value', inplace=True))
result = df.groupby('A', as_index=False).cumsum() assert_frame_equal(result, expected)
with option_context('mode.chained_assignment', None): for key, group in grouped: res = f(group) assert_frame_equal(res, result.ix[key])
def summarize(df, name=None): return Series({'count': 1, 'mean': 2, 'omissions': 3, }, name=name)
return Series({ 'count': 1, 'mean': 2, 'omissions': 3, }, name=df.iloc[0]['A'])
grouped = values.groupby(labels)
f = lambda x: len(set(map(id, x.index))) grouped.agg(f)
assert (len(x.base) > 0) return Decimal(str(x.mean()))
df = DataFrame({'foo': [0, 1], 'bar': [3, 4], 'val': np.random.randn(2)})
self.assertEqual(grouped.groups[k], e)
tm.assert_frame_equal(grouped.grouper.groupings[0].obj, df) self.assertEqual(grouped.ngroups, 2)
self.assertTrue(_int64_overflow_possible(gr.grouper.shape))
s = self.frame['C'].copy() s.name = None
grouped = self.df.groupby('A')
df = DataFrame({'foo': [1, 2], 'bar': [3, 4]}).astype(np.int64)
grouped.apply(f) grouped.aggregate(freduce) grouped.aggregate({'C': freduce, 'D': freduce}) grouped.transform(f)
assert_frame_equal(result_sort, df.groupby(col, sort=False).first())
assert_frame_equal(result_sort, df.groupby(col, sort=True).first()) assert_frame_equal(result_nosort, df.groupby(col, sort=False).first())
assert_frame_equal(result_sort, df.groupby(col, sort=False).first())
levels = pd.date_range('2014-01-01', periods=4) codes = np.random.randint(0, 4, size=100)
groups = grouped.groups tm.assertIsInstance(list(groups.keys())[0], datetime)
c = 24650000000000000
self.assertRaises(ValueError, lambda: series.groupby(bins).mean())
tm.assert_frame_equal(lexsorted_df, not_lexsorted_df)
by_columns = df.reset_index().groupby(idx_names).mean()
df_grouped.apply(lambda x: noddy(x.value, x.weight))
tm._skip_if_no_pytz() import pytz
df_reordered = df_original.sort_values(by='Quantity')
expected_list = [df_original.iloc[[0, 1, 5]], df_original.iloc[[2, 3]], df_original.iloc[[4]]] dt_list = ['2013-09-30', '2013-10-31', '2013-12-31']
df_original = df_original.set_index('Date') df_reordered = df_original.sort_values(by='Quantity')
actual = grouped_df.filter(lambda x: len(x) > 1) expected = df.iloc[expected_indexes] assert_frame_equal(actual, expected)
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
assert_series_equal(actual, expected)
actual = grouped_df.pid.transform(len) assert_series_equal(actual, expected)
actual = grouped_df.filter(lambda x: len(x) > 1) expected = df.iloc[expected_indexes] assert_frame_equal(actual, expected)
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
assert_series_equal(actual, expected)
actual = grouped_df.pid.transform(len) assert_series_equal(actual, expected)
actual = grouped_df.filter(lambda x: len(x) > 1) expected = df.iloc[expected_indexes] assert_frame_equal(actual, expected)
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
assert_series_equal(actual, expected)
actual = grouped_df.pid.transform(len) assert_series_equal(actual, expected)
actual = grouped_df.filter(lambda x: len(x) > 1) expected = df.iloc[expected_indexes] assert_frame_equal(actual, expected)
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
assert_series_equal(actual, expected)
actual = grouped_df.pid.transform(len) assert_series_equal(actual, expected)
actual = grouped_df.filter(lambda x: len(x) > 1) expected = df.iloc[expected_indexes] assert_frame_equal(actual, expected)
actual = grouped_ser.filter(lambda x: len(x) > 1) expected = ser.take(expected_indexes) assert_series_equal(actual, expected)
assert_series_equal(actual, expected)
actual = grouped_df.pid.transform(len) assert_series_equal(actual, expected)
filt = g.filter(lambda x: x['A'].sum() == 2) assert_frame_equal(filt, df.iloc[[0, 1]])
rng = pd.date_range('2014', periods=len(self.df)) self.df.index = rng
])
defined_but_not_allowed = ("(?:^Cannot.+{0!r}.+{1!r}.+try using the " "'apply' method$)")
items = [nan] * 5 + list(range(100)) + [nan] * 5 items2 = np.array(items, dtype='O')
np.argsort(np.array([[1, 2], [1, 3], [1, 2]], dtype='i')) np.argsort(items2, kind='mergesort')
raise RaisingObjectException(self.msg)
dtypes = [np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint32, np.uint64, np.float32, np.float64]
labels = np.array([0, 0, 0, 0, 0], dtype=np.int64)
for (op, args), targop in ops: for data in [s, s_missing]: expected = data.groupby(labels).transform(targop)
for df in [df, df2]: for gb_target in [dict(by=labels), dict(level=0), dict(by='string')
if op == 'shift': gb._set_selection_from_grouper()
df = pd.DataFrame({'key': ['b'] * 10, 'value': 2})
groups_single_key = test.groupby("cat") res = groups_single_key.agg('mean')
test_df = DataFrame({'groups': [0, 0, 1, 1], 'random_vars': [8, 7, 4, 5]})
df = pd.DataFrame({'A': [1, 2, 1], 'B': [1, 2, 3]}) g = df.groupby('A')
import nose from pandas import compat import pandas.formats.printing as printing import pandas.formats.format as fmt import pandas.util.testing as tm import pandas.core.config as cf
return adj.justify([x], *args, **kwargs)[0]
r = repr(df) for ix, l in enumerate(r.splitlines()): if not r.split()[cand_col] == '...': return False return True
df = DataFrame('hello', lrange(1000), lrange(5))
self.assertFalse(has_expanded_repr(df6)) self.assertFalse(has_truncated_repr(df6))
self.assertFalse(has_expanded_repr(df10)) self.assertTrue(has_vertically_truncated_repr(df10))
self.assertTrue(has_expanded_repr(df))
self.assertTrue(has_horizontally_truncated_repr(df))
self.assertTrue(has_expanded_repr(df)) self.assertTrue(has_vertically_truncated_repr(df))
repr(df)
_stdin = sys.stdin try: sys.stdin = None repr(df) finally: sys.stdin = _stdin
with option_context('display.unicode.east_asian_width', True):
with option_context('display.max_rows', 3, 'display.max_columns', 3):
buf.getvalue()
df = DataFrame({'text': ['some words'] + [None]*9})
df = DataFrame({'date' : [pd.Timestamp('20130101').tz_localize('UTC')] + [pd.NaT]*5})
df.pivot_table(index=[u('clé1')], columns=[u('clé2')])._repr_html_()
df = DataFrame({'foo': np.inf * np.empty(10)}) repr(df)
self.assertTrue(not "\t" in pp_t("a\tb", escape_chars=("\t", )))
biggie = DataFrame({'A': randn(200), 'B': tm.makeStringIndex(200)}, index=lrange(200))
df = df * 0 result = df.to_string() expected = (' 0\n' '0 0\n' '1 0\n' '2 -0')
repr(df)
biggie = DataFrame({'A': randn(200), 'B': tm.makeStringIndex(200)}, index=lrange(200))
self.assertEqual(result, expected_without_index)
with tm.ensure_clean('test.tex') as path: self.assertRaises(UnicodeEncodeError, df.to_latex, path)
self.frame.to_latex()
self.frame.to_latex(column_format='ccc')
df = DataFrame({'col1': [1], 'col2': ['a'], 'col3': [10.1]})
self.assertEqual(df.set_index('a').to_csv(decimal='^'), expected)
self.assertEqual( df.set_index(['a', 'b']).to_csv(decimal="^"), expected)
self.assertEqual( df.set_index(['a', 'b']).to_csv(float_format='%.2f'), expected)
result = self.ts[:0].to_string() self.assertEqual(result, 'Series([], Freq: B)')
with option_context('display.unicode.east_asian_width', True):
result = str(s2.index) self.assertTrue('NaT' in result)
s = Series(randn(8), None)
df = pd.DataFrame({0: [1, 2, 3]}) df.style._translate()
import collections from datetime import datetime, timedelta import re
result = isnull(['foo', 'bar']) assert (not result.any())
pidx = idx.to_period(freq='M') mask = isnull(pidx) assert (mask[0]) assert (not mask[1:].any())
self.assertFalse(com.is_number(np.bool_(False))) self.assertTrue(com.is_number(np.timedelta64(1, 'D')))
self.assertTrue(com.is_integer(np.timedelta64(1, 'D')))
assert remaining + nfalse == len(mask)
ncols = 8 for i in range(2 ** ncols):
test_locs([]) test_locs([0]) test_locs([1])
class HashableClass(object): pass
assert not com.is_hashable(np.array([]))
if compat.PY2:
state = com._random_state(5) tm.assert_equal(state.uniform(), npr.RandomState(5).uniform())
state2 = npr.RandomState(10) tm.assert_equal( com._random_state(state2).uniform(), npr.RandomState(10).uniform())
assert com._random_state() is np.random
with tm.assertRaises(ValueError): com._random_state('test')
assert (not com.is_timedelta64_ns_dtype(tdi.astype('timedelta64'))) assert (not com.is_timedelta64_ns_dtype(tdi.astype('timedelta64[h]')))
self.assertTrue(isinstance(cdf, CustomDataFrame))
cdf_series = cdf.col1 self.assertTrue(isinstance(cdf_series, CustomSeries)) self.assertEqual(cdf_series.custom_series_function(), 'OK')
cdf_rows = cdf[1:5] self.assertTrue(isinstance(cdf_rows, CustomDataFrame)) self.assertEqual(cdf_rows.custom_frame_function(), 'OK')
self.assertEqual(df.iloc[0:1, :].testattr, 'XXX')
class A(DataFrame):
assert_series_equal(df[:0].dtypes, ex_dtypes) assert_series_equal(df[:0].ftypes, ex_ftypes)
def _check_cast(df, v): self.assertEqual( list(set([s.dtype.name for _, s in compat.iteritems(df)]))[0], v)
casted = mn.astype('O') _check_cast(casted, 'object')
for tt in set([str, compat.text_type]): result = df.astype(tt)
from copy import deepcopy import sys import nose from distutils.version import LooseVersion
assert_frame_equal(joined, expected.ix[:, joined.columns])
tup3 = next(df3.itertuples()) self.assertFalse(hasattr(tup3, '_fields')) self.assertIsInstance(tup3, tuple)
mat = self.mixed_frame.as_matrix(['foo', 'A']) self.assertEqual(mat[0, 0], 'bar')
index, data = tm.getMixedTypeDict() mixed = DataFrame(data, index=index)
expected = f.sum(axis=0) result = f.sum(axis='index') assert_series_equal(result, expected)
f = lambda x: x.set_index('a', inplace=True) _check_f(data.copy(), f)
f = lambda x: x.reset_index(inplace=True) _check_f(data.set_index('a'), f)
f = lambda x: x.drop_duplicates(inplace=True) _check_f(data.copy(), f)
f = lambda x: x.sort_values('b', inplace=True) _check_f(data.copy(), f)
f = lambda x: x.sort_index(inplace=True) _check_f(data.copy(), f)
f = lambda x: x.sortlevel(0, inplace=True) _check_f(data.set_index(['a', 'b']), f)
f = lambda x: x.fillna(0, inplace=True) _check_f(data.copy(), f)
f = lambda x: x.replace(1, 0, inplace=True) _check_f(data.copy(), f)
f = lambda x: x.rename({1: 'foo'}, inplace=True) _check_f(data.copy(), f)
d = data.copy()['c']
f = lambda x: x.reset_index(inplace=True, drop=True) _check_f(data.set_index('a')['c'], f)
f = lambda x: x.fillna(0, inplace=True) _check_f(d.copy(), f)
f = lambda x: x.replace(1, 0, inplace=True) _check_f(d.copy(), f)
f = lambda x: x.rename({1: 'foo'}, inplace=True) _check_f(d.copy(), f)
self.assertEqual(pivoted.index.name, 'index') self.assertEqual(pivoted.columns.name, 'columns')
result = frame.pivot(columns='columns')
df['A'] = df['A'].astype(np.int16) df['B'] = df['B'].astype(np.float64)
result = df.unstack(fill_value=0.5)
result = data.unstack(fill_value='d') assert_frame_equal(result, expected)
self.assertRaises(ValueError, df2.stack, level=['animal', 0])
data = self.frame.unstack()
old_data = data.copy() for _ in range(4): data = data.unstack() assert_frame_equal(old_data, data)
rows = [[1, 1, 3, 4], [1, 2, 3, 4], [2, 1, 3, 4], [2, 2, 3, 4]]
expected = df.stack(level=level, dropna=True) if isinstance(expected, Series): assert_series_equal(result, expected) else: assert_frame_equal(result, expected)
dropped = df.dropna(axis=1, how='all') assert_frame_equal(dropped, df)
self.assertRaises(ValueError, df.dropna, axis=3)
df = DataFrame(np.random.randn(10, 3)) df.iloc[2:7, 0] = np.nan df.iloc[3:5, 2] = np.nan
df = DataFrame({ 'Date': [pd.NaT, Timestamp("2014-1-1")], 'Date2': [Timestamp("2013-1-1"), pd.NaT] })
df.fillna(np.nan)
result = df.fillna({'a': 0, 'b': 5, 'd': 7})
result = df.fillna(df.max()) expected = df.fillna(df.max().to_dict()) assert_frame_equal(result, expected)
with assertRaisesRegexp(NotImplementedError, 'column by column'): df.fillna(df.max(1), axis=1)
tm._skip_if_no_scipy() result = df.interpolate(axis=1, method='values') assert_frame_equal(result, expected)
result = df[['B', 'D']].interpolate(downcast=None) assert_frame_equal(result, df[['B', 'D']])
exit=False)
df = DataFrame(np.arange(12).reshape(3, 4), columns=dups, dtype='float64') self.assertRaises(ValueError, lambda: df[df.A > 6])
self.assertRaises(ValueError, lambda: df1 == df2)
for i in range(len(df.columns)): df.iloc[:, i]
df = DataFrame(np.arange(9).reshape(3, 3).T) df.columns = list('AAA') expected = df.iloc[:, 2]
result = df.set_index('C') result_nodrop = df.set_index('C', drop=False)
df2 = df.copy()
result = df.set_index(['A', 'B']) result_nodrop = df.set_index(['A', 'B'], drop=False)
df2 = df.copy() df2.set_index(['A', 'B'], inplace=True) assert_frame_equal(df2, expected)
with assertRaisesRegexp(ValueError, 'Index has duplicate keys'): df.set_index('A', verify_integrity=True)
result = df.set_index(df.C) self.assertEqual(result.index.name, 'C')
result = df.set_index(['A', df['B'].values], drop=False) expected = df.set_index(['A', 'B'], drop=False)
result = Series(i) assert_series_equal(result, expected)
df['B'] = i result = df['B'] assert_series_equal(result, expected, check_names=False) self.assertEqual(result.name, 'B')
result = i.to_series(keep_tz=True) assert_series_equal(result.reset_index(drop=True), expected)
df['D'] = i.to_pydatetime() result = df['D'] assert_series_equal(result, expected, check_names=False) self.assertEqual(result.name, 'D')
result = df.set_index(['a', 'x']) repr(result)
data = { 'A': {'foo': 0, 'bar': 1} }
self.assertRaises(TypeError, self.frame.rename)
result = df.reorder_levels([0, 1, 2]) assert_frame_equal(df, result)
result = df.reorder_levels(['L0', 'L1', 'L2']) assert_frame_equal(df, result)
self.frame.columns.name = 'columns' resetted = self.frame.reset_index() self.assertEqual(resetted.columns.name, 'columns')
df = self.frame.copy() resetted = self.frame.reset_index() df.reset_index(inplace=True) assert_frame_equal(df, resetted, check_names=False)
df = pd.DataFrame([[1, 2], [3, 4]], columns=pd.date_range('1/1/2013', '1/2/2013'), index=['A', 'B'])
self.assertTrue(isinstance(df.set_index(df.index).index, MultiIndex))
tm.assert_index_equal(df.set_index(df.index).index, mi)
self.assertTrue(isinstance(df.set_index( [df.index, df.index]).index, MultiIndex))
tm.assert_index_equal(df.set_index([df.index, df.index]).index, mi2)
def f(): df.quantile(.5, axis=1, numeric_only=False) self.assertRaises(TypeError, f)
if _np_version_under1p9: raise nose.SkipTest("Numpy version under 1.9")
if not _np_version_under1p9: raise nose.SkipTest("Numpy version is greater than 1.9")
result = df.quantile(.5) expected = Series([2.5], index=['b'])
tm.assert_frame_equal(lexsorted_df, not_lexsorted_df)
df1 = DataFrame(columns=['a', 'b'], data=[[1, 11], [0, 22]])
nonContigFrame = self.frame.reindex(self.ts1.index[::2])
newFrame = self.frame.reindex(self.frame.index, copy=False) self.assertIs(newFrame.index, self.frame.index)
newFrame = self.frame.reindex([]) self.assertTrue(newFrame.empty) self.assertEqual(len(newFrame.columns), len(self.frame.columns))
newFrame = self.frame.reindex(list(self.ts1.index)) self.assert_index_equal(newFrame.index, self.ts1.index)
result = self.frame.reindex() assert_frame_equal(result, self.frame) self.assertFalse(result is self.frame)
newFrame = self.frame.reindex(columns=[]) self.assertTrue(newFrame.empty)
result = df.reindex(lrange(15)) self.assertTrue(np.isnan(result.values[-5:]).all())
result = df.reindex(columns=lrange(5), fill_value=0.) expected = df.copy() expected[4] = 0. assert_frame_equal(result, expected)
result = df.reindex_axis(lrange(15), fill_value=0., axis=0) expected = df.reindex(lrange(15)).fillna(0) assert_frame_equal(result, expected)
arr = np.random.randn(10) df = DataFrame(arr, index=[1, 2, 3, 4, 5, 1, 2, 3, 4, 5])
self.assertRaises(ValueError, df.reindex, index=list(range(len(df))))
af, bf = self.intframe.align(other, join='inner', axis=1, method='pad') self.assert_index_equal(bf.columns, other.columns)
self.assertRaises(ValueError, self.frame.align, af.ix[0, :3], join='inner', axis=2)
idx = self.frame.index s = Series(range(len(idx)), index=idx)
self._check_align(empty, right, axis=ax, fill_axis=fax, how=kind, method=meth) self._check_align(empty, right, axis=ax, fill_axis=fax, how=kind, method=meth, limit=1)
self._check_align(left, empty, axis=ax, fill_axis=fax, how=kind, method=meth) self._check_align(left, empty, axis=ax, fill_axis=fax, how=kind, method=meth, limit=1)
self._check_align(empty, empty, axis=ax, fill_axis=fax, how=kind, method=meth) self._check_align(empty, empty, axis=ax, fill_axis=fax, how=kind, method=meth, limit=1)
X = np.arange(10 * 10, dtype='float64').reshape(10, 10) Y = np.ones((10, 1), dtype=int)
res1l, res1r = df1.align(df2, join='left') res2l, res2r = df2.align(df1, join='right')
res1, res2 = s.align(df) tm.assert_series_equal(res1, exp2) tm.assert_frame_equal(res2, exp1)
fcopy = self.frame.copy() fcopy['AA'] = 1
filtered = expected.filter(regex='^[0-9]+$') assert_frame_equal(filtered, expected)
with assertRaisesRegexp(TypeError, 'Must pass'): self.frame.filter(items=None)
filtered = self.mixed_frame.filter(like='foo') self.assertIn('foo', filtered)
filtered = fcopy.filter(regex='[A]+') self.assertEqual(len(filtered.columns), 2) self.assertIn('AA', filtered)
df = DataFrame({'aBBa': [1, 2], 'BBaBB': [1, 2], 'aCCa': [1, 2], 'aCCaBB': [1, 2]})
order = [3, 1, 2, 0] for df in [self.frame]:
order = [2, 1, -1] for df in [self.frame]:
result = df.take(order, axis=1) expected = df.ix[:, ['C', 'B', 'D']] assert_frame_equal(result, expected, check_names=False)
order = [4, 1, 2, 0, 3] for df in [self.mixed_frame]:
order = [4, 1, -2] for df in [self.mixed_frame]:
result = df.take(order, axis=1) expected = df.ix[:, ['foo', 'B', 'D']] assert_frame_equal(result, expected)
order = [1, 2, 0, 3] for df in [self.mixed_float, self.mixed_int]:
smaller = self.intframe.reindex(columns=['A', 'B', 'E']) self.assertEqual(smaller['E'].dtype, np.float64)
cols = self.frame.columns.copy() newFrame = self.frame.reindex_axis(cols, axis=1) assert_frame_equal(newFrame, self.frame)
recons = consolidated.consolidate() self.assertIsNot(recons, consolidated) assert_frame_equal(recons, consolidated)
for letter in range(ord('A'), ord('Z')): self.frame[chr(letter)] = chr(letter)
self.frame['E'] = 7. self.frame.values[6] = 6 self.assertTrue((self.frame.values[6] == 6).all())
values = self.mixed_float.as_matrix(['A', 'B', 'C', 'D']) self.assertEqual(values.dtype, np.float64)
values = self.mixed_int.as_matrix(['A', 'B', 'C']) self.assertEqual(values.dtype, np.int64)
result = df.get_dtype_counts().sort_values() expected = Series({'datetime64[ns]': 3})
}, index=range(3))
f('int64') f('float64')
if not compat.is_platform_windows(): f('M8[ns]')
df = DataFrame(self.frame, copy=True) column = df.columns[0]
blocks = df.as_blocks() for dtype, _df in blocks.items(): if column in _df: _df.ix[:, column] = _df[column] + 1
self.assertFalse(_df[column].equals(df[column]))
df = DataFrame(self.frame, copy=True) column = df.columns[0]
blocks = df.as_blocks(copy=False) for dtype, _df in blocks.items(): if column in _df: _df.ix[:, column] = _df[column] + 1
self.assertTrue(_df[column].equals(df[column]))
copy = self.mixed_frame.copy() self.assertIsNot(copy._data, self.mixed_frame._data)
self.mixed_frame._data.ndim
unpickled = self.round_trip_pickle(self.empty) repr(unpickled)
unpickled = self.round_trip_pickle(self.tzframe) assert_frame_equal(self.tzframe, unpickled)
self.mixed_frame['H'] = '1.' self.mixed_frame['I'] = '1'
converted = self.mixed_frame.copy() with assertRaisesRegexp(ValueError, 'invalid literal'): converted['H'].astype('int32')
df = DataFrame(index=[0, 1]) df[0] = nan wasCol = {}
df3 = DataFrame({"a": [1, 2, 3, 4], "b": [1, 2, 3, 4]})
df3.cov() df3.corr()
df = DataFrame({"a": [True, False], "b": [1, 0]})
expected = self.frame.cov() result = self.frame.cov(min_periods=len(self.frame))
self.frame['A'][:5] = nan self.frame['B'][:10] = nan cov = self.frame.cov()
b = b.reindex(columns=b.columns[::-1], index=b.index[::-1][10:]) del b['B']
result = df.describe(include=['bool'])
frame = DataFrame() ct1 = frame.count(1) tm.assertIsInstance(ct1, Series)
df = DataFrame(index=lrange(10)) result = df.count(1) expected = Series(0, index=df.index) tm.assert_series_equal(result, expected)
self._check_stat_op('sum', np.sum, frame=self.mixed_float.astype('float32'), has_numeric_only=True, check_dtype=False, check_less_precise=True)
df2 = DataFrame({0: [np.nan, 2], 1: [np.nan, 3], 2: [np.nan, 4]}, dtype=object)
cummin = self.tsframe.cummin() expected = self.tsframe.apply(Series.cummin) tm.assert_frame_equal(cummin, expected)
cummin = self.tsframe.cummin(axis=1) expected = self.tsframe.apply(Series.cummin, axis=1) tm.assert_frame_equal(cummin, expected)
df = DataFrame({'A': np.arange(20)}, index=np.arange(20))
cummin_xs = self.tsframe.cummin(axis=1) self.assertEqual(np.shape(cummin_xs), np.shape(self.tsframe))
cummax = self.tsframe.cummax() expected = self.tsframe.apply(Series.cummax) tm.assert_frame_equal(cummax, expected)
cummax = self.tsframe.cummax(axis=1) expected = self.tsframe.apply(Series.cummax, axis=1) tm.assert_frame_equal(cummax, expected)
df = DataFrame({'A': np.arange(20)}, index=np.arange(20))
cummax_xs = self.tsframe.cummax(axis=1) self.assertEqual(np.shape(cummax_xs), np.shape(self.tsframe))
df2.ix[0, 'foo'] = 'a'
cumsum = self.tsframe.cumsum() expected = self.tsframe.apply(Series.cumsum) tm.assert_frame_equal(cumsum, expected)
cumsum = self.tsframe.cumsum(axis=1) expected = self.tsframe.apply(Series.cumsum, axis=1) tm.assert_frame_equal(cumsum, expected)
df = DataFrame({'A': np.arange(20)}, index=np.arange(20))
cumsum_xs = self.tsframe.cumsum(axis=1) self.assertEqual(np.shape(cumsum_xs), np.shape(self.tsframe))
cumprod = self.tsframe.cumprod() expected = self.tsframe.apply(Series.cumprod) tm.assert_frame_equal(cumprod, expected)
cumprod = self.tsframe.cumprod(axis=1) expected = self.tsframe.apply(Series.cumprod, axis=1) tm.assert_frame_equal(cumprod, expected)
cumprod_xs = self.tsframe.cumprod(axis=1) self.assertEqual(np.shape(cumprod_xs), np.shape(self.tsframe))
df = self.tsframe.fillna(0).astype(int) df.cumprod(0) df.cumprod(1)
df = self.tsframe.fillna(0).astype(np.int32) df.cumprod(0) df.cumprod(1)
df = DataFrame(np.random.randint(0, 5, size=40).reshape((10, 4)))
self.mixed_frame['datetime'] = datetime.now() self.mixed_frame['timedelta'] = timedelta(days=1, seconds=1)
ranks0 = self.frame.rank(na_option='bottom') ranks1 = self.frame.rank(1, na_option='bottom')
ranks0 = self.frame.rank(na_option='top') ranks1 = self.frame.rank(1, na_option='top')
ranks0 = self.frame.rank(na_option='top', ascending=False) ranks1 = self.frame.rank(1, na_option='top', ascending=False)
ranks0 = self.frame.rank(na_option='bottom', ascending=False) ranks1 = self.frame.rank(1, na_option='bottom', ascending=False)
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): df.sort(axis=1)
frame.ix[5:10] = np.nan frame.ix[15:20, -2:] = np.nan
tm.assert_series_equal(result1, frame.apply(wrapper, axis=1), check_dtype=False, check_less_precise=check_less_precise)
if check_dtype: lcd_dtype = frame.values.dtype self.assertEqual(lcd_dtype, result0.dtype) self.assertEqual(lcd_dtype, result1.dtype)
tm.assertRaisesRegexp(ValueError, 'No axis named 2', f, axis=2) getattr(self.mixed_frame, name)(axis=0) getattr(self.mixed_frame, name)(axis=1)
result = diffs.min() self.assertEqual(result[0], diffs.ix[0, 'A']) self.assertEqual(result[1], diffs.ix[0, 'B'])
result = diffs.max() self.assertEqual(result[0], diffs.ix[2, 'A']) self.assertEqual(result[1], diffs.ix[2, 'B'])
from pandas.tseries.timedeltas import ( _coerce_scalar_to_timedelta_type as _coerce)
result = mixed.min(axis=1) expected = Series([1, 1, 1.], index=[0, 1, 2]) tm.assert_series_equal(result, expected)
result = mixed[['A', 'B']].min(1) expected = Series([timedelta(days=-1)] * 3) tm.assert_series_equal(result, expected)
bools = np.isnan(self.frame) bools.sum(1) bools.sum(0)
the_mean = self.mixed_frame.mean(axis=1) the_sum = self.mixed_frame.sum(axis=1, numeric_only=True) self.assert_index_equal(the_sum.index, the_mean.index)
self.mixed_frame.std(1) self.mixed_frame.var(1) self.mixed_frame.mean(1) self.mixed_frame.skew(1)
self.assertRaises(ValueError, f, axis=2)
df2.columns = ['A', 'C'] result = df1.isin(df2) expected['B'] = False tm.assert_frame_equal(result, expected)
df2.columns = ['B', 'B'] with tm.assertRaises(ValueError): df1.isin(df2)
expected = DataFrame(False, index=df1.index, columns=df1.columns) result = df1.isin(df2) tm.assert_frame_equal(result, expected)
result = df.drop_duplicates('AAA') expected = df[:2] tm.assert_frame_equal(result, expected)
with tm.assert_produces_warning(FutureWarning): result = df.drop_duplicates('AAA', take_last=True) expected = df.ix[[6, 7]] tm.assert_frame_equal(result, expected)
df2 = df.ix[:, ['AAA', 'B', 'C']]
expected = df2.drop_duplicates(['AAA', 'B']) tm.assert_frame_equal(result, expected)
df = pd.DataFrame([i] * 9 for i in range(16)) df = df.append([[1] + [0] * 8], ignore_index=True)
result = df.drop_duplicates('AAA') expected = df.iloc[[0, 1, 2, 6]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates(['AAA', 'B']) expected = df.iloc[[0, 1, 2, 3, 4, 6]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates(('AA', 'AB')) expected = df[:2] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates('A') expected = df.ix[[0, 2, 3]] tm.assert_frame_equal(result, expected)
with tm.assert_produces_warning(FutureWarning): result = df.drop_duplicates('A', take_last=True) expected = df.ix[[1, 6, 7]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates(['A', 'B']) expected = df.ix[[0, 2, 3, 6]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates('C') expected = df[:2] tm.assert_frame_equal(result, expected)
with tm.assert_produces_warning(FutureWarning): result = df.drop_duplicates('C', take_last=True) expected = df.ix[[3, 7]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates(['C', 'B']) expected = df.ix[[0, 1, 2, 4]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates('A') expected = df.iloc[[0, 2, 3, 5, 7]] tm.assert_frame_equal(result, expected)
result = df.drop_duplicates('C') expected = df.iloc[[0, 1, 5, 6]] tm.assert_frame_equal(result, expected)
df = orig.copy() df.drop_duplicates('A', inplace=True) expected = orig[:2] result = df tm.assert_frame_equal(result, expected)
orig2 = orig.ix[:, ['A', 'B', 'C']].copy()
expected = orig2.drop_duplicates(['A', 'B']) result = df2 tm.assert_frame_equal(result, expected)
df = DataFrame() tm.assert_frame_equal(df, df.round())
df = DataFrame({'col1': [1.123, 2.123, 3.123], 'col2': [1.234, 2.234, 3.234]})
tm.assert_frame_equal(np.round(df, decimals), expected_rounded)
round_list = [1, 2] with self.assertRaises(TypeError): df.round(round_list)
wrong_round_dict = {'col3': 2, 'col2': 1} tm.assert_frame_equal(df.round(wrong_round_dict), expected_partially_rounded)
non_int_round_dict = {'col1': 1, 'col2': 0.5} with self.assertRaises(TypeError): df.round(non_int_round_dict)
non_int_round_dict = {'col1': 1, 'col2': 'foo'} with self.assertRaises(TypeError): df.round(non_int_round_dict)
non_int_round_dict = {'col1': 1, 'col2': [1, 2]} with self.assertRaises(TypeError): df.round(non_int_round_dict)
non_int_round_Series = Series(non_int_round_dict) with self.assertRaises(TypeError): df.round(non_int_round_Series)
nan_round_Series = Series({'col1': nan, 'col2': 1})
with self.assertRaises(ValueError): df.round(nan_round_Series)
tm.assert_series_equal(df['col1'].round(1), expected_rounded['col1'])
df = DataFrame( {'col1': [1.123, 2.123, 3.123], 'col2': [1.234, 2.234, 3.234]})
df = DataFrame(np.random.randn(1000, 2))
b1 = b.reindex(index=reversed(b.index)) result = a.dot(b) tm.assert_frame_equal(result, expected)
result = a.dot(b['one']) tm.assert_series_equal(result, expected['one'], check_names=False) self.assertTrue(result.name is None)
row = a.ix[0].values
result = A.dot(b)
recons_data = DataFrame(test_data).to_dict("i")
return pd.DataFrame(dict([(c, s) for c, s in compat.iteritems(_intframe)]), dtype=np.int64)
result = df.assign(C=lambda x: x.B / x.A) assert_frame_equal(result, expected)
assert_frame_equal(df, original)
result = df.assign(C=[4, 2.5, 2]) assert_frame_equal(result, expected) assert_frame_equal(df, original)
result = df.assign(A=df.A + df.B) expected = df.copy() expected['A'] = [5, 7, 9] assert_frame_equal(result, expected)
result = df.assign(A=lambda x: x.A + x.B) assert_frame_equal(result, expected)
df = DataFrame(np.random.randint(0, 2, (4, 4)), columns=['a', 'b', 'c', 'd'])
df.insert(0, 'baz', df['c']) self.assertEqual(df.columns.name, 'some_name')
expected = DataFrame([[1, 3], [4, 6]], columns=[ 'A', 'C'], index=['X', 'Y']) assert_frame_equal(a, expected)
expected = Series([2, 5], index=['X', 'Y'], name='B') + 1 assert_series_equal(b, expected)
df = DataFrame({'b': [1.1, 2.2]}) df = df.rename(columns={}) df.insert(0, 'a', [1, 2])
for n in [4, 4000]:
result = eval("m{op}df".format(op=op_str)) assert_frame_equal(result, expected)
if op in ['+', '*']: result = getattr(df, op)(m) assert_frame_equal(result, expected)
elif op in ['-', '/']: result = getattr(df, rop)(m) assert_frame_equal(result, expected)
df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'b']})
with tm.assertRaises(UndefinedVariableError): df.query('@a > b > @c', engine=engine, parser=parser)
with tm.assertRaises(UndefinedVariableError): df.query('@a > b > c', engine=engine, parser=parser)
with tm.assertRaises(UndefinedVariableError): df.query('sin > 5', engine=engine, parser=parser)
res = df.query('index < b', engine=engine, parser=parser) expec = df[df.index < df.b] assert_frame_equal(res, expec)
res = df.query('index < 5', engine=engine, parser=parser) expec = df[df.index < 5] assert_frame_equal(res, expec)
with tm.assertRaises(UndefinedVariableError): df.query('df > 0', engine=self.engine, parser=self.parser)
with tm.assertRaises(SyntaxError): df.query('(@df>0) & (@df2>0)', engine=engine, parser=parser)
ts1 = Timestamp('2015-01-01', tz=None) ts2 = Timestamp('2015-01-01', tz='UTC') ts3 = Timestamp('2015-01-01', tz='EST')
result = df.append(series.to_dict(), ignore_index=True) assert_frame_equal(result, expected)
row = df.ix[4] row.name = 5 result = df.append(row) expected = df.append(df[-1:], ignore_index=True) assert_frame_equal(result, expected)
df1 = DataFrame([]) df2 = DataFrame([]) result = df1.append(df2) expected = df1.copy() assert_frame_equal(result, expected)
head, tail = self.frame[:5], self.frame[5:]
fcopy = self.frame.copy() fcopy['A'] = 1 del fcopy['C']
head, tail = reordered_frame[:10].copy(), reordered_frame head['A'] = 1
tail['A'][:10] = 0 combined = tail.combine_first(head) self.assertTrue((combined['A'][:10] == 0).all())
comb = self.frame.combine_first(self.empty) assert_frame_equal(comb, self.frame)
expected = left_f(df, Timestamp('20010109')) result = right_f(Timestamp('20010109'), df) assert_frame_equal(result, expected)
expected = left_f(df, Timestamp('nat')) result = right_f(Timestamp('nat'), df) assert_frame_equal(result, expected)
p = DataFrame({'first': [3, 4, 5, 8], 'second': [0, 0, 0, 3]})
result2 = DataFrame(p.values.astype('float64') % 0, index=p.index, columns=p.columns) assert_frame_equal(result2, expected)
p = DataFrame({'first': [3, 4, 5, 8], 'second': [0, 0, 0, 3]}) result = p / p
result2 = DataFrame(p.values.astype('float64') / 0, index=p.index, columns=p.columns) assert_frame_equal(result2, expected)
_check_unary_op(operator.inv)
result = d['a'] | d['b'] expected = Series([False, True]) assert_series_equal(result, expected)
result = d['a'].fillna(False) | d['b'] expected = Series([True, True]) assert_series_equal(result, expected)
ndim_5 = np.ones(self.frame.shape + (3, 4, 5)) with assertRaisesRegexp(ValueError, 'shape'): f(self.frame, ndim_5)
result = self.frame.add(self.frame[:0]) assert_frame_equal(result, self.frame * np.nan)
_test_seq(df, idx_ser.values, col_ser.values)
assert_frame_equal(df.add(row, axis=None), df + row)
assert_frame_equal(df.div(row), df / row) assert_frame_equal(df.div(col, axis=0), (df.T / col).T)
plus_empty = self.frame + self.empty self.assertTrue(np.isnan(plus_empty.values).all())
reverse = self.frame.reindex(columns=self.frame.columns[::-1])
added = self.frame + self.mixed_float _check_mixed_float(added, dtype='float64') added = self.mixed_float + self.frame _check_mixed_float(added, dtype='float64')
added = self.mixed_float + self.mixed_float2 _check_mixed_float(added, dtype=dict(C=None)) added = self.mixed_float2 + self.mixed_float _check_mixed_float(added, dtype=dict(C=None))
added = self.frame + self.mixed_int _check_mixed_float(added, dtype='float64')
series = self.frame.xs(self.frame.index[0])
ts = self.tsframe['A']
added = self.tsframe.add(ts, axis='index')
frame = self.tsframe[:1].reindex(columns=[]) result = frame.mul(ts, axis='index') self.assertEqual(len(result), len(ts))
expected = DataFrame([[False, False], [False, True], [True, True]]) result = df > b assert_frame_equal(result, expected)
expected = DataFrame([[False, False], [True, False], [False, False]]) result = df == b assert_frame_equal(result, expected)
self.assertRaises(ValueError, lambda: df == (2, 2)) self.assertRaises(ValueError, lambda: df == [2, 2])
comb = self.frame.combineAdd(self.frame) assert_frame_equal(comb, self.frame * 2)
comb = self.frame.combineAdd(self.empty) assert_frame_equal(comb, self.frame)
comb = self.frame.combineMult(self.frame)
comb = self.frame.combineMult(self.empty) assert_frame_equal(comb, self.frame)
X = X_orig.copy() result1 = (X[block1] + Z).reindex(columns=subs)
X = X_orig.copy() result1 = (X[block1] - Z).reindex(columns=subs)
s_orig = Series([1, 2, 3]) df_orig = DataFrame(np.random.randint(0, 5, size=10).reshape(-1, 5))
s = s_orig.copy() s2 = s s += 1.5 assert_series_equal(s, s2) assert_series_equal(s_orig + 1.5, s)
self.tsframe.to_csv(path) recons = DataFrame.from_csv(path)
self.tsframe.to_csv(path, index=False) recons = DataFrame.from_csv(path, index_col=None) assert_almost_equal(self.tsframe.values, recons.values)
with ensure_clean('__tmp_to_csv_from_csv5__') as path:
import pandas as pd
if cols is not None:
else: rs_c.columns = df.columns assert_frame_equal(df, rs_c, check_names=False)
df = mkdf(N, 3) df.columns = ['a', 'a', 'b'] _check_df(df, None)
cols = ['b', 'a'] _check_df(df, cols)
from pandas import NaT
s1 = make_dtnat_arr(chunksize + 5) s2 = make_dtnat_arr(chunksize + 5, 0)
recons.columns = df.columns
self.frame['E'] = np.inf self.frame['F'] = -np.inf
frame.to_csv(path) df = DataFrame.from_csv(path, index_col=[0, 1], parse_dates=False)
self.frame.index = old_index
tsframe = self.tsframe old_index = tsframe.index new_index = [old_index, np.arange(len(old_index))] tsframe.index = MultiIndex.from_arrays(new_index)
tsframe.to_csv(path) recons = DataFrame.from_csv(path, index_col=None) self.assertEqual(len(recons.columns), len(tsframe.columns) + 2)
tsframe.to_csv(path, index=False) recons = DataFrame.from_csv(path, index_col=None) assert_almost_equal(recons.values, self.tsframe.values)
self.tsframe.index = old_index
df = _make_frame() df.to_csv(path, tupleize_cols=False, index=False) result = read_csv(path, header=[0, 1], tupleize_cols=False) assert_frame_equal(df, result)
df = _make_frame() df.to_csv(path, tupleize_cols=False) result = read_csv(path, header=[0, 1], index_col=[ 0], tupleize_cols=False) assert_frame_equal(df, result)
df = _make_frame(True) df.to_csv(path, tupleize_cols=False)
with assertRaisesRegexp(CParserError, 'Passed header=\[0,1,2\] are too many ' 'rows for this multi_index of columns'): read_csv(path, tupleize_cols=False, header=lrange(3), index_col=0)
with assertRaisesRegexp(TypeError, 'cannot specify cols with a ' 'MultiIndex'): df.to_csv(path, tupleize_cols=False, columns=['foo', 'bar'])
tsframe[:0].to_csv(path) recons = DataFrame.from_csv(path) exp = tsframe[:0] exp.index = []
df = DataFrame({'A': [1, 2, 3], 'B': ['5,6', '7,8', '9,0']})
df_float.ix[30:50, 1:3] = np.nan
for i in ['0.4', '1.4', '2.4']: result[i] = to_datetime(result[i])
from pandas.util.testing import makeCustomDataframe as mkdf
result = read_csv(filename, index_col=0) result = result.rename(columns={'a.1': 'a'}) assert_frame_equal(result, df)
assert_frame_equal(recons, newdf, check_names=False)
csv_str = self.frame.to_csv(path=None) self.assertIsInstance(csv_str, str) recons = pd.read_csv(StringIO(csv_str), index_col=0) assert_frame_equal(self.frame, recons)
rs = read_csv(filename, compression="gzip", index_col=0) assert_frame_equal(df, rs)
rs = read_csv(filename, compression="bz2", index_col=0) assert_frame_equal(df, rs)
rs = read_csv(filename, compression="xz", index_col=0) assert_frame_equal(df, rs)
lzma = compat.import_lzma() f = lzma.open(filename, 'rb') assert_frame_equal(df, read_csv(f, index_col=0)) f.close()
import zipfile self.assertRaises(zipfile.BadZipfile, df.to_csv, filename, compression="zip")
test = read_csv(path, index_col=0)
datetime_frame_columns = datetime_frame.T
datetime_frame_columns.columns = ( datetime_frame_columns.columns .map(lambda x: x.strftime('%Y%m%d')))
nat_index = to_datetime( ['NaT'] * 10 + ['2000-01-01', '1/1/2000', '1-1-2000']) nat_frame = DataFrame({'A': nat_index}, index=nat_index)
times = pd.date_range("2013-10-26 23:00", "2013-10-27 01:00", tz="Europe/London", freq="H", ambiguous='infer')
df.astype(str)
df = DataFrame(np.ones((4, 2)))
df['foo'] = np.ones((4, 2)).tolist()
self.assertRaises(ValueError, df.__setitem__, tuple(['test']), np.ones((4, 2)))
df['foo2'] = np.ones((4, 2)).tolist()
result = DataFrame([DataFrame([])]) self.assertEqual(result.shape, (1, 0))
df = _make_mixed_dtypes_df('float') _check_mixed_dtypes(df)
df = _make_mixed_dtypes_df('float', dict(A=1, B='foo', C='bar')) _check_mixed_dtypes(df)
df = _make_mixed_dtypes_df('int') _check_mixed_dtypes(df)
a = np.random.rand(10).astype(np.complex64) b = np.random.rand(10).astype(np.complex128)
self.assertEqual(len(self.ts1), 30) self.assertEqual(len(self.ts2), 25)
self.assertEqual(len(DataFrame({})), 0)
idx = Index([0, 1, 2]) frame = DataFrame({}, index=idx) self.assertIs(frame.index, idx)
with tm.assertRaises(ValueError): DataFrame({'a': 0.7})
with tm.assertRaisesRegexp(ValueError, msg): DataFrame(np.empty(0), columns=list('abc'))
df1 = DataFrame({'a': [1, 2, 3], 'b': [3, 4, 5]}) df2 = DataFrame([df1, df1 + 10])
data = {'a': (1, 2, 3), 'b': (4, 5, 6)}
dates_as_str = ['1984-02-19', '1988-11-06', '1989-12-03', '1990-03-15']
td_as_int = [1, 2, 3, 4]
mat = empty((2, 3), dtype=float) frame = DataFrame(mat, columns=['A', 'B', 'C'], index=[1, 2])
frame = DataFrame(empty((0, 3))) self.assertEqual(len(frame.index), 0)
mat = ma.masked_all((2, 3), dtype=int) frame = DataFrame(mat, columns=['A', 'B', 'C'], index=[1, 2])
mat = ma.masked_all((2, 3), dtype=bool) frame = DataFrame(mat, columns=['A', 'B', 'C'], index=[1, 2])
frame = DataFrame(mat, columns=['A', 'B', 'C'], index=[1, 2], dtype=object) self.assertEqual(frame.values.dtype, object)
for comb in itertools.combinations(arrays, 3): names, data = zip(*comb) mrecs = mrecords.fromarrays(data, names=names)
comb = dict([(k, v.filled()) if hasattr( v, 'filled') else (k, v) for k, v in comb])
expected = DataFrame(comb, columns=names[::-1]) result = DataFrame(mrecs, columns=names[::-1]) assert_fr_equal(result, expected)
expected = DataFrame(comb, columns=names, index=[1, 2]) result = DataFrame(mrecs, index=[1, 2]) assert_fr_equal(result, expected)
df = DataFrame(index=lrange(10), columns=['a', 'b'], dtype=object) self.assertEqual(df.values.dtype, np.object_)
df = DataFrame(index=lrange(10), columns=['a', 'b'], dtype=int) self.assertEqual(df.values.dtype, np.object_)
df = DataFrame({}, columns=['foo', 'bar']) self.assertEqual(df.values.dtype, np.object_)
arr = randn(10) dm = DataFrame(arr, columns=['A'], index=np.arange(10)) self.assertEqual(dm.values.ndim, 2)
dm = DataFrame(columns=['A', 'B'], index=np.arange(10)) self.assertEqual(dm.values.shape, (10, 2))
def empty_gen(): return yield
import collections
expected = DataFrame([[2, 1]], columns=['b', 'a'])
row_one = OrderedDict() row_one['b'] = 2 row_one['a'] = 1
a = Series([], name='x') df = DataFrame(a) self.assertEqual(df.columns[0], 'x')
df = DataFrame([arr, s1]).T expected = DataFrame({1: s1, 0: arr}, columns=[0, 1]) tm.assert_frame_equal(df, expected)
s1 = Series([1, 2, 3], index=['a', 'b', 'c'], name='x')
s2 = Series([1, 2, 3], index=['a', 'b', 'c'])
df = DataFrame([[8, 5]], columns=['a', 'a']) edf = DataFrame([[8, 5]]) edf.columns = ['a', 'a']
expected = DataFrame(index=[0, 1], columns=[0, 1], dtype=object)
import pytz tz = pytz.timezone('US/Eastern') dt = tz.localize(datetime(2012, 1, 1))
i = date_range('1/1/2011', periods=5, freq='10s', tz='US/Eastern')
df = DataFrame([np.arange(5) for x in range(5)]) result = df.get_dtype_counts() expected = Series({'int64': 5})
df = DataFrame([1, 2]) result = df.get_dtype_counts() expected = Series({'int64': 1}) tm.assert_series_equal(result, expected)
indexer = np.arange(len(df.columns))[isnull(df.columns)]
else:
arr2 = np.zeros((2, 3)) tm.assert_frame_equal(DataFrame.from_records(arr2), DataFrame(arr2))
documents.append({'order_id': 10, 'quantity': 5})
result = DataFrame.from_records(documents, index=['order_id', 'quantity']) self.assertEqual(result.index.names, ('order_id', 'quantity'))
rows = [] rows.append([datetime(2010, 1, 1), 1])
if not is_platform_little_endian(): raise nose.SkipTest("known failure of test on non-little endian")
expected = DataFrame({'EXPIRY': [datetime(2005, 3, 1, 0, 0), None]})
result = (DataFrame.from_records(tuples, columns=columns) .reindex(columns=df.columns))
result2 = (DataFrame.from_records(recarray, columns=columns) .reindex(columns=df.columns)) result3 = (DataFrame.from_records(recarray2, columns=columns) .reindex(columns=df.columns))
result4 = (DataFrame.from_records(lists, columns=columns) .reindex(columns=df.columns))
result = DataFrame.from_records(tuples) tm.assert_index_equal(result.columns, pd.Index(lrange(8)))
columns_to_test = [columns.index('C'), columns.index('E1')]
columns = [] for dtype, b in compat.iteritems(df.blocks): columns.extend(b.columns)
df1 = DataFrame.from_records(df, index=['C']) tm.assert_index_equal(df1.index, Index(df.C))
self.assertRaises(ValueError, DataFrame.from_records, df, index=[2]) self.assertRaises(KeyError, DataFrame.from_records, df, index=2)
result = DataFrame.from_records([], index='foo', columns=['foo', 'bar'])
self.mixed_frame.ix[5:20, 'foo'] = nan self.mixed_frame.ix[-10:, 'A'] = nan
res = dfobj.replace(r'\s*\.\s*', nan, regex=True) assert_frame_equal(dfobj, res.fillna('.'))
res = dfmix.replace(r'\s*\.\s*', nan, regex=True) assert_frame_equal(dfmix, res.fillna('.'))
res = dfobj.replace(re.compile(r'\s*\.\s*'), nan, regex=True) assert_frame_equal(dfobj, res.fillna('.'))
res = dfmix.replace(re.compile(r'\s*\.\s*'), nan, regex=True) assert_frame_equal(dfmix, res.fillna('.'))
res = dfobj.copy() res.replace(r'\s*\.\s*', nan, regex=True, inplace=True) assert_frame_equal(dfobj, res.fillna('.'))
res = dfmix.copy() res.replace(r'\s*\.\s*', nan, regex=True, inplace=True) assert_frame_equal(dfmix, res.fillna('.'))
res = dfmix.copy() res.replace(regex=r'\s*\.\s*', value=nan, inplace=True) assert_frame_equal(dfmix, res.fillna('.'))
mix = {'a': lrange(4), 'b': list('ab..')} dfmix = DataFrame(mix)
df = DataFrame(index=['a', 'b']) assert_frame_equal(df, df.replace(5, 7))
result = df.replace({1: 'a', 4: 'b'}) assert_frame_equal(expected, result)
tsframe = self.tsframe.copy().astype(np.float32) tsframe['A'][:5] = nan tsframe['A'][-5:] = nan
df = DataFrame({'bools': [True, False, True]}) result = df.replace(False, True) self.assertTrue(result.values.all())
frame = DataFrame(index=np.arange(1000))
foo = repr(self.frame) self.frame.info(verbose=False, buf=buf)
no_index = DataFrame(columns=[0, 1, 3])
self.empty.info(buf=buf)
biggie = DataFrame(np.zeros((200, 4)), columns=lrange(4), index=lrange(200)) repr(biggie)
import warnings warn_filters = warnings.filters warnings.filterwarnings('ignore', category=FutureWarning, module=".*format")
self.assertIsNone(df._repr_latex_())
frame = DataFrame(np.random.randn(1500, 4), columns=['a', 'a', 'b', 'b']) frame.info(buf=io)
io = StringIO()
self.assertTrue(re.match(r"memory usage: [^+]+\+", res[-1]))
self.assertFalse(re.match(r"memory usage: [^+]+\+", res[-1]))
df_size = df.memory_usage().sum() exp_size = len(dtypes) * n * 8 + df.index.nbytes self.assertEqual(df_size, exp_size)
self.assertEqual(df.memory_usage().sum(), df.memory_usage(deep=True).sum())
diff = df.memory_usage(deep=True).sum() - sys.getsizeof(df) self.assertTrue(abs(diff) < 100)
a = 10000000000000000 b = a + 1 s = Series([a, b])
df = DataFrame(dict(time=[Timestamp('20130101 9:01'), Timestamp('20130101 9:02')], value=[1.0, 2.0]))
shiftedFrame = self.tsframe.shift(5) self.assert_index_equal(shiftedFrame.index, self.tsframe.index)
unshifted = self.tsframe.shift(0) assert_frame_equal(unshifted, self.tsframe)
shiftedFrame = self.tsframe.shift(5, freq=datetools.BDay()) self.assertEqual(len(shiftedFrame), len(self.tsframe))
df = DataFrame({'foo': []}) rs = df.shift(-1)
ps = tm.makePeriodFrame() shifted = ps.tshift(1) unshifted = shifted.tshift(-1)
shifted = self.tsframe.tshift(1) unshifted = shifted.tshift(-1)
truncated = ts.truncate() assert_frame_equal(truncated, ts)
expected = ts[1:3]
expected = ts[1:]
expected = ts[:3]
zero_length = self.tsframe.reindex([]) result = zero_length.asfreq('BM') self.assertIsNot(result, zero_length)
empty = DataFrame() self.assertIsNone(empty.last_valid_index()) self.assertIsNone(empty.first_valid_index())
applied = self.frame.apply(np.sqrt) assert_series_equal(np.sqrt(self.frame['A']), applied['A'])
applied = self.frame.apply(np.mean) self.assertEqual(applied['A'], np.mean(self.frame['A']))
applied = self.empty.apply(np.sqrt) self.assertTrue(applied.empty)
xp = DataFrame(index=['a']) rs = xp.apply(lambda x: x['a'], axis=1) assert_frame_equal(xp, rs)
self.assertEqual(x, [])
result = self.frame.apply(lambda x: x * 2, raw=True) expected = self.frame * 2 assert_frame_equal(result, expected)
expected = Series(np.nan, index=pd.Index([], dtype='int64')) assert_series_equal(result, expected)
result = self.frame.applymap(lambda x: (x, x)) tm.assertIsInstance(result['A'][0], tuple)
df = DataFrame(data=[1, 'a']) result = df.applymap(lambda x: x) self.assertEqual(result.dtypes[0], object)
with tm.assert_produces_warning(FutureWarning): frame.sort(columns='A') with tm.assert_produces_warning(FutureWarning): frame.sort()
mi = MultiIndex.from_tuples([[1, 1, 3], [1, 1, 1]], names=list('ABC')) df = DataFrame([[1, 2], [3, 4]], mi)
unordered = frame.ix[[3, 2, 4, 1]] sorted_df = unordered.sort_index(axis=0) expected = frame assert_frame_equal(sorted_df, expected)
sorted_df = frame.sort_values(by='A') indexer = frame['A'].argsort().values expected = frame.ix[frame.index[indexer]] assert_frame_equal(sorted_df, expected)
sorted_df = frame.sort_values(by=['A'], ascending=[False]) assert_frame_equal(sorted_df, expected)
sorted_df = frame.sort_values(by='A') assert_frame_equal(sorted_df, expected[::-1]) expected = frame.sort_values(by='A') assert_frame_equal(sorted_df, expected)
idf = df.set_index(['A', 'B'])
result = idf['C'].sort_index(ascending=[1, 0]) assert_series_equal(result, expected['C'])
with tm.assert_produces_warning(FutureWarning): df.sort_index(by='a')
with tm.assert_produces_warning(FutureWarning): df.sort_index(by=['a'])
with tm.assert_produces_warning(FutureWarning): df.sort_index(by=['a', 'b'])
df.sort_values(by=['a', 'b'])
with tm.assert_produces_warning(FutureWarning): df.sort_index(by=[('a', 1)]) expected = df.sort_values(by=[('a', 1)])
sl = self.frame[:20] self.assertEqual(20, len(sl.index))
result = self.frame[lambda x: 'A'] tm.assert_series_equal(result, self.frame.loc[:, 'A'])
d = self.tsframe.index[10] indexer = self.tsframe.index > d indexer_obj = indexer.astype(object)
indexer_obj = Series(indexer_obj, self.tsframe.index)
with tm.assert_produces_warning(UserWarning, check_stacklevel=False): indexer_obj = indexer_obj.reindex(self.tsframe.index[::-1]) subframe_obj = self.tsframe[indexer_obj] assert_frame_equal(subframe_obj, subframe)
for df in [self.tsframe, self.mixed_frame, self.mixed_float, self.mixed_int]:
for c in df.columns: if c not in bifw: bifw[c] = df[c] bifw = bifw.reindex(columns=df.columns)
df = DataFrame(data=np.random.randn(100, 50))
blah = DataFrame(np.empty([0, 1]), columns=['A'], index=DatetimeIndex([]))
k = np.array([], bool)
a = DataFrame(randn(20, 2), index=[chr(x + 65) for x in range(20)]) a.ix[-1] = a.ix[-2]
smaller = self.frame[:2]
df = DataFrame([[0, 0]]) df.iloc[0] = np.nan expected = DataFrame([[np.nan, np.nan]]) assert_frame_equal(df, expected)
df[df[:-1] < 0] = 2 np.putmask(values[:-1], values[:-1] < 0, 2) assert_almost_equal(df.values, values)
df[df[::-1] == 2] = 3 values[values == 2] = 3 assert_almost_equal(df.values, values)
dm['C'] = 1 self.assertEqual(dm['C'].dtype, np.int64)
dm['A'] = 'bar' self.assertEqual('bar', dm['A'][0])
from decimal import Decimal
dm = DataFrame(index=lrange(3), columns=lrange(3))
self.assertEqual(dm[2].dtype, np.object_)
foo = df['z']
expected = ix[5:11] result = ix[f.index[5]:f.index[10]] assert_frame_equal(expected, result)
assert_frame_equal(ix[:, :2], f.reindex(columns=['A', 'B']))
exp = f.copy() ix[5:10].values[:] = 5 exp.values[5:10] = 5 assert_frame_equal(f, exp)
cp = df.copy() cp.ix[4:10] = 0 self.assertTrue((cp.ix[4:10] == 0).values.all())
cp = df.copy() cp.ix[3:11] = 0 self.assertTrue((cp.ix[3:11] == 0).values.all())
frame = self.frame.copy() frame2 = self.frame.copy()
frame = self.frame.copy()
frame = self.frame.copy() frame2 = self.frame.copy()
frame = self.frame.copy() expected = self.frame.copy()
frame = self.frame.copy() frame2 = self.frame.copy() expected = self.frame.copy()
frame.ix[:, 1:3] = 4. expected.values[:, 1:3] = 4. assert_frame_equal(frame, expected)
frame.ix[:, 'B':'C'] = 4. assert_frame_equal(frame, expected)
sliced = self.frame.ix[:, -3:]
exp[2] = 5 assert_frame_equal(tmp, exp)
self.assertRaises(KeyError, df.ix.__setitem__, ([0, 1, 2], [2, 3, 4]), 5)
df = DataFrame({1: [1., 2., 3.], 2: [3, 4, 5]}) self.assertTrue(df._is_mixed_type)
self.assertIs(ix[:, :], f)
xs1 = ix[0] xs2 = f.xs(f.index[0]) assert_series_equal(xs1, xs2)
assert_series_equal(ix[:, 'A'], f['A'])
exp = f.copy() exp.values[5] = 4 ix[5][:] = 4 assert_frame_equal(exp, f)
xs = self.mixed_frame.ix[5] exp = self.mixed_frame.xs(self.mixed_frame.index[5]) assert_series_equal(xs, exp)
frame = self.frame.copy() expected = self.frame.copy()
frame = self.frame.copy() expected = self.frame.copy()
frame = self.frame.copy() expected = self.frame.copy()
frame = self.frame.copy() expected = self.frame.copy()
for col in f.columns: ts = f[col] for idx in f.index[::5]: self.assertEqual(ix[idx, col], ts[idx])
for j, col in enumerate(f.columns):
frame = self.frame.copy() expected = self.frame.copy()
mask = self.frame['A'][::-1] > 1
result = df.ix[1:2] expected = df.iloc[0:2] assert_frame_equal(result, expected)
self.assertRaises(TypeError, lambda: df.iloc[1.0:5])
result = df.ix[1.0:5] expected = df assert_frame_equal(result, expected) self.assertEqual(len(result), 5)
result = df.get_dtype_counts() expected = Series({'float64': 3, 'datetime64[ns]': 1}) assert_series_equal(result, expected)
df.ix['c', 'timestamp'] = nan self.assertTrue(com.isnull(df.ix['c', 'timestamp']))
df.ix['d', :] = nan
df = DataFrame(np.random.randn(5, 3), index=['foo', 'foo', 'bar', 'baz', 'bar'])
df = DataFrame(np.random.randn(5, 3), index=['foo', 'foo', 'bar', 'baz', 'bar'])
df = DataFrame({'a': [1, 2, 3]})
df = DataFrame(np.random.randn(4, 4), columns=list('AABC')) df.columns.name = 'foo'
with tm.assert_produces_warning(FutureWarning): df.irow(1)
result = df.iloc[slice(4, 8)] expected = df.ix[8:14] assert_frame_equal(result, expected)
def f(): result[2] = 0. self.assertRaises(com.SettingWithCopyError, f) exp_col = df[2].copy() exp_col[4:8] = 0. assert_series_equal(df[2], exp_col)
result = df.iloc[[1, 2, 4, 6]] expected = df.reindex(df.index[[1, 2, 4, 6]]) assert_frame_equal(result, expected)
with tm.assert_produces_warning(FutureWarning): df.icol(1)
result = df.iloc[:, slice(4, 8)] expected = df.ix[:, 8:14] assert_frame_equal(result, expected)
def f(): result[8] = 0. self.assertRaises(com.SettingWithCopyError, f) self.assertTrue((df[8] == 0).all())
result = df.iloc[:, [1, 2, 4, 6]] expected = df.reindex(columns=df.columns[[1, 2, 4, 6]]) assert_frame_equal(result, expected)
for idx in [['D', 'F'], ['A', 'C', 'B']]: verify_first_level(df, 'jim', idx, check_index_type=False)
series = self.frame.xs('A', axis=1) expected = self.frame['A'] assert_series_equal(series, expected)
series = self.frame.xs('A', axis=1) series[:] = 5 self.assertTrue((expected == 5).all())
def is_ok(s): return (issubclass(s.dtype.type, (np.integer, np.floating)) and s.dtype != 'uint8')
if check_dtypes: self.assertTrue((rs.dtypes == df.dtypes).all())
for df in [default_frame, self.mixed_frame, self.mixed_float, self.mixed_int]: cond = df > 0 _check_get(df, cond)
cond = (df > 0)[1:] _check_align(df, cond, _safe_add(df))
cond = df > 0 _check_align(df, cond, (_safe_add(df).values))
cond = df > 0 check_dtypes = all([not issubclass(s.type, np.integer) for s in df.dtypes]) _check_align(df, cond, np.nan, check_dtypes=check_dtypes)
df = default_frame err1 = (df + 1).values[0:2, :] self.assertRaises(ValueError, df.where, cond, err1)
def _check_set(df, cond, check_dtypes=True): dfi = df.copy() econd = cond.reindex_like(df).fillna(True) expected = dfi.mask(~econd)
cond = (df >= 0)[1:] _check_set(df, cond)
df = DataFrame(dict(A=date_range('20130102', periods=5), B=date_range('20130104', periods=5), C=np.random.randn(5)))
df = DataFrame(np.random.randn(2, 2)) mask = DataFrame([[False, False], [False, False]]) s = Series([0, 1])
df = DataFrame([[1, 2], [3, 4]], dtype='int64') mask = DataFrame([[False, False], [False, False]]) s = Series([0, np.nan])
d1 = df.copy().drop(1, axis=0) expected = df.copy() expected.loc[1, :] = np.nan
df = DataFrame(np.random.randn(5, 3)) cond = df > 0
df = DataFrame([[1, 2]]) res = df.mask(DataFrame([[True, False]])) expec = DataFrame([[nan, 2]]) assert_frame_equal(res, expec)
df['C'] = idx assert_series_equal(df['C'], Series(idx, name='C'))
from __future__ import print_function
result = expr._can_use_numexpr(operator.add, None, self.frame, self.frame, 'evaluate') self.assertFalse(result)
result = expr._can_use_numexpr(operator.add, '+', self.mixed, self.frame, 'evaluate') self.assertFalse(result)
result = expr._can_use_numexpr(operator.add, '+', self.frame2, self.frame2, 'evaluate') self.assertFalse(result)
result = expr._can_use_numexpr(operator.add, '+', self.frame, self.frame2, 'evaluate') self.assertTrue(result)
from __future__ import division, print_function
if hasattr(targ, 'dtype') and targ.dtype == 'm8[ns]': targ, res = _coerce_tds(targ, res) tm.assert_almost_equal(targ, res, check_dtype=check_dtype) return
if allow_obj == 'convert': targfunc = partial(self._badobj_wrap, func=targfunc, allow_complex=allow_complex) self.check_fun(testfunc, targfunc, 'arr_obj', **kwargs)
values = np.array([1, 2, 3]) self.assertTrue(np.allclose(nanops._ensure_numeric(values), values), 'Failed for numeric ndarray')
o_values = values.astype(object) self.assertTrue(np.allclose(nanops._ensure_numeric(o_values), values), 'Failed for object ndarray')
s_values = np.array(['foo', 'bar', 'baz'], dtype=object) self.assertRaises(ValueError, lambda: nanops._ensure_numeric(s_values))
self.variance = variance = 3.0 self.samples = self.prng.normal(scale=variance ** 0.5, size=100000)
samples_norm = self.samples samples_unif = self.prng.uniform(size=samples_norm.shape[0]) samples = np.vstack([samples_norm, samples_unif])
var = 1.0 / 12 tm.assert_almost_equal(variance_1, var, check_less_precise=2)
tm.assert_almost_equal(variance_0, (n - 1.0) / n * var, check_less_precise=2)
tm.assert_almost_equal(variance_2, (n - 1.0) / (n - 2.0) * var, check_less_precise=2)
data = Series(766897346 * np.ones(10)) for ddof in range(3): result = data.std(ddof=ddof) self.assertEqual(result, 0.0)
self.samples = np.sin(np.linspace(0, 1, 200)) self.actual_skew = -0.1875895205961754
self.samples = np.sin(np.linspace(0, 1, 200)) self.actual_kurt = -1.2058303433799713
from __future__ import (absolute_import, division, print_function, unicode_literals) import pandas.util.testing as tm
self.assertIs(Series.str, strings.StringMethods) self.assertIsInstance(Series(['']).str, strings.StringMethods)
invalid = Series([1]) with tm.assertRaisesRegexp(AttributeError, "only use .str accessor"): invalid.str self.assertFalse(hasattr(invalid, 'str'))
strs = 'google', 'wikimedia', 'wikipedia', 'wikitravel' ds = Series(strs)
tm.assertIsInstance(s, Series)
tm.assert_index_equal(s.index, ds.index)
self.assertTrue(isinstance(el, compat.string_types) or isnull(el))
self.assertEqual(s.dropna().values.item(), 'l')
self.assertEqual(i, 100) self.assertEqual(s, 1)
result = strings.str_cat(one) exp = 'aabbc' self.assertEqual(result, exp)
values = [u('foo'), u('foofoo'), NA, u('foooofooofommmfoo')]
result = strings.str_contains(values, 'foo', regex=False, case=False) expected = np.array([True, False, True, False]) tm.assert_numpy_array_equal(result, expected)
values = np.array([u'foo', NA, u'fooommm__foo', u'mmm_'], dtype=np.object_) pat = 'mmm[_]+'
values = Series([u("FOO"), NA, u("bar"), u("Blurg")])
values = Series([u('om'), NA, u('nom'), u('nom')])
mixed = Series(['aBAD', NA, 'bBAD', True, datetime.today(), 'fooBAD', None, 1, 2.])
values = Series([u('fooBAD__barBAD'), NA])
mixed = Series(['a', NA, 'b', True, datetime.today(), 'foo', None, 1, 2.])
values = Series([u('a'), u('b'), NA, u('c'), NA, u('d')])
values = Series(['fooBAD__barBAD', NA, 'foo'])
mixed = Series(['aBAD_BAD', NA, 'BAD_b_BAD', True, datetime.today(), 'foo', None, 1, 2.])
values = Series([u('fooBAD__barBAD'), NA, u('foo')])
mixed = Series(['aBAD_BAD', NA, 'BAD_b_BAD', True, datetime.today(), 'foo', None, 1, 2.])
values = Series([u('fooBAD__barBAD'), NA, u('foo')])
values = Series(['fooBAD__barBAD', NA, 'foo'])
mixed = Series(['aBAD_BAD', NA, 'BAD_b_BAD', True, datetime.today(), 'foo', None, 1, 2.])
values = Series([u('fooBAD__barBAD'), NA, u('foo')])
f = lambda: s_or_idx.str.extract('(?:[AB]).*', expand=False) self.assertRaises(ValueError, f)
result = s.str.extract('(_)', expand=False) exp = Series([NA, NA, NA], dtype=object) tm.assert_series_equal(result, exp)
values = Series(['fooBAD__barBAD', NA, 'foo'])
mixed = Series(['aBAD_BAD', NA, 'BAD_b_BAD', True, datetime.today(), 'foo', None, 1, 2.])
values = Series([u('fooBAD__barBAD'), NA, u('foo')])
f = lambda: s_or_idx.str.extract('(?:[AB]).*', expand=True) self.assertRaises(ValueError, f)
idx = Index(['a|b', 'a|c', 'b|c']) result = idx.str.get_dummies('|')
mixed = Series(['a_b', NA, 'asdf_cas_asdf', True, datetime.today(), 'foo', None, 1, 2.])
mixed = Series(['a_b', NA, 'asdf_cas_asdf', True, datetime.today(), 'foo', None, 1, 2.])
values = Series([u('foo'), u('fooo'), u('fooooo'), np.nan, u( 'fooooooo')])
mixed = Series(['fooBAD__barBAD', NA, 'foo', True, datetime.today(), 'BAD', None, 1, 2.])
values = Series([u('fooBAD__barBAD'), NA, u('foo'), u('BAD')])
mixed = Series(['a', NA, 'b', True, datetime.today(), 'ee', None, 1, 2. ])
values = Series([u('a'), u('b'), NA, u('c'), NA, u('eeeeee')])
mixed = Series(['a', NA, 'b', True, datetime.today(), 'c', 'eee', None, 1, 2.])
values = Series([u('a'), u('b'), NA, u('c'), NA, u('eeeeee')])
with tm.assertRaisesRegexp(TypeError, "fillchar must be a character, not str"): result = values.str.center(5, fillchar='XY')
values = Series([u('a_b_c'), u('c_d_e'), NA, u('f_g_h')])
s = Series(['bd asdf jfg', 'kjasdflqw asdfnfk'])
values = Series([u'a_b_c', u'c_d_e', NA, u'f_g_h'])
s = Series(['A|B|C'])
mixed = Series(['aafootwo', NA, 'aabartwo', True, datetime.today(), None, 1, 2.])
values = Series([u('aafootwo'), u('aabartwo'), NA, u('aabazqux')])
mixed = Series([' aa ', NA, ' bb \t\n', True, datetime.today(), None, 1, 2.])
values = Series([u(' aa '), u(' bb \n'), NA, u('cc ')])
mixed = Series(['a_b_c', NA, 'c_d_e', True, datetime.today(), None, 1, 2.])
values = Series([u('a_b_c'), u('c_d_e'), np.nan, u('f_g_h')])
s = Series(list('aabbcde')) with tm.assertRaisesRegexp(AttributeError, "You cannot add any new attribute"): s.str.xlabel = "a"
tm.assert_contains_all(self.strIndex, self.strIndex) tm.assert_contains_all(self.dateIndex, self.dateIndex)
arr = np.array(self.strIndex) index = Index(arr) tm.assert_contains_all(arr, index) tm.assert_index_equal(self.strIndex, index)
self.assertRaises(TypeError, Index, 0)
result = pd.infer_freq(df['date']) self.assertEqual(result, 'MS')
class ArrayLike(object):
self.assertRaises(TypeError, lambda: ind.view('i8'))
for i in list(set(self.indices.keys()) - set(restricted)): ind = self.indices[i]
ind.view('i8')
casted.get_loc(5)
self.intIndex.name = 'foobar' casted = self.intIndex.astype('i8') self.assertEqual(casted.name, 'foobar')
self.assertFalse(Index(['a', 'b', 'c']).equals(Index(['a', 'b'])))
self.assertFalse(Index(['a', 'b', 'c']).equals(['a', 'b', 'c']))
result = Index(['b', 'c', 'd'])
self.assert_index_equal(Index(['a', 'b', 'c', 'd']), result.insert(0, 'a'))
self.assert_index_equal(Index(['b', 'c', 'e', 'd']), result.insert(-1, 'e'))
self.assert_index_equal(result.insert(1, 'z'), result.insert(-2, 'z'))
null_index = Index([]) self.assert_index_equal(Index(['a']), null_index.insert(0, 'a'))
result = idx.delete(5)
i1 = Index(['a', 'b', 'c']) i2 = Index(['a', 'b', 'c'])
for idx in [self.strIndex, self.intIndex, self.floatIndex]: empty_idx = idx.__class__([])
self.assertRaises(IndexError, idx.__getitem__, empty_farr)
inter = first.intersection(first) self.assertIs(inter, first)
union = first.union(first) self.assertIs(union, first)
result = index.append([]) self.assert_index_equal(result, index)
index = Index(['a', 'b', 'c']) index2 = index + 'foo'
self.assertIn('a', index)
result = first.difference(second)
second.name = 'name' result = first.difference(second) self.assertEqual(result.name, 'name')
result = first.difference([]) self.assertTrue(tm.equalContents(result, first)) self.assertEqual(result.name, first.name)
result = first.difference(first) self.assertEqual(len(result), 0) self.assertEqual(result.name, first.name)
expected = idx1 ^ idx2 self.assertTrue(tm.equalContents(result, expected)) self.assertIsNone(result.name)
if not is_platform_windows(): formatted = index.format() expected = [str(index[0])] self.assertEqual(formatted, expected)
inc = timedelta(hours=4) dates = Index([dt + inc for dt in self.dateIndex], name='something')
idx = Index(np.arange(10))
self.assertRaises(ValueError, ser.drop, [3, 4])
int_idx = idx1.intersection(idx2)
union_idx = idx1.union(idx2) expected = idx2 self.assertEqual(union_idx.ndim, 1) self.assert_index_equal(union_idx, expected)
result = idx.isin(set(values)) tm.assert_numpy_array_equal(result, expected)
idx = Index([]) result = idx.isin(values) self.assertEqual(len(result), 0) self.assertEqual(result.dtype, np.bool_)
check_idx(Float64Index([1.0, 2.0, 3.0, 4.0]))
idx = Index(list('abcd')) self.assertTrue('str' in dir(idx))
with tm.assert_produces_warning(RuntimeWarning): expected = right_idx.astype(object).union(left_idx.astype(object)) tm.assert_index_equal(joined, expected)
idx = pd.Index([0, 1, 2])
self.assertEqual(idx.reindex(dt_idx.values)[0].name, None) self.assertEqual(idx.reindex(dt_idx.tolist())[0].name, None)
idx = pd.Index(list('abc'))
idx = pd.Index(list('abc'))
if PY3: coerce = lambda x: x else: coerce = unicode
with cf.option_context('display.unicode.east_asian_width', True):
if self._holder is None: return
self.assertRaises(TypeError, self._holder)
idx = self.create_index() self.assertRaises(NotImplementedError, idx.shift, 1) self.assertRaises(NotImplementedError, idx.shift, 1, 2)
expected = self.create_index() if not isinstance(expected, MultiIndex): expected.name = 'foo' result = pd.Index(expected) tm.assert_index_equal(result, expected)
idx = self.create_index()
idx.nbytes idx.values.nbytes
if isinstance(ind, MultiIndex): continue
name = ('A', 'B') ind.rename(name, inplace=True) self.assertEqual(ind.name, name) self.assertEqual(ind.names, [name])
if isinstance(ind, MultiIndex): continue
with tm.assert_produces_warning(FutureWarning): ind.order()
if k in ['catIndex']: continue
if k in ['boolIndex', 'tuples', 'empty']: continue
with tm.assertRaises(AttributeError): ind.freq
cases = [0.5, 'xxx'] methods = [idx.intersection, idx.union, idx.difference, idx.symmetric_difference]
with tm.assert_produces_warning(FutureWarning): first.sym_diff(second)
self.assertTrue(idx[0:4].equals(result.insert(0, idx[0])))
continue
result = idx.delete(len(idx))
index_a = self.create_index() if isinstance(index_a, PeriodIndex): return
with tm.assertRaises(Exception): func(idx)
result = func(idx) exp = Index(func(idx.values), name=idx.name) self.assert_index_equal(result, exp) self.assertIsInstance(result, pd.Float64Index)
if len(idx) == 0: continue else: with tm.assertRaises(Exception): func(idx)
with tm.assertRaises(Exception): func(idx)
result = func(idx) exp = func(idx.values) self.assertIsInstance(result, np.ndarray) tm.assertNotIsInstance(result, Index)
for name, index in self.indices.items(): if isinstance(index, MultiIndex): pass else: idx = index.copy()
expected = np.array([False] * len(idx), dtype=bool) self.assert_numpy_array_equal(idx._isnan, expected) self.assertFalse(idx.hasnans)
if not isinstance(idx, RangeIndex): result = idx * idx tm.assert_index_equal(result, idx ** 2)
result = idx / 1 expected = idx if PY3: expected = expected.astype('float64') tm.assert_index_equal(result, expected)
idx = self._holder(np.arange(5, dtype='int64'))
expected = Float64Index(arr) a = np.zeros(5, dtype='float64') result = fidx - a tm.assert_index_equal(result, expected)
index = self.create_index() expected = Index(index.values % 2) self.assert_index_equal(index % 2, expected)
for dtype in ['M8[ns]', 'm8[ns]']: self.assertRaises(TypeError, lambda: i.astype(dtype))
sliced = idx.slice_locs(np.nan) self.assertTrue(isinstance(sliced, tuple)) self.assertEqual(sliced, (0, 3))
exp = Float64Index([1.0, 2.0, 3.0], name='x') self.assert_index_equal(idx.fillna(2), exp)
exp = Index([1.0, 'obj', 3.0], name='x') self.assert_index_equal(idx.fillna('obj'), exp)
index = Int64Index([-5, 0, 1, 2]) expected = Index([-5, 0, 1, 2], dtype=np.int64) tm.assert_index_equal(index, expected)
index = Int64Index(iter([-5, 0, 1, 2])) tm.assert_index_equal(index, expected)
self.assertRaises(TypeError, Int64Index, 5)
arr = self.index.values new_index = Int64Index(arr, copy=True) tm.assert_index_equal(new_index, self.index) val = arr[0] + 3000
arr[0] = val self.assertNotEqual(new_index[0], val)
arr = np.array([1, '2', 3, '4'], dtype=object) with tm.assertRaisesRegexp(TypeError, 'casting'): Int64Index(arr)
arr = Index([1, 2, 3, 4]) tm.assertIsInstance(arr, Int64Index)
arr = Index([1, 2, 3, 4], dtype=object) tm.assertIsInstance(arr, Index)
res, lidx, ridx = self.index.join(other, how='inner', return_indexers=True)
ind = res.argsort() res = res.take(ind) lidx = lidx.take(ind) ridx = ridx.take(ind)
res, lidx, ridx = self.index.join(other_mono, how='inner', return_indexers=True)
data = ['foo', 'bar', 'baz'] self.assertRaises(TypeError, Int64Index, data)
data = ['0', '1', '2'] self.assertRaises(TypeError, Int64Index, data)
result = Index(np.array(ci)) self.assertIsInstance(result, Index) self.assertNotIsInstance(result, CategoricalIndex)
ci = self.create_index(categories=list('abc'))
ci = self.create_index()
idx = pd.Index(pd.Categorical(['a', 'b']))
self.assertRaises(ValueError, lambda: ci.set_categories( list('cab'), inplace=True))
self.assertFalse(0 in ci) self.assertFalse(1 in ci)
result = ci[:3].append(ci[3:]) tm.assert_index_equal(result, ci, exact=True)
result = ci.append([]) tm.assert_index_equal(result, ci, exact=True)
result = ci.append(['c', 'a']) expected = CategoricalIndex(list('aabbcaca'), categories=categories) tm.assert_index_equal(result, expected, exact=True)
self.assertRaises(TypeError, lambda: ci.append(['a', 'd']))
result = ci.insert(0, 'a') expected = CategoricalIndex(list('aaabbca'), categories=categories) tm.assert_index_equal(result, expected, exact=True)
result = ci.insert(-1, 'a') expected = CategoricalIndex(list('aabbcaa'), categories=categories) tm.assert_index_equal(result, expected, exact=True)
result = CategoricalIndex(categories=categories).insert(0, 'a') expected = CategoricalIndex(['a'], categories=categories) tm.assert_index_equal(result, expected, exact=True)
self.assertRaises(TypeError, lambda: ci.insert(0, 'd'))
result = ci.delete(10)
self.assertTrue(result.equals(ci)) self.assertIsInstance(result, Index) self.assertNotIsInstance(result, CategoricalIndex)
idx = self.create_index() expected = np.array([4, 0, 1, 5, 2, 3])
cidx2 = CategoricalIndex(list('aacded'), categories=list('edabc')) idx2 = Index(list('aacded'))
cidx3 = CategoricalIndex(list('aabbb'), categories=list('abc')) idx3 = Index(list('aabbb'))
if PY3: str(ci) else: compat.text_type(ci)
ci = CategoricalIndex(np.random.randint(0, 5, size=100)) if PY3: str(ci) else: compat.text_type(ci)
with cf.option_context('display.unicode.east_asian_width', True):
with tm.assertRaisesRegexp(ValueError, 'fill value must be in categories'): idx.fillna(2.0)
ind = self.index.set_names(new_names, level=[0, 1]) self.assertEqual(self.index.names, self.index_names) self.assertEqual(ind.names, new_names)
levels = self.index.levels new_levels = [[lev + 'a' for lev in level] for level in levels]
ind2 = self.index.set_levels(new_levels) assert_matching(ind2.levels, new_levels) assert_matching(self.index.levels, levels)
ind2 = self.index.copy() inplace_return = ind2.set_levels(new_levels, inplace=True) self.assertIsNone(inplace_return) assert_matching(ind2.levels, new_levels)
ind2 = self.index.set_levels(new_levels[0], level=0) assert_matching(ind2.levels, [new_levels[0], levels[1]]) assert_matching(self.index.levels, levels)
ind2 = self.index.set_levels(new_levels, level=[0, 1]) assert_matching(ind2.levels, new_levels) assert_matching(self.index.levels, levels)
ind2 = self.index.set_labels(new_labels) assert_matching(ind2.labels, new_labels) assert_matching(self.index.labels, labels)
ind2 = self.index.copy() inplace_return = ind2.set_labels(new_labels, inplace=True) self.assertIsNone(inplace_return) assert_matching(ind2.labels, new_labels)
ind2 = self.index.set_labels(new_labels[0], level=0) assert_matching(ind2.labels, [new_labels[0], labels[1]]) assert_matching(self.index.labels, labels)
ind2 = self.index.set_labels(new_labels, level=[0, 1]) assert_matching(ind2.labels, new_labels) assert_matching(self.index.labels, labels)
with tm.assertRaisesRegexp(TypeError, 'list of lists-like'): self.index.set_levels(levels[0])
with tm.assertRaisesRegexp(TypeError, 'list of lists-like'): self.index.set_labels(labels[0])
with tm.assertRaisesRegexp(TypeError, 'list-like'): self.index.set_names(names[0])
with tm.assertRaisesRegexp(TypeError, 'list of lists-like'): self.index.set_levels(levels[0], level=[0, 1])
with tm.assertRaisesRegexp(TypeError, 'list of lists-like'): self.index.set_labels(labels[0], level=[0, 1])
with tm.assertRaisesRegexp(ValueError, 'Length of names'): self.index.set_names(names[0], level=[0, 1])
new_vals = mi1.set_levels(levels2).values assert_almost_equal(vals2, new_vals) assert_almost_equal(mi1._tuples, vals) assert_almost_equal(mi1.values, vals)
mi1.set_levels(levels2, inplace=True) assert_almost_equal(mi1.values, vals2)
mi2.set_labels(labels2, inplace=True) assert_almost_equal(mi2.values, new_values)
names = self.index_names level_names = [level.name for level in self.index.levels] self.assertEqual(names, level_names)
with tm.assertRaisesRegexp(ValueError, length_error): self.index.copy().set_levels([['a'], ['b']])
with warnings.catch_warnings(): warnings.simplefilter('ignore')
assert_copy(copy.levels, original.levels)
assert_almost_equal(copy.labels, original.labels) self.assertIsNot(copy.labels, original.labels)
self.assertEqual(copy.names, original.names) self.assertIsNot(copy.names, original.names)
self.assertEqual(copy.sortorder, original.sortorder)
level_names = [level.name for level in self.index.levels] self.check_level_names(self.index, self.index.names)
new_names = [name + "a" for name in self.index.names] self.index.names = new_names self.check_level_names(self.index, new_names)
self.check_level_names(view, level_names) self.check_level_names(copy, level_names) self.check_level_names(shallow_copy, level_names)
shallow_copy.names = [name + "c" for name in shallow_copy.names] self.check_level_names(self.index, new_names)
tm.assert_numpy_array_equal(mi.values[:4], mi[:4].values)
result = self.index.append([]) self.assertTrue(result.equals(self.index))
assertRaisesRegexp(IndexError, '^Too many levels', self.index.reorder_levels, [2, 1, 0])
path = tm.get_data_path('mindex_073.pickle') obj = pd.read_pickle(path)
self.assertFalse(self.index.is_numeric())
self.assertEqual(self.index[2], ('bar', 'one'))
result = self.index[2:5] expected = self.index[[2, 3, 4]] self.assertTrue(result.equals(expected))
sorted_index, _ = index.sortlevel(0) sorted_index.slice_locs((1, 0, 1), (2, 1, 0))
major_axis = lrange(70000) minor_axis = lrange(10)
index = MultiIndex(levels=[major_axis, minor_axis], labels=[major_labels, minor_labels])
self.assertRaises(ValueError, index.truncate, 3, 1)
r1 = idx1.get_indexer(idx2._tuple_index) rexp1 = idx1.get_indexer(idx2) assert_almost_equal(r1, rexp1)
idx1 = Index(lrange(10) + lrange(10)) idx2 = Index(lrange(20))
pd.set_option('display.multi_sparse', False)
major_axis = Index(lrange(4)) minor_axis = Index(lrange(2))
major_axis = Index(['foo', 'bar', 'baz', 'qux']) minor_axis = Index(['one', 'two'])
mi2.names = ["A", "B"] self.assertTrue(mi2.is_(mi)) self.assertTrue(mi.is_(mi2))
the_union = self.index.union(self.index) self.assertIs(the_union, self.index)
the_int = self.index.intersection(self.index) self.assertIs(the_int, self.index)
empty = self.index[:2] & self.index[2:] expected = self.index[:0] self.assertTrue(empty.equals(expected))
result = self.index.difference(self.index.sortlevel(1)[0]) self.assertEqual(len(result), 0)
result = first.difference(first._tuple_index) self.assertTrue(result.equals(first[:0]))
result = first.difference([]) self.assertTrue(first.equals(result)) self.assertEqual(first.names, result.names)
dropped = self.index.drop(index, errors='ignore') expected = self.index[[0, 1, 2, 3, 4, 5]] self.assert_index_equal(dropped, expected)
self.assert_index_equal(lexsorted_mi, not_lexsorted_mi) with self.assert_produces_warning(PerformanceWarning): self.assert_index_equal(lexsorted_mi.drop('a'), not_lexsorted_mi.drop('a'))
new_index = self.index.insert(0, ('abc', 'three'))
msg = "Item must have length equal to number of levels" with assertRaisesRegexp(ValueError, msg): self.index.insert(0, ('foo2', ))
tm.assert_frame_equal(left, right, check_dtype=False) tm.assert_series_equal(ts, right['3rd'])
mask = np.array( [x[1] in exp_level for x in self.index], dtype=bool) exp_values = self.index.values[mask] tm.assert_numpy_array_equal(join_index.values, exp_values)
def check(nlevels, with_nulls): labels = np.tile(np.arange(500), 2) level = np.arange(500)
index = MultiIndex(levels=levels, labels=labels) self.assertFalse(index.has_duplicates)
check(4, False) check(4, True)
check(8, False) check(8, True)
if PY3: str(mi) else: compat.text_type(mi)
mi = MultiIndex.from_product([list('abcdefg'), range(10)], names=['first', 'second']) result = str(mi)
pass
with tm.assertRaises(NotImplementedError): pd.isnull(self.index)
assert not ind.is_monotonic
idx = self.index.copy() target = idx.copy() idx.names = target.names = [None, None]
groups = self.index.groupby(self.index) exp = dict((key, [key]) for key in self.index) tm.assert_dict_equal(groups, exp)
self.assertTrue((self.index == self.index).all())
result = df.loc[idx['2016-01-01':'2016-02-01', :], :] expected = df tm.assert_frame_equal(result, expected)
with assertRaises(KeyError): df['2016-01-01']
result = df.loc['2016'] expected = df tm.assert_frame_equal(result, expected)
result = df.loc['2016-01-01'] expected = df.iloc[0:6] tm.assert_frame_equal(result, expected)
result = df.loc['2016-01-02 12'] expected = df.iloc[9:12] tm.assert_frame_equal(result, expected)
result = df_swap.loc[idx[:, '2016-01-02'], :] expected = df_swap.iloc[[2, 3, 7, 8, 12, 13]] tm.assert_frame_equal(result, expected)
result = df.loc[('2016-01-01', 'a'), :] expected = df.iloc[[0, 3]] tm.assert_frame_equal(result, expected)
with assertRaises(KeyError): df_swap.loc['2016-01-01']
self.assertRaises(ValueError, lambda: DatetimeIndex( i.tz_localize(None).asi8, dtype=i.dtype, tz='US/Pacific'))
idx = DatetimeIndex(['2016-05-16', 'NaT', NaT, np.NaN])
idx = DatetimeIndex(['2016-05-16', 'NaT', NaT, np.NaN])
idx = DatetimeIndex(['2016-05-16', 'NaT', NaT, np.NaN])
i = pd.date_range('20130101', periods=3, tz='US/Eastern')
index = date_range('20130101', periods=3, tz='US/Eastern', name='foo') unpickled = self.round_trip_pickle(index) self.assert_index_equal(index, unpickled)
for tz in ['US/Eastern', 'Asia/Tokyo']: idx = pd.DatetimeIndex(['2011-01-01 09:00', pd.NaT, '2011-01-01 11:00'])
idx = PeriodIndex(['2016-05-16', 'NaT', NaT, np.NaN], freq='D')
idx = PeriodIndex(['2016-05-16', 'NaT', NaT, np.NaN], freq='D')
idx = pd.PeriodIndex( ['2011-01-01 09:00', pd.NaT, '2011-01-01 11:00'], freq='H')
idx = TimedeltaIndex([1e14, 'NaT', pd.NaT, np.NaN])
idx = TimedeltaIndex([1e14, 'NaT', pd.NaT, np.NaN])
idx = TimedeltaIndex([1e14, 'NaT', pd.NaT, np.NaN])
idx = TimedeltaIndex(['2H', '4H', '6H', '8H', '10H'], freq='2H', name='x')
idx = pd.TimedeltaIndex(['1 day', pd.NaT, '3 day'])
self.assertRaises(TypeError, lambda: Index(0, 1000))
index = RangeIndex(1, 5, 2) result = RangeIndex(index, copy=False) self.assertTrue(result.identical(index))
result = RangeIndex.from_range(range(5, 1)) expected = RangeIndex(0, 0, 1) self.assert_index_equal(result, expected, exact=True)
orig = RangeIndex(10) orig.name = 'original'
result = idx / 2
result = idx * idx expected = Index(idx.values * idx.values) tm.assert_index_equal(result, expected, exact=True)
idx = RangeIndex(0, 1000, 2) result = idx ** 2 expected = idx._int64index ** 2 tm.assert_index_equal(Index(result.values), expected, exact=True)
self.assertRaises(TypeError, lambda: RangeIndex(1, 5, dtype='float64'))
self.assert_index_equal(idx[0:4], result.insert(0, idx[0]))
result = idx.delete(len(idx))
if isinstance(self.index, RangeIndex): return
other = Int64Index(np.arange(25, 14, -1))
other = RangeIndex(25, 14, -1)
other = Int64Index(np.arange(25, 14, -1))
ind = res.argsort() res = res.take(ind) lidx = lidx.take(ind) ridx = ridx.take(ind)
other = RangeIndex(25, 14, -1)
other = Int64Index(np.arange(25, 14, -1))
other = Int64Index(np.arange(25, 14, -1))
other = Int64Index(np.arange(25, 14, -1))
other = RangeIndex(25, 14, -1)
i = RangeIndex(0, 1000) self.assertTrue(i.nbytes < i.astype(int).nbytes / 10)
i2 = RangeIndex(0, 10) self.assertEqual(i.nbytes, i2.nbytes)
self.assertRaises(TypeError, RangeIndex, 'foo', 'bar', 'baz')
self.assertRaises(TypeError, RangeIndex, '0', '1', '2')
idx = RangeIndex(5)
expected = Float64Index(arr) a = np.zeros(5, dtype='float64') result = fidx - a tm.assert_index_equal(result, expected)
pass
res = self.index[1] expected = 2 self.assertEqual(res, expected)
index = self.index[:] expected = self.index self.assert_index_equal(index, expected)
index = self.index[7:10:2] expected = Index(np.array([14, 18]), name='foo') self.assert_index_equal(index, expected)
index = self.index[-1:-5:-2] expected = Index(np.array([18, 14]), name='foo') self.assert_index_equal(index, expected)
index = self.index[2:100:4] expected = Index(np.array([4, 12]), name='foo') self.assert_index_equal(index, expected)
index = self.index[::-1] expected = Index(self.index.values[::-1], name='foo') self.assert_index_equal(index, expected)
arr = a.astype('U').astype(object) self.assertTrue(lib.max_len_string_array(arr), 3)
arr = a.astype('S').astype(object) self.assertTrue(lib.max_len_string_array(arr), 3)
tm.assertRaises(TypeError, lambda: lib.max_len_string_array(arr.astype('U')))
indices = indices[::-1] maybe_slice = lib.maybe_indices_to_slice(indices, len(target)) self.assertTrue(isinstance(maybe_slice, slice)) self.assert_numpy_array_equal(target[indices], target[maybe_slice])
indices = indices[::-1] maybe_slice = lib.maybe_indices_to_slice(indices, len(target)) self.assertTrue(isinstance(maybe_slice, slice)) self.assert_numpy_array_equal(target[indices], target[maybe_slice])
indices = indices[::-1] maybe_slice = lib.maybe_indices_to_slice(indices, len(target)) self.assertTrue(isinstance(maybe_slice, slice)) self.assert_numpy_array_equal(target[indices], target[maybe_slice])
indices = indices[::-1] maybe_slice = lib.maybe_indices_to_slice(indices, len(target)) self.assertTrue(isinstance(maybe_slice, slice)) self.assert_numpy_array_equal(target[indices], target[maybe_slice])
import nose
import nose
validate_args(self.fname, (None,), 2, dict(out=None))
as_stolen_buf = move_into_mutable_buffer(b[:-3])
self.assertEqual(bytearray(as_stolen_buf), b'test')
arr = np.array(list('abc'), dtype='S1') self.assertEqual(pd.lib.infer_dtype(arr), compare)
arr = arr.astype(object) self.assertEqual(pd.lib.infer_dtype(arr), compare)
with tm.assertRaisesRegexp(ValueError, msg): lib.maybe_convert_numeric( np.array(['foo_' + infinity], dtype=object), na_values, maybe_int)
from collections import namedtuple record = namedtuple('record', 'x y') r = record(5, 6) values = [r]
rows = [[1, 2, 3], [4, 5, 6]]
arr = np.array([None], dtype='O') result = lib.infer_dtype(arr) self.assertEqual(result, 'mixed')
from pandas import Categorical, Series arr = Categorical(list('abc')) result = lib.infer_dtype(arr) self.assertEqual(result, 'categorical')
dtypes = ['i1', 'i2', 'i4', 'i8', 'u1', 'u2', 'u4', 'u8']
from pandas.compat import range
x = Series(['A', 'A', np.nan, 'B', 3.14, np.inf]) labels, uniques = algos.factorize(x)
key = np.array([0, np.nan, 1], dtype='O') na_sentinel = -1
htable.get_labels(vals[:nvals], uniques, 0, -1) uniques.to_array() htable.get_labels(vals, uniques, 0, -1)
_test_vector_resize(tbl(), vect(), dtype, 0) _test_vector_resize(tbl(), vect(), dtype, 10)
expected = np_array_datetime64_compat( ['2015-01-03T00:00:00.000000000+0000', '2015-01-01T00:00:00.000000000+0000'], dtype='M8[ns]')
expected = np.array([31200, 45678, 10000], dtype='m8[ns]')
s = s.cat.as_ordered() result = s.value_counts() expected.index = expected.index.as_ordered() tm.assert_series_equal(result, expected, check_index_type=True)
old = Index([1, 4]) new = Index(lrange(5, 10)) filler = _algos.backfill_int64(old.values, new.values)
expected = np.argsort(a, kind='mergesort') assert (np.array_equal(result, expected))
former_encoding = sys.getdefaultencoding()
if former_encoding is not None and former_encoding != "utf-8":
s = Series([1], dtype='float32') with tm.assert_produces_warning(FutureWarning): result = s.convert_objects(convert_dates='coerce', convert_numeric=False) assert_series_equal(result, s)
r = s.copy().astype('O') r['a'] = '1' result = r._convert(numeric=True) assert_series_equal(result, s)
s = Series([1], dtype='float32') result = s._convert(datetime=True, coerce=True) assert_series_equal(result, s)
s = self.ts.copy() result = getattr(s, op)(s) self.assertEqual(result.name, self.ts.name)
cp = self.ts.copy() cp.name = 'changed' result = getattr(s, op)(cp) self.assertIsNone(result.name)
s = Series(list('abbcd'), dtype="category") self.assertTrue('cat' in dir(s))
getkeys = self.ts.keys self.assertIs(getkeys(), self.ts.index)
self.assertFalse(hasattr(self.series.iteritems(), 'reverse'))
if deep is None: s2 = s.copy() else: s2 = s.copy(deep=deep)
self.assertTrue(np.isnan(s2[0])) self.assertFalse(np.isnan(s[0]))
self.assertTrue(np.isnan(s2[0])) self.assertTrue(np.isnan(s[0]))
expected = Series([Timestamp('2012/01/01', tz='UTC')]) expected2 = Series([Timestamp('1999/01/01', tz='UTC')])
if deep is None or deep is True: assert_series_equal(s, expected) assert_series_equal(s2, expected2) else: assert_series_equal(s, expected2) assert_series_equal(s2, expected2)
np.unique(self.ts)
tsdf = DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'], index=date_range('1/1/2000', periods=1000))
s = Series([1]) result = s.item() self.assertEqual(result, 1) self.assertEqual(s.item(), s.iloc[0])
s = Series(np.random.randn(10)) result = np.ones_like(s) expected = Series(1, index=range(10), dtype='float64')
s = Series(np.random.randn(10)) tm.assert_almost_equal(s.ravel(order='F'), s.values.ravel(order='F'))
exp = Series([], dtype='float64', index=Index([], dtype='object')) assert_series_equal(result, exp)
exp = Series([], dtype='float64', index=Index([], dtype='float64')) assert_series_equal(result, exp)
s = Series(range(5)) with self.assertRaisesRegexp(AttributeError, 'only use .str accessor'): s.str.repeat(2)
td[2] = np.nan result = td.ffill() expected = td.fillna(0) expected[0] = np.nan assert_series_equal(result, expected)
self.assertRaises(ValueError, s.dropna, axis=1)
s = Series([0, 1, np.nan, 3, 4, 5])
non_ts = self.series.copy() non_ts[0] = np.NaN self.assertRaises(ValueError, non_ts.interpolate, method='time')
other_result = s.interpolate(method='values')
expected = Series([1., 3., 5., 7., np.nan, 11.])
self.assertRaises(ValueError, s.interpolate, method='linear', limit_direction='abc')
s = Series([1, 3, np.nan, np.nan, np.nan, 11])
s = Series([1, 3, np.nan, np.nan, np.nan, 7, 9, np.nan, np.nan, 12, np.nan])
s = Series([np.nan, np.nan, 5, 7, 9, np.nan])
s = Series([np.nan, np.nan, 5, 7, np.nan, np.nan])
tm._skip_if_no_scipy() s = Series([1, 2, 3]) result = s.interpolate(method='polynomial', order=1) assert_series_equal(result, s)
result = s.interpolate() assert_series_equal(result, s)
s = pd.Series([1., 2., 3.]) result = s.interpolate(limit=1) expected = s assert_series_equal(result, expected)
def test_spline_error(self): tm._skip_if_no_scipy()
exit=False)
series = self.series.copy() self.assertRaises(TypeError, setattr, series, 'index', None)
series = self.series.copy() self.assertRaises(Exception, setattr, series, 'index', np.arange(len(series) - 1))
series = self.series.copy() series.index = np.arange(len(series)) tm.assertIsInstance(series.index, Index)
rename_dict = dict(zip(self.ts.index, renamed.index)) renamed2 = self.ts.rename(rename_dict) assert_series_equal(renamed, renamed2)
s = ser.reset_index(drop=True) s2 = ser s2.reset_index(drop=True, inplace=True) assert_series_equal(s, s2)
result = s.reorder_levels([0, 1, 2]) assert_series_equal(s, result)
result = s.reorder_levels(['L0', 'L1', 'L2']) assert_series_equal(s, result)
q = Series(self.ts, dtype=object).quantile(0.9) self.assertEqual(q, percentile(self.ts.valid(), 90))
dts = self.ts.index.to_series() q = dts.quantile(.2) self.assertEqual(q, Timestamp('2000-01-10 19:12:00'))
tds = dts.diff() q = tds.quantile(.25) self.assertEqual(q, pd.to_timedelta('24:00:00'))
result = Series([np.timedelta64('NaT')]).sum() self.assertTrue(result is pd.NaT)
if _np_version_under1p9: raise nose.SkipTest("Numpy version is under 1.9")
self.assertEqual(q, q1)
if _np_version_under1p9: raise nose.SkipTest("Numpy version is under 1.9")
if not _np_version_under1p9: raise nose.SkipTest("Numpy version is greater than 1.9")
expErrMsg = "Interpolation methods other than " with tm.assertRaisesRegexp(ValueError, expErrMsg): self.ts.quantile(0.9, interpolation='nearest')
with tm.assertRaisesRegexp(ValueError, expErrMsg): q = Series(self.ts, dtype=object).quantile(0.7, interpolation='higher')
cases = [Series([]), Series([np.nan, np.nan])]
s = Series(date_range('20130101', periods=5, tz='US/Eastern'), name='xxx') for prop in ok_for_dt:
if prop != 'freq': compare(s, prop)
with pd.option_context('chained_assignment', 'raise'):
self.assert_numpy_array_equal(result, expected, check_dtype=False)
s = Series(date_range('20130101', periods=5, freq='D')) s.iloc[2] = pd.NaT
from pandas.tseries.common import (CombinedDatetimelikeProperties, DatetimeProperties) self.assertIs(Series.dt, CombinedDatetimelikeProperties)
s = Series([], index=[]) self.assertEqual(s.sum(), 0)
for dtype in ['int32', 'int64']: v = np.arange(5000000, dtype=dtype) s = Series(v)
int_ts = Series(np.ones(10, dtype=int), index=lrange(10)) self.assertAlmostEqual(np.median(int_ts), int_ts.median())
s = self.ts.iloc[[0]] result = s.var(ddof=1) self.assertTrue(isnull(result))
s = self.ts.iloc[[0]] result = s.sem(ddof=1) self.assertTrue(isnull(result))
s = Series(np.arange(5))
self.series[5:15] = np.NaN
if name not in ['max', 'min']: ds = Series(date_range('1/1/2001', periods=10)) self.assertRaises(TypeError, f, ds)
self.assertTrue(notnull(f(self.series))) self.assertTrue(isnull(f(self.series, skipna=False)))
nona = self.series.dropna() assert_almost_equal(f(nona), alternate(nona.values)) assert_almost_equal(f(self.series), alternate(nona.values))
try: self.assertTrue(nanops._USE_BOTTLENECK)
s = Series([1, 2, 3, None, 5]) f(s)
if check_objects: s = Series(bdate_range('1/1/2000', periods=10)) res = f(s) exp = alternate(s) self.assertEqual(res, exp)
if name not in ['sum', 'min', 'max']: self.assertRaises(TypeError, f, Series(list('abc')))
self.assertRaises(ValueError, f, self.series, axis=1)
if 'numeric_only' in compat.signature(f).args: self.assertRaisesRegexp(NotImplementedError, name, f, self.series, numeric_only=True)
ts = self.ts.copy() ts[::2] = np.NaN
s = Series(['abc', True])
s1 = Series([np.nan, True]) s2 = Series([np.nan, False])
self.assertRaises(NotImplementedError, s.any, bool_only=True, level=0) self.assertRaises(NotImplementedError, s.all, bool_only=True, level=0)
self.assertRaises(NotImplementedError, s.any, bool_only=True) self.assertRaises(NotImplementedError, s.all, bool_only=True)
s = Series([0, 1])
result = Series(dtype=float).sum() self.assertEqual(result, 0)
result = Series(dtype='m8[ns]').sum() self.assertEqual(result, Timedelta(0))
self.assertAlmostEqual(self.ts.corr(self.ts), 1)
self.assertAlmostEqual(self.ts[:15].corr(self.ts[5:]), 1)
self.assertTrue(np.isnan(self.ts[::2].corr(self.ts[1::2])))
cp = self.ts[:10].copy() cp[:] = np.nan self.assertTrue(isnull(cp.corr(cp)))
if scipy.__version__ < LooseVersion('0.9'): raise nose.SkipTest("skipping corr rank because of scipy version " "{0}".format(scipy.__version__))
self.assertAlmostEqual(self.ts.cov(self.ts), self.ts.std() ** 2)
self.assertAlmostEqual(self.ts[:15].cov(self.ts[5:]), self.ts[5:15].std() ** 2)
self.assertTrue(np.isnan(self.ts[::2].cov(self.ts[1::2])))
cp = self.ts[:10].copy() cp[:] = np.nan self.assertTrue(isnull(cp.cov(cp)))
self.assertTrue(isnull(self.ts[:15].cov(self.ts[5:], min_periods=12)))
b2 = b.reindex(index=reversed(b.index)) result = a.dot(b) assert_series_equal(result, expected)
assert_almost_equal(a.dot(b['1']), expected['1']) assert_almost_equal(a.dot(b2['1']), expected['1'])
s = Series([1.2345] * 100) s[::2] = np.nan result = s.unique() self.assertEqual(len(result), 2)
exp = Series(rankdata(filled), index=filled.index, name='ts') exp[mask] = np.nan
def cummin(x): return np.minimum.accumulate(x)
s = Series(date_range('jan-01-2013', 'jan-05-2013'))
result = s.isin(s[0:2].values.astype('datetime64[D]')) assert_series_equal(result, expected)
s = Series(pd.to_timedelta(lrange(5), unit='d')) result = s.isin(s[0:2]) assert_series_equal(result, expected)
td = Series(date_range('2012-1-1', periods=3, freq='D')) - \ Timestamp('20120101')
td[0] = np.nan
s1 = Series(date_range('20120101', periods=3)) s2 = Series(date_range('20120102', periods=3)) expected = Series(s2 - s1)
result = td.max() expected = Timedelta('2 days') self.assertEqual(result, expected)
self.series[5:15] = np.NaN
self.assertEqual(self.series[self.series.idxmin()], self.series.min()) self.assertTrue(isnull(self.series.idxmin(skipna=False)))
allna = self.series * nan self.assertTrue(isnull(allna.idxmin()))
from pandas import date_range s = Series(date_range('20130102', periods=6)) result = s.idxmin() self.assertEqual(result, 0)
data = np.random.randint(0, 11, size=10) result = np.argmin(Series(data)) self.assertEqual(result, np.argmin(data))
self.series[5:15] = np.NaN
self.assertEqual(self.series[self.series.idxmax()], self.series.max()) self.assertTrue(isnull(self.series.idxmax(skipna=False)))
allna = self.series * nan self.assertTrue(isnull(allna.idxmax()))
data = np.random.randint(0, 11, size=10) result = np.argmax(Series(data)) self.assertEqual(result, np.argmax(data))
Series(Series(["a", "c", "b"]).unique()).sort_values()
with tm.assert_produces_warning(FutureWarning): ts.sort()
df = DataFrame(np.random.randn(10, 4)) s = df.iloc[:, 0]
sorted_series = random_order.sort_index(ascending=False) assert_series_equal(sorted_series, self.ts.reindex(self.ts.index[::-1]))
rindex = list(self.ts.index) random.shuffle(rindex)
rindex = list(self.ts.index) random.shuffle(rindex)
sorted_series = random_order.sort_index(axis=0) assert_series_equal(sorted_series, self.ts)
with tm.assert_produces_warning(FutureWarning): self.ts.order()
ser = Series(['A', 'B'], [1, 2]) ser.sort_values()
Series([3., 2, 1, 2, 5], dtype='complex128'),
s = pd.Series(['a', 'b', 'c', 'd'], dtype='category')
x = Series(np.random.random(201), name='x') self.assertTrue(x.reshape(x.shape, ) is x)
idx = pd.DatetimeIndex(values, name='xxx') tm.assert_series_equal(idx.value_counts(), exp)
idx = pd.PeriodIndex(values, name='xxx') tm.assert_series_equal(idx.value_counts(), exp)
values = pd.Categorical([1, 2, 3, 1, 1, 3], ordered=True)
idx = pd.CategoricalIndex(values, name='xxx') tm.assert_series_equal(idx.value_counts(), exp)
idx = pd.CategoricalIndex(values, name='xxx') tm.assert_series_equal(idx.value_counts(), exp)
combined = series.combine_first(series_copy)
combined = series_copy.combine_first(series) self.assertTrue(np.isfinite(combined).all())
index = tm.makeStringIndex(20) floats = Series(tm.randn(20), index=index) strings = Series(tm.makeStringIndex(10), index=index[::2])
s = Series([1., 2, 3], index=[0, 1, 2]) result = s.combine_first(Series([], index=[])) assert_series_equal(s, result)
df = DataFrame([{"a": 1}, {"a": 3, "b": 2}]) df['c'] = np.nan
dtypes = map(np.dtype, ['float64', 'int8', 'uint8', 'bool', 'm8[ns]', 'M8[ns]'])
s == s2 s2 == s
s = Series([-1, 0, 1])
result = df['A'] - df['A'].shift() self.assertEqual(result.dtype, 'timedelta64[ns]')
maxa = df['A'].max() tm.assertIsInstance(maxa, Timestamp)
resultb = resulta + d assert_series_equal(df['A'], resultb)
value = rs[2] + np.timedelta64(timedelta(minutes=5, seconds=1)) rs[2] += np.timedelta64(timedelta(minutes=5, seconds=1)) self.assertEqual(rs[2], value)
s = Series([Timestamp('20130101 9:01'), Timestamp('20130101 9:02')])
assert_series_equal(result + td2, td1)
assert_series_equal(result + td2, td1)
startdate = Series(date_range('2013-01-01', '2013-01-03')) enddate = Series(date_range('2013-03-01', '2013-03-03'))
expected = s1.apply(lambda x: x / np.timedelta64(m, unit)) result = s1 / np.timedelta64(m, unit) assert_series_equal(result, expected)
result = s1.astype("timedelta64[{0}]".format(unit)) assert_series_equal(result, expected)
expected = s1.apply( lambda x: Timedelta(np.timedelta64(m, unit)) / x) result = np.timedelta64(m, unit) / s1
for op_str in ops: op = getattr(get_ser, op_str, None) with tm.assertRaisesRegexp(TypeError, 'operate'): op(test_ser)
if not _np_version_under1p8: result = td1[0] + dt1 expected = ( dt1.dt.tz_localize(None) + td1[0]).dt.tz_localize('US/Eastern') assert_series_equal(result, expected)
assert_series_equal(timedelta_series - NaT, nat_series_dtype_timedelta) assert_series_equal(-NaT + timedelta_series, nat_series_dtype_timedelta)
assert_series_equal(nat_series_dtype_timestamp - NaT, nat_series_dtype_timestamp) assert_series_equal(-NaT + nat_series_dtype_timestamp, nat_series_dtype_timestamp)
assert_series_equal(nat_series_dtype_timestamp + NaT, nat_series_dtype_timestamp) assert_series_equal(NaT + nat_series_dtype_timestamp, nat_series_dtype_timestamp)
assert_series_equal(nat_series_dtype_timedelta * 1.0, nat_series_dtype_timedelta) assert_series_equal(1.0 * nat_series_dtype_timedelta, nat_series_dtype_timedelta)
dt = Series(date_range('2012-1-1', periods=3, freq='D')) dt.iloc[2] = np.nan dt2 = dt[::-1]
result = dt2 - dt assert_series_equal(result, expected)
s = Series([(1, 1), (1, 2)])
ops = ['lt', 'le', 'gt', 'ge', 'eq', 'ne'] for op in ops: val = s[5]
s = Series(range(5)) s2 = Series(date_range('20010101', periods=5))
a = Series([True, False, True], list('bca')) b = Series([False, True, False, True], list('abcd'))
result = a & Series([]) expected = Series([False, False, False], list('bca')) assert_series_equal(result, expected)
result = a & Series([1], ['z']) expected = Series([False, False, False], list('bca')) assert_series_equal(result, expected)
index = list('bca') t = Series([True, False, True])
index = list('bca')
arr = Series(np.random.randn(10), index=np.arange(10), dtype=object)
self.assertRaises(TypeError, operator.add, datetime.now(), self.ts)
df = DataFrame({'A': self.ts})
op(a, b, axis=0)
s = Series(np.random.randn(5)) expected = s - s.index.to_series() result = s - s.index assert_series_equal(result, expected)
self.ts.to_csv(path, header=True) ts_h = Series.from_csv(path, header=0) self.assertTrue(ts_h.name == 'ts')
s = Series([1, 2, 3]) csv_str = s.to_csv(path=None) self.assertIsInstance(csv_str, str)
s = Series(self.ts.index) rs = s.tolist() self.assertEqual(self.ts.index[0], rs[0])
scalar = Series(0.5) self.assertNotIsInstance(scalar, float)
with tm.assert_produces_warning(FutureWarning): pd.TimeSeries(1, index=date_range('20130101', periods=3))
with tm.assert_produces_warning(FutureWarning): self.assertTrue(self.ts.is_time_series) self.assertTrue(self.ts.index.is_all_dates)
derived = Series(self.ts) with tm.assert_produces_warning(FutureWarning): self.assertTrue(derived.is_time_series) self.assertTrue(derived.index.is_all_dates)
self.assertEqual(id(self.ts.index), id(derived.index))
mixed = Series(['hello', np.NaN], index=[0, 1]) self.assertEqual(mixed.dtype, np.object_) self.assertIs(mixed[1], np.NaN)
m = MultiIndex.from_arrays([[1, 2], [3, 4]]) self.assertRaises(NotImplementedError, Series, m)
assert_series_equal(empty, empty2, check_index_type=False)
m = map(lambda x: x, range(10))
s = Series(index=np.array([None])) expected = Series(index=Index([None])) assert_series_equal(s, expected)
s = Series([1, 2, 3])
s = Series(tslib.iNaT, index=lrange(5)) self.assertFalse(isnull(s).all())
for t in ['s', 'D', 'us', 'ms']: self.assertRaises(TypeError, s.astype, 'M8[%s]' % t)
result = Series([datetime(2, 1, 1)]) self.assertEqual(result[0], datetime(2, 1, 1, 0, 0))
result = s.values self.assertIsInstance(result, np.ndarray) self.assertTrue(result.dtype == 'datetime64[ns]')
result = pd.concat([s.iloc[0:1], s.iloc[1:]]) assert_series_equal(result, s)
result = s.astype(object) expected = Series(DatetimeIndex(s._values).asobject) assert_series_equal(result, expected)
result = Series(s.values).astype('datetime64[ns, US/Eastern]') assert_series_equal(result, s)
self.assertTrue('datetime64[ns, US/Eastern]' in str(s))
result = s.shift() self.assertTrue('datetime64[ns, US/Eastern]' in str(result)) self.assertTrue('NaT' in str(result))
t = Series(date_range('20130101', periods=1000, tz='US/Eastern')) self.assertTrue('datetime64[ns, US/Eastern]' in str(t))
expected = Series( data=['A', 'B', 'C'], index=pd.to_timedelta([0, 10, 20], unit='s') )
assert_series_equal(result, expected)
import pandas import random
td = Series([timedelta(days=i) for i in range(3)]) self.assertEqual(td.dtype, 'timedelta64[ns]')
from pandas import tslib td = Series([timedelta(days=1), tslib.NaT], dtype='m8[ns]') self.assertEqual(td.dtype, 'timedelta64[ns]')
td = Series([np.timedelta64(300000000), pd.NaT]) self.assertEqual(td.dtype, 'timedelta64[ns]')
td.astype('int64')
self.assertRaises(TypeError, td.astype, 'int32')
def f(): Series([timedelta(days=1), 'foo'], dtype='m8[ns]')
td = Series([timedelta(days=i) for i in range(3)] + ['foo']) self.assertEqual(td.dtype, 'object')
str(self.empty)
self.series[5:7] = np.NaN str(self.series)
ots = self.ts.astype('O') ots[::2] = None repr(ots)
ser = Series(np.random.randn(100), name=0) rep_str = repr(ser) self.assertIn("Name: 0", rep_str)
ser = Series(np.random.randn(1001), name=0) rep_str = repr(ser) self.assertIn("Name: 0", rep_str)
s = Series([], dtype=np.int64, name='foo') self.assertEqual(repr(s), 'Series([], Name: foo, dtype: int64)')
repr(s)
repr(s)
with pd.option_context('max_rows', None):
ser.replace([np.nan], -1, inplace=True)
rs = ser.replace([np.nan, 'foo', 'bar'], -1)
rs = ser.replace({np.nan: -1, 'foo': -2, 'bar': -3})
rs2 = ser.replace([np.nan, 'foo', 'bar'], [-1, -2, -3]) tm.assert_series_equal(rs, rs2)
ser.replace([np.nan, 'foo', 'bar'], -1, inplace=True)
self.assertRaises(ValueError, ser.replace, [1, 2, 3], [np.nan, 0])
with tm.assertRaisesRegexp(TypeError, 'Cannot compare types .+'): ser.replace([1, 2], [np.nan, 0])
e = pd.Series([0, 1, 2, 3, 4]) tr, v = [3], [3.0] check_replace(tr, v, e)
e = pd.Series([0, 1, 2, 3.5, 4]) tr, v = [3], [3.5] check_replace(tr, v, e)
e = pd.Series([0, 1, 2, 3.5, 1]) tr, v = [3, 4], [3.5, True] check_replace(tr, v, e)
rs = ser.replace([np.nan, 'foo', 'bar'], -1)
rs = ser.replace({np.nan: -1, 'foo': -2, 'bar': -3})
rs2 = ser.replace([np.nan, 'foo', 'bar'], [-1, -2, -3]) tm.assert_series_equal(rs, rs2)
unshifted = self.ts.shift(0) assert_series_equal(unshifted, self.ts)
shifted4 = ps.shift(1, freq='B') assert_series_equal(shifted2, shifted4)
s = Series(date_range('2000-01-01 09:00:00', periods=5, tz='US/Eastern'), name='foo') result = s - s.shift()
ps = tm.makePeriodSeries() shifted = ps.tshift(1) unshifted = shifted.tshift(-1)
shifted = self.ts.tshift(1) unshifted = shifted.tshift(-1)
truncated = ts.truncate() assert_series_equal(truncated, ts)
expected = ts[1:3]
expected = ts[1:]
expected = ts[:3]
truncated = ts.truncate(after=self.ts.index[0] - offset) assert (len(truncated) == 0)
val1 = self.ts.asof(str(self.ts.index[7])) self.assertEqual(val1, self.ts[4])
self.assertEqual(self.ts.asof(self.ts.index[3]), self.ts[3])
d = self.ts.index[0] - datetools.bday self.assertTrue(np.isnan(self.ts.asof(d)))
rng = date_range('1/1/1990', periods=N, freq='H', tz='US/Eastern') ts = Series(np.random.randn(N), index=rng)
result = ts[datetime(1990, 1, 1, 4)] expected = ts[4] self.assertEqual(result, expected)
result = ts["1990-01-02"] expected = ts[24:48] assert_series_equal(result, expected)
rng = date_range('1/1/1990', periods=N, freq='H', tz='US/Eastern') ts = Series(np.random.randn(N), index=rng)
date = tz('US/Central').localize(datetime(1990, 1, 1, 3)) result[date] = 0 result[date] = ts[4] assert_series_equal(result, ts)
rng = date_range('1/1/1990', periods=N, freq='H', tz='America/New_York') ts = Series(np.random.randn(N), index=rng)
result = ts[ts.index[4]] expected = ts[4] self.assertEqual(result, expected)
val1 = ts.asof(str(ts.index[7])) self.assertEqual(val1, ts[4])
self.assertEqual(ts.asof(ts.index[3]), ts[3])
d = ts.index[0].to_timestamp() - datetools.bday self.assertTrue(np.isnan(ts.asof(d)))
self.ts.diff()
a = 10000000000000000 b = a + 1 s = Series([a, b])
rs = self.ts.diff(-1) xp = self.ts - self.ts.shift(-1) assert_series_equal(rs, xp)
rs = self.ts.diff(0) xp = self.ts - self.ts assert_series_equal(rs, xp)
s = Series(date_range('20130102', periods=5)) rs = s - s.shift(1) xp = s.diff() assert_series_equal(rs, xp)
nrs = rs - rs.shift(1) nxp = xp.diff() assert_series_equal(nrs, nxp)
corr1 = self.ts.autocorr()
corr2 = self.ts.autocorr(lag=1)
empty = Series() self.assertIsNone(empty.last_valid_index()) self.assertIsNone(empty.first_valid_index())
import math assert_series_equal(self.ts.apply(math.exp), np.exp(self.ts))
s = Series(index=[1, 2, 3]) rs = s.apply(lambda x: x) tm.assert_series_equal(s, rs)
def f(x): if not isinstance(x, pd.Timestamp): raise ValueError return str(x.tz)
merged = target.map(source.to_dict())
result = self.ts.map(lambda x: x * 2) self.assert_series_equal(result, self.ts * 2)
tm.assert_series_equal(df['labels'], df['expected_labels'], check_names=False)
def f(x): if not isinstance(x, pd.Timestamp): raise ValueError return str(x.tz)
s = Series(lrange(5)) del s[0]
s = Series()
d = self.ts.index[0] - datetools.bday self.assertRaises(KeyError, self.ts.__getitem__, d)
for s in [Series(), Series(index=list('abc'))]: result = s.get(None) self.assertIsNone(result)
with tm.assert_produces_warning(FutureWarning): s.iget(1)
with tm.assert_produces_warning(FutureWarning): s.irow(1)
with tm.assert_produces_warning(FutureWarning): s.iget_value(1)
result = s.iloc[slice(1, 3)] expected = s.ix[2:4] assert_series_equal(result, expected)
result[:] = 0 self.assertTrue((s[1:3] == 0).all())
result = s.iloc[[0, 2, 3, 4, 5]] expected = s.reindex(s.index[[0, 2, 3, 4, 5]]) assert_series_equal(result, expected)
result = s[list(mask)] expected = s[mask] assert_series_equal(result, expected) self.assert_index_equal(result.index, s.index[mask])
def f(): s[Series([], dtype=bool)]
result = s[omask] expected = s[mask] assert_series_equal(result, expected)
s2 = s.copy() cop = s.copy() cop[omask] = 5 s2[mask] = 5 assert_series_equal(cop, s2)
omask[5:10] = np.nan self.assertRaises(Exception, s.__getitem__, omask) self.assertRaises(Exception, s.__setitem__, omask, 5)
self.assertRaises(IndexError, self.ts.__getitem__, len(self.ts))
s = Series([]) self.assertRaises(IndexError, s.__getitem__, -1)
s = Series([1, 2, 3], ['a', 'b', 'c'])
s = pd.Series(4, index=list('ABCD')) result = s[lambda x: 'A'] self.assertEqual(result, s.loc['A'])
s2 = s.copy() s2[1] = 5 expected = s.append(Series([5], index=[1])) assert_series_equal(s2, expected)
s = Series(['a', 'b', 'c'], index=[0, 0.5, 1]) tmp = s.copy()
sl = self.series[10:20] sl[:] = 0 self.assertTrue((self.series[10:20] == 0).all())
series = Series(tm.makeIntIndex(20).astype(float), index=tm.makeIntIndex(20))
s = self.series.copy() s['foobar'] = 1
key = pd.Timestamp('2012-01-01') series = pd.Series() series[key] = 47 expected = pd.Series(47, [key]) assert_series_equal(series, expected)
expected = Series([np.nan, 2, 3])
with tm.assertRaisesRegexp(ValueError, 'tuple-index'): self.ts[:, 2] with tm.assertRaisesRegexp(ValueError, 'tuple-index'): self.ts[:, 2] = 2
result = self.ts[[slice(None, 5)]] expected = self.ts[:5] assert_series_equal(result, expected)
d1, d2 = self.ts.index[[5, 15]] result = self.ts.ix[d1:d2] expected = self.ts.truncate(d1, d2) assert_series_equal(result, expected)
mask = self.series > self.series.median() assert_series_equal(self.series.ix[mask], self.series[mask])
self.assertEqual(self.ts.ix[d1], self.ts[d1]) self.assertEqual(self.ts.ix[d2], self.ts[d2])
cp = s.copy() cp.ix[4:10] = 0 self.assertTrue((cp.ix[4:10] == 0).all())
cp = s.copy() cp.ix[3:11] = 0 self.assertTrue((cp.ix[3:11] == 0).values.all())
cond = Series([True, False, False, True, False], index=s.index) s2 = -(s.abs())
s = Series([1, 2]) s[[True, False]] = [0, 1] expected = Series([0, 2]) assert_series_equal(s, expected)
s = Series(range(10)).astype(float) s[8] = None result = s[8] self.assertTrue(isnull(result))
s = Series(list('abc'))
s = Series(list('abcdef'))
s = Series(list('abcdef'))
s = Series(list('abc'))
comb[comb < 1] = 5 expected = Series([5, 1, 2, 5, 1, 2], index=[0, 1, 2, 0, 1, 2]) assert_series_equal(comb, expected)
s = Series(np.random.randn(5)) cond = s > 0
d1, d2 = self.series.index[[5, 15]] result.ix[d1:d2] = 6
self.series.ix[d1] = 4 self.series.ix[d2] = 6 self.assertEqual(self.series[d1], 4) self.assertEqual(self.series[d2], 6)
s = pd.Series([1, 2, 3]) w = s.where(s > 1, 'X')
result = self.series.copy() result[mask] = self.series * 2 expected = self.series * 2 assert_series_equal(result[mask], expected[mask])
copy = self.series.copy() copy[ordered > 0] = 0
sel = self.series[ordered > 0] exp = self.series[self.series > 0] assert_series_equal(sel, exp)
s = Series(np.random.randn(6), index=[2, 2, 0, 0, 1, 1])
s = Series(len(index), index=index) s = s[::-1]
self.assertRaises(ValueError, s.drop, 'one', axis='columns')
_check_align(self.ts[:0], self.ts[:-5], how=kind) _check_align(self.ts[:0], self.ts[:-5], how=kind, fill=-1)
_check_align(self.ts[:-5], self.ts[:0], how=kind) _check_align(self.ts[:-5], self.ts[:0], how=kind, fill=-1)
_check_align(self.ts[:0], self.ts[:0], how=kind) _check_align(self.ts[:0], self.ts[:0], how=kind, fill=-1)
_check_align(self.ts[:0], self.ts[:-5], how=kind, method=meth) _check_align(self.ts[:0], self.ts[:-5], how=kind, method=meth, limit=1)
_check_align(self.ts[:-5], self.ts[:0], how=kind, method=meth) _check_align(self.ts[:-5], self.ts[:0], how=kind, method=meth, limit=1)
_check_align(self.ts[:0], self.ts[:0], how=kind, method=meth) _check_align(self.ts[:0], self.ts[:0], how=kind, method=meth, limit=1)
res1l, res1r = s1.align(s2, join='left') res2l, res2r = s2.align(s1, join='right')
try: self.assertTrue(np.may_share_memory(self.series.index, identity.index)) except (AttributeError): pass
result = self.ts.reindex() self.assertFalse((result is self.ts))
assert_series_equal(ts.reindex(i), ts.iloc[j], check_index_type=False)
reindexed = self.empty.reindex(self.ts.index, method='pad')
reindexed = self.ts.reindex(list(self.ts.index)) assert_series_equal(self.ts, reindexed)
ts = self.ts[::2] self.assertRaises(Exception, ts.reindex, self.ts.index, method='foo')
result = s.reindex(new_index).ffill() assert_series_equal(result, expected.astype('float64'))
reindexed_int = int_ts.reindex(self.ts.index)
self.assertEqual(reindexed_int.dtype, np.float_)
reindexed_int = int_ts.reindex(int_ts.index[::2]) self.assertEqual(reindexed_int.dtype, np.int_)
ts = self.ts[::2] bool_ts = Series(np.zeros(len(ts), dtype=bool), index=ts.index)
reindexed_bool = bool_ts.reindex(self.ts.index)
self.assertEqual(reindexed_bool.dtype, np.object_)
reindexed_bool = bool_ts.reindex(bool_ts.index[::2]) self.assertEqual(reindexed_bool.dtype, np.bool_)
day1 = datetime(2013, 3, 5) day2 = datetime(2013, 5, 5) day3 = datetime(2014, 3, 5)
ints = Series([1, 2, 3])
objects = Series([1, 2, 3], dtype=object)
bools = Series([True, False, True])
import pandas as pd import unittest import warnings
self.assertRaises(KeyError, self.cf.register_option, 'a', 1, 'doc')
self.assertRaises(KeyError, self.cf.describe_option, 'no.such.key')
self.assertRaises(KeyError, self.cf.get_option, 'no_such_option') self.cf.deprecate_option('KanBan')
self.assertRaises(KeyError, self.cf.get_option, 'no_such_option')
self.cf.deprecate_option('foo')
options.c = 1 self.assertEqual(len(holder), 1)
ctx = self.cf.option_context(option_name, context_value) self.assertEqual(self.cf.get_option(option_name), original_value)
with ctx: self.assertEqual(self.cf.get_option(option_name), context_value)
self.assertEqual(self.cf.get_option(option_name), original_value)
if 'numeric_only' in signature(f).args: self.assertRaisesRegexp(NotImplementedError, name, f, numeric_only=True)
item = self.panel['ItemA'] self.panel.items = new_items
for k, v in self.panel.iteritems(): pass
df = self.panel['ItemA']
xs = self.panel.major_xs(self.panel.major_axis[0]) result = func(xs, axis='major')
xs = self.panel.minor_xs(self.panel.minor_axis[0]) result = func(xs, axis='minor')
result = p.select(lambda x: x in ('foo', ), axis='items') self.assert_panel_equal(result, p.reindex(items=[]))
lp = self.panel.filter(['ItemA', 'ItemB']).to_frame() with tm.assertRaises(ValueError): self.panel['ItemE'] = lp
self.panel['ItemQ'] = 'foo' self.assertEqual(self.panel['ItemQ'].values.dtype, np.object_)
self.panel['ItemP'] = self.panel['ItemA'] > 0 self.assertEqual(self.panel['ItemP'].values.dtype, np.bool_)
idx = self.panel.major_axis[0] - bday self.assertRaises(Exception, self.panel.major_xs, idx)
self.assertRaises(Exception, self.panel.minor_xs, 'E')
self.panel['strings'] = 'foo' result = self.panel.xs('D', axis=2) self.assertIsNotNone(result.is_copy)
assert_panel_equal(p.ix[items, dates, cols], p.reindex(items=items, major=dates, minor=cols))
assert_panel_equal(p.ix[:, dates, cols], p.reindex(major=dates, minor=cols))
assert_panel_equal(p.ix[items, :, :], p.reindex(items=items))
result = p.ix[:, -1, :] expected = p.ix[:, p.major_axis[-1], :] assert_frame_equal(result, expected)
assert_frame_equal(p.ix[item], p[item]) assert_frame_equal(p.ix[item, :], p[item]) assert_frame_equal(p.ix[item, :, :], p[item])
assert_frame_equal(p.ix[:, date], p.major_xs(date)) assert_frame_equal(p.ix[:, date, :], p.major_xs(date))
assert_frame_equal(p.ix[:, :, 'C'], p.minor_xs('C'))
NS = slice(None, None)
for dtype in ['float64', 'int64']:
panel.loc['a2'] = df1.values tm.assert_frame_equal(panel.loc['a1'], df1) tm.assert_frame_equal(panel.loc['a2'], df1)
panel['a2'] = df2 tm.assert_frame_equal(panel.loc['a1'], df1) tm.assert_frame_equal(panel.loc['a2'], df2)
panel.loc['a2'] = df2 tm.assert_frame_equal(panel.loc['a1'], df1) tm.assert_frame_equal(panel.loc['a2'], df2)
result = func(p1, p2) self.assert_numpy_array_equal(result.values, func(p1.values, p2.values))
self.assertRaises(Exception, func, p1, tp)
self.assertRaises(Exception, func, p1, df)
result3 = func(self.panel, 0) self.assert_numpy_array_equal(result3.values, func(self.panel.values, 0))
wp = Panel(self.panel._data) self.assertIs(wp._data, self.panel._data)
wp = Panel(vals) self.assertIs(wp.values, vals)
wp = Panel(vals, copy=True) self.assertIsNot(wp.values, vals)
data = [[['foo', 'bar', 'baz']]] self.assertRaises(ValueError, Panel, data, dtype=float)
panel = Panel(items=lrange(3), major_axis=lrange(3), minor_axis=lrange(3), dtype='O') self.assertEqual(panel.values.dtype, np.object_)
wp = Panel.from_dict(d, intersect=True) self.assert_index_equal(wp.major_axis, itemb.index[5:])
d4 = {'A': None, 'B': None}
data['ItemB'] = data['ItemB'][:-1] self.assertRaises(Exception, Panel, data)
applied = self.panel.apply(np.sqrt) self.assertTrue(assert_almost_equal(applied.values, np.sqrt( self.panel.values)))
f = lambda x: ((x.T - x.mean(1)) / x.std(1)).T
result = result.astype('int64') expected = p.sum(0) assert_frame_equal(result, expected)
self.panel = Panel(np.random.rand(5, 5, 5))
result = self.panel.reindex(items=['ItemA', 'ItemB']) assert_frame_equal(result['ItemB'], ref)
new_major = list(self.panel.major_axis[:10]) result = self.panel.reindex(major=new_major) assert_frame_equal(result['ItemB'], ref.reindex(index=new_major))
self.assertRaises(Exception, self.panel.reindex, major_axis=new_major, major=new_major)
new_minor = list(self.panel.minor_axis[:2]) result = self.panel.reindex(minor=new_minor) assert_frame_equal(result['ItemB'], ref.reindex(columns=new_minor))
result = self.panel.reindex() assert_panel_equal(result, self.panel) self.assertFalse(result is self.panel)
smaller_major = self.panel.major_axis[::5] smaller = self.panel.reindex(major=smaller_major)
result = self.panel.reindex(major=self.panel.major_axis, copy=False) assert_panel_equal(result, self.panel) self.assertTrue(result is self.panel)
result = self.panel.reindex(items=self.panel.items, major=self.panel.major_axis, minor=self.panel.minor_axis, copy=False)
result = self.panel.truncate(before=None, after=None, axis='items')
result.fillna(value=0.0)
result = self.panel.swapaxes(0, 1) self.assertIs(result.items, self.panel.major_axis)
with tm.assertRaisesRegexp(TypeError, 'not enough/duplicate arguments'): self.panel.transpose('minor', maj='major', minor='items')
filtered = self.panel.to_frame() expected = self.panel.to_frame().dropna(how='any') assert_frame_equal(filtered, expected)
unfiltered = self.panel.to_frame(filter_observations=False) assert_panel_equal(unfiltered.to_panel(), self.panel)
self.assertEqual(unfiltered.index.names, ('major', 'minor'))
assert_frame_equal(wp['bool'], panel['bool'], check_names=False)
df = panel.to_frame() df['category'] = df['str'].astype('category')
idx = self.panel.items[0] idx_lag = self.panel.items[1] shifted = self.panel.shift(1, axis='items') assert_frame_equal(self.panel[idx], shifted[idx_lag])
ps = tm.makePeriodPanel() shifted = ps.tshift(1) unshifted = shifted.tshift(-1)
panel = _panel shifted = panel.tshift(1) unshifted = shifted.tshift(-1)
expected = Panel({"One": df}) check_drop('Two', 0, ['items'], expected)
wp = self.panel.to_panel() wp2 = wp.reindex(major=wp.major_axis[:-1]) lp2 = wp2.to_frame()
self.panel['foo'] = lp2['ItemA'] assert_series_equal(self.panel['foo'].reindex(lp2.index), lp2['ItemA'], check_names=False)
result = self.panel.add(self.panel.filter(['ItemA']))
wp = self.panel.to_panel() new_index = wp.major_axis[::5]
self.assertRaises(Exception, lp2.truncate, wp.major_axis[-2], wp.major_axis[2])
wide_means = self.panel.to_panel().mean('major') assert_frame_equal(means, wide_means)
wide_sums = self.panel.to_panel().sum('major') assert_frame_equal(sums, wide_sums)
df = pivot(np.array([]), np.array([]), np.array([]))
panel = Panel(np.random.rand(3, 3, 3)) with assert_produces_warning(): panel.major_xs(1, copy=False)
kwargs.pop(self._typ._info_axis_name, None)
result = obj.rename(**{axis: arg}) expected = obj.copy() setattr(expected, axis, list('abcd')) self._compare(result, expected)
args = [ str.lower, {x: x.lower() for x in idx}, Series({x: x.lower() for x in idx}), ]
result = obj.rename_axis(arg, axis=axis) expected = obj.copy() setattr(expected, axis, list('abcd')) self._compare(result, expected)
o = self._construct(n, **kwargs) result = o._get_numeric_data() self._compare(result, o)
result = o._get_bool_data() expected = self._construct(n, value='empty', **kwargs) self._compare(result, expected)
d0 = "a", "b", "c", "d" d1 = np.arange(4, dtype='int64') others = "e", 10
obj = self._construct(shape=0) self.assertRaises(ValueError, lambda: bool(obj))
f('int64') f('float64') f('M8[ns]')
for op in ['__eq__', '__le__', '__ge__']: v1 = getattr(o, op)(o) self.check_metadata(o, v1)
try: result = o.combine_first(o2) self.check_metadata(o, result) except (AttributeError): pass
try: result = o + o2 self.check_metadata(result) except (ValueError, AttributeError): pass
for op in ['__eq__', '__le__', '__ge__']:
v1 = getattr(o, op)(o)
try: o.head() except (NotImplementedError): raise nose.SkipTest('not implemented on {0}'.format( o.__class__.__name__))
self._compare(o.head(0), o.iloc[0:0]) self._compare(o.tail(0), o.iloc[0:0])
self._compare(o.head(len(o) + 1), o) self._compare(o.tail(len(o) + 1), o)
self._compare(o.head(-3), o.head(7)) self._compare(o.tail(-3), o.tail(7))
with tm.assertRaises(ValueError): o.sample(random_state='astring!')
with tm.assertRaises(ValueError): o.sample(n=3, frac=0.3)
with tm.assertRaises(ValueError): o.sample(n=-3) with tm.assertRaises(ValueError): o.sample(frac=-0.3)
with tm.assertRaises(ValueError): o.sample(n=3.2)
with tm.assertRaises(ValueError): o.sample(n=3, weights=[0, 1])
with tm.assertRaises(ValueError): bad_weights = [-0.1] * 10 o.sample(n=3, weights=bad_weights)
with tm.assertRaises(ValueError): weights_with_inf = [0.1] * 10 weights_with_inf[0] = np.inf o.sample(n=3, weights=weights_with_inf)
zero_weights = [0] * 10 with tm.assertRaises(ValueError): o.sample(n=3, weights=zero_weights)
nan_weights = [np.nan] * 10 with tm.assertRaises(ValueError): o.sample(n=3, weights=nan_weights)
weights_with_nan = [np.nan] * 10 weights_with_nan[5] = 0.5 self._compare( o.sample(n=1, axis=0, weights=weights_with_nan), o.iloc[5:6])
weights_with_None = [None] * 10 weights_with_None[5] = 0.5 self._compare( o.sample(n=1, axis=0, weights=weights_with_None), o.iloc[5:6])
def test_stat_unexpected_keyword(self): obj = self._construct(5) starwars = 'Star Wars' errmsg = 'unexpected keyword'
o = Series([1, 2, 3]) result = o._get_numeric_data() self._compare(result, o)
s = Series([True]) self.assertTrue(s.bool())
Series._metadata = _metadata Series.__finalize__ = _finalize
assert_series_equal(result.to_series(), s, check_index_type=check_index_type, check_categorical=check_categorical)
testit(tm.makeCategoricalIndex, check_index_type=False, check_categorical=False)
df = DataFrame([[True]]) self.assertTrue(df.bool())
df = tm.makeDataFrame()
desc1 = df.describe(include="all") desc2 = df.describe(include=[np.generic, "category"]) assert_frame_equal(desc1, desc2)
_metadata = DataFrame._metadata _finalize = DataFrame.__finalize__
DataFrame._metadata = _metadata DataFrame.__finalize__ = _finalize
df2 = DataFrame(np.ones(5), MultiIndex.from_arrays([l0, l1]))
with tm.assertRaisesRegexp(ValueError, 'not valid'): df = DataFrame(index=l0) df = getattr(df, fn)('US/Pacific', level=1)
df = DataFrame({'x': [1, 2, 3]})
assert_panel_equal(result.to_pandas(), p)
self.assertRaises(ValueError, lambda: result.to_pandas())
easy_weight_list = [0] * 10 easy_weight_list[5] = 1
s = Series(range(10)) with tm.assertRaises(ValueError): s.sample(n=3, weights='weight_column')
with tm.assertRaises(KeyError): df.sample(n=3, weights='not_a_real_column_name')
weights_less_than_1 = [0] * 10 weights_less_than_1[0] = 0.5 tm.assert_frame_equal( df.sample(n=1, weights=weights_less_than_1), df.iloc[:1])
assert_frame_equal(df.sample(n=1, axis='columns', weights=second_column_weight), df[['col2']])
with tm.assertRaises(ValueError): df.sample(n=1, axis=2)
with tm.assertRaises(ValueError): df.sample(n=1, axis=1, weights=[0.5] * 10)
easy_weight_list = [0] * 3 easy_weight_list[2] = 1
s2 = Series([0.001, 0, 10000], index=[3, 5, 10]) assert_frame_equal(df.loc[[3]], df.sample(1, weights=s2))
s3 = Series([0.01, 0], index=[3, 5]) assert_frame_equal(df.loc[[3]], df.sample(1, weights=s3))
s4 = Series([1, 0], index=[1, 2]) with tm.assertRaises(ValueError): df.sample(1, weights=s4)
df = tm.makeTimeDataFrame().reindex(columns=['A']) tm.assert_series_equal(df.squeeze(), df['A'])
empty_series = pd.Series([], name='five') empty_frame = pd.DataFrame([empty_series]) empty_panel = pd.Panel({'six': empty_frame})
tm.assert_series_equal(s.transpose(), s)
different = df1.copy() different['floats'] = different['floats'].astype('float32') self.assertFalse(df1.equals(different))
different_index = -index different = df2.set_index(different_index) self.assertFalse(df1.equals(different))
different = df2.copy() different.columns = df2.columns[::-1] self.assertFalse(df1.equals(different))
df3 = df1.set_index(['text'], append=True) df2 = df1.set_index(['text'], append=True) self.assertTrue(df3.equals(df2))
df3 = df1.set_index(['floats'], append=True) df2 = df1.set_index(['floats'], append=True) self.assertTrue(df3.equals(df2))
from datetime import datetime from pandas.compat import range, lrange import operator import nose
result = p.select(lambda x: x in ('foo',), axis='items') self.assert_panel4d_equal(result, p.reindex(items=[]))
self.panel4d['lQ'] = 'foo' self.assertEqual(self.panel4d['lQ'].values.dtype, np.object_)
self.panel4d['lP'] = self.panel4d['l1'] > 0 self.assertEqual(self.panel4d['lP'].values.dtype, np.bool_)
panel4dc = self.panel4d.copy() p = panel4dc.iloc[0]
self.panel4d['foo'] = 'bar'
self.assertRaises(Exception, func, p1, tp)
self.assertRaises(Exception, func, p1, p)
idx = self.panel4d.major_axis[0] - bday self.assertRaises(Exception, self.panel4d.major_xs, idx)
self.assertRaises(Exception, self.panel4d.minor_xs, 'E')
self.panel4d['strings'] = 'foo' result = self.panel4d.xs('D', axis=3) self.assertIsNotNone(result.is_copy)
assert_panel4d_equal(panel4d.ix[labels, items, dates, cols], panel4d.reindex(labels=labels, items=items, major=dates, minor=cols))
assert_panel4d_equal(panel4d.ix[:, items, dates, cols], panel4d.reindex(items=items, major=dates, minor=cols))
assert_panel4d_equal(panel4d.ix[:, :, dates, cols], panel4d.reindex(major=dates, minor=cols))
assert_panel4d_equal(panel4d.ix[:, items, :, :], panel4d.reindex(items=items))
panel4d = Panel4D(self.panel4d._data) self.assertIs(panel4d._data, self.panel4d._data)
panel4d = Panel4D(vals) self.assertIs(panel4d.values, vals)
panel4d = Panel4D(vals, copy=True) self.assertIsNot(panel4d.values, vals)
data = [[['foo', 'bar', 'baz']]] self.assertRaises(ValueError, Panel, data, dtype=float)
panel = Panel(items=lrange(3), major_axis=lrange(3), minor_axis=lrange(3), dtype='O') self.assertEqual(panel.values.dtype, np.object_)
data['l2'] = data['l2']['ItemB'] self.assertRaises(Exception, Panel4D, data)
result = self.panel4d.reindex(labels=['l1', 'l2']) assert_panel_equal(result['l2'], ref)
result = self.panel4d.reindex(items=['ItemA', 'ItemB']) assert_frame_equal(result['l2']['ItemB'], ref['ItemB'])
self.assertRaises(Exception, self.panel4d.reindex, major_axis=new_major, major=new_major)
result = self.panel4d.reindex() assert_panel4d_equal(result, self.panel4d) self.assertFalse(result is self.panel4d)
smaller_major = self.panel4d.major_axis[::5] smaller = self.panel4d.reindex(major=smaller_major)
result = self.panel4d.reindex( major=self.panel4d.major_axis, copy=False) assert_panel4d_equal(result, self.panel4d) self.assertTrue(result is self.panel4d)
result = self.panel4d.swapaxes(0, 1) self.assertIs(result.labels, self.panel4d.items)
r = self.frame.rolling(window=5)[1, 3] tm.assert_index_equal(r._selected_obj.columns, self.frame.columns[[1, 3]])
g[['A', 'C']]
df = DataFrame({'A': range(5), 'B': range(0, 10, 2)}) r = df.rolling(window=3)
s = Series(np.arange(100), name='foo')
s = pd.Series( np.random.randn(20), index=pd.date_range('1/1/2000', periods=20, freq='12H'))
tm._skip_if_no_scipy()
c(win_type='boxcar', window=2, min_periods=1) c(win_type='boxcar', window=2, min_periods=1, center=True) c(win_type='boxcar', window=2, min_periods=1, center=False)
w = rwindow.Window(Series([2, 4, 6]), window=[0, 2])
c(window=2) c(window=2, min_periods=1) c(window=2, min_periods=1, center=True) c(window=2, min_periods=1, center=False)
r = rwindow.Rolling(Series([2, 4, 6]), window=2)
c(min_periods=1) c(min_periods=1, center=True) c(min_periods=1, center=False)
e = rwindow.Expanding(Series([2, 4, 6]), window=2)
with self.assertRaises(ValueError): c(com=-0.5)
with self.assertRaises(ValueError): c(span=0.5)
with self.assertRaises(ValueError): c(halflife=0)
for alpha in (-0.5, 1.5): with self.assertRaises(ValueError): c(alpha=alpha)
e = rwindow.EWM(Series([2, 4, 6]), alpha=0.5)
assert_raises(NotImplementedError, f, roll)
Series(np.ones(10)).rolling(window=3, center=True, axis=0).mean()
with self.assertRaises(ValueError): Series(np.ones(10)).rolling(window=3, center=True, axis=1).mean()
with self.assertRaises(ValueError): (DataFrame(np.ones((10, 10))) .rolling(window=3, center=True, axis=2).mean())
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
rs = DataFrame(vals).rolling(5, win_type='boxcar', center=True).mean() tm.assert_frame_equal(DataFrame(xp), rs)
with self.assertRaises(AttributeError): (DataFrame(vals).rolling(5, win_type='boxcar', center=True) .std())
vals = Series(np.random.randn(10)) vals[4] = np.nan vals[8] = np.nan
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
tm._skip_if_no_scipy()
with warnings.catch_warnings(): warnings.filterwarnings("ignore", message=".*(empty slice|0 for slice).*", category=RuntimeWarning)
arr = np.arange(4)
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): result = mom.rolling_apply(arr, 10, np.sum) self.assertTrue(isnull(result).all())
if name is not None: self._check_structures(f, static_comp, name=name, has_min_periods=has_min_periods, has_time_rule=has_time_rule, fill_value=fill_value, has_center=has_center, **kwargs)
arr = randn(50) arr[:10] = np.NaN arr[-10:] = np.NaN
result = get_result(arr, 20, min_periods=15) self.assertTrue(np.isnan(result[23])) self.assertFalse(np.isnan(result[24]))
result0 = get_result(arr, 20, min_periods=0) result1 = get_result(arr, 20, min_periods=1) tm.assert_almost_equal(result0, result1)
if name is not None:
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): return f(obj, window=window, min_periods=min_periods, freq=freq, center=center, **kwargs)
if has_time_rule: win = 25 minp = 10
if has_center:
s = ['x%d' % x for x in range(12)]
result = s.ewm(com=com, adjust=adjust).mean() tm.assert_series_equal(result, expected)
arr = randn(50) arr[:10] = np.NaN arr[-10:] = np.NaN s = Series(arr)
self.assertTrue(np.isnan(result.values[:11]).all()) self.assertFalse(np.isnan(result.values[11:]).any())
result = func(Series([]), 50, min_periods=min_periods) tm.assert_series_equal(result, Series([]))
result2 = func(np.arange(50), span=10) self.assertEqual(result2.dtype, np.float_)
data = create_series() + create_dataframes()
expected = mock_mean(x) assert_equal(mean_x, expected.astype('float64'))
corr_x_x = corr(x, x)
expected = x * np.nan expected[count_x >= max(min_periods, 1)] = exp assert_equal(mean_x, expected)
expected[:] = np.nan assert_equal(corr_x_x, expected)
var_unbiased_x = var_unbiased(x) var_biased_x = var_biased(x) var_debiasing_factors_x = var_debiasing_factors(x) assert_equal(var_unbiased_x, var_biased_x * var_debiasing_factors_x)
assert_equal(var_x, cov_x_x)
assert_equal(var_x, std_x * std_x)
mean_x2 = mean(x * x) assert_equal(var_x, mean_x2 - (mean_x * mean_x))
continue
corr_x_y = corr(x, y) corr_y_x = corr(y, x) assert_equal(corr_x_y, corr_y_x)
cov_x_y = cov(x, y) cov_y_x = cov(y, x) assert_equal(cov_x_y, cov_y_x)
var_x_plus_y = var(x + y) var_y = var(y) assert_equal(cov_x_y, 0.5 * (var_x_plus_y - var_x - var_y))
std_y = std(y) assert_equal(corr_x_y, cov_x_y / (std_x * std_y))
mean_y = mean(y) mean_x_times_y = mean(x * y) assert_equal(cov_x_y, mean_x_times_y - (mean_x * mean_y))
with warnings.catch_warnings(): warnings.filterwarnings("ignore", message=".*(empty slice|0 for slice).*", category=RuntimeWarning)
for (x, is_constant, no_nans) in self.data: functions = self.base_functions
if no_nans: functions = self.base_functions + self.no_nan_functions for (f, require_min_periods, name) in functions: expanding_f = getattr( x.expanding(min_periods=min_periods), name)
with warnings.catch_warnings(): warnings.filterwarnings("ignore", message=".*(empty slice|0 for slice).*", category=RuntimeWarning)
for (x, is_constant, no_nans) in self.data: functions = self.base_functions
def test_rolling_cov(self): A = self.series B = A + randn(len(A))
a = tm.makeTimeSeries() b = tm.makeTimeSeries() a[:5] = np.nan b[:10] = np.nan
self.assertRaises(TypeError, rwindow._flex_binary_moment, 5, 6, None)
result = func(Series([]), Series([]), 50, min_periods=min_periods) tm.assert_series_equal(result, Series([]))
result = func( Series([1.]), Series([1.]), 50, min_periods=min_periods) tm.assert_series_equal(result, Series([np.NaN]))
continue
continue
with warnings.catch_warnings(): warnings.filterwarnings("ignore", message=".*incomparable objects.*", category=RuntimeWarning)
d = Series([1] * 5) x = d.rolling(window=5).skew() tm.assert_series_equal(all_nan, x)
d = Series(np.random.randn(5)) x = d.rolling(window=2).skew() tm.assert_series_equal(all_nan, x)
d = Series([1] * 5) x = d.rolling(window=5).kurt() tm.assert_series_equal(all_nan, x)
d = Series(np.random.randn(5)) x = d.rolling(window=3).kurt() tm.assert_series_equal(all_nan, x)
result = func(arr, min_periods=15) self.assertTrue(np.isnan(result[13])) self.assertFalse(np.isnan(result[14]))
result0 = func(arr, min_periods=0) result1 = func(arr, min_periods=1) tm.assert_almost_equal(result0, result1)
from pandas.compat import range import pandas.util.testing as tm from pandas import read_csv import os
self.polycollection_factor = 2
mapped = dict(zip(unique, colors)) return [mapped[v] for v in series.values]
result = conv.to_rgba(result)
result = patch.get_facecolor()[0]
labels = ax.get_xticklabels()
self.assertTrue(len(ax.get_children()) > 0)
points = ax.get_position().get_points() x_set.add(points[0][0]) y_set.add(points[0][1])
if return_type is None: return_type = 'dict'
if return_type is None: for r in self._flatten_visible(returned): self.assertIsInstance(r, Axes) return
ax = _check_plot_works(self.ts.plot, subplots=True) self._check_axes_shape(ax, axes_num=1, layout=(1, 1))
expected = np.array([1.0e-03, 1.0e-02, 1.0e-01, 1.0e+00])
axes = df.plot() self._check_ticks_props(axes, xrot=0)
ax = _check_plot_works(series.plot.pie, labels=None) self._check_text_labels(ax.texts, [''] * 5)
color_args = ['r', 'g', 'b'] ax = _check_plot_works(series.plot.pie, colors=color_args)
df = DataFrame(np.random.randn(30, 4), columns=list('abcd'))
self._check_text_labels(xlabels, [''] * len(xlabels)) ylabels = ax.get_yticklabels() self._check_text_labels(ylabels, [''] * len(ylabels))
with tm.assertRaises(ValueError): s.plot(yerr=np.arange(11))
with tm.assertRaises((ValueError, TypeError)): s.plot(yerr=s_err)
self._check_grid_settings(Series([1, 2, 3]), plotting._series_kinds + plotting._common_kinds)
for c in colors.cnames: result = plotting._get_standard_colors(num_colors=1, color=c) self.assertEqual(result, [c])
for c in colors.ColorConverter.colors: result = plotting._get_standard_colors(num_colors=1, color=c) self.assertEqual(result, [c])
ax = Series(np.arange(12) + 1).plot(color='green') self._check_colors(ax.get_lines(), linecolors=['green'])
for ax in axes[:2]:
df = DataFrame(np.random.rand(10, 3), index=list(string.ascii_letters[:10]))
warnings.simplefilter('error') try: df = DataFrame(np.random.randn(100, 4)) df.plot(subplots=True, layout=(3, 2))
fig, axes = self.plt.subplots(2, 3) df = DataFrame(np.random.rand(10, 3), index=list(string.ascii_letters[:10]))
df.plot(subplots=True, ax=axes)
fig, axes = self.plt.subplots(1, 1) df = DataFrame(np.random.rand(10, 1), index=list(string.ascii_letters[:10]))
for ax in axes[0:-1].ravel(): self._check_visible(ax.get_xticklabels(), visible=False)
for ax in axes[-1].ravel(): self._check_visible(ax.get_xticklabels(), visible=True)
for ax in axes[[0, 1, 2], [0]].ravel(): self._check_visible(ax.get_yticklabels(), visible=True)
self._check_legend_labels(ax, labels=['a']) self.assertEqual(len(ax.lines), 1)
ax = df.plot.bar(linewidth=2) for r in ax.patches: self.assertEqual(r.get_linewidth(), 2)
ax = df.plot.bar(stacked=True, linewidth=2) for r in ax.patches: self.assertEqual(r.get_linewidth(), 2)
ax = df.plot.bar(width=width) for r in ax.patches: self.assertEqual(r.get_width(), width / len(df.columns))
ax = df.plot.bar(stacked=True, width=width) for r in ax.patches: self.assertEqual(r.get_width(), width)
ax = df.plot.barh(width=width) for r in ax.patches: self.assertEqual(r.get_height(), width / len(df.columns))
ax = df.plot.barh(stacked=True, width=width) for r in ax.patches: self.assertEqual(r.get_height(), width)
axes = df.plot.bar(width=width, subplots=True) for ax in axes: for r in ax.patches: self.assertEqual(r.get_width(), width)
axes = df.plot.barh(width=width, subplots=True) for ax in axes: for r in ax.patches: self.assertEqual(r.get_height(), width)
df = DataFrame(randn(5, 5))
self.assertEqual(ax.patches[0].get_x(), -0.5) self.assertEqual(ax.patches[-1].get_x(), 3.5)
self.assertEqual(ax.patches[0].get_x(), -0.25) self.assertEqual(ax.patches[-1].get_x(), 5.15)
axes = df.plot(x='x', y='y', kind='scatter', subplots=True) self._check_axes_shape(axes, axes_num=1, layout=(1, 1))
self.assertEqual(ax.collections[0].cmap.name, 'Greys')
self.assertEqual(ax.collections[0].colorbar._label, 'z')
ax = df.plot.scatter(x='x', y='y', c='z', colorbar=False) self.assertIs(ax.collections[0].colorbar, None)
self.assertAlmostEqual(ax_min, min_edge - 0.25) self.assertAlmostEqual(ax_max, max_edge + 0.25)
self.assertTrue((axis.get_ticklocs() == np.arange(len(df))).all())
self.assertAlmostEqual(axis.get_ticklocs()[0], center)
self.assertAlmostEqual(axis.get_ticklocs()[0], edge)
expected = np.array([1., 10.])
if not PY3: axes = _check_plot_works(df.plot.box, subplots=True, logy=True)
rects = [x for x in ax.get_children() if isinstance(x, Rectangle)] self.assertAlmostEqual(rects[-1].get_height(), 1.0) tm.close()
axes = df.plot.hist(rot=50, fontsize=8, orientation='horizontal') self._check_ticks_props(axes, xrot=0, yrot=50, ylabelsize=8)
ax = df.ix[:, [0]].plot(color='DodgerBlue') self._check_colors(ax.lines, linecolors=['DodgerBlue'])
custom_colors = ['#F00', '#00F', '#FF0', '#000', '#FFF'] _check_plot_works(df.plot, color=custom_colors)
from matplotlib import cm default_colors = self._maybe_unpack_cycler(self.plt.rcParams)
axes = df.plot(subplots=True, color='k') for ax in axes: self._check_colors(ax.get_lines(), linecolors=['k']) tm.close()
axes = df.plot(subplots=True, color='green') for ax in axes: self._check_colors(ax.get_lines(), linecolors=['green']) tm.close()
custom_colors = ['#F00', '#00F', '#FF0', '#000', '#FFF'] _check_plot_works(df.plot, color=custom_colors, subplots=True, filterwarnings='ignore')
axes = df.ix[:, [0]].plot(color='DodgerBlue', subplots=True) self._check_colors(axes[0].lines, linecolors=['DodgerBlue'])
axes = df.plot(style='r', subplots=True) for ax in axes: self._check_colors(ax.get_lines(), linecolors=['r']) tm.close()
self._check_colors(handles[:len(jet_colors)], linecolors=jet_colors) for h in handles: self.assertEqual(h.get_alpha(), 0.5)
axes = df.ix[:, [0]].plot(kind='kde', color='DodgerBlue', subplots=True) self._check_colors(axes[0].lines, linecolors=['DodgerBlue'])
bp = df.plot.box(colormap=cm.jet, return_type='dict') _check_colors(bp, jet_colors[0], jet_colors[0], jet_colors[2]) tm.close()
bp = df.plot.box(color='DodgerBlue', return_type='dict') _check_colors(bp, 'DodgerBlue', 'DodgerBlue', 'DodgerBlue', 'DodgerBlue')
df.plot.box(color=dict(boxes='red', xxxx='blue'))
ax = df.plot.hexbin(x='A', y='B') self.assertEqual(ax.collections[0].cmap.name, 'BuGn')
self.assertEqual([x.get_text() for x in ax.get_legend().get_texts()], base_expected[:i] + base_expected[i + 1:])
import itertools ax = _check_plot_works(df.plot, yerr=itertools.repeat(0.1, len(df))) self._check_has_errorbars(ax, xerr=0, yerr=2)
ix = date_range('1/1/2000', '1/1/2001', freq='M') tdf = DataFrame(d, index=ix) tdf_err = DataFrame(d_err, index=ix)
import matplotlib.pyplot as plt plt.close('all') gs, axes = _generate_4_axes_via_gridspec()
for ax in axes: df.plot(x="a", y="b", title="title", ax=ax)
import matplotlib.pyplot as plt
for ax in axes: df.plot(x="a", y="b", title="title", ax=ax)
results[kind] = weakref.proxy(df.plot(kind=kind, **args))
tm.close() gc.collect() for key in results: with tm.assertRaises(ReferenceError): results[key].lines
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec
self._check_visible(ax.get_yticklabels(), visible=True) self._check_visible(ax.get_xticklabels(), visible=True) self._check_visible(ax.get_xticklabels(minor=True), visible=True)
height.groupby(gender).plot(alpha=0.5) tm.close()
self.assertEqual(len(res['a'].collections), 1)
self._assert_not_almost_equal_both(iter([1, 2, 3]), [1, 2, 3])
assert_numpy_array_equal(np.array([1]), 1)
assert_almost_equal(np.array([1.1, 2.000001]), np.array([1.1, 2.0]))
assert_index_equal(idx1, idx2, check_exact=False)
assert_index_equal(idx1, idx2, check_exact=False, check_less_precise=True)
s1 = Series([0.1235], dtype='float32') s2 = Series([0.1236], dtype='float32')
locales = tm.get_locales() self.assertTrue(len(locales) >= 1)
with assertRaises(ValueError): f() raise ValueError
from pandas import compat import nose
import matplotlib if str(matplotlib.__version__) >= LooseVersion('1.4'): raise nose.SkipTest("Matplotlib Regression in 1.4 and current dev.")
self._check_box_return_type(result, 'dict')
_check_plot_works(df.hist, sharex=True, sharey=True)
_check_plot_works(df.hist, figsize=(8, 10))
_check_plot_works(df.hist, bins=5)
self._check_ax_scales(ax, yaxis='log')
with tm.assertRaises(AttributeError): ser.hist(foo='bar')
with tm.assertRaises(ValueError): df.hist(layout=(1, 1))
with tm.assertRaises(ValueError): df.hist(layout=(1,)) with tm.assertRaises(ValueError): df.hist(layout=(-1, -1))
with tm.assert_produces_warning(UserWarning): axes = _check_plot_works(scatter_matrix, filterwarnings='always', frame=df, range_padding=.1) axes0_labels = axes[0][0].yaxis.get_majorticklabels()
res = df.groupby('gender').hist() tm.close()
axes = df.hist(by='D', rot=30) self._check_axes_shape(axes, axes_num=1, layout=(1, 1)) self._check_ticks_props(axes, xrot=30)
self._check_ax_scales(axes, yaxis='log')
with tm.assertRaises(AttributeError): plotting.grouped_hist(df.A, by=df.C, foo='bar')
df = self.hist_df
axes = df.groupby('classroom').boxplot(ax=axes)
axes = _check_plot_works(df.hist, by='classroom') self._check_axes_shape(axes, axes_num=3, layout=(2, 2))
df = self.hist_df
axes = df.hist(column='height', ax=axes)
ax1, ax2 = df.hist(column='height', by=df.gender, sharex=True)
self.assertTrue(ax1._shared_x_axes.joined(ax1, ax2)) self.assertTrue(ax2._shared_x_axes.joined(ax1, ax2))
self.assertFalse(ax1._shared_y_axes.joined(ax1, ax2)) self.assertFalse(ax2._shared_y_axes.joined(ax1, ax2))
self.assertTrue(ax1._shared_y_axes.joined(ax1, ax2)) self.assertTrue(ax2._shared_y_axes.joined(ax1, ax2))
self.assertFalse(ax1._shared_x_axes.joined(ax1, ax2)) self.assertFalse(ax2._shared_x_axes.joined(ax1, ax2))
self.assertTrue(ax1._shared_x_axes.joined(ax1, ax2)) self.assertTrue(ax2._shared_x_axes.joined(ax1, ax2))
c = self.factor.copy() c[0] = 'b' self.assertEqual(c[0], 'b') c[-1] = 'a' self.assertEqual(c[-1], 'a')
result = c.codes[np.array([100000]).astype(np.int64)] self.assertEqual(result, np.array([5], dtype='int8'))
def f(): Categorical([1, 2], [1, 2, 2])
c1 = Categorical(["a", "b", "c", "a"]) self.assertFalse(c1.ordered)
c1 = Categorical(["a", "b", "c", "a"]) c2 = Categorical(c1) tm.assert_categorical_equal(c1, c2)
cat = pd.Categorical([1, 2, 3, np.nan], categories=[1, 2, 3]) self.assertTrue(com.is_integer_dtype(cat.categories))
cat = pd.Categorical([np.nan, 1, 2, 3]) self.assertTrue(com.is_integer_dtype(cat.categories))
cat = pd.Categorical([np.nan, 1, 2., 3]) self.assertTrue(com.is_float_dtype(cat.categories))
with tm.assert_produces_warning(RuntimeWarning): c_old = Categorical([0, 1, 2, 0, 1, 2],
with tm.assert_produces_warning(None):
with tm.assert_produces_warning(None):
xrange = range
from pandas.core.index import MultiIndex MultiIndex.from_product([range(5), ['a', 'b', 'c']])
def f(): Categorical.from_codes([1, 2], [1, 2])
def f(): Categorical.from_codes(["a"], [1, 2])
def f(): Categorical.from_codes([0, 1, 2], ["a", "a", "b"])
def f(): Categorical.from_codes([-2, 1, 2], ["a", "b", "c"])
res_rev = cat_rev > cat_rev_base exp_rev = np.array([True, False, False]) self.assert_numpy_array_equal(res_rev, exp_rev)
def f(): cat > cat_rev
cat_unorderd = cat.set_ordered(False) self.assertFalse((cat > cat).any())
actual = repr(factor) self.assertEqual(actual, expected)
with option_context('display.unicode.east_asian_width', True):
def f(): s.categories = [1, 2, 3, 4]
def f(): s.categories = [1, 2]
c = c.set_categories([4, 3, 2, 1])
self.assert_numpy_array_equal(c._codes, np.array([3, 2, 1, 0, 3], dtype=np.int8))
self.assert_index_equal(c.categories, Index([4, 3, 2, 1]))
def f(): cat.rename_categories([1, 2, 3, 4])
def f(): cat.rename_categories([1, 2])
res = cat.reorder_categories(["c", "b", "a"]) self.assert_categorical_equal(cat, old) self.assert_categorical_equal(res, new)
res = cat.reorder_categories(["c", "b", "a"], inplace=True) self.assertIsNone(res) self.assert_categorical_equal(cat, new)
cat = Categorical(["a", "b", "c", "a"], ordered=True)
def f(): cat.reorder_categories(["a", "b", "d"])
def f(): cat.reorder_categories(["a", "b", "c", "d"])
res = cat.add_categories("d") self.assert_categorical_equal(cat, old) self.assert_categorical_equal(res, new)
res = cat.add_categories("d", inplace=True) self.assert_categorical_equal(cat, new) self.assertIsNone(res)
def f(): cat.add_categories(["d"])
res = cat.remove_categories("c") self.assert_categorical_equal(cat, old) self.assert_categorical_equal(res, new)
res = cat.remove_categories("c", inplace=True) self.assert_categorical_equal(cat, new) self.assertIsNone(res)
def f(): cat.remove_categories(["c"])
c = Categorical(["a", "b", "c", "a"]) with tm.assert_produces_warning(FutureWarning):
for i, j in [(0, 1), (0, 2), (1, 2)]: nulls = [null_values[i], null_values[j]]
def f(): c.codes = np.array([0, 1, 2, 0, 1], dtype='int8')
codes = c.codes
cat = Categorical(["a", "b", "b", "a"], ordered=False) cat.sort_values()
cat = Categorical([5, 2, np.nan, 2, np.nan], ordered=True) exp_categories = Index([2, 5])
cat = pd.Categorical(['a', 'b', 'c', 'd', 'a'])
self.assert_categorical_equal(cat, cat.shift(0))
diff = cat.memory_usage(deep=True) - sys.getsizeof(cat) self.assertTrue(abs(diff) < 100)
with tm.assert_produces_warning(UserWarning): Categorical([0, 1], name="a")
cat = pd.Categorical([1, 2, 3], ordered=True)
result = Categorical(['foo', 'bar', 'baz']) self.assertTrue(result.codes.dtype == 'int8')
result = result.remove_categories(['foo%05d' % i for i in range(300)]) self.assertTrue(result.codes.dtype == 'int8')
self.assertRaises( ValueError, lambda: DataFrame([pd.Categorical(list('abc')), pd.Categorical(list('abdefg'))]))
self.assertRaises(NotImplementedError, lambda: pd.Categorical(np.array([list('abcd')])))
result = list(df.grade.values) expected = np.array(df.grade.values).tolist() tm.assert_almost_equal(result, expected)
for t in df.itertuples(index=False): str(t)
def f(): s.set_categories([4, 3, 2, 1])
s.name = 'E' self.assert_series_equal(result2.sort_index(), s.sort_index())
result = self.cat.describe() self.assertEqual(len(result.columns), 1)
df = DataFrame({"a": [5, 15, 25]}) c = pd.cut(df.a, bins=[0, 10, 20, 30, 40])
df = DataFrame({"a": [5, 15, 25, -5]}) c = pd.cut(df.a, bins=[-10, 0, 10, 20, 30, 40])
for func in ('order', 'sort'): with tm.assert_produces_warning(FutureWarning): getattr(c, func)()
df.sort_values(by=["unsort"], ascending=False)
result = df.sort_values(by=['grade']) expected = df.iloc[[1, 2, 5, 0, 3, 4]] tm.assert_frame_equal(result, expected)
result = df.sort_values(by=['grade', 'id']) expected = df.iloc[[2, 1, 5, 4, 3, 0]] tm.assert_frame_equal(result, expected)
exp_df = pd.DataFrame({"cats": cats2, "values": values2}, index=idx2)
exp_col = pd.Series(cats, index=idx, name='cats')
exp_row = pd.Series(["b", 3], index=["cats", "values"], dtype="object", name="j")
exp_val = "b"
res_df = df.iloc[2:4, :] tm.assert_frame_equal(res_df, exp_df) self.assertTrue(com.is_categorical_dtype(res_df["cats"]))
res_row = df.iloc[2, :] tm.assert_series_equal(res_row, exp_row) tm.assertIsInstance(res_row["cats"], compat.string_types)
res_col = df.iloc[:, 0] tm.assert_series_equal(res_col, exp_col) self.assertTrue(com.is_categorical_dtype(res_col))
res_val = df.iloc[2, 0] self.assertEqual(res_val, exp_val)
res_df = df.loc["j":"k", :] tm.assert_frame_equal(res_df, exp_df) self.assertTrue(com.is_categorical_dtype(res_df["cats"]))
res_row = df.loc["j", :] tm.assert_series_equal(res_row, exp_row) tm.assertIsInstance(res_row["cats"], compat.string_types)
res_col = df.loc[:, "cats"] tm.assert_series_equal(res_col, exp_col) self.assertTrue(com.is_categorical_dtype(res_col))
res_val = df.loc["j", "cats"] self.assertEqual(res_val, exp_val)
res_df = df.ix["j":"k", :] tm.assert_frame_equal(res_df, exp_df) self.assertTrue(com.is_categorical_dtype(res_df["cats"]))
res_row = df.ix["j", :] tm.assert_series_equal(res_row, exp_row) tm.assertIsInstance(res_row["cats"], compat.string_types)
res_col = df.ix[:, "cats"] tm.assert_series_equal(res_col, exp_col) self.assertTrue(com.is_categorical_dtype(res_col))
res_val = df.ix["j", 0] self.assertEqual(res_val, exp_val)
res_val = df.iat[2, 0] self.assertEqual(res_val, exp_val)
res_val = df.at["j", "cats"] self.assertEqual(res_val, exp_val)
exp_fancy = df.iloc[[2]]
res_val = df.get_value("j", "cats") self.assertEqual(res_val, exp_val)
res_row = df.iloc[2] tm.assert_series_equal(res_row, exp_row) tm.assertIsInstance(res_row["cats"], compat.string_types)
df = orig.copy() df.iloc[2, 0] = "b" tm.assert_frame_equal(df, exp_single_cats_value)
def f(): df = orig.copy() df.iloc[2, 0] = "c"
df = orig.copy() df.iloc[2, :] = ["b", 2] tm.assert_frame_equal(df, exp_single_row)
def f(): df = orig.copy() df.iloc[2, :] = ["c", 2]
df = orig.copy() df.iloc[2:4, :] = [["b", 2], ["b", 2]] tm.assert_frame_equal(df, exp_multi_row)
df = orig.copy() df.iloc[2:4, 0] = ["b", "b"] tm.assert_frame_equal(df, exp_parts_cats_col)
df = orig.copy() df.loc["j", "cats"] = "b" tm.assert_frame_equal(df, exp_single_cats_value)
def f(): df = orig.copy() df.loc["j", "cats"] = "c"
df = orig.copy() df.loc["j", :] = ["b", 2] tm.assert_frame_equal(df, exp_single_row)
def f(): df = orig.copy() df.loc["j", :] = ["c", 2]
df = orig.copy() df.ix["j", 0] = "b" tm.assert_frame_equal(df, exp_single_cats_value)
def f(): df = orig.copy() df.ix["j", 0] = "c"
df = orig.copy() df.ix["j", :] = ["b", 2] tm.assert_frame_equal(df, exp_single_row)
def f(): df = orig.copy() df.ix["j", :] = ["c", 2]
df = orig.copy() df.ix["j":"k", 0] = ["b", "b"] tm.assert_frame_equal(df, exp_parts_cats_col)
df = orig.copy() df.iat[2, 0] = "b" tm.assert_frame_equal(df, exp_single_cats_value)
def f(): df = orig.copy() df.iat[2, 0] = "c"
df = orig.copy() df.at["j", "cats"] = "b" tm.assert_frame_equal(df, exp_single_cats_value)
def f(): df = orig.copy() df.at["j", "cats"] = "c"
tm.assert_frame_equal(df, exp_fancy)
df = orig.copy() df.set_value("j", "cats", "b") tm.assert_frame_equal(df, exp_single_cats_value)
res_rev = cat_rev > cat_rev_base exp_rev = Series([True, False, False]) tm.assert_series_equal(res_rev, exp_rev)
def f(): cat > cat_rev
cat = Series(Categorical(list("abc")))
cat = Series(Categorical(list("abc"), ordered=True))
self.assertTrue((a == a).all()) self.assertFalse((a != a).all())
s = Series(list('abc'), dtype='category') s2 = Series(list('abd'), dtype='category')
expected = df.copy()
res = df.fillna(value={"cats": 3, "vals": "b"}) tm.assert_frame_equal(res, df_exp_fill)
def cmp(a, b): tm.assert_almost_equal( np.sort(np.unique(a)), np.sort(np.unique(b)))
tm.assert_almost_equal(np.array(s), np.array(s.values))
tm.assert_series_equal(result, s, check_categorical=False)
s = pd.Series(pd.Categorical([1, 2, 3, 4])) self.assertRaises(TypeError, lambda: np.sum(s))
for op in ['__add__', '__sub__', '__mul__', '__truediv__']: self.assertRaises(TypeError, lambda: getattr(s, op)(2))
self.assertRaises(TypeError, lambda: np.log(s))
from pandas.tseries.common import Properties from pandas.tseries.index import date_range, DatetimeIndex from pandas.tseries.period import period_range, PeriodIndex from pandas.tseries.tdi import timedelta_range, TimedeltaIndex
] _special_func_names = [f[0] for f in special_func_defs]
_ignore_names = ['tz_localize']
df1 = pd.DataFrame( np.arange(18, dtype='int64').reshape(6, 3), columns=["a", "b", "c"])
exit=False)
import re from datetime import datetime
fill_error = re.compile("Incompatible type for fill_value")
data.take(indexer, out=out)
data.take(indexer, out=out, axis=i)
data.take(indexer, out=out, axis=i)
result = algos.take_nd(arr, indexer, axis=0) expected = arr.take(indexer, axis=0) expected[-1] = np.nan tm.assert_almost_equal(result, expected)
result = algos.take_nd(arr, indexer, axis=1) expected = arr.take(indexer, axis=1) expected[:, -1] = np.nan tm.assert_almost_equal(result, expected)
out = np.empty((len(indexer), arr.shape[1]), dtype='float32')
import datetime import itertools import nose
self.assertEqual(str(df2).splitlines()[0].split(), ['red'])
result = df.copy().sort_index(axis=1) expected = df.iloc[:, [0, 2, 1, 3]] assert_frame_equal(result, expected)
result = df.copy() result[('red', extra)] = 'world' result = result.sort_index(axis=1) assert_frame_equal(result, expected)
result = s.ix[[(2000, 3, 10), (2000, 3, 13)]] expected = s.reindex(s.index[49:51]) assert_series_equal(result, expected)
self.assertRaises(KeyError, s.__getitem__, (2000, 3, 4))
self.assertRaises(IndexError, s.__getitem__, len(self.ymd))
result = s[(x > 0 for x in s)] expected = s[s > 0] assert_series_equal(result, expected)
df[df[:-1] < 0] = 2 np.putmask(values[:-1], values[:-1] < 0, 2) assert_almost_equal(df.values, values)
result = self.frame.ix[:4] expected = self.frame[:4] assert_frame_equal(result, expected)
cp = self.frame.copy() cp.ix[:4] = 0
cp = df.copy() cp['a'] = cp['b'].values assert_frame_equal(cp['a'], cp['b'])
df['B', '1'] = [1, 2, 3] df['A'] = df['B', '1']
result = self.frame.xs('two', level='second')
def f(x): x[:] = 10
result = df.xs(('a', 4), level=['one', 'four'])
def f(x): x[:] = 10
self.assertRaises(KeyError, frame.ix.__getitem__, 3)
result = self.frame.ix[2] expected = self.frame.xs(self.frame.index[2]) assert_series_equal(result, expected)
result = df.ix[:, :np.int32(3)] expected = df.reindex(columns=df.columns[:3]) assert_frame_equal(result, expected)
a_sorted = self.frame['A'].sortlevel(0)
self.assertEqual(a_sorted.index.names, self.frame.index.names)
rs = self.frame.copy() rs.sortlevel(0, inplace=True) assert_frame_equal(rs, self.frame.sortlevel(0))
index = MultiIndex.from_arrays([np.arange(4000)] * 3) df = DataFrame(np.random.randn(4000), index=index, dtype=np.int64)
result = df.sortlevel(0) self.assertTrue(result.index.lexsort_depth == 3)
index = MultiIndex.from_arrays([np.arange(4000)] * 3) df = DataFrame(np.random.randn(4000), index=index, dtype=np.int32)
df = tm.makeTimeDataFrame() assertRaisesRegexp(TypeError, 'hierarchical', df.count, level=0)
unstacked = self.ymd.unstack() unstacked.unstack()
self.ymd.astype(int).unstack()
self.ymd.astype(np.int32).unstack()
unstacked = self.ymd.unstack() restacked = unstacked.stack() assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack() unstacked = unstacked.sort_index(axis=1, ascending=False) restacked = unstacked.stack() assert_frame_equal(restacked, self.ymd)
unstacked = self.ymd.unstack(1).unstack(1)
result = self.ymd.unstack(0).stack(-2) expected = self.ymd.unstack(0).stack(0)
result = df.unstack(2)
s = self.ymd['A'] s_unstacked = s.unstack(['year', 'month']) assert_frame_equal(s_unstacked, expected['A'])
with assertRaisesRegexp(ValueError, "level should contain"): unstacked.stack([0, 'month'])
unstacked = self.ymd.unstack(['year', 'month'])
df.unstack(['b', 'c'])
NUM_ROWS = 1000
idf.unstack('E')
levels = [[0, 1], [0, 1, 2, 3]] labels = [[0, 0, 1, 1], [0, 2, 0, 2]]
df.groupby(level='three')
df['foo'].values[:] = 0 self.assertTrue((df['foo'].values == 0).all())
df['foo', 'four'] = 'foo' df = df.sortlevel(0, axis=1)
def f(): df['foo']['one'] = 2 return df
leftside = grouped.agg(aggf) rightside = getattr(self.series, op)(level=level, skipna=skipna) assert_series_equal(leftside, rightside)
level_index = frame._get_axis(axis).levels[level]
df['A'].ix[14] = 5 self.assertEqual(df['A'][14], 5)
self.ymd['E'] = 'foo' self.ymd['F'] = 2
stacked = result.stack() assert_series_equal(s, stacked.reindex(s.index))
index = MultiIndex(levels=[[0, 1]] + [level] * 8, labels=[np.arange(2).repeat(500)] + [labels] * 8)
result = idf.drop(drop_idx.index, level=0).reset_index() expected = df[-df.var1.isin(drop_idx.index)]
l[0] = ["faz", "boo"] df.index = l repr(df)
idx = MultiIndex.from_arrays(([1, 2, 3, 1, 2, 3], [1, 1, 1, 1, 2, 2]))
df.set_index(index)
date1 = datetime.date.today() date2 = datetime.datetime.today() date3 = Timestamp.today()
import nose
res = df.loc[lambda x: x.A > 2] tm.assert_frame_equal(res, df.loc[df.A > 2])
res = df.loc[lambda x: 1, lambda x: 'A'] self.assertEqual(res, df.loc[1, 'A'])
df = pd.DataFrame({'X': [1, 2, 3, 4], 'Y': list('aabb')}, index=list('ABCD'))
res = df.loc[lambda x: ['A', 'C']] tm.assert_frame_equal(res, df.loc[['A', 'C']])
df = pd.DataFrame({'X': [1, 2, 3, 4], 'Y': list('aabb')}, index=list('ABCD'))
df = pd.DataFrame({'X': [1, 2, 3, 4], 'Y': list('aabb')}, index=list('ABCD'))
res = df.iloc[lambda x: [1, 3]] tm.assert_frame_equal(res, df.iloc[[1, 3]])
res = df.iloc[[1, 3], lambda x: 0] tm.assert_series_equal(res, df.iloc[[1, 3], 0])
df = pd.DataFrame({'X': [1, 2, 3, 4], 'Y': list('aabb')}, index=list('ABCD'))
res = df.copy() res.iloc[lambda x: [1, 3]] = 0 exp = df.copy() exp.iloc[[1, 3]] = 0 tm.assert_frame_equal(res, exp)
import sys import nose import itertools import warnings from datetime import datetime
if values: return f.values[i]
return f.ix[i]
if method == 'indexer': method = 'ix' key = obj._get_axis(axis)[key]
try: xp = getattr(obj, method).__getitem__(_axify(obj, key, axis)) except: xp = getattr(obj, method).__getitem__(key)
axes = [slice(None)] * obj.ndim axes[axis] = key return tuple(axes)
for o in self._objs:
if values: expected = f.values[i] else: expected = f for a in reversed(i): expected = expected.__getitem__(a)
if fails is True: if result == 'fail': result = 'ok (fail)'
if fails is not None: if isinstance(detail, fails): result = 'ok (%s)' % type(detail).__name__ _print(result) return
for o in objs: if o not in self._objs: continue
expected = Series(np.ones(n), index=index) s = Series(np.zeros(n), index=index) s[s == 0] = 1 assert_series_equal(s, expected)
s = Series(['2014-01-01', '2014-02-02'], dtype='datetime64[ns]') expected = Timestamp('2014-02-02')
df = DataFrame(np.random.random_sample((20, 5)), columns=list('ABCDE')) expected = df
with tm.assertRaisesRegexp( IndexError, 'single positional indexer is out-of-bounds'): df.iloc[30] self.assertRaises(IndexError, lambda: df.iloc[-30])
with tm.assertRaisesRegexp( IndexError, 'single positional indexer is out-of-bounds'): s.iloc[30] self.assertRaises(IndexError, lambda: s.iloc[-30])
result = s.iloc[18:30] expected = s.iloc[18:] assert_series_equal(result, expected)
def check(result, expected): str(result) result.dtypes assert_frame_equal(result, expected)
df = DataFrame({'A': [2, 3, 5], 'B': [7, 11, 13]}) s = df['A']
expected = pd.Series(['a'], index=['A']) result = expected.iloc[[-1]] assert_series_equal(result, expected)
result = df.iloc[0, 0] self.assertTrue(isnull(result))
df = concat([df1, df2], axis=1) assert_frame_equal(df.iloc[:, :4], df1) assert_frame_equal(df.iloc[:, 4:], df2)
rs = df.iloc[[0, 1]] xp = df.xs(4, drop_level=False) assert_frame_equal(rs, xp)
try: expected = df.ix[key] except KeyError: self.assertRaises(KeyError, lambda: df.loc[key]) continue
df1 = df.copy() df2 = df.copy()
s = Series([1, 2, 3, 4], index=list('abde'))
s = Series(range(5), [-2, -1, 1, 2, 3])
result = df[df.a > df.a[3]] expected = df.iloc[4:] assert_frame_equal(result, expected)
def f(): df.loc[df.new_col == 'new', 'time'] = v
index = pd.date_range('2015-01-01', periods=2, tz='utc')
assert_series_equal(ser[sel], ser)
result = ser.copy() result[sel] = 1 expected = pd.Series(1, index=index) assert_series_equal(result, expected)
assert_series_equal(ser.loc[sel], ser)
result = ser.copy() result.loc[sel] = 1 expected = pd.Series(1, index=index) assert_series_equal(result, expected)
self.assertEqual(ser[index[1]], 1)
result = ser.copy() result[index[1]] = 5 expected = pd.Series([0, 5], index=index) assert_series_equal(result, expected)
self.assertEqual(ser.loc[index[1]], 1)
result = ser.copy() result.loc[index[1]] = 5 expected = pd.Series([0, 5], index=index) assert_series_equal(result, expected)
df.iloc[[0, 1], [0, 1]] = df.iloc[[0, 1], [0, 1]] assert_frame_equal(df, expected)
def check(result, expected): tm.assert_numpy_array_equal(result, expected) tm.assertIsInstance(result, np.ndarray)
from itertools import product
result = df.loc[10, :] expected = df.ix[10, :] assert_frame_equal(result, expected)
expected = df[10] assert_frame_equal(result, expected)
self.assertRaises(KeyError, df.loc.__getitem__, tuple([[1, 2], [1, 2]]))
df = DataFrame([['a'], ['b']], index=[1, 2], columns=['value'])
keys1 = ['@' + str(i) for i in range(5)] val1 = np.arange(5, dtype='int64')
df = DataFrame({1: [1, 2], 2: [3, 4], 'a': ['a', 'b']})
result = df.iloc[4:8] expected = df.ix[8:14] assert_frame_equal(result, expected)
result = df.iloc[[0, 1, 3]] expected = df.ix[[0, 2, 6]] assert_frame_equal(result, expected)
result = df.iloc[[-1, 1, 3], [-1, 1]] expected = df.ix[[18, 2, 6], [6, 2]] assert_frame_equal(result, expected)
result = df.iloc[[-1, -1, 1, 3], [-1, 1]] expected = df.ix[[18, 18, 2, 6], [6, 2]] assert_frame_equal(result, expected)
s = Series(index=lrange(1, 5)) result = df.iloc[s.index] expected = df.ix[[2, 4, 6, 8]] assert_frame_equal(result, expected)
df = DataFrame( np.random.randn(10, 4), index=list('abcdefghij'), columns=list('ABCD'))
result = df.iloc[-1, -1] exp = df.ix['j', 'D'] self.assertEqual(result, exp)
self.assertRaises(IndexError, df.iloc.__getitem__, tuple([10, 5]))
self.assertRaises(ValueError, df.iloc.__getitem__, tuple(['j', 'D']))
result = p.iloc[1:3] expected = p.loc[['B', 'C']] assert_panel_equal(result, expected)
result = p.iloc[[0, 2]] expected = p.loc[['A', 'C']] assert_panel_equal(result, expected)
self.assertRaises(IndexError, p.iloc.__getitem__, tuple([10, 5]))
self.assertRaises(ValueError, p.iloc.__getitem__, tuple(['j', 'D']))
df.describe()
df.columns = list('aaaa') result = df.iloc[3:5, 0:2] str(result) result.dtypes
def f(): df.ix[2:5, 'bar'] = np.array([2.33j, 1.23 + 0.1j, 2.2])
df.ix[2:5, 'bar'] = np.array([2.33j, 1.23 + 0.1j, 2.2, 1.0])
rs = mi_int.iloc[:, 2] xp = mi_int.ix[:, 2] assert_series_equal(rs, xp)
rs = mi_int.iloc[2, 2] xp = mi_int.ix[:, 2].ix[2] self.assertEqual(rs, xp)
rs = mi_labels.iloc[2, 2] xp = mi_labels.ix['j'].ix[:, 'j'].ix[0, 0] self.assertEqual(rs, xp)
rs = mi_labels.loc['i'] xp = mi_labels.ix['i'] assert_frame_equal(rs, xp)
rs = mi_labels.loc[:, 'j'] xp = mi_labels.ix[:, 'j'] assert_frame_equal(rs, xp)
rs = mi_labels.loc[('i', 'X')] xp = mi_labels.ix[('i', 'X')] assert_frame_equal(rs, xp)
attributes = ['Attribute' + str(i) for i in range(1)] attribute_values = ['Value' + str(i) for i in range(5)]
df = DataFrame( np.arange(12).reshape(-1, 1), index=pd.MultiIndex.from_product([[1, 2, 3, 4], [1, 2, 3]]))
from numpy.random import randint, choice, randn cols = ['jim', 'joe', 'jolie', 'joline', 'jolia']
for i, k in enumerate(key): mask &= df.iloc[:, i] == k
df = pd.DataFrame(vals, columns=cols) a, b = pd.concat([df, df]), df.drop_duplicates(subset=cols[:-1])
result = s.xs(0, level=0) expected = Series([1], index=[0]) assert_series_equal(result, expected)
with self.assert_produces_warning(PerformanceWarning): tm.assert_frame_equal(df.ix[key], df.iloc[2:])
result = df.loc[(slice(None), [1]), :] expected = df.iloc[[0, 3]] assert_frame_equal(result, expected)
result = df.loc[:, (slice(None), ['foo'])] expected = df.iloc[:, [1, 3]] assert_frame_equal(result, expected)
self.assertRaises(KeyError, lambda: df.loc[slice(None), [1]])
result = df.xs(1, level=2, drop_level=False) assert_frame_equal(result, expected)
result = df1.loc[(slice('A1')), :] expected = df1.iloc[0:10] assert_frame_equal(result, expected)
result = df1.loc[(slice('A2')), :] expected = df1 assert_frame_equal(result, expected)
result = df1.loc[(slice(None), slice('B2')), :] expected = df1 assert_frame_equal(result, expected)
idx = pd.IndexSlice
def f(): df.loc['A1', (slice(None), 'foo')]
df.loc(axis=0)[:, :, ['C1', 'C3']] = -10
result = df.loc(axis=1)[:, 'foo'] expected = df.loc[:, (slice(None), 'foo')] assert_frame_equal(result, expected)
def f(): df.loc(axis=-1)[:, :, ['C1', 'C3']]
df = DataFrame({'date': [pd.Timestamp('20130101').tz_localize('UTC'), pd.NaT]}) expected = df.dtypes
import datetime df = DataFrame({'date': [datetime.datetime(2012, 1, 1), datetime.datetime(1012, 1, 2)]}) expected = df.dtypes
df = DataFrame({'text': ['some words'] + [None] * 9}) expected = df.dtypes
idx = pd.IndexSlice
df = df_orig.copy()
def f(): df.loc['bar'] *= 2
self.assertRaises(KeyError, lambda: s.loc[['D']])
result = s.loc[[]] expected = s.iloc[[]] assert_series_equal(result, expected)
df = DataFrame(randn(4, 3), index=list('ABCD')) expected = df.ix[['E']]
df = DataFrame( np.random.randn(5, 5), columns=['A', 'B', 'B', 'B', 'A'])
df['test'] = df['a'].apply(lambda x: '_' if x == 'aaa' else x)
p = Panel(randn(4, 4, 4))
panel = tm.makePanel()
class TestObject:
np.random.seed(0) index = range(3) columns = list('abc')
arr = np.array( [[[1, 2, 3], [0, 0, 0]], [[0, 0, 0], [0, 0, 0]]], dtype=np.float64)
def f(): df.ix[4, 'c'] = [0, 1, 2, 3]
NUM_ROWS = 100 NUM_COLS = 10 col_names = ['A' + num for num in map(str, np.arange(NUM_COLS).tolist())] index_cols = col_names[:5]
for name, df2 in grp: new_vals = np.arange(df2.shape[0]) df.ix[name, 'new_col'] = new_vals
df2.ix[mask, cols] = dft.ix[mask, cols] assert_frame_equal(df2, expected)
df = DataFrame(tm.getSeriesData()) df['foo'] = 'bar'
df = DataFrame(index=[0, 1], columns=[0]) df.ix[1, 0] = [1, 2, 3] df.ix[1, 0] = [1, 2]
class TO(object):
df = DataFrame(index=[0, 1], columns=[0]) df.ix[1, 0] = TO(1) df.ix[1, 0] = np.nan result = DataFrame(index=[0, 1], columns=[0])
result = df.iloc[np.array([True] * len(mask), dtype=bool)] assert_frame_equal(result, df)
df_orig = DataFrame( [['1', '2', '3', '.4', 5, 6., 'foo']], columns=list('ABCDEFG'))
s_orig = Series([1, 2, 3])
s = s_orig.copy()
df = df_orig.copy()
df = DataFrame([[True, 1], [False, 2]], columns=["female", "fitness"])
df = DataFrame(columns=['A', 'B'])
dt1 = Timestamp('20130101 09:00:00') dt2 = Timestamp('20130101 10:00:00')
ser = Series([0.1, 0.2], index=[1, 2])
expected = Series([np.nan, 0.2, np.nan], index=[3, 2, 3]) result = ser.loc[[3, 2, 3]] assert_series_equal(result, expected, check_index_type=True)
self.assertRaises(KeyError, lambda: ser.loc[[3, 3, 3]])
self.assertRaises(KeyError, lambda: ser.loc[[3, 3, 3]])
def f(): df.loc[100.0, :] = df.ix[0]
df.loc['a', :] = df.ix[0]
df = DataFrame()
expected = DataFrame(columns=['foo'], index=pd.Index( [], dtype='int64'))
expected = DataFrame( {0: Series(1, index=range(4))}, columns=['A', 'B', 0])
df = DataFrame(Series()) assert_frame_equal(df, DataFrame({0: Series()}))
df.loc[0]['z'].iloc[0] = 1. result = df.loc[(0, 0), 'z'] self.assertEqual(result, 1)
df.loc[(0, 0), 'z'] = 2 result = df.loc[(0, 0), 'z'] self.assertEqual(result, 2)
with option_context('chained_assignment', None):
df = DataFrame({"aa": lrange(5), "bb": [2.2] * 5})
df["cc"] = 0.0
df["bb"]
repr(df)
df['bb'].iloc[0] = 0.17 df._clear_item_cache() self.assertAlmostEqual(df['bb'][0], 0.17)
cont = ['one', 'two', 'three', 'four', 'five', 'six', 'seven']
if do_ref: df.ix[0, "c"]
df.ix[7, 'c'] = 1
from string import ascii_letters as letters
x = df.iloc[[0, 1, 2]] self.assertIsNotNone(x.is_copy) x = df.iloc[[0, 1, 2, 4]] self.assertIsNotNone(x.is_copy)
self.assertIsNone(df.is_copy) df['letters'] = df['letters'].apply(str.lower) self.assertIsNone(df.is_copy)
df = DataFrame({'a': [1]}).dropna() self.assertIsNone(df.is_copy) df['a'] += 1
df = DataFrame(np.arange(0, 9), columns=['count']) df['group'] = 'b'
df2['y'] = ['g', 'h', 'i']
s = Series(ser) result = s.value_counts() str(result)
for s in [Series(range(5)), Series(range(5), index=range(1, 6))]:
rhs = -2 * df.iloc[3:0:-1, 2:0:-1]
right = df.copy() right.iloc[1:4, 1:3] *= -2
run_tests(df, rhs, right)
assert_series_equal(s[l_slc], s.iloc[i_slc]) assert_series_equal(s.ix[l_slc], s.iloc[i_slc])
result = MultiIndex.from_arrays([range(10 ** 6), range(10 ** 6)]) assert (not (10 ** 6, 0) in result)
(["foo", "bar", "baz"], [None, "bar", "baz"]),
(["foo", "bar", "baz"], [None, "bar", "baz"]),
self.assertRaises(KeyError, lambda: df.loc['d'])
result = self.df.loc[['c', 'a']] expected = self.df.iloc[[4, 0, 1, 5]] assert_frame_equal(result, expected, check_index_type=True)
self.assertRaises(KeyError, lambda: self.df2.loc['e'])
self.assertRaises(KeyError, lambda: self.df2.loc[['a', 'd']])
res = df.loc[['a', 'a', 'b']]
rw_array = np.eye(10) rw_df = DataFrame(rw_array)
cats = list('cabe')
self.assertRaises(ValueError, lambda: self.df2.reindex(['a', 'a']))
result = df3[df3.index < 2] expected = df3.iloc[[4]] assert_frame_equal(result, expected)
self.assertRaises(TypeError, lambda: df4[df4.index < 2]) self.assertRaises(TypeError, lambda: df4[df4.index > 1])
s = pd.Series([1, 2, 3, 4]) self.assertEqual(s.index.dtype, np.int64)
tm.assert_series_equal(temp, pd.Series([True, True, True, False])) self.assertEqual(temp.dtype, np.bool)
pass
pass
if tm.is_platform_32bit(): raise nose.SkipTest("32-bit platform buggy: {0} -> {1}".format (from_key, to_key))
exp = pd.Series(self.rep[to_key], index=index, name='yyy', dtype=from_key)
def test_replace_conversion_series_from_object(self): from_key = 'object' for to_key in self.rep: self._assert_replace_conversion(from_key, to_key, how='series')
raise nose.SkipTest("doesn't work as in PY3")
for idxr, getitem in [(lambda x: x.ix, False), (lambda x: x.iloc, False), (lambda x: x, True)]:
if getitem and isinstance(s, DataFrame): error = KeyError else: error = TypeError self.assertRaises(error, f)
def f(): s.loc[3.0]
self.assertFalse(3.0 in s)
def f(): s.iloc[3.0] = 0 self.assertRaises(TypeError, f)
if s.index.inferred_type in ['categorical']: pass elif s.index.inferred_type in ['datetime64', 'timedelta64', 'period']:
s = Series(np.arange(len(i)), index=i) s[3] self.assertRaises(TypeError, lambda: s[3.0])
for idxr in [lambda x: x.ix, lambda x: x, lambda x: x.iloc]:
for idxr in [lambda x: x.ix, lambda x: x]:
for index in [tm.makeIntIndex, tm.makeRangeIndex]:
for idxr, getitem in [(lambda x: x.ix, False), (lambda x: x.loc, False), (lambda x: x, True)]:
for idxr, getitem in [(lambda x: x.ix, False), (lambda x: x.loc, False), (lambda x: x, True)]:
self.assertTrue(3.0 in s)
result = idxr(s)[indexer] self.check(result, s, 3, getitem)
s2 = s.copy()
self.assertRaises(KeyError, lambda: idxr(s)[3.5])
self.assertTrue(3.0 in s)
expected = s.iloc[3] s2 = s.copy()
self.assertRaises(TypeError, lambda: s.iloc[3.0])
for l in [slice(3.0, 4), slice(3, 4.0), slice(3.0, 4.0)]:
for l in [slice(3.0, 4), slice(3, 4.0), slice(3.0, 4.0)]:
s = Series(range(5), index=index)
for l in [slice(3.0, 4), slice(3, 4.0), slice(3.0, 4.0)]:
if oob: indexer = slice(0, 0) else: indexer = slice(3, 5) self.check(result, s, indexer, False)
def f(): s[l]
for l in [slice(-6, 6), slice(-6.0, 6.0)]:
if oob: indexer = slice(0, 0) else: indexer = slice(-6, 6) self.check(result, s, indexer, False)
def f(): s[slice(-6.0, 6.0)]
def f(): s[l]
for l in [slice(3.0, 4), slice(3, 4.0), slice(3.0, 4.0)]:
def f(): s[l] = 0
for index in [tm.makeIntIndex, tm.makeRangeIndex]:
for l in [slice(0.0, 1), slice(0, 1.0), slice(0.0, 1.0)]:
def f(): s[l]
for l in [slice(-10, 10), slice(-10.0, 10.0)]:
def f(): s[slice(-10.0, 10.0)]
def f(): s[l]
for l in [slice(3.0, 4), slice(3, 4.0), slice(3.0, 4.0)]:
def f(): s[l] = 0
s = Series(np.arange(5), index=np.arange(5) * 2.5, dtype=np.int64)
result1 = s[5.0] result2 = s.loc[5.0] result3 = s.ix[5.0] self.assertEqual(result1, result2) self.assertEqual(result1, result3)
self.assertRaises(KeyError, lambda: s.loc[4]) self.assertRaises(KeyError, lambda: s.ix[4]) self.assertRaises(KeyError, lambda: s[4])
expected = Series([2, 0], index=Float64Index([5.0, 0.0]))
result1 = s.loc[2:5] result2 = s.ix[2:5] result3 = s[2:5]
parsed_url = parse_url(filepath_or_buffer) s3_host = os.environ.get('AWS_S3_HOST', 's3.amazonaws.com')
if compat.PY3: try: text = compat.bytes_to_str( text, encoding=(kwargs.get('encoding') or get_option('display.encoding')) ) except: pass
lines = text[:10000].split('\n')[:-1][:10]
with option_context('display.max_colwidth', 999999): objstr = obj.to_string(**kwargs)
from datetime import datetime, date, time, MINYEAR
pass
if _is_url(io): io = _urlopen(io) io, _, _ = get_filepath_or_buffer(io)
data = io.read() self.book = xlrd.open_workbook(file_contents=data)
try: dt = xldate.xldate_as_tuple(cell_contents, epoch1904)
val = int(cell_contents) if val == cell_contents: cell_contents = val
if LooseVersion(xlrd.__VERSION__) >= LooseVersion("0.9.3"): xlrd_0_9_3 = True else: xlrd_0_9_3 = False
if isinstance(sheetname, list): sheets = sheetname ret_dict = True elif sheetname is None: sheets = self.sheet_names ret_dict = True else: sheets = [sheetname]
sheets = list(set(sheets))
if not com.is_list_like(header): offset = 1 + header else: offset = 1 + max(header)
try: parser = TextParser(data, header=header, index_col=index_col, has_index_names=has_index_names, na_values=na_values, thousands=thousands, parse_dates=parse_dates, date_parser=date_parser, skiprows=skiprows, skip_footer=skip_footer, squeeze=squeeze, **kwds)
output[asheetname] = DataFrame()
while len(row) > 0 and (row[0] == '' or row[0] is None): row = row[1:] return row
last = row[0] for i in range(1, len(row)): if row[i] == '' or row[i] is None: row[i] = last else: last = row[i] return row
return none_fill(row[0]), row[1:]
i = index_col if not com.is_list_like(index_col) else max(index_col) return none_fill(row[i]), row[:i] + [''] + row[i + 1:]
book = None curr_sheet = None path = None
if isinstance(path, string_types): ext = os.path.splitext(path)[-1] else: ext = 'xls' if engine == 'xlwt' else 'xlsx'
def __enter__(self): return self
from openpyxl.workbook import Workbook
self.book = Workbook() if self.book.worksheets: self.book.remove_sheet(self.book.worksheets[0])
from openpyxl.cell import get_column_letter
if style: first_row = startrow + cell.row + 1 last_row = startrow + cell.mergestart + 1 first_col = startcol + cell.col + 1 last_col = startcol + cell.mergeend + 1
continue
from openpyxl.cell import get_column_letter
if style_kwargs: first_row = startrow + cell.row + 1 last_row = startrow + cell.mergestart + 1 first_col = startcol + cell.col + 1 last_col = startcol + cell.mergeend + 1
continue
from openpyxl.styles import NumberFormat return NumberFormat(**number_format_dict)
return number_format_dict['format_code']
sheet_name = self._get_sheet_name(sheet_name)
if style_kwargs: first_row = startrow + cell.row + 1 last_row = startrow + cell.mergestart + 1 first_col = startcol + cell.col + 1 last_col = startcol + cell.mergeend + 1
continue
import xlwt engine_kwargs['engine'] = engine super(_XlwtWriter, self).__init__(path, **engine_kwargs)
import xlsxwriter
if num_format_str is None and style_dict is None: return None
xl_format = self.book.add_format()
if style_dict.get('font'): font = style_dict['font'] if font.get('bold'): xl_format.set_bold()
if style_dict.get('borders'): xl_format.set_border()
import os import sys import logging
try: FLAGS(flags) except gflags.FlagsError as e: print('%s\nUsage: %s ARGS\n%s' % (e, str(flags), FLAGS)) sys.exit(1)
logging.getLogger().setLevel(getattr(logging, FLAGS.logging_level))
credentials = storage.get() if credentials is None or credentials.invalid: credentials = tools.run(flow, storage)
_RE_WHITESPACE = re.compile(r'[\r\n]+|\s{2,}')
if 'class_' in attrs: attrs['class'] = attrs.pop('class_')
query = '//table//*[re:test(text(), %r)]/ancestor::table' xpath_expr = u(query) % pattern
if kwargs: xpath_expr += _build_xpath_expr(kwargs)
r = parse(self.io, parser=parser)
if not _is_url(self.io): r = fromstring(self.io, parser=parser)
_expand_elements(body)
retained = None for flav in flavor: parser = _parser_dispatch(flav) p = parser(io, compiled_match, attrs, encoding)
from datetime import datetime, date import time import re import copy import itertools import warnings import os
_version = '0.15.2'
_default_encoding = 'UTF-8'
if encoding is None: if PY3: encoding = _default_encoding return encoding
_TYPE_MAP = {
_AXES_MAP = { DataFrame: [0], Panel: [1, 2], Panel4D: [1, 2, 3], }
_table_mod = None _table_file_open_policy_is_strict = False
if LooseVersion(tables.__version__) < '3.0.0': raise ImportError("PyTables version >= 3.0.0 is required")
try: _table_file_open_policy_is_strict = ( tables.file._FILE_OPEN_POLICY == 'strict') except: pass
if 'where' in kwargs: kwargs['where'] = _ensure_term(kwargs['where'], scope_level=1)
except (TypeError, ValueError): exists = False
store = HDFStore(path_or_buf, **kwargs) auto_close = True
try: store.close() except: pass
if self._mode in ['a', 'w'] and mode in ['r', 'r+']: pass elif mode in ['w']:
if self.is_open: raise PossibleDataLossError( "Re-opening the file [{0}] with mode [{1}] " "will delete the current file!" .format(self._path, self._mode) )
if self.is_open: self.close()
if self._mode == 'r' and 'Unable to open/create file' in str(e): raise IOError(str(e)) raise
where = _ensure_term(where, scope_level=1) s = self._create_storer(group) s.infer_axes()
def func(_start, _stop, _where): return s.read(start=_start, stop=_stop, where=_where, columns=columns, **kwargs)
it = TableIterator(self, s, func, where=where, nrows=s.nrows, start=start, stop=stop, iterator=iterator, chunksize=chunksize, auto_close=auto_close)
tbls = [self.get_storer(k) for k in keys] s = self.get_storer(selector)
axis = list(set([t.non_index_axes[0][0] for t in tbls]))[0]
objs = [t.read(where=_where, columns=columns, **kwargs) for t in tbls]
return concat(objs, axis=axis, verify_integrity=False).consolidate()
it = TableIterator(self, s, func, where=where, nrows=nrows, start=start, stop=stop, iterator=iterator, chunksize=chunksize, auto_close=auto_close)
s = self.get_node(key) if s is not None: s._f_remove(recursive=True) return None
if where is None and start is None and stop is None: s.group._f_remove(recursive=True)
else: if not s.is_table: raise ValueError( 'can only remove with where on objects written as tables') return s.delete(where=where, start=start, stop=stop)
axis = list(set(range(value.ndim)) - set(_AXES_MAP[type(value)]))[0]
if data_columns is None: data_columns = d[selector]
for k, v in d.items(): dc = data_columns if k == selector else None
val = value.reindex_axis(v, axis=axis)
_tables() s = self.get_storer(key) if s is None: return
def _check_if_open(self): if not self.is_open: raise ClosedFileError("{0} file is not open!".format(self._path))
try: kwargs['format'] = _FORMAT_MAP[format.lower()] except: raise TypeError("invalid HDFStore format specified [{0}]" .format(format))
if pt is None: if value is None:
if format == 'table': pt += u('_table')
if u('table') not in pt: try: return globals()[_STORER_MAP[pt]](self, group, **kwargs) except: error('_STORER_MAP')
if tt is None:
if value is not None:
if group is not None and not append: self._handle.remove_node(group, recursive=True) group = None
if getattr(value, 'empty', None) and (format == 'table' or append): return
s.write(obj=value, append=append, complib=complib, **kwargs)
if self.s.is_table: if nrows is None: nrows = 0 if start is None: start = 0 if stop is None: stop = nrows stop = min(nrows, stop)
current = self.start while current < self.stop:
if self.chunksize is not None: if not self.s.is_table: raise TypeError( "can only use an iterator or chunksize on a table")
if coordinates: where = self.s.read_coordinates(where=self.where) else: where = self.where
results = self.func(self.start, self.stop, where) self.close() return results
if values.dtype.fields is not None: values = values[self.cname]
if 'freq' in kwargs: kwargs['freq'] = None self.values = Index(values, **kwargs)
if key in ['freq', 'index_name']: ws = attribute_conflict_doc % (key, existing_value, value) warnings.warn(ws, AttributeConflictWarning, stacklevel=6)
idx[key] = None setattr(self, key, None)
if self.typ is None: self.typ = getattr(self.description, self.cname, None)
elif inferred_type == 'string' or dtype == 'object': self.set_atom_string( block, block_items, existing_col, min_itemsize, nan_rep, encoding)
else: self.set_atom_data(block)
inferred_type = lib.infer_dtype(data.ravel()) if inferred_type != 'string':
for i, item in enumerate(block_items):
data_converted = _convert_string_array(data, encoding) itemsize = data_converted.itemsize
if existing_col is not None: eci = existing_col.validate_col(itemsize) if eci > itemsize: itemsize = eci
self.ordered = values.ordered self.typ = self.get_atom_data(block, kind=codes.dtype.name) self.set_data(_block_shape(codes))
self.meta = 'category' self.set_metadata(block.values.categories)
self.update_info(info)
values = values.asi8.reshape(block.shape)
self.tz = _get_tz(block.values.tz) self.update_info(info)
if values.dtype.fields is not None: values = values[self.cname]
meta = _ensure_decoded(self.meta)
if self.dtype is not None: dtype = _ensure_decoded(self.dtype)
if dtype == u('datetime64'):
self.data = _set_tz(self.data, self.tz, coerce=True)
categories = self.metadata self.data = Categorical.from_codes(self.data.ravel(), categories=categories, ordered=self.ordered)
if _ensure_decoded(self.kind) == u('string'): self.data = _unconvert_string_array( self.data, nan_rep=nan_rep, encoding=encoding)
def _class_to_alias(self, cls): return self._index_type_map.get(cls, '')
return alias
ret = np.empty(shape, dtype=dtype)
ret = _set_tz(ret, getattr(attrs, 'tz', None), coerce=True)
setattr(node._v_attrs, '%s_name%d' % (key, i), name)
label_key = '%s_label%d' % (key, i) self.write_array(label_key, lab)
empty_array = self._is_empty_array(value.shape) transposed = False
atom = _tables().Atom.from_dtype(value.dtype)
self._handle.create_array(self.group, key, value.asi8)
if self.is_shape_reversed: shape = shape[::-1]
raise Exception( "invalid combinate of [%s] on appending data [%s] vs " "current table [%s]" % (c, sv, ov))
if k == 'values': continue if k not in q: raise ValueError( "min_itemsize has the key [%s] which is not an axis or " "data_column" % k)
self._indexables.extend([ IndexCol(name=name, axis=axis, pos=i) for i, (axis, name) in enumerate(self.attrs.index_cols) ])
dc = set(self.data_columns) base_pos = len(self._indexables)
if v.is_indexed: index = v.index cur_optlevel = index.optlevel cur_kind = index.kind
self.validate_version(where)
if not self.infer_axes(): return False
self.selection = Selection(self, where=where, **kwargs) values = self.selection.select()
for a in self.axes: a.set_info(self.info) a.convert(values, nan_rep=self.nan_rep, encoding=self.encoding)
if data_columns is True: data_columns = axis_labels elif data_columns is None: data_columns = []
if isinstance(min_itemsize, dict):
return [c for c in data_columns if c in axis_labels]
axes = [obj._get_axis_number(a) for a in axes]
if len(axes) != self.ndim - 1: raise ValueError( "currently only support ndim-1 indexers in an AppendableTable")
self.non_index_axes = [] self.data_columns = []
if nan_rep is None: nan_rep = 'nan'
index_axes_map = dict() for i, a in enumerate(obj.axes):
append_axis = list(a) if existing_table is not None: indexer = len(self.non_index_axes) exist_axis = existing_table.non_index_axes[indexer][1] if append_axis != exist_axis:
if sorted(append_axis) == sorted(exist_axis): append_axis = exist_axis
info = _get_info(self.info, i) info['names'] = list(a.names) info['type'] = a.__class__.__name__
self.index_axes = [ index_axes_map[a].set_pos(j).update_info(self.info) for j, a in enumerate(axes) ] j = len(self.index_axes)
if validate: for a in self.axes: a.maybe_set_size(min_itemsize=min_itemsize)
for a in self.non_index_axes: obj = _reindex_axis(obj, a[0], a[1])
self.values_axes = [] for i, (b, b_items) in enumerate(zip(blocks, blk_items)):
klass = DataCol name = None
if (data_columns and len(b_items) == 1 and b_items[0] in data_columns): klass = DataIndexableCol name = b_items[0] self.data_columns.append(name)
self.validate_min_itemsize(min_itemsize)
self.validate_metadata(existing_table)
if validate: self.validate(existing_table)
if columns is not None: columns = list(columns)
if columns is not None and self.is_multi_index: for n in self.levels: if n not in columns: columns.insert(0, n)
for axis, labels in self.non_index_axes: obj = _reindex_axis(obj, axis, labels, columns)
if self.selection.filter is not None: for field, op, filt in self.selection.filter.format():
if field == axis_name:
if self.is_multi_index: filt = filt.union(Index(self.levels))
elif field in axis_values:
values = _ensure_index(getattr(obj, field).values) filt = _ensure_index(filt)
if isinstance(obj, DataFrame): axis_number = 1 - axis_number takers = op(values, filt) return obj.ix._getitem_axis(takers, axis=axis_number)
if expectedrows is None: expectedrows = max(self.nrows_expected, 10000)
d['description'] = dict([(a.cname, a.typ) for a in self.axes])
self.validate_version(where)
if not self.infer_axes(): return False
self.validate_version()
if not self.infer_axes(): return False
for a in self.axes: if column == a.name:
key = _factor_indexer(N[1:], labels)
for c in self.values_axes:
sorted_values = c.take_data().take(sorter, axis=0) if sorted_values.ndim == 1: sorted_values = sorted_values.reshape( (sorted_values.shape[0], 1))
mgr = BlockManager([block], [items] + levels) obj = self.obj_type(mgr)
if self.is_transposed: obj = obj.transpose( *tuple(Series(self.data_orientation).argsort()))
long_index = MultiIndex.from_arrays( [i.values for i in self.index_axes])
tuple_index = long_index._tuple_index
if len(objs) == 1: wp = objs[0] else: wp = concat(objs, axis=0, verify_integrity=False).consolidate()
wp = self.process_axes(wp, columns=columns)
self.create_axes(axes=axes, obj=obj, validate=append, min_itemsize=min_itemsize, **kwargs)
options = self.create_description(complib=complib, complevel=complevel, fletcher32=fletcher32, expectedrows=expectedrows)
self.set_attrs()
self._handle.create_table(self.group, **options)
self.set_info()
for a in self.axes: a.validate_and_set(self, append)
self.write_data(chunksize, dropna=dropna)
masks = [] if dropna:
if len(masks): mask = masks[0] for m in masks[1:]: mask = mask & m mask = mask.ravel() else: mask = None
indexes = [a.cvalues for a in self.index_axes] nindexes = len(indexes) bindexes = [] for i, idx in enumerate(indexes):
if i > 0 and i < nindexes: repeater = np.prod( [indexes[bi].shape[0] for bi in range(0, i)]) idx = np.tile(idx, repeater)
if chunksize is None: chunksize = 100000
for v in values: if not np.prod(v.shape): return
for i, idx in enumerate(indexes): rows[names[i]] = idx
for i, v in enumerate(values): rows[names[i + nindexes]] = v
if mask is not None: m = ~mask.ravel().astype(bool, copy=False) if not m.all(): rows = rows[m]
if not self.infer_axes(): return None
table = self.table self.selection = Selection( self, where, start=start, stop=stop, **kwargs) values = self.selection.select_coords()
l = Series(values).sort_values() ln = len(l)
diff = l.diff() groups = list(diff[diff > 1].index)
if not len(groups): groups = [0]
if groups[-1] != ln: groups.append(ln)
if groups[0] != 0: groups.insert(0, 0)
return ln
if values.ndim == 1 and isinstance(values, np.ndarray): values = values.reshape((1, values.shape[0]))
df = self.process_axes(df, columns=columns)
if s.name == 'values': s.name = None return s
self._indexables = [GenericIndexCol(name='index', axis=0)]
df.index = df.index.set_names([ None if self._re_levels.search(l) else l for l in df.index.names ])
if other is not None: other = _ensure_index(other) if (other is None or labels.equals(other)) and labels.equals(ax): return obj
atom = _tables().Int64Col() return IndexCol(np.asarray(values, dtype=np.int64), 'integer', atom, index_name=index_name)
if encoding is not None and len(data): data = Series(data.ravel()).str.encode( encoding).values.reshape(data.shape)
if itemsize is None: itemsize = lib.max_len_string_array(com._ensure_object(data.ravel()))
encoding = _ensure_encoding(encoding) if encoding is not None and len(data):
values = conv(values)
if self.terms is not None: self.condition, self.filter = self.terms.evaluate()
except: with open(path, 'rb') as fh: return pc.load(fh, encoding=encoding, compat=True)
if com.is_datetime64_dtype(arr): arr = arr.view(com._NS_DTYPE)
iterator = kwds.get('iterator', False) chunksize = kwds.get('chunksize', None) nrows = _validate_nrows(kwds.pop('nrows', None))
parser = TextFileReader(filepath_or_buffer, **kwds)
'parse_dates': False, 'keep_date_col': False, 'dayfirst': False, 'date_parser': None,
header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True,
dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=None, nrows=None,
na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True,
parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False,
iterator=False, chunksize=None,
compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='"', quoting=csv.QUOTE_MINIMAL, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False,
error_bad_lines=True, warn_bad_lines=True,
skip_footer=0,
doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=_c_parser_defaults['low_memory'], buffer_lines=None, memory_map=False, float_precision=None):
if delimiter is None: delimiter = sep
if widths is not None: colspecs, col = [], 0 for w in widths: colspecs.append((col, col + w)) col += w
self.engine = engine self._engine = None
self.options, self.engine = self._clean_options(options, engine) if 'has_index_names' in kwds: self.options['has_index_names'] = kwds['has_index_names']
if argname == 'mangle_dupe_cols' and not value: raise ValueError('Setting mangle_dupe_cols=False is ' 'not supported yet') else: options[argname] = value
if engine == 'c': if options['skip_footer'] > 0: fallback_reason = "the 'c' engine does not support"\ " skip_footer" engine = 'python'
keep_default_na = result.pop('keep_default_na')
na_values, na_fvalues = _clean_na_values(na_values, keep_default_na)
if engine != 'c': if com.is_integer(skiprows): skiprows = lrange(skiprows) skiprows = set() if skiprows is None else set(skiprows)
result['names'] = names result['converters'] = converters result['na_values'] = na_values result['na_fvalues'] = na_fvalues result['skiprows'] = skiprows
index, columns, col_dict = self._create_index(ret)
ic = self.index_col if ic is None: ic = []
index_names = header.pop(-1) index_names, names, index_col = _clean_index_names(index_names, self.index_col)
field_count = len(header[0])
if len(ic): col_names = [r[0] if len(r[0]) and 'Unnamed' not in r[0] else None for r in header] else: col_names = [None] * len(header)
if self.mangle_dupe_cols:
if indexnamerow: coffset = len(indexnamerow) - len(columns) index = index.set_names(indexnamerow[:coffset])
columns = self._maybe_make_multi_index_columns(columns, self.col_names)
for i in reversed(sorted(to_remove)): data.pop(i) if not self._implicit_index: columns.pop(i)
for c in reversed(sorted(to_remove)): data.pop(c) col_names.remove(c)
if self.parse_dates is not None: data, names = _process_date_conversion( data, self._date_conv, self.parse_dates, self.index_col, self.index_names, names, keep_date_col=self.keep_date_col)
kwds['allow_leading_cols'] = self.index_col is not False
self.usecols = _validate_usecols_arg(self._reader.usecols)
self.names, self.index_names, self.col_names, passed_names = ( self._extract_multi_indexer_columns( self._reader.header, self.index_names, self.col_names, passed_names ) )
self.orig_names = self.names[:]
self._first_chunk = False
arrays = []
data = sorted(data.items()) data = dict((k, v) for k, (i, v) in zip(names, data))
data = sorted(data.items())
names = list(self.orig_names) names = self._maybe_dedup_names(names)
alldata = [x[1] for x in data]
names = self._maybe_make_multi_index_columns(names, self.col_names)
data = bz2.decompress(f.read()) f = StringIO(data)
elif compat.PY3 and isinstance(f, compat.BytesIO): from io import TextIOWrapper
if hasattr(f, 'readline'): self._make_reader(f) else: self.data = f
self._col_indices = None self.columns, self.num_original_columns = self._infer_columns()
self.orig_names = list(self.columns)
noconvert_columns = set()
if sniff_sep: line = f.readline() while self.pos in self.skiprows: self.pos += 1 line = f.readline()
self._first_chunk = False
names = self._maybe_dedup_names(self.orig_names) return _get_empty_meta(names, self.index_col, self.index_names)
count_empty_content_vals = count_empty_vals(content[0]) indexnamerow = None if self.has_index_names and count_empty_content_vals == len(columns): indexnamerow = content[0] content = content[1:]
def get_chunk(self, size=None): if size is None: size = self.chunksize return self.read(nrows=size)
clean_conv = {}
if isinstance(header, (list, tuple, np.ndarray)): have_mi_columns = True header = list(header) + [header[-1] + 1] else: have_mi_columns = False header = [header]
if not self.names: raise EmptyDataError( "No columns to parse from file")
self._handle_usecols(columns, names)
self._handle_usecols([names], names) columns = [names] num_original_columns = ncols
implicit_first_cols = 0 if line is not None: if self.index_col is not False: implicit_first_cols = len(line) - self.num_original_columns
orig_names = list(columns) self.num_original_columns = len(columns) return line, orig_names, columns
self._implicit_index = True if self.index_col is None: self.index_col = lrange(implicit_first_cols)
(index_name, columns_, self.index_col) = _clean_index_names(columns, self.index_col)
zipped_content = list(lib.to_object_array( content, min_width=col_len).T) zip_len = len(zipped_content)
if rows is not None: if len(self.buf) >= rows: new_rows, self.buf = self.buf[:rows], self.buf[rows:]
else: rows -= len(self.buf)
if self.skiprows: new_rows = [row for i, row in enumerate(new_rows) if i + self.pos not in self.skiprows]
for new_name, colspec in compat.iteritems(parse_spec): if new_name in data_dict: raise ValueError('Date column %s already in dict' % new_name)
index_col = list(index_col)
if isinstance(index_names[0], compat.string_types)\ and 'Unnamed' in index_names[0]: index_names[0] = None
dtype = dict((columns[k] if com.is_integer(k) else k, v) for k, v in compat.iteritems(dtype))
result = set() for v in na_values: try: v = float(v) if not np.isnan(v): result.add(v) except: pass return result
if v == int(v): v = int(v) result.append("%s.0" % v) result.append(str(v))
return [line[fromm:to].strip(self.delimiter) for (fromm, to) in self.colspecs]
self.colspecs = kwds.pop('colspecs')
@contextmanager def urlopen(*args, **kwargs): with closing(_urlopen(*args, **kwargs)) as f: yield f
to_return = (list(maybe_read_encoded_stream(req, encoding, compression)) + [compression]) return tuple(to_return)
filepath_or_buffer = _stringify_path(filepath_or_buffer) return _expand_user(filepath_or_buffer), None, compression
if sys.version_info[1] <= 6: @contextmanager def ZipFile(*args, **kwargs): with closing(zipfile.ZipFile(*args, **kwargs)) as zf: yield zf else: ZipFile = zipfile.ZipFile
return csv.reader(f, dialect=dialect, **kwds)
if type(indicator) == str: indicator = [indicator]
with urlopen(url) as response: data = response.read()
data = data.sort(columns='id') data.index = pandas.Index(lrange(data.shape[0])) return data
return datetime.strptime(datestr, "%d%b%y:%H:%M:%S")
ieee1 = xport1 & 0x00ffffff
ieee2 = xport2
ieee1 &= 0xffefffff
contents = filepath_or_buffer.read() try: contents = contents.encode(self._encoding) except: pass self.filepath_or_buffer = compat.BytesIO(contents)
line1 = self._get_row() if line1 != _correct_line1: raise ValueError("Header record is not an XPORT file.")
field = field.ljust(140)
ix = np.flatnonzero(last_card == 2314885530818453536)
buf = self._read_bytes(const.endianness_offset, const.endianness_length) if buf == b'\x01': self.byte_order = "<" else: self.byte_order = ">"
def _process_subheader_counts(self, offset, length): pass
pass
gs = DataReader("GS", "yahoo")
aapl = DataReader("AAPL", "google")
vix = DataReader("VIXCLS", "fred")
url = _YAHOO_COMPONENTS_URL + 's={0}&f={1}&e=.csv&h={2}'
raise RemoteDataError("No data fetched using " "{0!r}".format(method.__name__))
zip_file_path = '{0}/{1}_TXT.zip'.format(_FAMAFRENCH_URL, name)
CUR_MONTH = dt.datetime.now().month CUR_YEAR = dt.datetime.now().year CUR_DAY = dt.datetime.now().day
>>> aapl = Options('aapl', 'yahoo')
>>> calls = aapl.get_call_data()
>>> aapl.calls
>>> puts = aapl.get_put_data()
>>> aapl.puts
>>> cut_calls = aapl.get_near_stock_price(call=True, above_below=3)
>>> forward_data = aapl.get_forward_data(8, call=True, put=True)
try: import zlib
compressor = None
if isinstance(path_or_buf, compat.string_types):
if hasattr(path_or_buf, 'read') and compat.callable(path_or_buf.read): return read(path_or_buf)
7: np.dtype('int64'), 'category': 'category' }
if hasattr(np, 'float128'): c2f_dict['complex256'] = np.float128
if dtype == np.object_: return v.tolist()
v = v.tostring() return ExtType(0, zlib.compress(v))
if dtype == np.object_: return v.tolist()
v = v.tostring() return ExtType(0, blosc.compress(v, typesize=dtype.itemsize))
return ExtType(0, v.tostring())
return np.fromstring(values, dtype=dtype)
if tz is not None: result = result.tz_localize('UTC').tz_convert(tz) return result
if isinstance(self.path, compat.string_types):
needs_closing = False fh = self.path
except (TypeError, ValueError): exists = False
numpy = self.numpy if numpy: self._parse_numpy()
if use_dtypes: if self.dtype is False: return data, False elif self.dtype is True: pass
try: data = data.astype('float64') result = True except: pass
try: data = data.astype('float64') result = True except: pass
if len(data) and (data.dtype == 'float' or data.dtype == 'object'):
try: new_data = data.astype('int64') if (new_data == data).all(): data = new_data result = True except: pass
if data.dtype == 'int':
try: data = data.astype('int64') result = True except: pass
if not len(data): return data, False
new_obj = DataFrame(new_obj, index=self.obj.index) new_obj.columns = self.obj.columns self.obj = new_obj
convert_dates = self.convert_dates if convert_dates is True: convert_dates = [] convert_dates = set(convert_dates)
if not isinstance(k, compat.string_types): k = str(k) if level == 0: newkey = k else: newkey = prefix + '.' + k
if not isinstance(v, dict):
if isinstance(data, dict): data = [data]
records = [] lengths = []
lengths.append(len(recs))
for k, v in compat.iteritems(meta_vals): if meta_prefix is not None: k = meta_prefix + k
if self.verbose: self._print(error_message) else: raise StreamingInsertError(error_message + '\nEnable verbose logging to ' 'see all errors')
}
schema = query_reply['schema']
while 'rows' in query_reply and current_row < total_rows: page = query_reply['rows'] result_pages.append(page) current_row += len(page)
self._print('Got {} rows.\n'.format(total_rows))
dtype_map = {'INTEGER': np.dtype(float), 'FLOAT': np.dtype(float), 'TIMESTAMP': 'M8[ns]'}
if col_order is not None: if sorted(col_order) == sorted(final_df.columns): final_df = final_df[col_order] else: raise InvalidColumnOrder( 'Column order does not match this DataFrame.' )
final_df._data = final_df._data.downcast(dtypes='infer')
warnings.warn("generate_bq_schema is deprecated and will be removed in " "a future version", FutureWarning, stacklevel=2)
for c_data in conversion_data: if dtype == c_data[0]: if data[col].max() <= np.iinfo(c_data[1]).max: dtype = c_data[1] else: dtype = c_data[2]
self.off = np.array(self.off, dtype=np.int32) self.val = np.array(self.val, dtype=np.int32)
self.len = 4 + 4 + 4 * self.n + 4 * self.n + self.text_len
bio.write(struct.pack(byteorder + 'i', self.len))
labname = self._encode(_pad_bytes(self.labname[:32], 33)) bio.write(labname)
for i in range(3): bio.write(struct.pack('c', null_byte))
bio.write(struct.pack(byteorder + 'i', self.n))
bio.write(struct.pack(byteorder + 'i', self.text_len))
for offset in self.off: bio.write(struct.pack(byteorder + 'i', offset))
for value in self.val: bio.write(struct.pack(byteorder + 'i', value))
for text in self.txt: bio.write(self._encode(text + null_string))
value = compat.long(value) if value < 2147483648 else float(value) self._str = self.MISSING_VALUES[value]
return "%s(%s)" % (self.__class__, self)
(32768, 'Q'),
}
contents = path_or_buf.read() try: contents = contents.encode(self._default_encoding) except: pass self.path_or_buf = BytesIO(contents)
self.col_sizes = lmap(lambda x: self._calcsize(x), self.typlist)
self.fmtlist = ["%td" if x.startswith("%td") else x for x in self.fmtlist]
self._seek_variable_labels = self._get_seek_variable_labels()
def _get_dtypes(self, seek_vartypes):
return self._seek_value_label_names + (33 * self.nvar) + 20 + 17
self.data_location = self.path_or_buf.tell()
s = s.partition(b"\0")[0] return s.decode(self._encoding or self._default_encoding)
return
return
@Appender('DEPRECATED: ' + _data_method_doc) def data(self, **kwargs):
if (self.nobs == 0) and (nrows is None): self._can_read_value_labels = True self._data_read = True return DataFrame(columns=self.varlist)
if self._dtype is None:
if self.byteorder != self._native_byteorder: data = data.byteswap().newbyteorder()
if index is None: ix = np.arange(self._lines_read - read_lines, self._lines_read) data = data.set_index(ix)
for col, typ in zip(data, self.typlist): if type(typ) is int: data[col] = data[col].apply( self._null_terminate, convert_dtype=True)
cat_data = Categorical(data[col], ordered=order_categoricals) categories = [] for category in cat_data.categories: if category in value_label_dict[label]: categories.append(value_label_dict[label][category]) else:
return fname
itemsize = max_len_string_array(com._ensure_object(column.values)) return chr(max(itemsize, 1))
self._prepare_pandas(data)
values[values == -1] = get_base_missing_value(dtype) data_formatted.append((col, values, index))
if name in self.RESERVED_WORDS: name = '_' + name
if name[0] >= '0' and name[0] <= '9': name = '_' + name
while columns.count(name) > 0: name = '_' + str(duplicate_var_id) + name name = name[:min(len(name), 32)] duplicate_var_id += 1
try: orig_name = orig_name.encode('utf-8') except: pass converted_names.append( '{0} -> {1}'.format(orig_name, name))
if self._convert_dates: for c, o in zip(columns, original_columns): if c != o: self._convert_dates[c] = self._convert_dates[o] del self._convert_dates[o]
data = self._check_column_names(data)
data = _cast_to_stata_types(data)
data = self._replace_nans(data)
data = self._prepare_categoricals(data)
if self._convert_dates is not None: for key in self._convert_dates: self.fmtlist[key] = self._convert_dates[key]
self._write(_pad_bytes("", 5)) self._prepare_data() self._write_data() self._write_value_labels() self._file.close()
for typ in self.typlist: self._write(typ)
for name in self.varlist: name = self._null_terminate(name, True) name = _pad_bytes(name[:32], 33) self._write(name)
srtlist = _pad_bytes("", 2 * (nvar + 1)) self._write(srtlist)
for fmt in self.fmtlist: self._write(_pad_bytes(fmt, 49))
if self._convert_dates is not None: for i, col in enumerate(data): if i in convert_dates: data[col] = _datetime_to_stata_elapsed_vec(data[col], self.fmtlist[i])
return read_stata(file, convert_dates=True)
with tm.ensure_clean() as path: empty_ds.to_stata(path, write_index=False) empty_ds2 = read_stata(path) tm.assert_frame_equal(empty_ds, empty_ds2)
with StataReader(self.dta1_114) as rdr:
expected['float_miss'] = expected['float_miss'].astype(np.float32)
w = [x for x in w if x.category is UserWarning]
self.assertEqual(len(w), 3)
tm.assert_frame_equal(parsed_114, expected, check_datetimelike_compat=True) tm.assert_frame_equal(parsed_115, expected, check_datetimelike_compat=True) tm.assert_frame_equal(parsed_117, expected, check_datetimelike_compat=True)
expected = pd.concat([expected[col].astype('category') for col in expected], axis=1)
tm.assert_frame_equal(written_and_read_again.set_index('index'), original, check_index_type=False)
raw = read_stata(self.dta_encoding) encoded = read_stata(self.dta_encoding, encoding="latin-1") result = encoded.kreis1849[0]
self.assertEqual(len(w), 1)
self.assertEqual(len(w), 1)
with tm.assert_produces_warning(InvalidColumnName): original.to_stata(path)
expected = self.read_csv(self.csv15) expected['date_td'] = expected['date_td'].apply(datetime.strptime, args=('%Y-%m-%d',))
columns = ['int_', 'long_', 'byte_'] expected = expected[columns] reordered = read_stata(self.dta15_117, convert_dates=True, columns=columns) tm.assert_frame_equal(expected, reordered)
original = pd.concat([original[col].astype('category') for col in original], axis=1)
original.to_stata(path) written_and_read_again = self.read_dta(path) res = written_and_read_again.set_index('index') tm.assert_frame_equal(res, expected, check_categorical=False)
self.assertEqual(len(w), 1)
parsed_115 = read_stata(self.dta19_115) parsed_117 = read_stata(self.dta19_117) tm.assert_frame_equal(expected, parsed_115, check_categorical=False) tm.assert_frame_equal(expected, parsed_117, check_categorical=False)
from_chunks = pd.concat(read_stata(fname, chunksize=4)) tm.assert_frame_equal(parsed, from_chunks)
with warnings.catch_warnings(record=True) as w: warnings.simplefilter("always") parsed = read_stata( fname, convert_categoricals=convert_categoricals, convert_dates=convert_dates)
itr = read_stata( fname, iterator=True, convert_dates=convert_dates, convert_categoricals=convert_categoricals) pos = 0 for j in range(5):
self.drop_table('test_frame1')
self.drop_table('test_frame1')
self.assertRaises(ValueError, df.to_sql, 'test_complex', self.conn)
self.assertRaises(ValueError, sql.to_sql, temp_frame, 'test_index_label', self.conn, if_exists='replace', index_label='C')
res1 = sql.read_sql_query("select * from test_chunksize", self.conn)
res2 = DataFrame() i = 0 sizes = [5, 5, 5, 5, 2]
if self.mode == 'sqlalchemy': res3 = DataFrame() i = 0 sizes = [5, 5, 5, 5, 2]
df = DataFrame([[1, 2], [3, 4]], columns=[u'\xe9', u'b']) df.to_sql('test_unicode', self.conn, index=False)
sql.to_sql(self.test_frame1, 'test_frame', self.conn)
sql.to_sql(self.test_frame1, 'test_frame', self.conn)
iris = self._make_iris_table_metadata()
with tm.assert_produces_warning(): sql.to_sql(df, "test_frame3_legacy", self.conn, flavor="sqlite", index=False)
create_sql = sql.get_schema(self.test_frame1, 'test', 'sqlite') self.assertTrue('CREATE' in create_sql)
if not SQLALCHEMY_INSTALLED: raise nose.SkipTest('SQLAlchemy not installed')
self.conn.connect()
df = DataFrame(data={'i64': [2**62]}) df.to_sql('test_bigint', self.conn, index=False) result = sql.read_sql_table('test_bigint', self.conn)
self.assertTrue(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
if com.is_datetime64_dtype(col.dtype):
self.assertEqual(col[0], Timestamp('2000-01-01 08:00:00'))
self.assertEqual(col[1], Timestamp('2000-06-01 07:00:00'))
self.assertEqual(col[0], Timestamp( '2000-01-01 08:00:00', tz='UTC'))
self.assertEqual(col[1], Timestamp( '2000-06-01 07:00:00', tz='UTC'))
df = sql.read_sql_table("types_test_data", self.conn) check(df.DateColWithTz)
df = sql.read_sql_table("types_test_data", self.conn)
result = sql.read_sql_table('test_datetime', self.conn) result = result.drop('index', axis=1) tm.assert_frame_equal(result, df)
result = sql.read_sql_table('test_datetime', self.conn) tm.assert_frame_equal(result, df)
df.to_sql("test_read_write", self.conn, index=False) df2 = sql.read_sql_table("test_read_write", self.conn)
result = sql.read_sql_table('test_nan', self.conn) tm.assert_frame_equal(result, df)
result = sql.read_sql_query('SELECT * FROM test_nan', self.conn) tm.assert_frame_equal(result, df)
result = sql.read_sql_table('test_nan', self.conn) tm.assert_frame_equal(result, df)
df.loc[2, 'B'] = None
result = sql.read_sql_table('test_nan', self.conn) tm.assert_frame_equal(result, df)
result = sql.read_sql_query('SELECT * FROM test_nan', self.conn) tm.assert_frame_equal(result, df)
self.assertEqual(np.round(df['f64'].iloc[0], 14), np.round(res['f64'].iloc[0], 14))
cls.driver = None
self.assertTrue(issubclass(df.BoolCol.dtype.type, np.integer), "BoolCol loaded with incorrect type")
self.assertFalse(issubclass(df.DateCol.dtype.type, np.datetime64), "DateCol loaded with incorrect type")
df = DataFrame({'a': [1, 2]}, dtype='int64') df.to_sql('test_bigintwarning', self.conn, index=False)
self.assertTrue(issubclass(df.BoolCol.dtype.type, np.integer), "BoolCol loaded with incorrect type")
res2 = sql.read_sql("CALL get_testdb();", self.conn) tm.assert_frame_equal(df, res2)
self.conn.execute("DROP SCHEMA IF EXISTS other CASCADE;") self.conn.execute("CREATE SCHEMA other;")
self.conn.execute("DROP SCHEMA IF EXISTS other CASCADE;") self.conn.execute("CREATE SCHEMA other;")
self.assertEqual(self._get_sqlite_column_type( 'dtype_test', 'B'), 'INTEGER')
df = DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])
self.assertRaises(ValueError, df.to_sql, "", self.conn, flavor=self.flavor)
try: cls.connect() except cls.driver.err.OperationalError: raise nose.SkipTest( "{0} - can't connect to MySQL server".format(cls))
self.setUp()
result.index = frame.index
con_x = self.conn the_sum = sum([my_c0[0] for my_c0 in con_x.execute("select * from mono_df")]) self.assertEqual(the_sum, 3)
self.assertRaises(ValueError, sql.to_sql, frame=df_if_exists_1, con=self.conn, name=table_name, flavor='sqlite', if_exists='notvalidvalue') clean_up(table_name)
self.conn = pymysql.connect(host='localhost', user='root', passwd='', db='pandas_nosetest')
self.setUp()
result.index = frame.index result.index.name = frame.index.name
expected.index = index expected.index.names = result.index.names tm.assert_frame_equal(expected, result)
self.assertRaises(ValueError, sql.to_sql, frame=df_if_exists_1, con=self.conn, name=table_name, flavor='mysql', if_exists='notvalidvalue') clean_up(table_name)
self.assertRaises(ValueError, read_csv, 's3://pandas-test/tips.csv' + ext, compression=comp)
self.assertRaises(ValueError, read_csv, 's3://pandas-test/tips.csv' + ext, compression=comp)
with tm.assertRaisesRegexp(boto.exception.S3ResponseError, 'S3ResponseError: 403 Forbidden'): read_csv('s3://cant_get_it/')
if hash(np.int64(-1)) != -2: raise nose.SkipTest("skipping because of windows hash on Python" " 3.2.2")
data = "A;B\n1;2\n3;4"
df = self.read_csv(StringIO(data), header=[0, 1, 2, 3], index_col=[ 0, 1], tupleize_cols=False) tm.assert_frame_equal(df, expected)
self.assertRaises(ValueError, self.read_csv, StringIO(data), header=[0, 1, 2, 3], index_col=[0, 1], as_recarray=True, tupleize_cols=False)
self.assertRaises(ValueError, self.read_csv, StringIO(data), header=[0, 1, 2, 3], index_col=[0, 1], names=['foo', 'bar'], tupleize_cols=False)
self.assertRaises(ValueError, self.read_csv, StringIO(data), header=[0, 1, 2, 3], index_col=[0, 1], usecols=['foo', 'bar'], tupleize_cols=False)
self.assertRaises(ValueError, self.read_csv, StringIO(data), header=[0, 1, 2, 3], index_col=['foo', 'bar'], tupleize_cols=False)
data = '1,2,3\n4,5,6'
expected = DataFrame(columns=['a', 'b', 'c'])
expected = self.read_csv(self.csv1, index_col=0, parse_dates=True)
msg = 'Only length-1 decimal markers supported' with tm.assertRaisesRegexp(ValueError, msg): self.read_csv(StringIO(data), decimal='')
self.read_csv(StringIO(data)) self.read_table(StringIO(data), sep=',')
df = self.read_csv(StringIO(self.data1), nrows=3.0) tm.assert_frame_equal(df, expected)
reader = self.read_csv(StringIO(self.data1), index_col=0, iterator=True) df = self.read_csv(StringIO(self.data1), index_col=0)
lines = list(csv.reader(StringIO(self.data1))) parser = TextParser(lines, index_col=0, chunksize=2)
parser = TextParser(lines, index_col=0, chunksize=2, skiprows=[1]) chunks = list(parser) tm.assert_frame_equal(chunks[0], df[1:3])
df2 = self.read_csv(StringIO(data2)) tm.assert_frame_equal(df2, df)
raise nose.SkipTest("failing on %s" % ' '.join(platform.uname()).strip())
path = '%s.csv' % tm.rands(10) self.assertRaises(IOError, self.read_csv, path)
from io import TextIOWrapper s = TextIOWrapper(s, encoding='utf-8')
result = self.read_table(path, encoding='utf-16') self.assertEqual(len(result), 50)
data = 'A,B\n0,0\n0,'
self.assertTrue(type(df.a[0]) is np.float64) self.assertEqual(df.a.dtype, np.float)
if self.engine == 'c' and self.low_memory: warning_type = DtypeWarning
data = "65248E10 11\n55555E55 22\n"
data = '\n hello\nworld\n' result = self.read_csv(StringIO(data), header=None) self.assertEqual(len(result), 2)
expected = DataFrame([[4, 5, 6]], columns=['a', 'b', 'c'])
data = 'a,b,c\n4,5,6\n ' result = self.read_csv(StringIO(data)) tm.assert_frame_equal(result, expected)
data = 'a,b,c\n4,5,6\n#comment' result = self.read_csv(StringIO(data), comment='#') tm.assert_frame_equal(result, expected)
data = 'a,b,c\n4,5,6\n\r' result = self.read_csv(StringIO(data)) tm.assert_frame_equal(result, expected)
data = 'a,b,c\n4,5,6#comment' result = self.read_csv(StringIO(data), comment='#') tm.assert_frame_equal(result, expected)
data = 'a,b,c\n4,5,6\nskipme' result = self.read_csv(StringIO(data), skiprows=[2]) tm.assert_frame_equal(result, expected)
data = "a,b,c\n4,5,6\n\\" self.assertRaises(Exception, self.read_csv, StringIO(data), escapechar='\\')
data = 'a,b,c\n4,5,6\n"\\' self.assertRaises(Exception, self.read_csv, StringIO(data), escapechar='\\')
data = 'a,b,c\n4,5,6\n"' self.assertRaises(Exception, self.read_csv, StringIO(data), escapechar='\\')
msg = "Expected \d+ fields in line \d+, saw \d+" with tm.assertRaisesRegexp(ValueError, msg): df = self.read_csv(StringIO(csv))
names = ['Dummy', 'X', 'Dummy_2']
errmsg = "No columns to parse from file"
data = "\n" self.assertRaises(EmptyDataError, self.read_csv, StringIO(data))
data = "\n\n\n" self.assertRaises(EmptyDataError, self.read_csv, StringIO(data))
data = 'a b c\n1 2 3' msg = 'is not supported'
data = 'a b c\n1 2 3' msg = 'does not support'
self.assertRaises(ValueError, self.read_csv, StringIO(self.ts_data), index_col=True)
index_col, expected = None, DataFrame([], columns=list('xyz')), tm.assert_frame_equal(self.read_csv( StringIO(data), index_col=index_col), expected)
index_col, expected = False, DataFrame([], columns=list('xyz')), tm.assert_frame_equal(self.read_csv( StringIO(data), index_col=index_col), expected)
self.assertRaises(parser.CParserError, TextReader, StringIO(data), delimiter=',', header=5, as_recarray=True)
self.assertRaises(parser.CParserError, TextReader, StringIO(data), delimiter=',', header=5, as_recarray=True)
pass
df = self.read_csv(StringIO(data.replace(',', ' ')), comment='#', delim_whitespace=True) tm.assert_almost_equal(df.values, expected)
result = result.astype(float) tm.assert_frame_equal(result, df)
self.assertRaises(TypeError, self.read_csv, path, dtype={'A': 'foo', 'B': 'float64'}, index_col=0)
self.assertRaises(TypeError, self.read_csv, path, dtype={'A': 'timedelta64', 'B': 'float64'}, index_col=0)
tm._skip_if_32bit() from decimal import Decimal
for num in np.linspace(1., 2., num=500): text = 'a\n{0:.25}'.format(num)
self.assertEqual(roundtrip_val, float(text[2:]))
self.read_csv(self.csv1, memory_map=True)
data = "1,2\n3,4,5"
max_row_range = 10000 num_files = 100
pool = ThreadPool(8) results = pool.map(self.read_csv, files) first_result = results[0]
num_tasks = 4 file_name = '__threadpool_reader__.csv' num_rows = 100000
result = self.read_csv(StringIO(data), names=['a', 'b'], header=None, usecols=[0, 1])
self.assertRaises(ValueError, self.read_csv, StringIO(data), names=['a', 'b'], usecols=[1], header=None)
data = 'a,b,c\n4,apple,bat,5.7\n8,orange,cow,10'
data = 'a b c\n4 apple bat 5.7\n8 orange cow 10'
from io import TextIOWrapper s = TextIOWrapper(s, encoding='utf-8')
df = self.read_csv(StringIO(data), header=None, parse_dates=date_spec, date_parser=conv.parse_date_time) self.assertIn('nominal', df)
self.read_csv(log_file, index_col=0, parse_dates=[0], date_parser=f)
df.to_csv(path) result = self.read_csv(path, index_col=0, parse_dates=['B']) tm.assert_frame_equal(result, df)
df2 = self.read_csv(StringIO(data), index_col=[1, 0], parse_dates=True) self.assertIsInstance(df2.index.levels[1][0], (datetime, np.datetime64, Timestamp))
import pytz data = StringIO("Date,x\n2012-06-13T01:39:00Z,0.5")
start = datetime(2010, 1, 1) end = datetime(2013, 1, 27)
start = datetime(2010, 1, 1) end = datetime(2013, 1, 27)
raise nose.SkipTest('unreliable test, receive partial components back for nasdaq_100')
self.assertTrue('AAPL' in df.index) self.assertTrue('GOOG' in df.index) self.assertTrue('AMZN' in df.index)
web.get_data_yahoo('GOOG')
pan = web.get_data_yahoo('XOM', '2013-01-01', '2013-12-31', interval='d') self.assertEqual(len(pan), 252)
pan = web.get_data_yahoo('XOM', '2013-01-01', '2013-12-31', interval='w') self.assertEqual(len(pan), 53)
pan = web.get_data_yahoo('XOM', '2013-01-01', '2013-12-31', interval='m') self.assertEqual(len(pan), 12)
pan = web.get_data_yahoo('XOM', '2013-01-01', '2013-12-31', interval='v') self.assertEqual(len(pan), 4)
self.assertRaises(ValueError, web.get_data_yahoo, 'XOM', interval='NOT VALID')
sl = ['AAPL', 'AMZN', 'GOOG'] web.get_data_yahoo(sl, '2012')
self.assertTrue(np.issubdtype(result.dtype, np.floating))
self.assertTrue(np.issubdtype(pan.values.dtype, np.floating))
self.assertRaises(ValueError, self.aapl.get_options_data, month=3) self.assertRaises(ValueError, self.aapl.get_options_data, year=1992)
import numpy as np from numpy.random import randint
self.assertEqual(df.iloc[1][1], 'Harry Carney')
actual = self.get_exceldf(basename, 'Sheet1') tm.assert_frame_equal(actual, expected)
def test_reader_converters(self):
actual = self.get_exceldf(basename, 'Sheet1', converters=converters) tm.assert_frame_equal(actual, expected)
def test_read_excel_blank(self): actual = self.get_exceldf('blank', 'Sheet1') tm.assert_frame_equal(actual, DataFrame())
refdf = pd.DataFrame([[1, 'foo'], [2, 'bar'], [3, 'baz']], columns=['a', 'b'])
if sys.version_info[:2] < (2, 6): raise nose.SkipTest("file:// not supported with Python < 2.6")
import platform raise nose.SkipTest("failing on %s" % ' '.join(platform.uname()).strip())
tm._skip_if_no_pathlib()
tm._skip_if_no_localpath()
read_excel(xlsx, 'Sheet1', index_col=0)
import xlrd
check_names = True if not r_idx_names and r_idx_levels > 1: check_names = False
with tm.assertRaises(NotImplementedError): pd.read_excel(os.path.join(self.dirpath, 'test1' + self.ext), chunksize=100)
with tm.assertRaises(NotImplementedError): pd.read_excel(os.path.join(self.dirpath, 'test1' + self.ext), parse_dates=True)
f = os.path.join(self.dirpath, 'test_squeeze' + self.ext)
merge_cells = True
self.frame.to_excel(path, 'test1') recons = read_excel(path, 'test1', index_col=0) tm.assert_frame_equal(self.frame, recons)
self.frame.to_excel(path, 'Sheet1') recons = read_excel(path, index_col=0) tm.assert_frame_equal(self.frame, recons)
float_frame = frame.astype(float) recons = read_excel(path, 'test1', convert_float=False) tm.assert_frame_equal(recons, float_frame, check_index_type=False, check_column_type=False)
df = self.frame.copy() df = df.set_index(['A', 'B'])
tsf = self.tsframe.copy() with ensure_clean(self.ext) as path:
tm.assert_frame_equal(rs2, df_expected)
def test_to_excel_multiindex_cols(self): _skip_if_no_xlrd()
tsframe = self.tsframe.copy() new_index = [tsframe.index, np.arange(len(tsframe.index))] tsframe.index = MultiIndex.from_arrays(new_index)
frame1 = DataFrame({'a': [10, 20], 'b': [30, 40], 'c': [50, 60]})
frame2 = frame1.copy() multi_index = MultiIndex.from_tuples([(70, 80), (90, 100)]) frame2.index = multi_index
frame2.to_excel(path, 'test1', index=False)
reader = ExcelFile(path) frame3 = read_excel(reader, 'test1')
tm.assert_frame_equal(frame1, frame3)
if j > 1: with tm.assertRaises(NotImplementedError): res = roundtrip(df, use_headers, index=False) else: res = roundtrip(df, use_headers)
self.assertEqual(res.shape, (nrows - 1, ncols + i))
_skip_if_no_xlrd()
_skip_if_no_xlrd()
_skip_if_no_xlrd()
_skip_if_no_xlrd()
_skip_if_no_xlrd()
def test_bytes_io(self): _skip_if_no_xlrd()
def test_write_lists_dict(self): _skip_if_no_xlrd()
_skip_if_no_xlsxwriter()
warnings.simplefilter("ignore") _skip_if_no_openpyxl() import openpyxl
cell = read_worksheet.cell('B2')
merge_cells = False
merge_cells = False
merge_cells = False
called_save = [] called_write_cells = []
data_csv = pd.read_csv(self.file01.replace(".xpt", ".csv")) numeric_as_float(data_csv)
data = read_sas(self.file01, format="xport") tm.assert_frame_equal(data, data_csv)
reader = read_sas(self.file01, format="xport", iterator=True) data = reader.read(10) tm.assert_frame_equal(data, data_csv.iloc[0:10, :])
reader = read_sas(self.file01, format="xport", chunksize=10) data = reader.get_chunk() tm.assert_frame_equal(data, data_csv.iloc[0:10, :])
data = read_sas(self.file01) tm.assert_frame_equal(data, data_csv)
data_csv = pd.read_csv(self.file01.replace(".xpt", ".csv")) data_csv = data_csv.set_index("SEQN") numeric_as_float(data_csv)
data = read_sas(self.file01, index="SEQN", format="xport") tm.assert_frame_equal(data, data_csv, check_index_type=False)
data_csv = pd.read_csv(self.file02.replace(".xpt", ".csv")) numeric_as_float(data_csv)
data_csv = pd.read_csv(self.file03.replace(".xpt", ".csv"))
with tm.assert_produces_warning(FutureWarning, check_stacklevel=False): from pandas.io.wb import search, download, get_countries
except ValueError as e: raise nose.SkipTest("No indicators returned data: {0}".format(e))
if len(result) > 0: raise nose.SkipTest("Invalid results")
if len(result) > 0: raise nose.SkipTest("Invalid results")
try: data = pandas.read_pickle(vf) except (ValueError) as e: if 'unsupported pickle protocol:' in str(e): return else: raise
comparator = "compare_{typ}_{dt}".format(typ=typ, dt=dt) comparator = getattr(self, comparator, self.compare_element) comparator(result, expected, typ, version)
freq = result.index.freq tm.assert_equal(freq + Day(1), Day(2))
if LooseVersion(version) < '0.16.0': tm.assert_series_equal(result, expected, check_categorical=False) else: tm.assert_series_equal(result, expected)
if LooseVersion(version) < '0.16.0': tm.assert_frame_equal(result, expected, check_categorical=False) else: tm.assert_frame_equal(result, expected)
writer(expected, path)
result = pd.read_pickle(path) self.compare_element(result, expected, typ)
exit=False)
with warnings.catch_warnings(record=True): import pandas.io.ga as ga
df1 = self.read_html(self.spam_data, '.*Water.*') df2 = self.read_html(self.spam_data, 'Unit') assert_framelist_equal(df1, df2)
for arg in [True, False]: with tm.assertRaises(TypeError): read_html(self.spam_data, header=arg)
if is_platform_windows(): if '16' in encoding or '32' in encoding: continue raise
class A(object):
tm.assert_almost_equal(tuple(x), x_rec)
tm.assert_almost_equal(tuple(x), x_rec)
tm.assert_almost_equal(tuple(x), x_rec)
i = Index([Timestamp('20130101'), Timestamp('20130103')]) i_rec = self.encode_decode(i) self.assert_index_equal(i, i_rec)
for n in range(10): for s, i in self.d.items(): i_rec = self.encode_decode(i) assert_series_equal(i, i_rec)
for n in range(10): for s, i in self.d.items(): i_rec = self.encode_decode(i) assert_categorical_equal(i, i_rec)
df = DataFrame([1, 2, 3], index=date_range('1/1/2013', '1/3/2013')) result = self.encode_decode(df) assert_frame_equal(result, df)
self.assertRaises(NotImplementedError, self.encode_decode, obj)
for block in value._data.blocks: self.assertTrue(block.values.flags.writeable)
for block in value._data.blocks: self.assertTrue(block.values.flags.writeable) block.values[0] += rhs[block.dtype]
self.assertEqual( str(w.message), 'copying data after decompressing; this may mean that' ' decompress is caching its result', )
self.assertEqual(buf, control_buf)
char_unpacked[0] = ord(b'b')
for encoding in self.utf_encodings: for frame in compat.itervalues(self.frame): result = self.encode_decode(frame, encoding=encoding) assert_frame_equal(result, frame)
skip_compression = PY3 and is_platform_windows()
if not len(os.path.dirname(path)): path = create_tempfile(path)
tables.parameters.MAX_NUMEXPR_THREADS = 1 tables.parameters.MAX_BLOSC_THREADS = 1 tables.parameters.MAX_THREADS = 1
tm.reset_testing_mode()
tm.set_testing_mode()
df = DataFrame({'a': tm.rands_array(100, size=10)}, index=tm.rands_array(100, size=10))
with ensure_clean_path(self.path) as path:
path = "" self.assertRaises(IOError, read_hdf, path, 'df')
with ensure_clean_store(self.path) as store: df = tm.makeDataFrame()
self.assertTrue(list(store) == [])
store._handle.create_group(store._handle.root, 'bah')
with ensure_clean_store(self.path) as store:
warnings.filterwarnings( 'ignore', category=tables.NaturalNameWarning) store['node())'] = tm.makeDataFrame() self.assertIn('node())', store)
_maybe_remove(store, 'df2') store.append('df2', df)
store.get_node('df2')._v_attrs.pandas_version = None self.assertRaises(Exception, store.select, 'df2')
if mode in ['r', 'r+']: self.assertRaises(IOError, HDFStore, path, mode=mode)
if mode in ['r', 'r+']: def f():
self.assertRaises(PossibleDataLossError, store.open, 'w') store.close() self.assertFalse(store.is_open)
store = HDFStore(path, mode='a', driver='H5FD_CORE', driver_core_backing_store=0) store['df'] = df store.append('df2', df)
self.assertFalse(os.path.exists(path))
result = store.a tm.assert_series_equal(result, s) result = getattr(store, 'a') tm.assert_series_equal(result, s)
self.assertRaises(AttributeError, getattr, store, 'd')
for x in ['mode', 'path', 'handle', 'complib']: getattr(store, "_%s" % x)
self.assertRaises( ValueError, store.put, 'b', df[10:], append=True)
self.assertRaises(ValueError, store.put, 'c', df[10:], append=True)
store.put('c', df[:10], format='table', append=False) tm.assert_frame_equal(df[:10], store['c'])
self.assertRaises(ValueError, store.put, 'b', df, format='fixed', complib='zlib')
self.assertRaises(ValueError, store.put, 'b', df, format='fixed', complib='blosc')
df = DataFrame(np.random.randn(50, 100)) self._check_roundtrip(df, tm.assert_frame_equal)
warnings.filterwarnings('ignore', category=PerformanceWarning) store.put('df', df) warnings.filterwarnings('always', category=PerformanceWarning)
ss = tm.makeStringSeries() ts = tm.makeTimeSeries() ns = Series(np.arange(100))
expected = ns[ns > 60] result = store.select('ns', Term('foo>60')) tm.assert_series_equal(result, expected)
check('fixed', tm.makePeriodIndex)
index = tm.makeUnicodeIndex if compat.PY3: check('table', index) check('fixed', index) else:
self.assertRaises(TypeError, check, 'table', index) with tm.assert_produces_warning( expected_warning=PerformanceWarning): check('fixed', index)
values = [[_try_decode(x) for x in y] for y in values]
df_with_missing = DataFrame( {'col1': [0, np.nan, 2], 'col2': [1, np.nan, np.nan]})
self.assertRaises(TypeError, store.select, 'df1', ( 'columns=A', Term('index>df.index[4]')))
with ensure_clean_store(self.path) as store:
with ensure_clean_store(self.path) as store:
df['int16_2'] = Series([1] * len(df), dtype='int16') self.assertRaises(ValueError, store.append, 'df', df)
df['float_3'] = Series([1.] * len(df), dtype='float64') self.assertRaises(ValueError, store.append, 'df', df)
indexers = ['items', 'major_axis', 'minor_axis']
result = store.select('p4d', ['labels=l1']) expected = p4d.reindex(labels=['l1']) assert_panel4d_equal(result, expected)
store.append('s3', wp, min_itemsize={'major_axis': 20}) self.assertRaises(ValueError, store.append, 's3', wp2)
store.append('s4', wp) self.assertRaises(ValueError, store.append, 's4', wp2)
result = store.select('df', [Term('B>0')]) expected = df[df.B > 0] tm.assert_frame_equal(result, expected)
def check_col(key, name, size): self.assertEqual(getattr(store.get_storer( key).table.description, name).itemsize, size)
store.append('df_dc', df_dc, data_columns=[ 'B', 'C', 'string', 'string2'])
np.random.seed(1234) p = tm.makePanel()
_maybe_remove(store, 'f2') store.put('f2', df) self.assertRaises(TypeError, store.create_table_index, 'f2')
with ensure_clean_store(self.path) as store: store.append('df2', df) store.append('df2', df)
with ensure_clean_store(self.path) as store:
df = DataFrame(np.random.randn(10, 3), index=index, columns=['A', 'B', 'C'])
p4d = tm.makePanel4D() self.assertRaises(TypeError, store.put, 'p4d', p4d)
with ensure_clean_store(self.path) as store:
df_empty = DataFrame(columns=list('ABC')) store.append('df', df_empty) self.assertRaises(KeyError, store.select, 'df')
df = DataFrame(columns=list('ABC')) store.put('df2', df) assert_frame_equal(store.select('df2'), df)
p_empty = Panel(items=list('ABC')) store.append('p', p_empty) self.assertRaises(KeyError, store.select, 'p')
store.put('p2', p_empty) assert_panel_equal(store.select('p2'), p_empty)
self.assertRaises(TypeError, store.append, 'df', np.arange(10))
self.assertRaises(TypeError, store.append, 'df', Series(np.arange(10)))
df = tm.makeDataFrame() store.append('df', df)
self.assertRaises(ValueError, store.append, 'df_i8', df1)
if not compat.PY3: l.append(('unicode', u('\\u03c3')))
for n, f in l: df = tm.makeDataFrame() df[n] = f self.assertRaises( TypeError, store.append, 'df1_%s' % n, df)
self.assertRaises(TypeError, store.append, 'df_unimplemented', df)
_maybe_remove(store, 'df') store.append('df', df, data_columns=True) result = store.select('df') assert_frame_equal(result, df)
_maybe_remove(store, 'df2') store.put('df2', df) result = store.select('df2') assert_frame_equal(result, df)
self.assertRaises(KeyError, store.remove, 'a_nonexistent_store')
store['a'] = ts store['b'] = df del store['a'] del store['b'] self.assertEqual(len(store), 0)
crit1 = Term('index>foo') self.assertRaises(KeyError, store.remove, 'a', [crit1])
_maybe_remove(store, 'wp') store.put('wp', wp, format='table')
n = store.remove('wp', []) self.assertTrue(n == 120)
_maybe_remove(store, 'wp7')
_maybe_remove(store, 'wp') store.put('wp', wp, format='table') date = wp.major_axis[len(wp.major_axis) // 2]
_maybe_remove(store, 'wp2') store.put('wp2', wp, format='table')
terms = [ (("labels=['l1', 'l2']"),), Term("labels=['l1', 'l2']"), ]
res = store.select('wpneg', Term('items == -1')) expected = Panel({-1: wpneg[-1]}) tm.assert_panel_equal(res, expected)
result = store.select('df', 'index>datetime.datetime(2013,1,5)') assert_frame_equal(result, expected)
index = np.random.randn(10) s = Series(np.random.randn(10), index=index) self._check_roundtrip(s, tm.assert_series_equal)
expected_warning = Warning if PY35 else PerformanceWarning with tm.assert_produces_warning(expected_warning=expected_warning, check_stacklevel=False): ser = Series(values, [0, 'y']) self._check_roundtrip(ser, func)
df.values[0, 0] = np.nan df.values[5, 3] = np.nan
self._check_roundtrip(df[:0], tm.assert_frame_equal)
with ensure_clean_store(self.path) as store: store.append('df', df) store.append('df', df)
arr = np.random.binomial(n=1, p=.01, size=(1000, 10)) df = DataFrame(arr).to_sparse(fill_value=0)
self._check_double_roundtrip(df, tm.assert_frame_equal, compression=False, check_frame_type=True)
self._check_double_roundtrip(df, tm.assert_frame_equal, compression='zlib', check_frame_type=True)
df[0] = np.zeros(1000)
self._check_double_roundtrip(df, tm.assert_frame_equal, compression=False, check_frame_type=True)
self._check_double_roundtrip(df, tm.assert_frame_equal, compression='zlib', check_frame_type=True)
_maybe_remove(store, 'wp') store.put('wp', wp, format='table') store.select('wp')
_maybe_remove(store, 'wp') store.put('wp2', wp) store.select('wp2')
df.iloc[0] = np.nan expected = df[df['values'] > 2.0]
with ensure_clean_store(self.path) as store: df = tm.makeDataFrame()
with ensure_clean_store(self.path) as store:
chunksize = 1e4
with ensure_clean_store(self.path) as store:
result = store.select('df') tm.assert_frame_equal(expected, result)
where = "index >= '%s'" % beg_dt result = store.select('df', where=where) tm.assert_frame_equal(expected, result)
where = "index <= '%s'" % end_dt result = store.select('df', where=where) tm.assert_frame_equal(expected, result)
with ensure_clean_store(self.path) as store:
results = [s for s in store.select('df', chunksize=chunksize)] result = concat(results) tm.assert_frame_equal(expected, result)
chunksize = 1e4
with ensure_clean_store(self.path) as store:
with ensure_clean_store(self.path) as store:
chunksize = int(1e4)
with ensure_clean_store(self.path) as store:
where = "index <= '%s'" % end_dt results = [s for s in store.select( 'df', where=where, chunksize=chunksize)]
tm.assert_equal(0, len(results))
df = DataFrame(dict( A=Series(lrange(3), index=date_range('2000-1-1', periods=3, freq='H'))))
df = tm.makeTimeDataFrame() store.append('df_time', df) self.assertRaises( ValueError, store.select, 'df_time', [Term("index>0")])
self.assertRaises(NotImplementedError, store.select, 'df', '~(string="bar")')
result = read_hdf(hh, 'df', where=Term( 'l1', '=', selection.index.tolist())) assert_frame_equal(result, expected)
store = HDFStore(hh) result = store.select('df', where='l1=l') assert_frame_equal(result, expected) store.close()
store = HDFStore(hh)
self.assertRaises(NotImplementedError, store.select, 'df', "columns=['A'] | columns=['B']")
self.assertRaises(NotImplementedError, store.select, 'df', "columns=['A','B'] & columns=['C']")
with ensure_clean_store(self.path) as store:
df['x'] = 'none' df.ix[2:7, 'x'] = ''
df['int'] = 1 df.ix[2:7, 'int'] = 2
self.assertRaises(KeyError, store.select_column, 'df', 'foo')
result = store.select_column('df', 'index') tm.assert_almost_equal(result.values, Series(df.index).values) self.assertIsInstance(result, Series)
self.assertRaises( ValueError, store.select_column, 'df', 'values_block_0')
result = store.select_column('df3', 'string', start=2) tm.assert_almost_equal(result.values, df3['string'].values[2:])
c = store.select_as_coordinates('df') assert((c.values == np.arange(len(df.index))).all())
_maybe_remove(store, 'df')
with ensure_clean_store(self.path) as store:
result = store.select('df', where=where) tm.assert_frame_equal(result, expected)
result = store.select('df', where=where) tm.assert_frame_equal(result, expected)
where = [True] * 10 where[-2] = False result = store.select('df2', where=where) expected = df.loc[where] tm.assert_frame_equal(result, expected)
result = store.select('df2', start=5, stop=10) expected = df[5:10] tm.assert_frame_equal(result, expected)
self.assertRaises(Exception, store.select_as_multiple, None, where=['A>0', 'B>0'], selector='df1')
if LooseVersion(tables.__version__) < '3.1.0': raise nose.SkipTest('tables version does not support fix for nan ' 'selection bug: GH 4858')
df = DataFrame(dict(A=np.random.rand(20), B=np.random.rand(20))) store.append('df', df)
result = store.select( 'df', start=30, stop=40) expected = df.iloc[30:40, :] tm.assert_frame_equal(result, expected)
comparator(retrieved, obj)
store1 = HDFStore(path)
store1 = HDFStore(path) store2 = HDFStore(path)
store = HDFStore(path, mode='w') store.append('df', df)
store = HDFStore(path, mode='w') store.append('df', df)
with ensure_clean_path(self.path) as path:
if PY35 and is_platform_windows(): raise nose.SkipTest("native2 read fails oddly on windows / 3.5")
store.select('df2', typ='legacy_frame')
with tm.assert_produces_warning( expected_warning=IncompatibilityWarning): self.assertRaises( Exception, store.select, 'wp1', Term('minor_axis=B'))
if keys is None: keys = store.keys() self.assertEqual(set(keys), set(tstore.keys()))
for k in tstore.keys(): if tstore.get_storer(k).is_table: new_t = tstore.get_storer(k) orig_t = store.get_storer(k)
if propindexes: for a in orig_t.axes: if a.is_indexed: self.assertTrue(new_t[a.name].is_indexed)
df = tm.makeDataFrame()
self.assertTrue('/df2 ' in str(store)) self.assertTrue('/df2/meta/values_block_0/meta' in str(store)) self.assertTrue('/df2/meta/values_block_1/meta' in str(store))
store.append('df3', df)
df3 = df.copy() df3['s'].cat.remove_unused_categories(inplace=True)
df = DataFrame({"B": [1, 2], "A": ["x", "y"]})
tm._skip_if_no_pathlib()
tm._skip_if_no_localpath()
from pandas.tslib import maybe_get_tz gettz = lambda x: maybe_get_tz('dateutil/' + x)
with ensure_clean_store(self.path) as store:
expected = df[df.A >= df.A[3]] result = store.select('df_tz', where=Term('A>=df.A[3]')) self._compare_with_tz(result, expected)
with ensure_clean_store(self.path) as store:
df = DataFrame(dict(A=Series(lrange(3), index=date_range( '2000-1-1', periods=3, freq='H', tz=gettz('US/Eastern')))))
with ensure_clean_store(self.path) as store:
self._compare_with_tz(store.select( 'df_tz', where=Term('A>=df.A[3]')), df[df.A >= df.A[3]])
with ensure_clean_store(self.path) as store:
df = DataFrame(dict(A=Series(lrange(3), index=date_range( '2000-1-1', periods=3, freq='H', tz='US/Eastern'))))
rng = date_range('1/1/2000', '1/30/2000') frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
rng = date_range('1/1/2000', '1/30/2000', tz='UTC') frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
rng = date_range('1/1/2000', '1/30/2000', tz='US/Eastern') frame = DataFrame(np.random.randn(len(rng), 4), index=rng)
with ensure_clean_store(self.path) as store:
orig_tz = os.environ.get('TZ')
from pandas.compat import range, lrange, StringIO, OrderedDict import os
assert_almost_equal(df.values, unser.values, check_dtype=check_numpy_dtype) self.assert_index_equal(df.columns, unser.columns, exact=check_column_type)
if numpy is True and df.shape == (0, 0): assert unser.shape[0] == 0 else: assert_almost_equal(df.values, unser.values, check_dtype=check_numpy_dtype)
unser.index = [str(i) for i in unser.index] unser.columns = [str(i) for i in unser.columns]
_check_all_orients(self.frame) self.assertEqual(self.frame.to_json(), self.frame.to_json(orient="columns"))
_check_all_orients(self.categorical, sort='sort', raise_ok=ValueError)
_check_all_orients(self.empty_frame, check_index_type=False, check_column_type=False)
_check_all_orients(self.tsframe)
df_mixed.columns = df_mixed.columns.astype('unicode')
_check_all_orients(self.series) self.assertEqual(self.series.to_json(), self.series.to_json(orient="index"))
self.assertEqual(self.empty_series.index.dtype, np.object_) _check_all_orients(self.empty_series, check_index_type=False)
json = self.tsframe.to_json() result = read_json(json) assert_frame_equal(result, self.tsframe)
df = self.tsframe.copy() df['date'] = Timestamp('20130101')
result = read_json(json, date_unit=unit) assert_frame_equal(result, df)
result = read_json(json, date_unit=None) assert_frame_equal(result, df)
tz_range = pd.date_range('20130101', periods=3, tz='US/Eastern') tz_naive = tz_range.tz_convert('utc').tz_localize(None)
df = pd.DataFrame(np.random.randn(10, 4)) df.ix[:8] = np.nan
helper(not_html_encoded, ensure_ascii=True) helper(not_html_encoded, ensure_ascii=False)
helper(not_html_encoded, ensure_ascii=True, encode_html_chars=False) helper(not_html_encoded, ensure_ascii=False, encode_html_chars=False)
helper(html_encoded, ensure_ascii=True, encode_html_chars=True) helper(html_encoded, ensure_ascii=False, encode_html_chars=True)
self.assertEqual(input, ujson.decode(output)) tm.assert_numpy_array_equal( np.array(input), ujson.decode(output, numpy=True))
self.assertRaises(TypeError, ujson.encode, input, double_precision='9') self.assertRaises(TypeError, ujson.encode, input, double_precision=None)
outp = Series(ujson.decode(ujson.encode(s))).sort_values() self.assertTrue((s == outp).values.all())
outp = Index(ujson.decode(ujson.encode(i)), name='index') tm.assert_index_equal(i, outp)
arr = np.arange(15, dtype=np.float64) arr[7:12] = nan arr[-1:] = nan
arr = np.arange(15, dtype=np.float64) arr[7:12] = nan arr[-1:] = nan
try:
sys.path.insert(0, '.')
with tm.assert_produces_warning(FutureWarning): df = make_mixed_dataframe_v2(10) gbq.generate_bq_schema(df)
pass
pass
pass
pass
with tm.assertRaises(gbq.TableCreationError): gbq.to_gbq(df, destination_table, PROJECT_ID)
with tm.assertRaises(gbq.TableCreationError): gbq.to_gbq(df, destination_table, PROJECT_ID, if_exists='fail')
gbq.to_gbq(df, destination_table, PROJECT_ID, chunksize=10000)
gbq.to_gbq(df, destination_table, PROJECT_ID, if_exists='append')
with tm.assertRaises(gbq.InvalidSchema): gbq.to_gbq(df_different_schema, destination_table, PROJECT_ID, if_exists='append')
gbq.to_gbq(df, destination_table, PROJECT_ID, chunksize=10000)
gbq.to_gbq(df_different_schema, destination_table, PROJECT_ID, if_exists='replace')
pass
pass
pass
pass
if ver < '0.8.2': from sqlalchemy import BigInteger from sqlalchemy.ext.compiler import compiles
format = 's' if format is None else format return to_datetime(col, errors='coerce', unit=format, utc=True)
return (to_datetime(col, errors='coerce') .astype('datetime64[ns, UTC]'))
if parse_dates is True or parse_dates is None or parse_dates is False: parse_dates = []
for col_name, df_col in data_frame.iteritems(): if com.is_datetime64tz_dtype(df_col): data_frame[col_name] = _handle_date_column(df_col)
result = list(lzip(*result)[0])
self.table = self._create_table_setup()
self.table = self.pd_sql.get_table(self.name, self.schema)
self.table = self.table.tometadata(self.pd_sql.meta) self.table.create()
d = b.values.astype('M8[us]').astype(object)
if b._can_hold_na: mask = isnull(d) d[mask] = None
elif isinstance(index, string_types): return [index] elif isinstance(index, list): return index else: return None
from sqlalchemy.schema import MetaData meta = MetaData(self.pd_sql, schema=schema)
if parse_dates is True or parse_dates is None or parse_dates is False: parse_dates = []
col_type = self._get_dtype(sql_col.type)
self.frame[col_name] = df_col.astype(col_type, copy=False)
if col_type is np.dtype('int64') or col_type is bool: self.frame[col_name] = df_col.astype( col_type, copy=False)
if col_name in parse_dates: try: fmt = parse_dates[col_name] except TypeError: fmt = None self.frame[col_name] = _handle_date_column( df_col, format=fmt)
if not sqltype.timezone: return datetime return DatetimeTZDtype
return datetime
from sqlalchemy import Numeric for column in tbl.columns: if isinstance(column.type, Numeric): column.type.asdecimal = False
uname = _get_unicode_name(name) if not len(uname): raise ValueError("Empty table or column name specified")
_SQL_WILDCARD = { 'mysql': '%s', 'sqlite': '?' }
_SQL_GET_IDENTIFIER = { 'mysql': _get_valid_mysql_name, 'sqlite': _get_valid_sqlite_name, }
import sqlite3 sqlite3.register_adapter(time, lambda _: _.strftime("%H:%M:%S.%f")) super(SQLiteTable, self).__init__(*args, **kwargs)
import warnings warnings.warn("The pandas.io.ga module is deprecated and will be " "removed in a future version.", FutureWarning, stacklevel=2)
return self.df.index.tolist()[section]
value = value.toPyObject()
dtype = self.df[col].dtype if dtype != object: value = None if value == '' else dtype.type(value)
self.setDataFrame(dataFrame)
factors = list(r['factor'](obj)) level = list(r['levels'](obj)) result = [level[index-1] for index in factors] return result
vec = vec.astype(int)
df = r['faithful']
frame["E"] = [np.nan for item in frame["A"]]
frame["F"] = ["text" if item % 2 == 0 else np.nan for item in range(30)]
exit=False)
import versioneer cmdclass = versioneer.get_cmdclass()
from distutils.core import setup, Command _have_setuptools = False
sdist_class = cmdclass['sdist']
if is_platform_windows(): extra_compile_args=[] else: extra_compile_args=['-Wno-unused-function']
libraries = ['m'] if not is_platform_windows() else []
_move_ext = Extension('pandas.util._move', depends=[], sources=['pandas/util/move.c']) extensions.append(_move_ext)
if not s: return
timeseries_custom_bmonthend_incr = \ Benchmark("date + cme",setup)
results.columns = db._results.c.keys() results = results.join(bench['name'], on='checksum').set_index("checksum") return results
GitRepo._parse_commit_log = _parse_wrapper(args.base_commit)
module_dependencies=dependencies)
(repo.shas, repo.messages, repo.timestamps, repo.authors) = _parse_commit_log(None,REPO_PATH, args.base_commit)
shutil.rmtree(TMP_DIR)
totals = totals.dropna(
gc.collect()
results = results[args.burnin:]
args.outdf = os.path.realpath(args.outdf)
args.log_file = os.path.realpath(args.log_file)
os.chdir(os.path.dirname(os.path.abspath(__file__)))
def _parse_commit_log(this,repo_path,base_commit=None): from vbench.git import _convert_timezones from pandas import Series from dateutil import parser as dparser
stamp = dparser.parse(stamp)
timestamps = _convert_timezones(timestamps)
def _parse_wrapper(base_commit): def inner(repo_path): return _parse_commit_log(repo_path,base_commit) return inner
module_dependencies=dependencies)
results[k] = v
index_float64_get = Benchmark('idx[1]', setup, name='index_float64_get', start_date=datetime(2014, 4, 13))
eval_frame_add_all_threads = \ Benchmark("pd.eval('df + df2 + df3 + df4')", common_setup, name='eval_frame_add_all_threads', start_date=datetime(2013, 7, 21))
eval_frame_chained_cmp_all_threads = \ Benchmark("pd.eval('df < df2 < df3 < df4')", common_setup, name='eval_frame_chained_cmp_all_threads', start_date=datetime(2013, 7, 21))
df_float.ix[30:500,1:3] = np.nan
@test_parallel(num_threads=2) def pg2(): f()
groupby_frame_nth_none = Benchmark("df.groupby(0).nth(0)", setup, start_date=datetime(2014, 3, 1))
bmark.name = bmark_name return bmark
del bmark
plt.figure(figsize=(10, 6)) ax = plt.gca() bmk.plot(DB_PATH, ax=ax)
frame_ctor_list_of_dict = Benchmark("DataFrame(dict_list)", setup, start_date=datetime(2011, 12, 20))
globals().update(dynamic_benchmarks)
frame_iteritems = Benchmark('f()', setup, start_date=datetime(2010, 6, 1))
os.path.join(os.path.dirname(__file__), '..', '../..', 'sphinxext')
templates_path = ['_templates', '_templates/autosummary']
source_suffix = '.rst'
master_doc = 'index'
project = u'pandas' copyright = u'2008-2011, the pandas development team'
import pandas
version = '%s' % (pandas.__version__)
release = version
autosummary_generate = True
exclude_trees = []
pygments_style = 'sphinx'
html_theme = 'agogo'
html_theme_path = ['themes']
html_title = 'Vbench performance benchmarks for pandas'
html_static_path = ['_static']
html_use_modindex = True
htmlhelp_basename = 'performance'
latex_documents = [ ('index', 'performance.tex', u'pandas vbench Performance Benchmarks', u'Wes McKinney', 'manual'), ]
import glob autosummary_generate = glob.glob("*.rst")
try: from pandas.core.index import MultiIndex except ImportError: pass
for i in range(i+1,20):
raise NotImplementedError
raise NotImplementedError
self.df = DataFrame(np.random.randn(10000, 50)) self.df.to_csv('__test__.csv')
goal_time = 0.2
PeriodIndex(date_range('1985', periods=1000).to_pydatetime(), freq='D')
try: from pandas.core.index import MultiIndex except ImportError: pass
LONG_VERSION_PY = {} HANDLERS = {}
p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None)) break
p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None)) break
git_describe = describe_out
dirty = git_describe.endswith("-dirty") pieces["dirty"] = dirty if dirty: git_describe = git_describe[:git_describe.rindex("-dirty")]
pieces["distance"] = int(mo.group(2))
pieces["short"] = mo.group(3)
pieces["closest-tag"] = None count_out = run_command(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
rendered = "0+untagged.%%d.g%%s" %% (pieces["distance"], pieces["short"]) if pieces["dirty"]: rendered += ".dirty"
rendered = "0.post.dev%%d" %% pieces["distance"]
rendered = "0.post%%d" %% pieces["distance"] if pieces["dirty"]: rendered += ".dev0" rendered += "+g%%s" %% pieces["short"]
rendered = "0.post%%d" %% pieces["distance"] if pieces["dirty"]: rendered += ".dev0"
rendered = pieces["short"]
rendered = pieces["short"]
for i in cfg.versionfile_source.split('/'): root = os.path.dirname(root)
git_describe = describe_out
dirty = git_describe.endswith("-dirty") pieces["dirty"] = dirty if dirty: git_describe = git_describe[:git_describe.rindex("-dirty")]
pieces["distance"] = int(mo.group(2))
pieces["short"] = mo.group(3)
pieces["closest-tag"] = None count_out = run_command(GITS, ["rev-list", "HEAD", "--count"], cwd=root)
rendered = "0+untagged.%d.g%s" % (pieces["distance"], pieces["short"]) if pieces["dirty"]: rendered += ".dirty"
rendered = "0.post.dev%d" % pieces["distance"]
rendered = "0.post%d" % pieces["distance"] if pieces["dirty"]: rendered += ".dev0" rendered += "+g%s" % pieces["short"]
rendered = "0.post%d" % pieces["distance"] if pieces["dirty"]: rendered += ".dev0"
rendered = pieces["short"]
rendered = pieces["short"]
del sys.modules["versioneer"]
from distutils.core import Command
if cfg.versionfile_build: target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build) print("UPDATING %s" % target_versionfile) write_to_version_file(target_versionfile, versions)
if "setuptools" in sys.modules: from setuptools.command.sdist import sdist as _sdist else: from distutils.command.sdist import sdist as _sdist
self.distribution.metadata.version = versions["version"] return _sdist.run(self)
target_versionfile = os.path.join(base_dir, cfg.versionfile_source) print("UPDATING %s" % target_versionfile) write_to_version_file(target_versionfile, self._versioneer_generated_versions)
do_vcs_install(manifest_in, cfg.versionfile_source, ipy) return 0
results.columns = ['dont_sort', 'sort']
import sqlite3 create_sql_indexes = True
if create_sql_indexes: conn.execute('create index left_ix on left(key, key2)') conn.execute('create index right_ix on right(key, key2)')
g() elapsed = (time.time() - start) / niter gc.enable()
counts = {}
for k, v in zip(x, data): try: counts[k] += v except KeyError: counts[k] = v
data = np.random.randn(N)
from pandas import * from pandas.util.testing import rands
dma1 = pandas.DataFrame(arr1, idx1, idx2) dma2 = pandas.DataFrame(arr2, idx1[::-1], idx2[::-1])
lar1 = la.larry(arr1, [idx1, idx2]) lar2 = la.larry(arr2, [idx1[::-1], idx2[::-1]])
arr = np.random.randn(N, N) lar = la.larry(arr) dma = pandas.DataFrame(arr, lrange(N), lrange(N))
filename_numpy = '/Users/wesm/tmp/numpy.npz' filename_larry = '/Users/wesm/tmp/archive.hdf5' filename_pandas = '/Users/wesm/tmp/pandas_tmp'
try: os.unlink(filename_numpy) except: pass try: os.unlink(filename_larry) except: pass try: os.unlink(filename_pandas) except: pass
numpy_f = lambda: numpy_roundtrip(filename_numpy, arr, arr) numpy_time = timeit(numpy_f, iterations) / iterations
return table
import numpy as np import itertools import collections import scipy.ndimage as ndi from pandas.compat import zip, range
import numpy as np
if len(panels) == 0: return None elif len(panels) == 1: return panels[0] elif len(panels) == 2 and panels[0] == panels[1]: return panels[0]
from __future__ import print_function from pandas import DataFrame from pandas.compat import range, zip import timeit
wes_timer = timeit.Timer(stmt='_tseries.fast_unique(arr)', setup=setup % sz)
filename_numpy = '/Users/wesm/tmp/numpy.npz' filename_larry = '/Users/wesm/tmp/archive.hdf5' filename_pandas = '/Users/wesm/tmp/pandas_tmp'
try: os.unlink(filename_numpy) except: pass try: os.unlink(filename_larry) except: pass
try: os.unlink(filename_numpy) except: pass try: os.unlink(filename_larry) except: pass
os.path.join(os.path.dirname(__file__), '..', '../..', 'sphinxext')
autosummary_generate = False
templates_path = ['../_templates']
source_suffix = '.rst'
source_encoding = 'utf-8'
master_doc = 'index'
project = u('pandas') copyright = u('2008-2014, the pandas development team')
import pandas
version = '%s' % (pandas.__version__)
release = version
exclude_trees = []
pygments_style = 'sphinx'
html_theme = 'nature_with_gtoc'
html_theme_path = ['themes']
html_static_path = ['_static']
html_use_modindex = True
htmlhelp_basename = 'pandas'
extlinks = {'issue': ('https://github.com/pydata/pandas/issues/%s', 'GH'), 'wiki': ('https://github.com/pydata/pandas/wiki/%s', 'wiki ')}
'pd.options.display.encoding="utf8"' ]
priority = 0.5
def remove_flags_docstring(app, what, name, obj, options, lines): if what == "attribute" and name.endswith(".flags"): del lines[:]
self.start_lineno = start_lineno self.end_lineno = end_lineno self.text = text
self.start_lineno = min(self.start_lineno, start[0]) self.end_lineno = max(self.end_lineno, end[0])
self.current_block = NonComment(0, 0)
self.blocks = []
self.index = {}
self.current_block.add(string, start, end, line)
block = Comment(start[0], end[0], string) self.blocks.append(block) self.current_block = block
from docutils.parsers.rst import directives from docutils import nodes
return True
import jinja2 def format_template(template, **kw): return jinja2.Template(template).render(**kw)
rst_file = document.attributes['source'] rst_dir = os.path.dirname(rst_file)
output_base = output_base.replace('.', '-')
is_doctest = contains_doctest(code) if 'format' in options: if options['format'] == 'python': is_doctest = False else: is_doctest = True
source_rel_name = relpath(source_file_name, setup.confdir) source_rel_dir = os.path.dirname(source_rel_name) while source_rel_dir.startswith(os.path.sep): source_rel_dir = source_rel_dir[1:]
dest_dir = os.path.abspath(os.path.join(setup.app.builder.outdir, source_rel_dir))
if not os.path.exists(dest_dir): os.makedirs(dest_dir)
compile(text, '<string>', 'exec') return False
stdout = sys.stdout sys.stdout = StringIO()
old_sys_argv = sys.argv sys.argv = [code_path]
if not all_exists: all_exists = (j > 0) break images.append(img)
plt.close('all')
run_code(code_piece, code_path, ns)
results.append((code_piece, images))
i = len(commonprefix([start_list, path_list]))
for i in range(min(len(start_list), len(path_list))): if start_list[i].lower() != path_list[i].lower(): break else: i += 1
NumpyDocString.__init__(self, docstring, config=config)
def _str_header(self, name, symbol='`'): return ['.. rubric:: ' + name, '']
param_obj = getattr(self._obj, param, None) if not (callable(param_obj) or isinstance(param_obj, property) or inspect.isgetsetdescriptor(param_obj)): param_obj = None
info = {} for key in domain_keys.get(domain, []): value = signode.get(key) if not value: value = '' info[key] = value if not info: continue
uri = resolve_target(domain, info) if not uri: continue
continue
if what == "class" and name.endswith(".Categorical"): cfg['class_members_list'] = False
app.add_domain(NumpyPythonDomain) app.add_domain(NumpyCDomain)
for element in t.nodes[:-1]: self._dispatch(element) self._write(", ")
last_element = t.nodes[-1] self._dispatch(last_element)
self._write("]")
self._write("()")
for element in t.nodes[:-1]: self._dispatch(element) self._write(", ")
last_element = t.nodes[-1] self._dispatch(last_element)
self._write(str(t))
from __future__ import division, absolute_import, print_function
from __future__ import division, absolute_import, print_function
from __future__ import division, absolute_import, print_function
from __future__ import division, absolute_import, print_function
from __future__ import division, absolute_import, print_function
try: indent = min(len(s) - len(s.lstrip()) for s in docstring if s.strip()) except ValueError: indent = 0
if not name.startswith('_'): doc['Traits'].append((name, trait, comment.splitlines()))
numpydoc.setup(app, get_doc_object)
all_nodes = dict([(n.attrib['id'], n) for n in root])
import re
from pygments.lexer import Lexer, do_insertions from pygments.lexers.agile import (PythonConsoleLexer, PythonLexer, PythonTracebackLexer) from pygments.token import Comment, Generic
line_re = re.compile('.*?\n')
insertions.append((len(curcode), [(0, Generic.Error, output_prompt.group())])) curcode += line[output_prompt.end():]
highlighting.lexers['ipython'] = IPythonConsoleLexer()
import os import re import sys import tempfile import ast from pandas.compat import zip, range, map, lmap, u, cStringIO as StringIO import warnings
try: from hashlib import md5 except ImportError: from md5 import md5
import sphinx from docutils.parsers.rst import directives from docutils import nodes from sphinx.util.compat import Directive
try: from traitlets.config import Config except ImportError: from IPython import Config from IPython import InteractiveShell from IPython.core.profiledir import ProfileDir from IPython.utils import io from IPython.utils.py3compat import PY3
COMMENT, INPUT, OUTPUT = range(3)
break
decorator = line_stripped continue
matchin = rgxin.match(line) if matchin: lineno, inputline = int(matchin.group(1)), matchin.group(2)
return super(DecodingStringIO, self).write(data.decode('utf8', 'replace'))
config = Config() config.InteractiveShell.autocall = False config.InteractiveShell.autoindent = False config.InteractiveShell.colors = 'NoColor'
tmp_profile_dir = tempfile.mkdtemp(prefix='profile_') profname = 'auto_profile_sphinx_build' pdir = os.path.join(tmp_profile_dir,profname) profile = ProfileDir.create_profile_dir(pdir)
IP = InteractiveShell.instance(config=config, profile_dir=profile)
io.stdout = self.cout io.stderr = self.cout
#from IPython.utils.io import Tee
self.IP = IP self.user_ns = self.IP.user_ns self.user_global_ns = self.IP.user_global_ns
self.directive = None
self._pyplot_imported = False
for line in exec_lines: self.process_input_line(line, store_history=False)
source_raw = splitter.raw_reset()
outfile = os.path.relpath(os.path.join(savefig_dir,filename), source_dir)
self.cout.set_encodings(self.output_encoding)
with warnings.catch_warnings(record=True) as ws: for i, line in enumerate(input_lines): if line.endswith(';'): is_semicolon = True
if is_verbatim: self.process_input_line('')
self.process_input_line(line, store_history=store_history)
if not is_verbatim: self.process_input_line(line, store_history=store_history)
ret.append(rest)
filename = self.state.document.current_source lineno = self.state.document.current_line
content = '\n'.join([TAB + line for line in content])
if image_file is not None: self.save_image(image_file)
#raise Exception("No backend was set, but @figure was used!") import matplotlib matplotlib.use('agg')
self.process_input_line('import matplotlib.pyplot as plt', store_history=False) self._pyplot_imported = True
if line_stripped.startswith('@'): output.extend([line]) if 'savefig' in line:
if line_stripped.startswith('#'): output.extend([line]) continue
config = self.state.document.settings.env.config
rgxin = config.ipython_rgxin rgxout = config.ipython_rgxout promptin = config.ipython_promptin promptout = config.ipython_promptout mplbackend = config.ipython_mplbackend exec_lines = config.ipython_execlines hold_count = config.ipython_holdcount
(savefig_dir, source_dir, rgxin, rgxout, promptin, promptout, mplbackend, exec_lines, hold_count) = self.get_config_options()
matplotlib.use(mplbackend)
self.shell = EmbeddedSphinxShell(exec_lines, self.state)
self.shell.directive = self
self.shell.process_input_line('bookmark ipy_savedir %s'%savefig_dir, store_history=False) self.shell.clear_cout()
self.shell.process_input_line('bookmark -d ipy_savedir', store_history=False) self.shell.clear_cout()
rgxin, rgxout, promptin, promptout = self.setup()
if 'python' in self.arguments: content = self.content self.content = self.shell.process_pure_python(content)
self.teardown()
def setup(app): setup.app = app
app.add_config_value('ipython_mplbackend', 'agg', 'env')
execlines = ['import numpy as np', 'import matplotlib.pyplot as plt'] app.add_config_value('ipython_execlines', execlines, 'env')
def test():
@doctest In [2]: x.upper() Out[2]: 'HELLO WORLD'
@savefig test_hist.png width=4in In [151]: hist(np.random.randn(10000), 100);
In [151]: plt.clf()
In [151]: ylabel('number')
examples = examples[1:]
os.system('rm source/html-styling.html') os.system('cd build; rm -f html/pandas.zip;')
os.system('cd build; rm -f html/pandas.zip; zip html/pandas.zip -r -q html/* ') print("\n")
if os.system('sphinx-build -b latex -d build/doctrees ' 'source build/latex'): raise SystemExit("Building LaTeX failed.")
if os.system('sphinx-build -b latex -d build/doctrees ' 'source build/latex'): raise SystemExit("Building LaTeX failed.")
html()
fancy = False
stochastic = False
uniform_batch_size = False
uniform_batch_size = True
fancy = None
stochastic = None
_base_iterator_cls = None
while length != self.batch_size: batch = self._base_iterator.next()
self.lengths = [len(s) for s in self._sequence_data] self.len_unique = np.unique(self.lengths)
if self.total_curr_counts == 0: self.reset() raise StopIteration()
while True: self.len_idx = np.mod(self.len_idx+1, len(self.len_unique)) curr_len = self.len_unique[self.len_idx] if self.len_curr_counts[curr_len] > 0: break
curr_batch_size = np.minimum(self._batch_size, self.len_curr_counts[curr_len]) curr_pos = self.len_indices_pos[curr_len]
curr_indices = self.len_indices[curr_len][curr_pos:curr_pos + curr_batch_size]
self.len_indices_pos[curr_len] += curr_batch_size self.len_curr_counts[curr_len] -= curr_batch_size self.total_curr_counts -= curr_batch_size return curr_indices
assert is_flat_specs(data_specs)
if hasattr(self._dataset, 'get'): rval = self._next(next_index) else: rval = self._fallback_next(next_index)
if hasattr(self, 'usesTime') and self.usesTime(): record.asctime = self.formatTime(record, self.datefmt)
if not record.exc_text: record.exc_text = self.formatException(record.exc_info)
s = s + record.exc_text.decode(sys.getfilesystemencoding())
top_level_logger.propagate = False
top_level_logger.setLevel(logging.DEBUG if debug else logging.INFO)
while top_level_logger.handlers: top_level_logger.handlers.pop()
fmt = CustomFormatter() handler = CustomStreamHandler(stdout=stdout, stderr=stderr, formatter=fmt) top_level_logger.addHandler(handler)
top_level_logger.propagate = True
top_level_logger.setLevel(logging.NOTSET)
while top_level_logger.handlers: top_level_logger.handlers.pop()
number_aware_alphabetical_key = cmp_to_key(number_aware_alphabetical_cmp)
new_message = ', '.join(str(arg) for arg in new_exc.args)
yaml_parse = None control = None cuda = None
if six.PY3: py_integer_types = (int, np.integer) py_number_types = (int, float, complex, np.number) else:
new_f.func_name = f.func_name return new_f
return wrapper
unknown = [k for k, w in known.items() if not w] known = dict((k, w) for k, w in known.items() if w)
return first_line.split(':')[2][0:10]
protocol_str = '0'
lush_magic = { 507333717: 'uint8', 507333716: 'int32', 507333713: 'float32', 507333715: 'float64' }
if config_file_path.endswith(suffix_to_strip): config_file_full_stem = config_file_path[0:-len(suffix_to_strip)] else: config_file_full_stem = config_file_path
assert False
encoding = {'encoding': 'latin-1'} if six.PY3 else {}
reraise_as("Couldn't open {0}".format(filepath))
reraise_as(IOError("Cannot open " + path + " but can open " + parent + "."))
assert False
image = image * 255. image = np.cast['uint8'](image)
if len(image.shape) == 3 and image.shape[2] == 1: image = image[:, :, 0]
fd, name = mkstemp(suffix='.png') os.close(fd)
out_shape = [(ishp + tsp) * tshp - tsp for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)]
if output_pixel_vals: channel_defaults = [0, 0, 0, 255] else: channel_defaults = [0., 0., 0., 1.]
dt = out_array.dtype if output_pixel_vals: dt = 'uint8' out_array[:, :, i] = np.zeros(out_shape, dtype=dt) + \ channel_defaults[i]
out_array[:, :, i] = tile_raster_images( X[i], img_shape, tile_shape, tile_spacing, scale_rows_to_unit_interval, output_pixel_vals)
H, W = img_shape Hs, Ws = tile_spacing
dt = X.dtype if output_pixel_vals: dt = 'uint8' out_array = np.zeros(out_shape, dtype=dt)
this_img = scale_to_unit_interval( this_x.reshape(img_shape))
import logging import os import inspect import zipfile from tempfile import TemporaryFile
import numpy import theano from pylearn2.datasets.utlc import load_ndarray_dataset, load_sparse_dataset from pylearn2.utils import subdict, sharedX
expected = inspect.getargspec(load_ndarray_dataset)[0][1:] data = load_ndarray_dataset(conf['dataset'], **subdict(conf, expected))
if conf.get('normalize_on_the_fly', False): return data
if (valid_repr.shape[1] > valid_repr.shape[0]): valid_repr = numpy.dot(valid_repr, valid_repr.T) test_repr = numpy.dot(test_repr, test_repr.T)
valid_repr = numpy.floor((valid_repr / valid_repr.max())*999) test_repr = numpy.floor((test_repr / test_repr.max())*999)
valid_file = TemporaryFile() test_file = TemporaryFile()
valid_file.seek(0) test_file.seek(0)
if not conf.get('sparse', False): valid_set = valid_set.get_value(borrow=True) test_set = test_set.get_value(borrow=True)
if features is not None: valid_set = valid_set[:, features] test_set = test_set[:, features]
valid_repr = transform_valid(valid_set) test_repr = transform_test(test_set)
save_submission(conf, valid_repr, test_repr)
n_valid = valid_repr.shape[0] n_test = test_repr.shape[0]
import logging import os import functools from itertools import repeat import warnings
from pylearn2.utils.rng import make_np_rng
x, y, z = repr.get_value(borrow=True).T do_3d_scatter(x, y, z)
if classes is not None: label = label[:, classes]
if scipy.sparse.issparse(train): idx = label.sum(axis=1).nonzero()[0] return (train[idx], label[idx])
condition = label.any(axis=1) return tuple(var.compress(condition, axis=0) for var in (train, label))
masks = numpy.asarray([subset.sum(axis=0) for subset in data]).squeeze() nz_feats = combine(masks).nonzero()[0]
flo = numpy.floor sub = numpy.subtract mul = numpy.multiply div = numpy.divide mod = numpy.mod
self.batch_size = batch_size if (isinstance(dataset[0], theano.Variable)): self.dataset = [set.get_value(borrow=True) for set in dataset] else: self.dataset = dataset
set_limit = numpy.ceil(numpy.divide(set_sizes, set_batch)) self.limit = map(int, set_limit)
set_tsign = sub(set_limit, flo(div(set_sizes, set_batch))) set_tsize = mul(set_tsign, flo(div(set_range, set_limit)))
index_tab = [] for i in xrange(3): index_tab.extend(repeat(i, set_range[i]))
self.seed = seed rng = make_np_rng(seed, which_method="permutation") self.permut = rng.permutation(index_tab)
index = counter[chosen] minibatch = self.dataset[chosen][ index * self.batch_size:(index + 1) * self.batch_size ] counter[chosen] = (counter[chosen] + 1) % self.limit[chosen] yield minibatch
assert len(str(e))
from pylearn2.utils import utlc
handlers = logger.handlers level = logger.getEffectiveLevel()
assert handlers == logger.handlers assert level == logger.getEffectiveLevel()
iterator = SequentialSubsetIterator(10, 3, 4) for i in range(4): iterator.next()
iterator = SequentialSubsetIterator(10, 3, 3) for i in range(3): iterator.next()
iterator = SequentialSubsetIterator(10, 3, 5)
self.n_unique_specs = 0
assert source == '' return None
if isinstance(source, (tuple, list)): source, = source
spec_mapping = tuple( self._fill_mapping(sub_space, sub_source) for sub_space, sub_source in safe_zip( space.components, source))
rval = [None] * self.n_unique_specs
self._fill_flat(nested, self.spec_mapping, rval)
return None
idx = mapping if isinstance(flat, (tuple, list)): assert 0 <= idx < len(flat) return flat[idx] else: assert idx == 0 return flat
idx = mapping if isinstance(flat, CompositeSpace): assert 0 <= idx < len(flat.components) return flat.components[idx] else: assert idx == 0 return flat
assert self.n_unique_specs == 1
res_r = int(numpy.floor(last_pool_r/rs)) + 1 res_c = int(numpy.floor(last_pool_c/cs)) + 1
window = tensor.alloc(0.0, batch, channel, res_r, res_c, pr, pc) window.name = 'unravlled_winodows_' + name
res_r = int(numpy.floor(last_pool_r/rs)) + 1 res_c = int(numpy.floor(last_pool_c/cs)) + 1
window = tensor.alloc(0.0, batch, channel, res_r, res_c, pr, pc) window.name = 'unravlled_winodows_' + name
n_classes = n_classes.astype(theano.config.floatX) return sm * (1 - n_classes * min_val) + min_val
precision = [1.] recall = [0.] tp = 0 fp = 0 fn = len(pos_scores) count = fn
p_shared = sharedX(zv[:, 0:rows:pool_rows, 0:cols:pool_cols, :]) h_shared = sharedX(zv) z_shared = sharedX(zv)
p_shared = sharedX(zv[:, :, 0:rows:pool_rows, 0:cols:pool_cols]) h_shared = sharedX(zv) z_shared = sharedX(zv)
p_shared = sharedX(zv[:, 0:rows:pool_rows, 0:cols:pool_cols, :]) h_shared = sharedX(zv) z_shared = sharedX(zv)
grad_shared = sharedX(zv) z_shared = sharedX(zv)
grad_shared = sharedX(zv) z_shared = sharedX(zv)
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
from pylearn2.gui.patch_viewer import PatchViewer
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
buckets = 10 bucket_width = 1. / float(buckets) for i in xrange(buckets): lower_lim = i * bucket_width upper_lim = (i+1) * bucket_width
from pylearn2.gui.patch_viewer import PatchViewer
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
assert max(pd.max(), hd.max()) < .17
assert np.all((ps == 0) + (ps == 1)) assert np.all((hs == 0) + (hs == 1))
alpha = 1.5 beta = 0.75
ave = kl(Y, Y_hat, 1)
ave = elemwise_kl(Y, Y_hat)
for i in xrange(len(p)): assert p[i] == precision[i], (i, p[i], precision[i]) assert recall[i] == recall[i]
mean = X.mean(axis=1) if subtract_mean:
ddof = 1
if X.shape[1] == 1: ddof = 0
normalizers[normalizers < min_divisor] = 1.
messages = [
beta1 = sqrt_inner_product(bs)
bnorm = beta1 n_params = len(bs)
# dbar = dbarn epln = eplnn dlta = cs * dbar + sn * alpha gbar = sn * dbar - cs * alpha
xnorml = xnorm dl2s = [x for x in xs] xs = [x + tau * d for x, d in zip(xs, ds)]
return None, phi_a1
rvals, _ = scan( armijo, outputs_info=states, n_steps=n_iters, name='armijo', mode=theano.Mode(linker='cvm'), profile=profile)
c1 = TT.as_tensor_variable(c1) c2 = TT.as_tensor_variable(c2) maxiter = n_iters
xmin = TT.switch(cond, constant(numpy.nan), a + (-B + TT.sqrt(radical)) / (3 * A)) return xmin
D = fa C = fpa db = b - a * one
phi_aj = phi(a_j) derphi_aj = derphi(a_j)
phi_aj = phi(a_j) derphi_aj = derphi(a_j)
if obj <= best_obj: best_obj = obj best_alpha = alpha best_alpha_ind = ind
else: assert self.line_search_mode == 'exhaustive'
step_size = x if self.verbose: logger.info('best objective: {0}'.format(mn)) assert not np.isnan(mn)
if new_weight == 1.: self.new_weight.set_value(.01)
cur_out = self._func(*augmented) rval = [x + y for x, y in safe_zip(rval, cur_out)]
skiprows = 1 if headers else 0 x = np.loadtxt(test_path, delimiter=delimiter, skiprows=skiprows)
dataset = obj
HIDDEN_SIZE = 1000 SALT_PEPPER_NOISE = 0.4 GAUSSIAN_NOISE = 0.5
_mbce = MeanBinaryCrossEntropy() reconstruction_cost = lambda a, b: _mbce.cost(a, b) / ds.X.shape[1]
mb_data = MNIST(which_set='test').X[105:106, :]
pw = ParzenWindows(MNIST(which_set='test').X, .20) print(pw.get_ll(history))
_rcost = MeanBinaryCrossEntropy() reconstruction_cost = lambda a, b: _rcost.cost(a, b) / ds.X.shape[1]
c = GSNCost( [ (0, 1.0, reconstruction_cost),
(2, 2.0, classification_cost)
gsn._corrupt_switch = False
#np.sum(np.abs(y_hat - y), axis=1) != 0
data.extend([np.ones((1, 784))] * 2)
already_fixed = {}
currently_fixing = []
rval = shared(obj.get_value()) obj.__getstate__ = None
main(args=[])
from __future__ import print_function
train_algo = SGD( learning_rate = 0.1, cost = MeanSquaredReconstructionError(), batch_size = 10, monitoring_batches = 10, monitoring_dataset = trainset, termination_criterion = EpochCounter(max_epochs=MAX_EPOCHS_UNSUPERVISED), update_callbacks = None )
layer_trainers[-1].main_loop()
train_object.algorithm.termination_criterion.prop_decrease = 0.5 train_object.algorithm.termination_criterion.N = 1
from pylearn2.utils import serial
from pylearn2.datasets import cifar10
from pylearn2.datasets import preprocessing
train = cifar10.CIFAR10(which_set="train")
pipeline.items.append( preprocessing.ExtractPatches(patch_shape=(8, 8), num_patches=150000) )
pipeline.items.append(preprocessing.GlobalContrastNormalization( sqrt_bias=10., use_std=True))
pipeline.items.append(preprocessing.ZCA())
save_path.replace('\\', r'\\')
if not os.path.isdir(os.path.join(local_path, 'h5')): os.makedirs(os.path.join(local_path, 'h5'))
train = SVHN('splitted_train', path=local_path) check_dtype(train)
pipeline = preprocessing.Pipeline() pipeline.items.append(preprocessing.GlobalContrastNormalization(batch_size=5000)) pipeline.items.append(preprocessing.LeCunLCN((32,32)))
train.apply_preprocessor(pipeline, can_fit=True) del train
valid = SVHN('valid', path=local_path) check_dtype(valid) valid.apply_preprocessor(pipeline, can_fit=False)
test = SVHN('test', path=local_path) check_dtype(test) test.apply_preprocessor(pipeline, can_fit=False)
assert test.X.shape[0] % batch_size == 0
train.algorithm.termination_criterion = EpochCounter(max_epochs=1) train.extensions.pop(0) train.save_freq = 0 train.main_loop()
train.algorithm.termination_criterion = EpochCounter(max_epochs=1) train.extensions.pop(0) train.save_freq = 0 train.main_loop()
layers = [model.visible_layer] + model.hidden_layers
sampling_updates = model.get_sampling_updates(layer_to_state, theano_rng) assert layer_to_state[model.visible_layer] in sampling_updates
layer_to_state = model.make_layer_to_state(m) vis_sample = layer_to_state[model.visible_layer]
depth = len(b_list)
_sample_even_odd(W_list, b_list, new_nsamples, beta, odd=marginalize_odd) _activation_even_odd(W_list, b_list, new_nsamples, beta, odd=not marginalize_odd)
new_nsamples[not marginalize_odd] += pa_bias * (1. - beta)
depth = len(b_list)
keep_idx = numpy.arange(not marginalize_odd, depth, 2) for i in keep_idx: fe -= T.dot(samples[i], b_list[i]) * beta
log_ais_w = numpy.zeros(batch_size, dtype=floatX)
dlogz = log_mean(log_ais_w)
inference_fn(x)
nsamples[0].set_value(x) for ii, psample in enumerate(psamples): if ii > 0: nsamples[ii].set_value(psample.get_value())
x_likelihood = numpy.sum((-energy_fn(1.0) + hq - log_z)[:batch_size0])
likelihood = (i * likelihood + x_likelihood) / (i + batch_size0)
depth = len(samples)
wip1 = W_list[i+1] hi_mean += T.dot(samples[i+1], wip1.T) * beta
wi = W_list[i] hi_mean += T.dot(samples[i-1], wi) * beta
W_list = [None] + W_list
depth = len(b_list)
marginalize_odd = (depth % 2) == 0
fe_bp_h1 = free_energy_at_beta(W_list, b_list, nsamples, beta, pa_bias, marginalize_odd=marginalize_odd) free_energy_fn = theano.function([beta], fe_bp_h1)
metrics = {'ais': estimate_likelihood} datasets = {'mnist': MNIST}
from pylearn2.utils import serial from pylearn2.datasets import cifar10 from pylearn2.datasets import preprocessing
num_labels_by_type = numpy.array(norb.SmallNORB.num_labels_by_type, 'int') num_labels_by_type[instance_index] = len(new_to_old_instance)
figure.subplots_adjust(bottom=0.05)
return None
grid_indices = [0, ] * 5
label_to_row_indices = _make_label_to_row_indices(dataset.y)
object_image_index = [0, ] blank_image_index = [0, ] blank_label = _get_blank_label(dataset)
grid_dimension = [0, ]
for axes in all_axes: axes.get_xaxis().set_visible(False) axes.get_yaxis().set_visible(False)
lines[grid_dimension[0]] = '==> ' + lines[grid_dimension[0]]
image_pair = tuple(image_pair[0, :, :, :, 0])
num_dimensions += 1
image_index[0] = add_mod(image_index[0], step, len(row_indices))
gd = grid_dimension[0] grid_indices[gd] = add_mod(grid_indices[gd], step, len(grid_to_short_label[gd]))
image_index[0] = min(image_index[0], len(row_indices))
disable_left_right = (is_blank(grid_indices) and not (grid_dimension[0] in (0, 5)))
ele = (ele * 2 * numpy.pi) / 360. azi = (azi * 2 * numpy.pi) / 360.
grad = numpy.gradient(a) grad_x, grad_y = grad
frgd_img = to_img(data.X[i], 28) frgd_img = frgd_img.convert('L')
textid = 14 while textid == 14: textid = rng.randint(1, 113)
output['texture_id'][i] = textid output['texture_pos'][i] = (px, py)
frgd_arr = to_array(frgd_img) mask_arr = frgd_arr > 0.1
blend_arr = copy(patch_arr) blend_arr[mask_arr] = frgd_arr[mask_arr]
frgd_img = to_img(blend_arr, os)
if not os.path.isdir(orig_path): raise IOError("You need to download the SVHN format2 dataset MAT files " "before running this conversion script.")
cmp_mode = 'equal'
_, model_0_path, model_1_path = sys.argv
record = 0 clean = intersect while len(clean) > 0: bad_channel = [] for channel in clean: channel_0 = channels_0[channel] channel_1 = channels_1[channel]
if record == channel_0.length: bad_channel.append(channel) continue
libv = LibVersion() libv.print_versions() libv.print_exp_env_info(args.print_theano)
import pylearn2.config.yaml_parse
ret_list.append('%s: %s,' % (key, val))
hyper_parameters = expand(flatten(state.hyper_parameters), dict_type=ydict)
final_yaml_str = yaml_template % hyper_parameters
train_obj = pylearn2.config.yaml_parse.load(final_yaml_str)
train_obj.main_loop() state.results = jobman.tools.resolve(state.extract_results)(train_obj) return channel.COMPLETE
import argparse import gc import logging import os
import numpy as np
if os.getenv('DISPLAY') is None: try: import matplotlib matplotlib.use('Agg') except: pass
from pylearn2.utils import serial from pylearn2.utils.logger import ( CustomStreamHandler, CustomFormatter, restore_defaults )
phase_variable = 'PYLEARN2_TRAIN_PHASE' phase_value = 'phase%d' % (number + 1) os.environ[phase_variable] = phase_value
subobj.main_loop(time_budget=time_budget)
del subobj gc.collect()
print_monitor_cv.main(filename)
print_monitor_cv.main(filename, all=True)
os.remove(filename)
return s
prompt = len(channels.values()) > 1
for code in sorted_codes: print(code + '. ' + codebook[code])
else: final_codes ,= set(codebook.keys())
for idx, code in enumerate(sorted(final_codes)):
x = np.arange(len(channel.batch_record))
num_rows = max_num_channels // num_columns if num_rows * num_columns < max_num_channels: num_rows += 1
window_height = window_width * ((num_rows * 1.8) / num_columns) figure, all_axes = pyplot.subplots(num_rows, num_columns, squeeze=False, figsize=(window_width, window_height))
for axes_row in all_axes: for axes in axes_row: axes.get_xaxis().set_visible(False) axes.get_yaxis().set_visible(False)
mid = int(np.floor(kernel_shape/ 2.)) centered_X = X - convout[:,:,mid:-mid,mid:-mid]
sum_sqr_XX = conv2d(T.sqr(X), filters=filters, border_mode='full')
csv_file = open(path, 'r')
row = reader.next()
y = y[:m]
csv_file = open(path, 'r')
row = next(reader)
y = y[:m]
if rbm.nvis < rbm.nhid: width = rbm.nvis type = 'vis' else: width = rbm.nhid type = 'hid'
block_bits = width if (not max_bits or width < max_bits) else max_bits block_size = 2 ** block_bits
visbias_a = visbias
assert rbmA_params[0].shape[0] == rbmB_params[0].shape[0] assert len(rbmA_params[1]) == len(rbmB_params[1])
rbmA_params = [numpy.asarray(q, dtype=config.floatX) for q in rbmA_params] rbmB_params = [numpy.asarray(q, dtype=config.floatX) for q in rbmB_params]
v_sample = tensor.matrix('ais_v_sample') beta = tensor.scalar('ais_beta')
self.log_ais_w = numpy.zeros(n_runs, dtype=config.floatX)
if key_betas is not None: betas = numpy.hstack((betas, key_betas)) betas.sort()
state = self.v_sample0 ki = 0
if self.key_betas is not None and \ ki < len(self.key_betas) and \ bp1 == self.key_betas[ki]:
state = self.sample_fn(bp1, state)
dlogz = self.log_mean(log_ais_w)
self._build_data_specs()
if self._dirty: self.redo_theano()
myiterator = d.iterator(mode=i, batch_size=b, num_batches=n, data_specs=self._flat_data_specs, return_tuple=True, rng=sd)
if len(self._flat_data_specs[1]) == 0: X = () self.run_prereqs(X, d) a(*X)
self.run_prereqs(X, d) a(*X) actual_ne += self._flat_data_specs[0].np_batch_size(X)
self._build_data_specs()
batch_names = ['monitoring_%s' % s for s in self._flat_data_specs[1]] theano_args = self._flat_data_specs[0].make_theano_batch(batch_names)
c_mapping = DataSpecsMapping(channel.data_specs) channel_inputs = c_mapping.flatten(channel.graph_input, return_tuple=True) inputs = c_mapping.flatten(nested_theano_args[i + 1], return_tuple=True)
self.accum.append(function(theano_args, givens=g, updates=u, mode=self.theano_function_mode, name=function_name))
if not hasattr(self, '_datasets'): self._datasets = [self._dataset] del self._dataset
if '_dataset' in d: d['_datasets'] = [d['_dataset']] del d['_dataset']
m_space, m_source = model.get_monitoring_data_specs() spaces.append(m_space) sources.append(m_source)
nested_ipt = mapping.nest(ipt)
channels[prefix + name] = (raw_channels[name], cost_ipt, (spaces[i], sources[i]))
model_channels = model.get_monitoring_channels(nested_ipt[-1]) channels = {} for name in model_channels: channels[name] = (model_channels[name], nested_ipt[-1], (spaces[-1], sources[-1])) custom_channels.update(channels)
if hasattr(self, "doc"): doc = self.doc else: doc = None
from __future__ import print_function
import theano from theano import tensor try: from theano.sparse import SparseType except ImportError: warnings.warn("Could not import theano.sparse.SparseType") from theano.compile.mode import get_default_mode
self._params.update(l._params)
repr = [inputs]
del configure_custom
cluster_ids, mu = milk.kmeans(X, k)
if contains_nan(mu): logger.info('nan found') return X
for i in xrange(k): dists[:, i] = numpy.square((X - mu[i, :])).sum(axis=1)
mmd = min_dists.mean()
break
min_dist_inds = dists.argmin(axis=1)
return False
get_input_space = Model.get_input_space get_output_space = Model.get_output_space
import numpy from theano import tensor
self.nfac = nfac
self.aes = self._layers
self._corrupt_switch = True
self._sample_switch = True
self._bias_switch = True
for i in xrange(1, len(self.aes)): assert (self.aes[i].weights.get_value().shape[0] == self.aes[i - 1].nhid)
act_enc = activation_funcs[i + 1] act_dec = act_enc if i != 0 else activation_funcs[0] aes.append( Autoencoder(layer_sizes[i], layer_sizes[i + 1], act_enc, act_dec, tied_weights=tied) )
set_idxs = safe_zip(*minibatch)[0]
steps = [self.activations[:]]
for _ in xrange(len(self.aes) + walkback): steps.append(self._update(self.activations, clamped=clamped))
def wrap_f_init(*args): data = f_init(*args) length = len(data) / 2 return data[:length], data[length:] return wrap_f_init
state = (self._corrupt_switch, self._sample_switch, self._bias_switch)
return self._compiled_cache[2:]
f_init = compile_f_init() cc = self._compiled_cache self._compiled_cache = (state, indices, f_init, cc[3]) return self._compiled_cache[2:]
f_init = compile_f_init() f_step = compile_f_step() self._compiled_cache = (state, indices, f_init, f_step) return self._compiled_cache[2:]
if not include_first: results = results[1:]
for i, val in minibatch: if val is not None: activations[i] = val
odds = filter(lambda i: i not in skip_idxs, range(1, len(activations), 2))
precor = [None] * len(self.activations) for idx, val in evens_copy + odds_copy: assert precor[idx] is None precor[idx] = val assert None not in precor
clamped_val = clamp * initial
if symbolic: activations[idx] = T.switch(clamp, initial, activations[idx]) else: activations[idx] = np.switch(clamp, initial, activations[idx])
if self._sample_switch: self._apply_corruption(activations, self._layer_samplers, idx_iter) return activations
act_func = None if i == 0: act_func = self.aes[0].act_dec else: act_func = self.aes[i - 1].act_enc
if act_func is not None: activations[i] = act_func(activations[i])
data = np.asarray(data[skip:skip+trials])[:, 0, :, :]
labels = np.zeros_like(mean) labels[np.arange(labels.shape[0]), am] = 1.0
import logging
ml_cost = (self.free_energy_given_v(pos_v).mean() - self.free_energy_given_v(neg_v).mean())
zero_mean = rng.normal(size=shape) * self.sigma return zero_mean + v_mean
W_irange = 2 / numpy.sqrt(nvis * nhid)
self.B = sharedX(numpy.zeros(self.nvis) + B0, name='B', borrow=True)
batch_size = v.shape[0]
h_mean = self.mean_h_given_v(v) h_mean_shape = (batch_size, self.nhid) h_sample = rng.binomial(size=h_mean_shape, n = 1, p = h_mean, dtype = h_mean.dtype)
v_mean, v_var = self.mean_var_v_given_h_s(h_sample, s_sample) v_mean_shape = (batch_size, self.nvis) v_sample = rng.normal(size=v_mean_shape) * tensor.sqrt(v_var) + v_mean
return StackedBlocks(layers)
self.learning_rates = {} self.base_lr = theano._asarray(base_lr, dtype=theano.config.floatX)
self.iteration = sharedX(theano._asarray(0, dtype='int32'), name='iter')
self.annealed = sharedX(base_lr, 'annealed')
ups[self.annealed] = annealed ups[self.iteration] = self.iteration + 1
learn_rates = [annealed * self.learning_rates[p] for p in self.params]
l_ups, learn_rates = self.learning_rate_updates(gradients) safe_update(ups, l_ups)
p_up = dict(self.sgd_updates(self.params, gradients, learn_rates))
safe_update(ups, p_up)
return ups
self.input_space = VectorSpace(dim=self.nvis) self.input_source = 'features' self.latent_space = VectorSpace(dim=self.nhid)
if self.kl_integrator is None: self.kl_integrator = find_integrator_for(self.prior, self.posterior)
z = self.sample_from_p_z(num_samples=num_samples, **kwargs) theta = self.decode_theta(z) X = self.sample_from_p_x_given_z(num_samples=num_samples, theta=theta)
conditional_probs = T.nnet.sigmoid(conditional_params[0]) return self.theano_rng.uniform( size=(num_samples, self.ndim), dtype=theano.config.floatX ) < conditional_probs
return T.nnet.sigmoid(conditional_params[0])
import logging import sys
from pylearn2.blocks import Block from pylearn2.utils import sharedX
self._params = []
if mean is None: mean = X.mean(axis=0) X = X - mean
v, W = self._cov_eigen(X)
self.W = sharedX(W, name='W') self.v = sharedX(v, name='v') self.mean = sharedX(mean, name='mean')
self._update_cutoff()
if self.whiten: W = W / tensor.sqrt(self.v[:self.component_cutoff])
v, W = v[::-1], W[:, ::-1] return v, W
logger.info('computing mean') self.mean_ = numpy.asarray(X.mean(axis=0))[0, :]
return v[::-1], W.T[:, ::-1]
return v[::-1], W[:, ::-1]
mean = X.mean(axis=0) mean_matrix = csr_matrix(mean.repeat(n).reshape((d, n))).T X = X - mean_matrix
return v[::-1], W[:, ::-1]
self._update_cutoff()
self.n_observations = 0 self.minibatch_index = 0
self.Xt = numpy.zeros([self.n_eigen + self.minibatch_size, self.n_dim])
self.x_sum = numpy.zeros([self.n_dim])
self.Ut = numpy.zeros([self.n_eigen, self.n_dim])
row = self.n_eigen + self.minibatch_index self.Xt[row] = x
self.x_sum *= self.gamma self.x_sum += x
normalizer = (1.0 - pow(self.gamma, self.n_observations)) / \ (1.0 - self.gamma)
if self.centering: self.Xt[row] -= self.x_sum / normalizer
for i in range(self.n_eigen + self.minibatch_size): self.G[i,i] += self.regularizer
self.Ut = numpy.dot(self.V[:,-self.n_eigen:].transpose(), self.Xt)
self.Xt[:self.n_eigen,:] = self.Ut
normalizer = (1.0 - pow(self.gamma, self.n_observations - self.minibatch_index)) /\ (1.0 - self.gamma)
if not hasattr(self, 'mask_weights'): self.mask_weights = None
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
self.desired_space = Conv2DSpace(shape=space.shape, channels=space.num_channels, axes=('c', 0, 1, 'b'))
raise NotImplementedError()
from pylearn2.costs.mlp import L1WeightDecay as _L1WD from pylearn2.costs.mlp import WeightDecay as _WD
dropout_input_mask_value = 0.
assert layer_name is None
if not hasattr(coeffs, '__iter__'): coeffs = [coeffs] * len(self.layers)
if not hasattr(coeffs, '__iter__'): coeffs = [coeffs] * len(self.layers)
if not hasattr(self, 'non_redundant'): self.non_redundant = False if not hasattr(self, 'mask_weights'): self.mask_weights = None
range_ = T.tile(range_.dimshuffle(0, 'x'), (1, self.binary_target_dim)).flatten()
raise NotImplementedError()
raise NotImplementedError()
raise NotImplementedError()
p = T.switch(p > 0., p, self.left_slope * p) return p
if not hasattr(self, 'detector_normalization'): self.detector_normalization = None
mkn = max_kernel_norm dn = detector_normalization on = output_normalization
assert not value or all(0 <= v < self.num_layers for v in value) self.inputs_to_layers[key] = sorted(value)
if len(cur_state_below) == 1: cur_state_below, = cur_state_below
assert not any([key in rval for key in contrib]) assert all([key in params for key in contrib])
scal_points = new_W / norms.dimshuffle('x',0)
dot_update = (old_W * scal_points).sum(axis=0)
import functools import operator
import numpy import theano from theano import tensor from theano.compat.six.moves import zip as izip, reduce
from pylearn2.blocks import Block, StackedBlocks from pylearn2.models import Model from pylearn2.utils import sharedX from pylearn2.utils.theano_graph import is_pure_elemwise from pylearn2.utils.rng import make_np_rng, make_theano_rng from pylearn2.space import VectorSpace
self.s_rng = make_theano_rng(seed, which_method="uniform")
acts = self._hidden_input(inputs) hiddens = self.act_enc(acts) act_grad = tensor.grad(hiddens.sum(), acts) return act_grad
act_grad = self._activation_grad(inputs) jacobian = self.weights * act_grad.dimshuffle(0, 'x', 1) return jacobian
return StackedBlocks(layers)
if 'extensions' not in d: self.extensions = []
self._disallow_censor_updates()
if not hasattr(self, 'names_to_del'): self.names_to_del = set() self.names_to_del = self.names_to_del.union(names)
assert isinstance(num_steps, py_integer_types) assert num_steps > 0
if num_steps != 1: for i in xrange(num_steps): layer_to_state = self.sample(layer_to_state, theano_rng, layer_to_clamp, num_steps=1) return layer_to_state
assert len(self.dbm.hidden_layers) > 0
if layer_to_clamp is None: layer_to_clamp = OrderedDict()
layer_to_updated = OrderedDict()
if i == 0: layer_below = self.dbm.visible_layer else: layer_below = self.dbm.hidden_layers[i-1] state_below = layer_to_state[layer_below] state_below = layer_below.upward_state(state_below)
this_sample = this_layer.sample(state_below=state_below, state_above=state_above, layer_above=layer_above, theano_rng=theano_rng)
for i, this_layer in list(enumerate(self.dbm.hidden_layers))[1::2]:
layer_below = self.dbm.hidden_layers[i-1]
this_sample = this_layer.sample(state_below=state_below, state_above=state_above, layer_above=layer_above, theano_rng=theano_rng)
assert all([layer in layer_to_updated for layer in layer_to_state]) assert all([layer in layer_to_state for layer in layer_to_updated]) assert all([(layer_to_state[layer] is layer_to_updated[layer]) == layer_to_clamp[layer] for layer in layer_to_state])
if Y is not None: state_above = dbm.hidden_layers[-1].downward_state(Y) layer_above = dbm.hidden_layers[-1] assert len(dbm.hidden_layers) > 1
if len(dbm.hidden_layers) > 2: state_below = dbm.hidden_layers[-3].upward_state(H_hat[-3]) else: state_below = dbm.visible_layer.upward_state(V)
H_hat[-1] = Y
for layer, state in safe_izip(dbm.hidden_layers, H_hat): upward_state = layer.upward_state(state) layer.get_output_space().validate(upward_state)
state_below=dbm.hidden_layers[-2].upward_state(H_hat[-1])))
assert V is orig_V assert drop_mask is orig_drop_mask
SuperWeightDoubling = WeightDoubling
drop_mask_Y = T.zeros_like(Y)
state_below=dbm.hidden_layers[-2].upward_state(H_hat[-1])))
assert V is orig_V assert drop_mask is orig_drop_mask
if Y is not None: H_hat[-1] = Y
assert (niter > 1) == (len(dbm.hidden_layers) > 1)
for layer, state in safe_izip(dbm.hidden_layers, H_hat): upward_state = layer.upward_state(state) layer.get_output_space().validate(upward_state)
assert V is orig_V assert drop_mask is orig_drop_mask
if Y is not None: H_hat[-1] = Y
assert (niter > 1) == (len(dbm.hidden_layers) > 1)
assert V is orig_V assert drop_mask is orig_drop_mask
layer_to_chains[self.dbm.visible_layer] = inputs
if rbm == rbm_list[-1]: if targets: assert len(rbm.hidden_layers) == 2 else: assert len(rbm.hidden_layers) == 1 else: assert len(rbm.hidden_layers) == 1
assert len(self.hidden_layers) > 0
assert len(self.hidden_layers) > 0
if not hasattr(self, 'rng'): self.setup_rng()
if not hasattr(self, 'freeze_set'): self.freeze_set = set([])
if not hasattr(self, 'freeze_set'): self.freeze_set = set([])
assert not any([key in rval for key in contrib]) assert all([key in params for key in contrib])
layers = [self.visible_layer] + self.hidden_layers
layers = [self.visible_layer] + self.hidden_layers
if layer_to_clamp is None: layer_to_clamp = OrderedDict()
for layer in layer_to_state: old = layer_to_state[layer] new = updated[layer] if layer_to_clamp[layer]: assert new is old else: add_updates(old, new)
del self.bias_from_marginals
rval = -(self.beta * T.dot(state, self.bias))
raise NotImplementedError()
raise NotImplementedError()
del self.bias_from_marginals
init_bias = \ init_sigmoid_bias_from_array(bias_from_marginals.X / 2. + 0.5)
rval = -(self.beta * T.dot(state, self.ising_bias()))
del self.bias_from_marginals
rval = -T.dot(state, self.bias)
if not hasattr(self, 'mask_weights'): self.mask_weights = None if not hasattr(self, 'max_col_norm'): self.max_col_norm = None
raise NotImplementedError()
raise NotImplementedError()
assert len(state) == 2 if isinstance(coeffs, str): coeffs = float(coeffs) assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
assert len(state) == 2 if isinstance(coeffs, str): coeffs = float(coeffs) assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
raise NotImplementedError()
if not hasattr(self, 'W_lr_scale'): self.W_lr_scale = None
raise NotImplementedError()
if not hasattr(self, 'needs_reformat'): self.needs_reformat = self.needs_reshape del self.needs_reshape
if not hasattr(self, 'needs_reformat'): self.needs_reformat = self.needs_reshape del self.needs_reshape
log_prob_of = (Y * log_prob).sum(axis=1) masked = log_prob_of * drop_mask_Y assert masked.ndim == 1
self.batch_axis=list(axes).index('b') self.axes_to_sum = list(range(len(axes))) self.axes_to_sum.remove(self.batch_axis)
assert len(state) == 2 assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")
default_z += T.alloc(*([0.]+[shape[elem] for elem in self.h_space.axes])).astype(default_z.dtype) assert default_z.ndim == 4
assert len(state) == 2 assert isinstance(coeffs, float) _, state = state state = [state] coeffs = [coeffs]
warnings.warn("Do you really want to regularize the detector units to be more active than the pooling units?")
h_rows, h_cols = self.h_space.shape num_h = float(h_rows * h_cols) rval[self.transformer._filters] = 1. /num_h rval[self.b] = 1. / num_h
default_z += T.alloc(*([0.]+[shape[elem] for elem in self.h_space.axes])).astype(default_z.dtype) assert default_z.ndim == 4
raise NotImplementedError()
raise NotImplementedError()
X = dataset.get_design_matrix() m = X.shape[0] assert X.shape[1] == self.nvis
self.prev_floatX = config.floatX config.floatX = 'float64'
config.floatX = self.prev_floatX
self.prev_floatX = config.floatX config.floatX = 'float64'
example_input[0, 0] = -2.5
example_input[1, 3] = 0.0 example_input[1, 4] = 1.0
np.random.seed(12345)
mlp_model = MLP( layers=[mlp_nonlinearity(dim=output_channels, layer_name='mlp', irange=1.0)], batch_size=batch_size, nvis=nvis )
assert_allclose(f(x_mlp).flatten(), g(x).flatten(), rtol=1e-5, atol=5e-5)
assert_raises(NotImplementedError, conv_model.cost, Y, Y_hat) assert_raises(NotImplementedError, mlp_model.cost, Y1, Y1_hat)
from nose.plugins.skip import SkipTest from theano import config from theano import function from theano.sandbox import cuda from theano import tensor as T
f = function([X], output, mode="DEBUG_MODE") f(np.zeros((1, 1)).astype(X.dtype))
axes = ['b', 0, 1, 'c'] random.shuffle(axes) axes = tuple(axes) print('axes:', axes)
rng = np.random.RandomState() mean = rng.uniform(1e-6, 1. - 1e-6, (rows, cols, channels))
axes = ['b', 0, 1, 'c'] random.shuffle(axes) axes = tuple(axes) print('axes:', axes)
for pool_size in [1, 2, 5]: n = num_pools * pool_size
input_space = VectorSpace(1) class DummyDBM(object): def __init__(self): self.rng = rng layer.set_dbm(DummyDBM()) layer.set_input_space(input_space)
mean = layer.mf_update( state_below=T.alloc(0., 1, 1), state_above=None, layer_above=None)
dbm = make_random_basic_binary_dbm( rng = rng, pool_size_1 = pool_size_1, )
p_idx = rng.randint(num_p)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
expected_p, expected_h = h1.mf_update( state_below = v.upward_state(v_state), state_above = h2.downward_state(h2_state), layer_above = h2)
wtf_numpy = np.zeros((pool_size_1,)) for i in xrange(pool_size_1): wtf_numpy[i] = on_probs[i] on_probs = wtf_numpy
for pool_size in [1, 2, 5]: do_test(pool_size)
dbm = make_random_basic_binary_dbm( rng = rng, pool_size_1 = pool_size_1,
p_idx = rng.randint(num_p)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
expected_p, expected_h = h1.mf_update( state_below = v.upward_state(v_state), state_above = h2.downward_state(h2_state), layer_above = h2)
wtf_numpy = np.zeros((pool_size_1,)) for i in xrange(pool_size_1): wtf_numpy[i] = on_probs[i] on_probs = wtf_numpy
do_test(1)
dbm = make_random_basic_binary_dbm( rng = rng, pool_size_1 = pool_size_1, )
p_idx = rng.randint(num_p)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] h1_state = layer_to_state[h1] h2_state = layer_to_state[h2]
expected_p, expected_h = h1.mf_update( state_below = v.upward_state(v_state), state_above = h2.downward_state(h2_state), layer_above = h2)
for pool_size in [1, 2, 5]: do_test(pool_size)
num_vis = rng.randint(1,11) n_classes = rng.randint(1, 11)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] y_state = layer_to_state[y]
expected_y = y.mf_update( state_below = v.upward_state(v_state))
energy = dbm.energy(V = v_state, hidden = [y_state]) unnormalized_prob = T.exp(-energy) assert unnormalized_prob.ndim == 1 unnormalized_prob = unnormalized_prob[0] unnormalized_prob = function([], unnormalized_prob)
wtf_numpy = np.zeros((n_classes,)) for i in xrange(n_classes): wtf_numpy[i] = probs[i] probs = wtf_numpy
num_vis = rng.randint(1,11) n_classes = rng.randint(1, 11)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] y_state = layer_to_state[y]
expected_y = y.mf_update( state_below = v.upward_state(v_state))
energy = dbm.energy(V = v_state, hidden = [y_state]) unnormalized_prob = T.exp(-energy) assert unnormalized_prob.ndim == 1 unnormalized_prob = unnormalized_prob[0] unnormalized_prob = function([], unnormalized_prob)
wtf_numpy = np.zeros((n_classes,)) for i in xrange(n_classes): wtf_numpy[i] = probs[i] probs = wtf_numpy
num_vis = rng.randint(1,11) n_classes = rng.randint(1, 11)
layer_to_state = dbm.make_layer_to_state(1) v_state = layer_to_state[v] y_state = layer_to_state[y]
expected_y = y.mf_update( state_below = v.upward_state(v_state))
cause_copy = sharedX(np.zeros((num_samples,))).dimshuffle(0,'x') v_state = v_state[0,:] + cause_copy y_state = y_state[0,:] + cause_copy
num_examples = 40 theano_rng = MRG_RandomStreams(2012+11+1)
visible_layer = BinaryVector(nvis=100) hidden_layer = BinaryVectorMaxPool(detector_layer_dim=500, pool_size=1, layer_name='h', irange=0.05, init_bias=-2.0) model = DBM(visible_layer=visible_layer, hidden_layers=[hidden_layer], batch_size=100, niter=1)
np.testing.assert_equal(mlp.get_total_input_dimension(['h0', 'h1']), 4) inp = theano.tensor.matrix()
l = [] for mask in xrange(16): l.append(mlp.masked_fprop(inp, mask)) outsum = reduce(lambda x, y: x + y, l)
mlp_first_part = MLP( layers=[ first_indep_layer ], input_space=VectorSpace(features_in_first_mlp), input_source=('features0') )
mlp_second_part = MLP( layers=[ second_indep_layer ], input_space=VectorSpace(features_in_second_mlp), input_source=('features1') )
shared_dataset = np.random.rand(20, 19).astype(theano.config.floatX)
train_composite = Train(dataset_composite, mlp_composite, SGD(0.0001, batch_size=20)) train_composite.algorithm.termination_criterion = EpochCounter(1) train_composite.main_loop()
X_composite = mlp_composite.get_input_space().make_theano_batch() X_first_part = mlp_first_part.get_input_space().make_theano_batch() X_second_part = mlp_second_part.get_input_space().make_theano_batch()
fl = mlp_composite.layers[0]
assert mlp_composite.get_input_space() == fl.get_input_space()
for i in range(0, 4): np.testing.assert_allclose(fl.get_params()[i].eval(), mlp_composite.get_params()[i].eval())
features_in_first_mlp = 5 features_in_second_mlp = 10 targets_in_first_mlp = 2 targets_in_second_mlp = 2
conv_first_part = ConvElemwise(8, [2, 2], 'sf1', SigmoidConvNonlinearity(), .1) mlp_first_part = MLP(layers=[conv_first_part], input_space=Conv2DSpace(shape=[5, 5], num_channels=2))
train_composite = Train(shared_dataset, mlp_composite, SGD(0.1, batch_size=5, monitoring_dataset=shared_dataset)) train_composite.algorithm.termination_criterion = EpochCounter(1) train_composite.main_loop()
X_composite = mlp_composite.get_input_space().make_theano_batch() X_first_part = mlp_first_part.get_input_space().make_theano_batch() X_second_part = mlp_second_part.get_input_space().make_theano_batch()
self.prev_floatX = config.floatX config.floatX = 'float64'
config.floatX = self.prev_floatX
self.prev_floatX = config.floatX config.floatX = 'float64'
if kl > tol or not (kl <= tol): raise AssertionError("KL divergence between two " "equivalent models should be 0 but is "+ str(kl))
return dict((_instantiate(k, bindings), _instantiate(v, bindings)) for k, v in six.iteritems(proxy))
elif isinstance(proxy, six.string_types): return preprocess(proxy) else: return proxy
if not isinstance(content, str): raise AssertionError("Expected content to be of type str, got " + str(type(content)))
pieces = modulename.split('.') str_e = str(e) found = True in [piece.find(str(e)) != -1 for piece in pieces]
reraise_as(ImportError("Could not import %s; ImportError was %s" % (modulename, str_e)))
yaml.add_multi_constructor('!obj:', multi_constructor_obj) yaml.add_multi_constructor('!pkl:', multi_constructor_pkl) yaml.add_multi_constructor('!import:', multi_constructor_import)
"corruptor" : *corr
loaded = yaml.load(yamlfile) logger.info(loaded) assert loaded['corruptor'] is loaded['dae'].corruptor
assert_(loaded['a'].yaml_src.find("${TEST_VAR}") != -1) del environ['TEST_VAR']
raised = False fmt = OneHotFormatter(max_labels=50) try: fmt.theano_expr(theano.tensor.vector(dtype=theano.config.floatX)) except TypeError: raised = True assert raised
raised = False try: fmt.format(numpy.zeros(10, dtype='float64')) except TypeError: raised = True assert raised
raised = False try: fmt = OneHotFormatter(max_labels=-10) except ValueError: raised = True assert raised
raised = False try: fmt = OneHotFormatter(max_labels=10, dtype='invalid') except TypeError: raised = True assert raised
raised = False try: fmt.theano_expr(theano.tensor.itensor3()) except ValueError: raised = True assert raised
result = list_files('.py') for path in result: logger.info(path)
COMMENT_WITH_NL = tokenize.generate_tokens(['#\n'].pop).send(None)[1] == '#\n'
try: length = len(line.decode('utf-8')) except UnicodeError: pass
yield found + 1, "E201 whitespace after '%s'" % char
indent_next = logical_line.endswith(':')
last_indent = start if verbose >= 3: print("... " + line.rstrip())
rel_indent[row] = expand_indent(line) - indent_level
close_bracket = (token_type == tokenize.OP and text in ']})')
if start[1] != indent[depth]: yield (start, "E124 closing bracket does not match " "visual indentation")
if hang_closing: yield start, "E133 closing bracket is missing indentation"
yield (start, "E128 continuation line " "under-indented for visual indent")
if close_bracket and not hang_closing: yield (start, "E123 closing bracket does not match " "indentation of opening bracket's line") hangs[depth] = hang
indent[depth] = start[1]
pass
(index < 2 or tokens[index - 2][1] != 'class') and not keyword.iskeyword(prev_text)): yield prev_end, "E211 whitespace before '%s'" % text
continue
if need_space is not True and not need_space[1]: yield (need_space[0], "E225 missing whitespace around operator") need_space = False
pass
yield prev_end, "E225 missing whitespace around operator"
pass
need_space = (prev_end, start != prev_end)
yield prev_end, "E225 missing whitespace around operator" need_space = False
self.blank_lines += 1 del self.tokens[0]
text = text.rstrip('\r\n') self.tokens = [(token_type, text) + token[2:]] self.check_logical()
self.elapsed = 0 self.total_errors = 0 self.counters = dict.fromkeys(self._benchmark_keys, 0) self.messages = {}
if code in self.expected: return if self.print_filename and not self.file_errors: print(self.filename) self.file_errors += 1 self.total_errors += 1 return code
options.ignore = tuple(DEFAULT_IGNORE.split(','))
options.ignore = ('',) if options.select else tuple(options.ignore)
if ((filename_match(filename, filepatterns) and not self.excluded(filename, root))): runner(os.path.join(root, filename))
(new_options, __) = parser.parse_args([])
(options, __) = parser.parse_args(arglist, values=new_options)
if not arglist and not parse_argv: arglist = [] (options, args) = parser.parse_args(arglist) options.reporter = None
try: indent = min(len(s) - len(s.lstrip()) for s in docstring if s.strip()) except ValueError: indent = 0
def _str_header(self, name, symbol='`'): return ['**' + name + '**'] + [symbol*(len(name)+4)]
out = self._str_indent(out,indent) return '\n'.join(out)
module = inspect.getmodule(method) if module is not None: if not module.__name__.startswith('pylearn2'): return method_errors
import scipy.sparse
assert not isinstance(batch, list)
if batch is None or (isinstance(batch, tuple) and len(batch) == 0): return True
assert result == any(subbatch_results), ("composite batch had a " "mixture of numeric and " "symbolic subbatches. This " "should never happen.") return result
return (isinstance(batch, np.ndarray) or scipy.sparse.issparse(batch) or str(type(batch)) == "<type 'CudaNdarray'>")
return theano._asarray(arg, dtype=dtype)
self._validate(is_numeric, batch)
self._check_sizes(space)
self.validate(batch)
if '_dtype' not in state_dict: self._dtype = theano.config.floatX
super(IndexSpace, self)._validate_impl(is_numeric, batch)
owner = batch.owner assert 'Subtensor' in str(owner.op) batch = owner.inputs[0]
super(VectorSpace, self)._validate_impl(is_numeric, batch)
return 1
super(VectorSequenceSpace, self)._validate_impl(is_numeric, batch)
return 1
super(IndexSequenceSpace, self)._validate_impl(is_numeric, batch)
default_axes = ('b', 0, 1, 'c')
self.shape = tuple(shape) self.num_channels = num_channels if axes is None: axes = self.default_axes assert len(axes) == 4 self.axes = tuple(axes)
if not hasattr(self, 'num_channels'): self.num_channels = self.nchannels
super(Conv2DSpace, self)._validate_impl(is_numeric, batch)
batch = _undo_op(batch, 'Cast')
if space.axes != self.axes: batch = _undo_op(batch, 'DimShuffle', strict=True)
def __init__(self): super(NullSpace, self).__init__()
self._validate(is_numeric, batch) return 0
from pylearn2.space import (SimplyTypedSpace, VectorSpace, Conv2DSpace, CompositeSpace, VectorSequenceSpace, IndexSequenceSpace, IndexSpace, NullSpace, is_symbolic_batch) from pylearn2.utils import function, safe_zip
if not (isinstance(from_space, VectorSpace) and from_space.sparse): kwargs['batch_size'] = batch_size
fallback_dtype = theano.config.floatX
if isinstance(space, VectorSpace) and space.sparse: del kwargs["batch_size"]
return None, None
if isinstance(from_space, CompositeSpace): if isinstance(to_space, Conv2DSpace): return (NotImplementedError, "CompositeSpace does not know how to format as " "Conv2DSpace")
if isinstance(to_space, CompositeSpace):
n_dtypes = 2 old_nchannels = shape[2] shape[2] = old_nchannels / 2 assert shape[2] * 2 == old_nchannels, \
composite_dtypes = ((None, 'int8'), ('complex128', theano.config.floatX))
for from_space in composite_spaces: for to_dtype in composite_dtypes: test_get_origin_batch(from_space, to_dtype) test_make_shared_batch(from_space, to_dtype) test_make_theano_batch(from_space, to_dtype) test_dtype_setter(from_space, to_dtype)
VS = VectorSpace(dim=27) VS_sparse = VectorSpace(dim=27, sparse=True)
VS_batch = VS.make_theano_batch() new_SVS_batch = VS.format_as(VS_batch, VS_sparse) new_VS_batch = VS.undo_format_as(new_SVS_batch, VS_sparse) assert new_VS_batch is VS_batch assert new_SVS_batch is not VS_batch
VS_batch = VS.make_theano_batch() new_CS_batch = VS.format_as(VS_batch, CS) new_VS_batch = VS.undo_format_as(new_CS_batch, CS) assert new_VS_batch is VS_batch
VS = VectorSpace(dim=27) VS_sparse = VectorSpace(dim=27, sparse=True)
from __future__ import print_function
theano.config.warn.sum_div_dimshuffle_bug = False
if self.corruption_level < 1e-5: return x
pvals = T.alloc(1.0 / num_classes, num_classes) one_hot = self.s_rng.multinomial(size=(num_examples,), pvals=pvals)
super(BinomialSampler, self).__init__(0, *args, **kwargs)
super(MultinomialSampler, self).__init__(0, *args, **kwargs)
assert len(corruptors) >= 1 self._corruptors = corruptors
costMatrix *= T.neq(Y, -1) return model.cost_from_cost_matrix(costMatrix)
return total / len(model_output)
use = zipped[:1]
use = zipped[1:]
return total / coeff_sum
if len(self.costs) > 1: output = self._get_samples_from_model(model, data)
get_space = lambda i: (model.aes[i].get_input_space() if i == 0 else model.aes[i - 1].get_output_space())
spaces = map(lambda c: get_space(c[0]), self.costs)
expected_energy_p = model.energy( layer_to_chains[model.visible_layer], [layer_to_chains[layer] for layer in model.hidden_layers] ).mean()
assert isinstance(model.hidden_layers[-1], Softmax)
assert isinstance(model.hidden_layers[-1], Softmax) layer_to_clamp[model.hidden_layers[-1]] = True layer_to_pos_samples[model.hidden_layers[-1]] = Y hid = model.hidden_layers[:-1]
updates, layer_to_chains = model.get_sampling_updates( layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps, return_layer_to_updated=True)
updates, layer_to_chains = model.get_sampling_updates( layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps, return_layer_to_updated=True)
assert isinstance(model.hidden_layers[-1], dbm.Softmax)
gsu = model.get_sampling_updates updates, layer_to_chains = gsu(layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps, return_layer_to_updated=True)
layer_to_chains = model.sampling_procedure.sample( layer_to_chains, self.theano_rng, layer_to_clamp=layer_to_clamp, num_steps=1 )
layer_to_chains = model.sampling_procedure.sample( layer_to_chains, self.theano_rng, num_steps=self.num_gibbs_steps )
cost = None
layers = model.get_all_layers() states = [final_state['V_hat']] + final_state['H_hat']
Y = None
inpaint_cost = 0.5 * inpaint_cost + 0.5 * new_inpaint_cost
total_cost += l1_act_cost
size = tuple([X.shape[i] for i in xrange(X.ndim)]) if self.sync_channels: del size[X_space.axes.index('c')]
rval = -T.mean(log_hx)-T.mean(log_one_minus_hy) rval.name = 'NCE('+X_name+')'
sampler_updates = self.sampler.updates()
pos_v = data neg_v = self.sampler.particles
ml_cost = (model.free_energy(pos_v).mean() - model.free_energy(neg_v).mean())
X_dense = theano.sparse.dense_from_sparse(X) noise = self.random_stream.binomial(size=X_dense.shape, n=1, prob=self.one_ratio, ndim=None)
P = noise + X_dense P = theano.tensor.switch(P > 0, 1, 0) P = tensor.cast(P, theano.config.floatX)
reg_units = theano.tensor.abs_(model.encode(X)).sum(axis=1).mean()
before_activation = model.reconstruct_without_dec_acti(X, P)
X_dense = theano.sparse.dense_from_sparse(X) noise = self.random_stream.binomial(size=X_dense.shape, n=1, prob=self.ratio, ndim=None)
P = noise + X_dense P = theano.tensor.switch(P > 0, 1, 0) P = tensor.cast(P, theano.config.floatX)
L1_units = theano.tensor.abs_(model.encode(X)).sum(axis=1).mean()
if per_example is None: return None
message = "Error while calling " + str(type(self)) + ".expr" reraise_as(TypeError(message))
composite_space = CompositeSpace(spaces) sources = tuple(sources) return (composite_space, sources)
self.get_data_specs(model)[0].validate(data)
penalty = penalty + abs(var ** self.p).sum()
fn = getattr(model, '%s_data_specs' % self.method)
assert left is not right assert left.fixed_vars is not right.fixed_vars assert left.on_load_batch is not right.on_load_batch
if self._channel_name is None: v = monitor.channels['objective'].val_record else: v = monitor.channels[self._channel_name].val_record
if v[-1] < (1. - self.prop_decrease) * self.best_value: self.countdown = self.N else: self.countdown = self.countdown - 1
return self.countdown > 0
self._original = None
preprocessor = CentralWindow(self._window_shape) for data in self._center: preprocessor.apply(data)
self._original = dict((data, _zero_pad( data.get_topological_view().astype('float32'), self._pad_randomized)) for data in randomize_now)
self.randomize_datasets(randomize_now)
if tag_key is None: tag_key = self.__class__.__name__ self._tag_key = tag_key
self.best_cost = self.coeff * np.inf self.best_model = None
model.tag[self._tag_key]['best_cost'] = self.best_cost
if self.negative_class_index is None: y = T.eq(y, self.positive_class_index)
p = mp.Process(target=train_mlp) p.start()
monitor = lm.LiveMonitor() monitor.update_channels(['train_objective'], start=0, end=2) assert(len(monitor.channels['train_objective'].val_record) == 2)
monitor = lm.LiveMonitor() monitor.update_channels(['train_objective'], start=1, end=2) assert(len(monitor.channels['train_objective'].val_record) == 1)
p.join()
assert_raises( AssertionError, monitor.update_channels, 0 )
assert_raises( AssertionError, monitor.update_channels, [] )
assert_raises( AssertionError, monitor.update_channels, ['train_objective'], start=2, end=1 )
ddata = DummyDataset(axes=('c', 0, 1, 'b')) topo = ddata.get_topological_view()
wf = wf_cls(window_shape=(3, 3), randomize=[ddata], flip=flip)
self.counter = 0
rsp_msg = rsqt_msg.get_response()
st_mode = st.st_mode read_all = stat.S_IRUSR read_all |= stat.S_IRGRP read_all |= stat.S_IROTH
os.chmod(fn, st_mode | read_all)
n_rows = 1 n_cols = np.ceil(n_plots*1./n_rows) n_cols = int(n_cols) half_perimeter = n_cols + 1
max_row = np.sqrt(n_plots) max_row = np.round(max_row) max_row = int(max_row)
colors_hue = np.arange(n_colors) colors_hue = as_floatX(colors_hue) colors_hue *= 1./n_colors
colors_hsv = np.ones((n_colors, 3)) colors_hsv[:, 2] *= .75 colors_hsv[:, 0] = colors_hue
colors_hsv = colors_hsv.reshape((1, )+colors_hsv.shape) colors_rgb = matplotlib.colors.hsv_to_rgb(colors_hsv) colors_rgb = colors_rgb[0]
n_min = plots.shape[1] n_min -= int(np.ceil(plots.shape[1] * self.share)) plots = plots[:, n_min:]
x = np.arange(plots.shape[1]) x += n_min
if self.per_second: seconds = channels['training_seconds_this_epoch'].val_record seconds = np.array(seconds) seconds = seconds.cumsum() x = seconds[x]
plt.figure() for i in xrange(self.n_colors): plt.plot(x, plots[i], color=self.colors_rgb[i], alpha=.5)
for plot in self.plots: if plot.freq is None: plot.freq = self.freq
logger.error("'{0}' not found " "but mandatory".format(this_check)) return False
logger.warning("no '{0}' found".format(this_check))
pass
while f1 != f2: f1=f2 (f2,ext)=os.path.splitext(f1)
from __future__ import print_function
dataset_sources="sources.lst" dataset_web="http://www.stevenpigeon.org/secret" dataset_conf_path="" dataset_data_path="" root_conf_path=None root_data_path=None user_conf_path=None user_data_path=None super_powers=False
packages_sources={} installed_packages_list={}
while f1 != f2: f1=f2 (f2,ext)=os.path.splitext(f1)
dst_path = os.path.dirname(os.path.abspath(dst_filename)) dst_temp_filename=os.tempnam(dst_path);
shutil.move(src_filename, dst_filename)
if not os.path.exists(dataset_conf_path): os.makedirs(dataset_conf_path)
pass
for line in f:
pass
for line in installed_list_file:
read_from_file(os.path.join(dataset_conf_path,"installed.lst"))
atomic_replace(os.path.join(dataset_conf_path,"installed.lst.2"), os.path.join(dataset_conf_path,"installed.lst"))
try: atomic_replace(temp_filename,local_dst) except Exception as e: raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
if file_access_rights(local_dst,os.W_OK,check_above=True):
try: atomic_replace(temp_filename,local_dst) except Exception as e: raise IOError("[ac] %s %s --> %s" % (str(e),temp_filename,local_dst))
this_tar_file=tarfile.open(tar_filename,"r:bz2")
try: this_tar_file.extractall(dest_path) except Exception as e: raise IOError("[tar] error while extracting '%s'" %tar_filename) else: pass
try: subprocess.check_call( script, stdout=sys.stdout, stderr=sys.stderr ) except Exception: os.chdir(cwd) raise
os.chdir(cwd)
unpack_tarball(src,dst) run_scripts(dst+package.name, scripts=["getscript","postinst"] )
del installed_packages_list[package.name]
package.where=dataset_data_path;
pass
install_package(package,temp_filename,dataset_data_path) update_installed_list("i",package)
if packages_to_upgrade==[]:
packages_really_to_upgrade=[] for this_package in packages_to_upgrade: if this_package in installed_packages_list:
installed_date=installed_packages_list[this_package].timestamp
logger.info(this_package) packages_really_to_upgrade.append(this_package)
pass
if not all_packages: logger.warning("[up] '{0}' is not installed, " "cannot upgrade.".format(this_package)) pass
pass
pass
if this_package in installed_packages_list:
packages_really_to_remove.append(this_package)
pass
pass
warnings.simplefilter("ignore", RuntimeWarning)
try: set_defaults() except Exception as e: logger.exception(e)
pass
for line in installed_list_file: l=line.rstrip().split(' ') if l: self.installed_packages_list[l[0]]=\ this_package=self.package_info(
pass
x=dataset_resolver() logger.info(x.resolve_dataset("toaster-oven")) logger.info(x.resolve_dataset("fake-dataset"))
try: iter(update_callbacks) self.update_callbacks = update_callbacks except TypeError: self.update_callbacks = [update_callbacks]
if self._count >= self.start: return self.final_momentum return self._init_momentum
mean_square_grad = sharedX(param.get_value() * 0.) mean_square_dx = sharedX(param.get_value() * 0.)
new_mean_squared_grad = ( self.decay * mean_square_grad + (1 - self.decay) * T.sqr(grads[param]) )
new_mean_square_dx = ( self.decay * mean_square_dx + (1 - self.decay) * T.sqr(delta_x_t) )
updates[mean_square_grad] = new_mean_squared_grad updates[mean_square_dx] = new_mean_square_dx updates[param] = param + delta_x_t
sum_square_grad = sharedX(param.get_value() * 0.)
new_sum_squared_grad = ( sum_square_grad + T.sqr(grads[param]) )
epsilon = lr_scalers.get(param, 1.) * learning_rate scale = T.maximum(self.eps, T.sqrt(new_sum_squared_grad)) delta_x_t = (-epsilon / scale * grads[param])
updates[sum_square_grad] = new_sum_squared_grad updates[param] = param + delta_x_t
mean_square_grad = sharedX(param.get_value() * 0.)
self.mean_square_grads[param.name] = mean_square_grad
new_mean_squared_grad = (self.decay * mean_square_grad + (1 - self.decay) * T.sqr(grads[param]))
updates[mean_square_grad] = new_mean_squared_grad updates[param] = param + delta_x_t
has_force_batch_size = getattr(model, "force_batch_size", False) train_dataset_is_uneven = \ dataset.get_num_examples() % self.batch_size != 0
nested_args = mapping.nest(theano_args) fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args) self.on_load_batch = fixed_var_descr.on_load_batch
cost_value.name = 'objective'
updates.update(dict(safe_zip(params, [param - learning_rate * lr_scalers.get(param, 1.) * grads[param] for param in params])))
for param in self.params: value = param.get_value(borrow=True) if not isfinite(value): raise RuntimeError("NaN in " + param.name)
for param in self.params: value = param.get_value(borrow=True) if not isfinite(value): raise RuntimeError("NaN in " + param.name)
return
new_lr = self._base_lr / (self.decay_factor ** self._count) if new_lr <= self.min_lr: self._min_reached = True new_lr = self.min_lr
try: model.add_polyak_channels(self._worker.param_to_mean, algorithm.monitoring_dataset) except AttributeError: pass
space, source = model.get_monitoring_data_specs()
learn_more = model.train_batch(dataset, batch_size) model.monitor.report_batch(batch_size) if not learn_more: break
cost = SumOfCosts([SumOfParams(), (0., DummyCost())]) model = DummyModel(shapes, lr_scalers=scales) dataset = ArangeDataset(1) momentum = 0.5
cost = SumOfCosts([SumOfParams(), (0., DummyCost())]) model = DummyModel(shapes, lr_scalers=scales) dataset = ArangeDataset(1) momentum = 0.5
cost = SumOfCosts([SumOfOneHalfParamsSquared(), (0., DummyCost())]) model = DummyModel(shapes, lr_scalers=scales) dataset = ArangeDataset(1) decay = 0.95
pstate['sg2'] += param_val ** 2 dx_t = - (scale * learning_rate / np.sqrt(pstate['sg2']) * param_val) rval += [param_val + dx_t]
try: AdaGrad(-1.0) allows_null = True except AssertionError: allows_null = False assert not allows_null
cost = SumOfCosts([SumOfOneHalfParamsSquared(), (0., DummyCost())])
return X
assert X.ndim == 2 return T.nnet.softmax(X*self.P)
assert X.ndim == 4 return T.nnet.softmax(X.reshape((X.shape[0], self.dim)) * self.P)
monitoring_dataset = DenseDesignMatrix(X=X, y=Y)
termination_criterion = EpochCounter(5)
monitoring_dataset = DenseDesignMatrix(X=X)
termination_criterion = EpochCounter(5)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
lr_tracker = LearningRateTracker() algorithm = SGD(learning_rate, cost, batch_size=batch_size, monitoring_batches=3, monitoring_dataset=monitoring_dataset, termination_criterion=termination_criterion, update_callbacks=[linear_decay, lr_tracker], set_batch_size=False)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
lr_tracker = LearningRateTracker() algorithm = SGD(learning_rate, cost, batch_size=batch_size, monitoring_batches=3, monitoring_dataset=monitoring_dataset, termination_criterion=termination_criterion, update_callbacks=[annealed_rate, lr_tracker], set_batch_size=False)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 15 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 6 termination_criterion = EpochCounter(epoch_num)
epoch_num = 5
monitoring_dataset = DenseDesignMatrix(X=X)
epoch_num = 2
monitoring_dataset = DenseDesignMatrix(X=X)
dummy = 'void'
monitor_lr2 = MonitorBasedLRAdjuster(channel_name=dummy)
epoch_num = 1
monitoring_train = DenseDesignMatrix(X=X) monitoring_test = DenseDesignMatrix(X=Y)
epoch_num = 1
monitoring_train = DenseDesignMatrix(X=X) monitoring_test = DenseDesignMatrix(X=Y)
m = 15 monitoring_dataset = get_topological_dataset(rng, rows, cols, channels, m)
termination_criterion = EpochCounter(5)
termination_criterion = EpochCounter(5)
termination_criterion = EpochCounter(5)
termination_criterion = EpochCounter(5)
disturb_mem.disturb_mem() rng = np.random.RandomState([2012, 11, 27])
w = rng.randn(num_features)
cost = SumOfCosts([SumOfParams(), (0., DummyCost())])
return X
cost = SumOfCosts([SumOfParams(), (0., DummyCost())])
train_with_monitoring_datasets( train_dataset=dataset1, monitoring_datasets=no_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
train_with_monitoring_datasets( train_dataset=dataset3, monitoring_datasets=no_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
train_with_monitoring_datasets( train_dataset=dataset1, monitoring_datasets=even_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
train_with_monitoring_datasets( train_dataset=dataset1, monitoring_datasets=uneven_monitoring_datasets, model_force_batch_size=False, train_iteration_mode='sequential', monitor_iteration_mode='sequential')
assert X.ndim == 2 return T.nnet.softmax(X*self.P)
monitoring_dataset = DenseDesignMatrix(X=X)
termination_criterion = EpochCounter(5)
disturb_mem.disturb_mem() rng = np.random.RandomState([2012, 11, 27, 8])
w = rng.randn(num_features)
w = rng.randn(num_features)
assert all(called)
assert unsup_counter.get_value() == train_batches assert sup_counter.get_value() == train_batches
assert grad_counter.get_value() == train_batches * updates_per_batch
nested_args = mapping.nest(theano_args) fixed_var_descr = self.cost.get_fixed_var_descr(model, nested_args) self.on_load_batch = fixed_var_descr.on_load_batch
def capture(f, mapping=mapping): new_f = lambda *args: f(mapping.flatten(args, return_tuple=True)) return new_f
from theano.tensor.nnet.conv import conv2d, ConvOp
return conv2d( x, self._filters, image_shape=self._img_shape, filter_shape=self._filters_shape, subsample=self._subsample, border_mode=self._border_mode, )
channels=3
kern_data_minor = kern_data.transpose([0,2,3,1]).copy() img_data_minor = img_data.transpose([0,2,3,1]).copy()
bias = T.dvector() kerns = T.dvector() input = T.dmatrix() rng = N.random.RandomState(3423489)
AT_xT = self.rmul_T(self.transpose_left(x, False)) rval = self.transpose_right(AT_xT, True) return rval
A_xT = self.rmul(self.transpose_right(x, True)) rval = self.transpose_left(A_xT, True) return rval
xT_AT = self.lmul_T(self.transpose_right(x, False)) rval = self.transpose_left(xT_AT, False) return rval
xT_A = self.lmul(self.transpose_left(x, True)) rval = self.transpose_right(xT_A, True) return rval
def tile_columns(self, **kwargs): raise NotImplementedError('override me')
x_s = x2[:,offset:offset+size] xWlist.append( W.lmul( x_s.reshape( (n_rows,)+W.col_shape()), T)) offset += size
xWlist = [W.lmul(x,T).flatten(2) for W in self._Wlist] rval = tensor.join(1, *xWlist)
def _tile_columns(self): raise NotImplementedError('TODO')
B, IR, IC, C = ishp4 K, KR, KC, CH = kshp4
patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH, RasterOrders.row_col_channel, RasterOrders.row_col_channel, subsample, border_mode, flip_patches=True).tocsc()
patch_stack = patches.reshape((B*OR*OC, KR*KC*CH))
output = tensor.dot(patch_stack, kerns.flatten(2).T).reshape((B, OR, OC, K))
B, C, IR, IC = ishp4 K, CH, KR, KC = kshp4
patch_extractor = sp_extract_patches(IR, IC, KR, KC, CH, RasterOrders.channel_row_col, RasterOrders.channel_row_col, subsample, border_mode, flip_patches=True).tocsc()
patch_stack = patches.reshape((B*OR*OC, KR*KC*CH))
output = tensor.dot(patch_stack, kerns.flatten(2).T).reshape((B, OR, OC, K))
if N.size(imgshp)==2: imgshp = (1,)+imgshp
indices, indptr, spmat_shape, sptype, outshp = \ convolution_indices.conv_eval(imgshp, maxpoolshp, maxpoolshp, mode='valid')
return convolution_indices.evaluate(inshp, kshp, offset, nkern, mode=mode, ws=False)
if N.size(imshp)==2: inshp = (1,)+imshp
lbound = N.array([kshp[0]-1,kshp[1]-1]) if mode=='valid' else N.zeros(2) ubound = lbound + (inshp[1:]-kshp+1) if mode=='valid' else fulloutshp
topleft = N.array([kshp[0]-1,kshp[1]-1])
spmatshp = (outsize*N.prod(kshp)*inshp[0],insize) if ws else\ (nkern*outsize,insize) spmat = scipy_sparse.lil_matrix(spmatshp)
z,zz = 0,0
tapi, ntaps = 0, 0
for ky in oy+N.arange(kshp[0]): for kx in ox+N.arange(kshp[1]):
if all((ky,kx) >= topleft) and all((ky,kx) < botright):
iy,ix = N.array((ky,kx)) - topleft col = iy*inshp[2]+ix +\
(y,x) = (oy,ox) if mode=='full' else (oy,ox) - topleft
row = (y*outshp[1]+x)*inshp[0]*ksize + l + fmapi*ksize if ws else\ y*outshp[1] + x
ntaps += 1
spmat = spmat.ensure_sorted_indices()
if numpy.size(imgshp)==2: imgshp = (1,)+imgshp
indices, indptr, spmat_shape, sptype, outshp, kmap = \ convolution_indices.sparse_eval(imgshp, kshp, nkern, step, mode)
raise NotImplementedError('partial sum')
from nose.plugins.skip import SkipTest import theano.sandbox.cuda as cuda_ndarray if cuda_ndarray.cuda_available == False: raise SkipTest('Optional package cuda disabled')
fgraph = f.maker.env
def run_autoencoder( self,
import matplotlib.pyplot as plt
try: grad_not_implemented = theano.gradient.grad_not_implemented except: def grad_not_implemented(op, idx, ipt): return None
raise NotImplementedError("non-square filter shape", (frows, fcols))
fgroups = hgroups filters_per_group = hcolors_per_group
def left_op(imgs): return self.op(imgs, self.s_filters)
weights_format = ('v', 'h')
self.colors = [np.asarray([1, 1, 0]), np.asarray([1, 0, 1]), np.asarray([0, 1, 0])]
self.xmin = xlim[0] self.xmax = xlim[1] self.delta_x = (self.xmax-self.xmin)/float(self.cols-1)
raise NotImplementedError()
W[0, 1] = .5 W[0, 2] = 1. W[0, 3] = 2.
W[0, 1] = .5 W[0, 2] = 1. W[0, 3] = 2.
self.path = preprocess(self.path) X, y = self._load_data()
dtype = 'uint8' ntrain = 50000
_logger.info('loading file %s' % datasets['test_batch']) data = serial.load(datasets['test_batch'])
Xs = {'train': x[0:ntrain], 'test': data['data'][0:ntest]}
assert start >= 0 assert stop > start assert stop <= X.shape[0] X = X[start:stop, :] y = y[start:stop, :] assert X.shape[0] == y.shape[0]
rval = X.copy()
rval = X.copy()
original_image_shape = (96, 96)
azimuth_degrees = numpy.arange(0, 341, 20)
label_type_to_index = {'category': 0, 'instance': 1, 'elevation': 2, 'azimuth': 3, 'lighting': 4}
num_labels_by_type = (len(_categories),
X = X.reshape(-1, 2 * numpy.prod(self.original_image_shape))
axes = ('b', 's', 0, 1, 'c') view_converter = StereoViewConverter(datum_shape, axes)
mono_shape = shape[:s_index] + (1, ) + shape[(s_index + 1):]
self.dataset_remote_dir = "" self.dataset_local_dir = ""
if self.dataset_local_dir == "": return filename
if not os.path.exists(remote_name): log.error("Error : Specified file %s does not exist" % remote_name) return filename
local_name = os.path.join(self.dataset_local_dir, os.path.relpath(remote_name, self.dataset_remote_dir))
if not os.path.exists(local_name):
if not self.check_enough_space(remote_name, local_name): log.warning(common_msg + "File %s not cached: Not enough free space" % remote_name) self.release_writelock() return filename
self.copy_from_server_to_local(remote_name, local_name) log.info(common_msg + "File %s has been locally cached to %s" % (remote_name, local_name))
self.get_readlock(local_name) self.release_writelock()
return ((storage_used + storage_need) < (storage_total * max_disk_usage))
atexit.register(self.release_readlock, lockdirName=lockdirName)
if (os.path.exists(lockdirName) and os.path.isdir(lockdirName)): os.rmdir(lockdirName)
datasetCache = cache.datasetCache filename = datasetCache.cache_file(filename)
data[s].ndim = len(data[s].shape)
inner_img = img[:, ring_w:img.shape[1] - ring_w, ring_w:img.shape[2] - ring_w]
inner_img = inner_img.reshape(len(output), -1) end_idx = start_idx + inner_img.shape[1] output[:, start_idx: end_idx] = inner_img
idx = 0 start_idx = end_idx for rd in rings: start_idx = downsample_ring(img, idx, rd, output, start_idx) idx += rd
img[:, ring_w:ring_w + inner_h, ring_w:ring_w + inner_w] = inner_img
idx = 0 start_idx = end_idx for rd in rings: start_idx = restore_ring(img, idx, rd, dense_input, start_idx) idx += rd
output[:, i:i + width, j:j + width] = dense_input[ :, idx][:, None, None] idx += 1
out_size = get_encoded_size(img_h, img_w, rings) output = numpy.zeros((batch_size, out_size * chans))
for chan_i in xrange(chans): channel = topo_X[..., chan_i] start_idx = foveate_channel(channel, rings, output, start_idx)
for chan_i in xrange(out_shp[-1]): channel = output[..., chan_i] start_idx = defoveate_channel(channel, rings, dense_X, start_idx)
region[np.logical_and(firstring, righthalf)] = 2 region[np.logical_and(secondring, np.logical_not(righthalf))] = 2
image_dtype = numpy.dtype(image_dtype)
self.label_index_to_name = ('category', 'instance', 'elevation', 'azimuth', 'lighting condition')
if which_norb == 'big': self.label_index_to_name = (self.label_index_to_name +
self.label_name_to_index = {} for index, name in enumerate(self.label_index_to_name): self.label_name_to_index[name] = index
image_length = 96 if which_norb == 'small' else 108
self.X_memmap_info = None self.y_memmap_info = None
result = super(NORB, self).get_topological_view(mat)
mono_shape = shape[:s_index] + (1, ) + shape[(s_index + 1):]
del result['X'] del result['y']
def get_memmap_info(memmap): assert isinstance(memmap, numpy.memmap)
'mode': 'r+' if memmap.mode in ('r+', 'w+') else 'r'}
for memmap in (self.X, self.y): memmap.flush() memmap.setflags(write=False)
data_dir = string_utils.preprocess('${PYLEARN2_DATA_PATH}') info['filename'] = os.path.join(data_dir, info['filename'])
assert hasattr(self, 'shape')
version = float('.'.join(numpy.version.version.split('.')[:2]))
datasetCache = cache.datasetCache im_path = datasetCache.cache_file(im_path)
tmp = X[i, :].copy() X[i, :] = X[j, :] X[j, :] = tmp
self._iter_mode = resolve_iterator_class('sequential')
data_specs[0].np_validate(data) assert not [contains_nan(X) for X in data] raise NotImplementedError()
npy_filename_root = os.path.join(preprocess('${PYLEARN2_DATA_PATH}'), 'icml07data', 'npy', npy_filename)
assert np.isfinite(data_x).all() assert np.isfinite(data_y).all() assert data_x.shape[0] == data_y.shape[0]
self.X_topo_space = self.view_converter.topo_space
sel = np.zeros(self.num_examples, dtype=bool) sel[next_index] = True next_index = sel
if make_new: self.filters = tables.Filters(complib='blosc', complevel=5) self.make_data(which_set, path)
h5file, node = self.init_hdf5(h_file_n, ([sizes[which_set], image_size], [sizes[which_set], 1]), title="SVHN Dataset", y_dtype='int')
rng = make_np_rng(None, 322, which_method="shuffle")
if which_set in ['train', 'test']: data_x, data_y = load_data("{0}{1}_32x32.mat".format(path, which_set))
data_y = data_y - 1
path = preprocess(path) data_x, data_y = self.make_data(which_set, path)
if center and scale: data_x -= 127.5 data_x /= 127.5 elif center: data_x -= 127.5 elif scale: data_x /= 255.
rng = make_np_rng(None, 322, which_method="shuffle")
if which_set in ['train', 'test']: data_x, data_y = load_data("{0}{1}_32x32.mat".format(path, which_set))
'float64': 0x1E3D4C53, 'int32': 0x1E3D4C54, 'uint8': 0x1E3D4C55, 'int16': 0x1E3D4C56
ndim = _read_int32(f) if debug: logger.debug('header ndim {0}'.format(ndim))
transformer = self.transformer_dataset.transformer out_space = self.data_specs[0] if isinstance(out_space, CompositeSpace): out_space = out_space.components[0]
rval_space = out_space
rval = transform(raw_batch)
rval = (transform(raw_batch[0]),) + raw_batch[1:]
FOOD_CONTAINER = 3 FRUIT = 4 FURNITURE = 6 INSECTS = 7 LARGE_OMNIVORES_HERBIVORES = 11 MEDIUM_MAMMAL = 12
if example_range: ex_range = slice(example_range[0], example_range[1]) else: ex_range = slice(None)
data_x = data['images'][set_indices] data_x = np.cast['float32'](data_x) data_x = data_x[ex_range] data_x = data_x.reshape(data_x.shape[0], image_size ** 2)
if which_set != 'unlabeled': data_y = data['labs_ex'][set_indices] data_y = data_y[ex_range] - 1
view_converter = dense_design_matrix.DefaultViewConverter((image_size, image_size, 1), axes)
super(TFD, self).__init__(X=data_x, y=data_y, y_labels=y_labels, view_converter=view_converter)
self.X -= union.mean(axis=0, dtype='float64') std = union.std(axis=0, dtype='float64') std[std < 1e-3] = 1e-3 self.X /= std
tables = None
if not hasattr(view_converter, 'topo_space'): raise NotImplementedError("Not able to get a topo_space " "from this converter: %s" % view_converter)
self.X_topo_space = view_converter.topo_space
self._iter_mode = resolve_iterator_class('sequential') self._iter_topo = False self._iter_targets = False self._iter_data_specs = (self.X_space, 'features')
space, source = data_specs if isinstance(space, CompositeSpace): sub_spaces = space.components sub_sources = source else: sub_spaces = (space,) sub_sources = (source,)
if not hasattr(view_converter, 'topo_space'): raise NotImplementedError("Not able to get a topo_space " "from this converter: %s" % view_converter)
self.X_topo_space = view_converter.topo_space
self.X_topo_space = self.view_converter.topo_space assert not contains_nan(self.X)
init_space, source = self.data_specs X_space, init_y_space = init_space.components new_y_space = VectorSpace(dim=num_classes) new_space = CompositeSpace((X_space, new_y_space)) self.data_specs = (new_space, source)
self.X_topo_space = self.view_converter.topo_space
atom = (tables.Int32Atom() if config.floatX == 'float32' else tables.Int64Atom())
[self.shape[i] for i in (2, 0, 1)])
rval = np.transpose(rval, tuple(self.axes.index(axis) for axis in ('b', 0, 1, 'c')))
if 'axes' not in d: d['axes'] = ['b', 0, 1, 'c'] self.__dict__.update(d)
if 'topo_space' not in self.__dict__: self._update_topo_space()
y = np.array([[y_i] for y_i in y]) assert min(y) == 0 assert max(y) == 2
data_x = np.cast[config.floatX](data['data']) data_x = data_x[MNISTPlus.idx[which_set]]
data_y = None if label_type is not None: data_y = data[label_type].reshape(-1, 1)
if label_type == 'azimuth': data_y = np.cast[config.floatX](data_y / 360.)
data_y = data_y[MNISTPlus.idx[which_set]]
if which_set == 'test': content = content[1:] content = content[:-1]
num_examples = {'train': 32561, 'test': 16281}[which_set] assert len(content) == num_examples, (len(content), num_examples)
content = map(lambda l: l[:-1].split(', '), content)
features = map(lambda l: l[:-1], content) targets = map(lambda l: l[-1], content) del content
space, source = data_specs if isinstance(space, CompositeSpace): sub_spaces = space.components sub_sources = source else: sub_spaces = (space,) sub_sources = (source,)
return mini_batch
assert start >= 0 assert stop > start assert stop <= X.shape[0] X = X[start:stop, :] y = y[start:stop] assert X.shape[0] == y.shape[0]
rval = X.copy()
if not hasattr(self, 'center'): self.center = False if not hasattr(self, 'gcn'): self.gcn = False
rval = X.copy()
if not hasattr(self, 'center'): self.center = False if not hasattr(self, 'gcn'): self.gcn = False
raise NotImplementedError()
im_path = serial.preprocess(im_path) label_path = serial.preprocess(label_path)
datasetCache = cache.datasetCache im_path = datasetCache.cache_file(im_path) label_path = datasetCache.cache_file(label_path)
data_train = Avicenna(which_set='train', standardize=True) assert data_train.X.shape == (150205, 120)
assert np.allclose(dt.mean(dtype='float64'), 0) assert np.allclose(dt.std(dtype='float64'), 1.)
trainer = yaml_parse.load(design_matrix_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
trainer = yaml_parse.load(topo_view_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
trainer = yaml_parse.load(convert_to_one_hot_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
trainer = yaml_parse.load(load_all_yaml % {'filename': filename}) trainer.main_loop()
os.remove(filename)
train, valid, test, transfer = utlc.load_ndarray_dataset("ule", normalize=True, transfer=True) assert train.shape[0] == transfer.shape[0]
train, valid, test, transfer = utlc.load_sparse_dataset("ule", normalize=True, transfer=True) assert train.shape[0] == transfer.shape[0]
norb_train = FoveatedNORB(which_set="train", scale=1, restrict_instances=[4, 6, 7, 8])
topo_tensors = norb.get_topological_view(single_tensor=False) expected_topo_tensors = tuple(expected_topo_tensor[:, i, ...] for i in range(2))
for norb in (SmallNORB('train', stop=1000), NORB(which_norb='small', which_set='train')): test_impl(norb)
{0: 'animal', 1: 'human', 2: 'airplane', 3: 'truck', 4: 'car', 5: 'blank'},
dict(safe_zip(range(10), range(10))),
dict(safe_zip(range(9), numpy.arange(9) * 5 + 30)),
dict(safe_zip(range(0, 36, 2), numpy.arange(0, 360, 20))),
dict(safe_zip(range(5), range(5))),
dict(safe_zip(range(-5, 6), range(-5, 6))),
dict(safe_zip(range(-5, 6), range(-5, 6))),
dict(safe_zip(range(-19, 20), range(-19, 20))),
dict(safe_zip(range(2), (0.8, 1.3))))
for (label_to_value_map, label_to_value_func) in zip(label_to_value_maps, norb.label_to_value_funcs): for label, expected_value in six.iteritems(label_to_value_map): actual_value = label_to_value_func(label) assert expected_value == actual_value
ddm = get_rnd_design_matrix() folds = ddm.split_dataset_nfolds(10) assert folds[0].shape[0] == np.ceil(ddm.get_num_examples() / 10)
d1 = DenseDesignMatrix(topo_view=topo_view) slice_d = from_dataset(d1, 5) assert slice_d.X.shape[1] == d1.X.shape[1] assert slice_d.X.shape[0] == 5
rng = np.random.RandomState([2014, 11, 4]) start = 0 stop = 990 num_examples = 1000 num_feat = 5 num_classes = 2
y = zca_dataset.mapback(zca_dataset.X) assert_allclose(x[start:stop], y)
y = zca_dataset.mapback_for_viewer(zca_dataset.X) z = x/np.abs(x).max(axis=0) assert_allclose(z[start:stop], y, rtol=1e-2)
y = zca_dataset.adjust_for_viewer(x.T).T z = x/np.abs(x).max(axis=0) assert_allclose(z, y)
assert zca_dataset.has_targets()
from pylearn2.datasets import vector_spaces_dataset
def test_split(): skip_if_no_data() n_train = 100 n_valid = 200 n_test = 300
preprocessor = GlobalContrastNormalization(subtract_mean=True, sqrt_bias=0.0, use_std=True)
preprocessor = GlobalContrastNormalization(subtract_mean=False, sqrt_bias=0.0, use_std=False)
assert preprocessor.P_.shape == (self.X.shape[1], self.X.shape[1]) assert_allclose(np.dot(preprocessor.P_, preprocessor.inv_P_), identity, rtol=1e-4)
assert_allclose(np.cov(preprocessed_X.transpose(), bias=1), identity, rtol=1e-4, atol=1e-4)
preprocessor = ZCA(filter_bias=0.0, n_components=3) preprocessed_X = self.get_preprocessed_data(preprocessor)
preprocessor = ZCA(filter_bias=0.0, n_drop_components=2) preprocessed_X = self.get_preprocessed_data(preprocessor) assert_allclose(zca_truncated_X, preprocessed_X, rtol=1e-3)
sut = PCA(self.num_components) sut.apply(self.dataset, True)
assert (np.diag(cm)[:-1] > np.diag(cm)[1:]).all()
np.testing.assert_almost_equal(np.diag(cm), np.ones(cm.shape[0]))
my_pca_preprocessor.apply(training_set, can_fit = True) my_pca_preprocessor.apply(test_set, can_fit = False)
if dataset.y is not None: dataset.y = numpy.repeat(dataset.y, num_patches / X.shape[0])
if dataset.y is not None: dataset.y = dataset.y[::patches.shape[0] / reassembled_shape[0]]
dspace.np_validate(batch) return batch
to_input = self.to_input(batch) return self.orig_view_converter.get_formatted_batch(to_input, dspace)
if not self._whiten and can_fit: assert proc_var[0] > orig_var.max()
self.matrices_save_path = None
for key, matrix in matrices.items(): del result[key]
if 'matrices_save_path' not in state: state['matrices_save_path'] = None
state = dict(state.items() + matrices.items()) del matrices
self.mean_ = numpy.mean(X, axis=0) X -= self.mean_
mid = int(numpy.floor(kernel_shape / 2.)) centered_X = X - convout[:, mid:-mid, mid:-mid, :]
transformer = Conv2D(filters=filters, batch_size=len(input), input_space=input_space, border_mode='full') sum_sqr_XX = transformer.lmul(X ** 2)
X = np.cast['float32'](X) X = X.reshape(-1, 2 * 96 * 96)
y = NORBSmall.load(which_set, 'cat') y_extra = NORBSmall.load(which_set, 'info')
self.class_names = [array[0].encode('utf-8') for array in train['class_names'][0]]
X = np.cast['float32'](train['X'])
y = train['y'][:, 0] - 1 assert y.shape == (5000,)
self.class_names = [array[0].encode('utf-8') for array in test['class_names'][0]]
y = test['y'][:, 0] - 1 assert y.shape == (8000,)
assert X.shape == (96 * 96 * 3, 100000) assert X.dtype == 'uint8'
assert x.ndim == 4 axes = self.input_space.axes assert len(axes) == 4
axes = self.output_axes assert len(axes) == 4
dummy_v = T.tensor4() dummy_v.name = 'dummy_v'
axes = self.input_space.axes assert len(axes) == 4
axes = self.input_space.axes assert len(axes) == 4
assert last_row % stride[0] == 0 num_row_steps = last_row / stride[0] + 1
assert x.ndim == 4 x_axes = self.input_axes assert len(x_axes) == 4
rval_axes = self.output_axes assert len(rval_axes) == 4
axes = self.input_axes assert len(axes) == 4
dummy_v = T.tensor4() sqfilt = T.square(self._filters)
rval, xdummy = z_hs.owner.op.grad((dummy_v, sqfilt), (x,))
axes = self.input_space.axes assert len(axes) == 4
self = layer
check_cuda(str(type(self)))
self.input_space = input_space
self._conv_op = GpuDnnConv() self._desc = GpuDnnConvDesc(border_mode=border_mode, subsample=self._subsample, conv_mode='conv')
assert x.ndim == 4 axes = self._input_space.axes assert len(axes) == 4
axes = self._output_axes assert len(axes) == 4
output = f(np.transpose(self.image, map_to_another_axes)) output_def = np.array(f_def(self.image)) output = np.transpose(output, map_to_default)
X, = data assert X.shape[0] == self.counter_idx + 1 assert X[0,0] == self.counter_idx prereq_counter = self.counter prereq_counter.set_value(prereq_counter.get_value() + 1)
@functools.wraps(fn) def wrapped(*args, **kwargs): orig_mode = config.mode if orig_mode in ["DebugMode", "DEBUG_MODE"]: config.mode = "FAST_RUN"
scipy_works = False
assert_not_debug_mode()
assert config.mode == "DEBUG_MODE" config.mode = orig_mode
cost = sum(costs) model_terms = sum([param.sum() for param in model.get_params()]) cost = cost * model_terms return cost
return (NullSpace(), '')
return (NullSpace(), '')
self.model.enforce_constraints()
continue_learning = (self.model.continue_learning() and extension_continue) assert continue_learning in [True, False, 0, 1] while continue_learning: if self.exceeded_time_budget(t0, time_budget): break
except StopIteration: log.info("Extension requested training halt.") continue_learning = False
self._samples = samples self._sigma = sigma
assert abs(exact_logz - logz) < 0.01*exact_logz
from pylearn2.datasets.mnist import MNIST dataset = MNIST(which_set='train') data = numpy.asarray(dataset.X, dtype=config.floatX)
ais_nodata('mnistvh.mat', do_exact=do_exact, betas=betas)
continue
warnings.warn("TODO: add unit test that iterators uneven property is set correctly.")
#end class
to_string(monitor)
train.save = MethodType(only_run_extensions, train)
model.dataset = dataset
space, source = data_specs if not isinstance(source, tuple): source = (source,)
target_source = self.add_mask_source(self.get_target_space(), 'targets') return target_source
rng = self.mlp.rng if self.irange is None: raise ValueError("Recurrent layer requires an irange value in " "order to initialize its weight matrices")
W = rng.uniform(-self.irange, self.irange, (input_dim, self.dim))
U = rng.randn(self.dim, self.dim) U, _ = scipy.linalg.qr(U)
b = np.zeros((self.dim,))
W, U, b = self._params if self.weight_noise: W = self.add_noise(W) U = self.add_noise(U)
z = mask[:, None] * z + (1 - mask[:, None]) * state_before
rng = self.mlp.rng if self.irange is None: raise ValueError("Recurrent layer requires an irange value in " "order to initialize its weight matrices")
W = rng.uniform(-self.irange, self.irange, (input_dim, self.dim * 4))
b = np.zeros((self.dim * 4,))
z = mask[:, None] * z + (1 - mask[:, None]) * state_before
rng = self.mlp.rng if self.irange is None: raise ValueError("Recurrent layer requires an irange value in " "order to initialize its weight matrices")
W = rng.uniform(-self.irange, self.irange, (input_dim, self.dim * 3))
b = np.zeros((self.dim * 3,))
BLACKLIST = [ 'CompositeLayer',
b01c_shape = [result.shape[0], space.shape[0], space.shape[1], space.num_channels] result = result.flatten() result = tensor.reshape(result, newshape=b01c_shape, ndim=4)
time_step = 5 return np.zeros((time_step, batch_size, self.dim), dtype=dtype)
self.space._validate_impl(is_numeric, batch[0])
time_step = 5 rval = np.zeros((time_step, batch_size), dtype=dtype) rval[:3, :1] = 1 rval[:4, 1:] = 1 return rval
return getattr(self.cost, attr)
f = function([X, y], [gradients[W].sum(), clipped_gradients[W].sum()], allow_input_downcast=True)
np.testing.assert_allclose(f([[1]], [[0]]), [20, 20 / np.sqrt(2)])
if len(set(ml)) != 1: raise ValueError("Composite space is empty or containing " "incompatible index spaces") return ml[0]
self._load_data(which_set, context_len, data_mode)
return SequenceDatasetIterator(self, data_specs, subset_iterator, return_tuple=return_tuple)
num_braces = 0
num_braces = 0
layer_1_detector = FilterActs()(images, filters)
output = FilterActs()(images, filters)
num_braces = 0
num_braces = 0
nb_channel = int(get_scalar_constant_value(images.shape[0])) assert nb_channel % 16 == 0
num_braces = 0
assert images.type.broadcastable == acts.type.broadcastable assert images.type.broadcastable == denoms.type.broadcastable assert images.type.broadcastable == dout.type.broadcastable
num_braces = 0
num_braces = 0
rows_broadcastable = False cols_broadcastable = False
flops = kerns[1] * kerns[2] * 2 #nb flops by output image flops *= out[1] * out[2] flops *= images[0] * kerns[3] * images[3] return flops
num_braces = 0
FilterActs = None WeightActs = None
rows_broadcastable = False cols_broadcastable = False
num_braces = 0
filter_rows_broadcastable = False filter_cols_broadcastable = False output_channels_broadcastable = hid_grads.type.broadcastable[0]
assert images[3] == kerns[3] flops = kerns[1] * kerns[2] * 2 #nb flops by output image flops *= out[1] * out[2] flops *= images[3] * kerns[0] * images[0] return flops
headers = super(WeightActs, self).c_headers() headers.append('weight_acts.cuh') return headers
num_braces = 0
p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols) ) func = function([z], [p, h], mode = mode_with_gpu)
p, h = max_pool_c01b(z, (pool_rows, pool_cols) ) func = function([z], [p, h], mode = mode_without_gpu)
p, h = prob_max_pool_c01b(z, (pool_rows, pool_cols), top_down = t) func = function([z, t], [p, h], mode = mode_with_gpu)
p, h = max_pool_c01b(z, (pool_rows, pool_cols), top_down = t) func = function([z, t], [p, h], mode = mode_without_gpu)
rng = np.random.RandomState([2012, 10, 9]) batch_size = 5 rows = 10 cols = 9 channels = 3 filter_rows = 4 filter_cols = filter_rows + 1 num_filters = 6
rng = np.random.RandomState([2012, 10, 9]) batch_size = 5 rows = 10 cols = 9 channels = 3 filter_rows = 4 filter_cols = filter_rows num_filters = 6
continue theano.tests.unittest_tools.verify_grad(op, [a.get_value()])
cost_weights = rng.normal(size=(num_filters, rows - filter_rows + 1, cols - filter_cols + 1, batch_size)) cost = (constant(cost_weights) * output).sum()
theano_rng = MRG_RandomStreams(2013*5*4) cost_weights = theano_rng.normal(size=output_conv2d.shape, dtype=output_conv2d.dtype) cost = (cost_weights * output).sum()
images_grad, filters_grad = grad(cost, [images, filters]) reference_cost = (cost_weights * output_conv2d).sum() images_conv2d_grad, filters_conv2d_grad = grad(reference_cost, [images, filters])
p_shared = sharedX(zv[:,0:rows:pool_rows,0:cols:pool_cols,:]) h_shared = sharedX(zv) z_shared = sharedX(zv)
grad_shared = sharedX(zv) z_shared = sharedX(zv)
def make_thunk(self, node, storage_map, compute_map, no_recycling): if not convnet_available(): raise RuntimeError('Could not compile cuda_convnet')
theano.compile.debugmode.default_make_thunk.append( get_unbound_function(BaseActs.make_thunk))
if convnet_available.compiled: _logger.debug('already compiled') return True
if convnet_available.compile_error: _logger.debug('error last time') return False
if not cuda.cuda_available: convnet_available.compile_error = True _logger.debug('cuda unavailable') return False
success = convnet_compile() if success: convnet_available.compiled = True else: convnet_available.compile_error = False _logger.debug('compilation success: %s', success)
convnet_available.compiled = False convnet_available.compile_error = False
if should_recompile(): _logger.debug('recompiling')
open(libcuda_convnet_so).close()
nvcc_compiler.add_standard_rpath(cuda_convnet_loc)
return function([], T.cast(T.argmax(self.estimated_rewards), 'int32'))
for k in xrange(n_folds): this_blocks = [] for i, layer in enumerate(layers): this_blocks.append(layer[k]) this_stacked_blocks = StackedBlocks(this_blocks) stacked_blocks.append(this_stacked_blocks)
self._folds = stacked_blocks
this_algorithm = deepcopy(algorithm) this_algorithm._set_monitoring_dataset(datasets)
this_extensions = deepcopy(extensions)
for trainer in self.trainers: for extension in trainer.extensions: extension.on_save(trainer.model, trainer.dataset, trainer.algorithm)
for extension in self.cv_extensions: extension.on_save(self.trainers)
if valid_size < 1.0: valid_size /= 1.0 - np.true_divide(self.n_test, self.n) self.valid_size = valid_size
if valid_size < 1.0: valid_size /= 1.0 - np.true_divide(self.n_test, self.n) self.valid_size = valid_size
datasets = list(datasets[label] for label in data_subsets.keys()) if len(datasets) == 1: datasets, = datasets
os.remove(filename)
this_yaml = test_yaml_which_set % {'which_set': 'train'} trainer = yaml_parse.load(this_yaml) trainer.main_loop()
this_yaml = test_yaml_which_set % {'which_set': ['train', 'test']} trainer = yaml_parse.load(this_yaml) trainer.main_loop()
this_yaml = test_yaml_which_set % {'which_set': 'valid'} try: trainer = yaml_parse.load(this_yaml) trainer.main_loop() raise AssertionError except ValueError: pass
this_yaml = test_yaml_which_set % {'which_set': 'bogus'} try: yaml_parse.load(this_yaml) raise AssertionError except ValueError: pass
trainer = yaml_parse.load(test_yaml_layer0 % {'layer0_filename': layer0_filename}) trainer.main_loop()
trainer = yaml_parse.load(test_yaml_layer1 % {'layer0_filename': layer0_filename, 'layer1_filename': layer1_filename}) trainer.main_loop()
trainer = yaml_parse.load(test_yaml_layer2 % {'layer0_filename': layer0_filename, 'layer1_filename': layer1_filename, 'layer2_filename': layer2_filename}) trainer.main_loop()
trainer = yaml_parse.load(test_yaml_layer3 % {'layer0_filename': layer0_filename, 'layer1_filename': layer1_filename, 'layer2_filename': layer2_filename}) trainer.main_loop()
os.remove(layer0_filename) os.remove(layer1_filename)
sys.exit(0)
pythonpath = os.environ.get('PYTHONPATH', '') pythonpath = throot + ':' + pythonpath os.environ['PYTHONPATH'] = pythonpath
#sys.path.append(os.path.abspath('some/directory'))
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'numpydoc', 'sphinx.ext.autosummary'] #, 'ext']
templates_path = ['.templates']
source_suffix = '.txt'
master_doc = 'index'
project = 'Pylearn2' copyright = '2011-2015, LISA lab'
version = 'dev' release = 'dev'
#today = '' today_fmt = '%B %d, %Y'
#unused_docs = []
exclude_dirs = ['images', 'scripts', 'sandbox']
#default_role = None
#add_function_parentheses = True
#add_module_names = True
#show_authors = False
pygments_style = 'sphinx'
#html_style = 'default.css' html_theme = 'solar' html_theme_path = ["./themes"]
#html_title = None
#html_short_title = None
#html_logo = 'images/theano_logo-200x67.png' #html_logo = 'images/theano_logo_allblue_200x46.png'
#html_favicon = None
html_last_updated_fmt = '%b %d, %Y'
html_use_smartypants = True
#html_sidebars = {}
#html_additional_pages = {}
#html_use_modindex = True
#html_use_index = True
#html_split_index = False
#html_copy_source = True
#html_use_opensearch = ''
#html_file_suffix = ''
htmlhelp_basename = 'theanodoc'
#latex_paper_size = 'letter'
latex_font_size = '11pt'
latex_documents = [ ('index', 'pylearn2.tex', 'Pylearn2 Documentation', 'LISA lab, University of Montreal', 'manual'), ]
#latex_logo = 'images/snake_theta2-trans.png' latex_logo = None
#latex_use_parts = False
#latex_preamble = ''
#latex_appendices = []
#latex_use_modindex = True
from __future__ import absolute_import import copy import errno import fnmatch import glob import hashlib import logging import os import shutil from datetime import datetime from salt.exceptions import FileserverConfigError
import salt.ext.six as six try: import hglib HAS_HG = True except ImportError: HAS_HG = False
import salt.utils import salt.utils.url import salt.fileserver from salt.utils.event import tagify
__virtualname__ = 'hg'
return repo.branches()
pass
hglib.init(rp_) new_remote = True
try: shutil.rmtree(repo['lockfile']) except OSError as exc: _add_error(failed, repo, exc)
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
os.remove(destdir) os.makedirs(destdir)
os.remove(hashdir) os.makedirs(hashdir)
repo['repo'].close() continue
return []
if not relpath.startswith('../'): ret.add(os.path.join(repo['mountpoint'], relpath))
return []
from __future__ import absolute_import import logging
AUTH_PROVIDERS = ('pygit2',) AUTH_PARAMS = ('user', 'password', 'pubkey', 'privkey', 'passphrase', 'insecure_auth')
import salt.utils.gitfs from salt.exceptions import FileserverConfigError
__virtualname__ = 'git'
return __virtualname__
return []
import os import errno import logging
import salt.fileserver import salt.utils from salt.utils.event import tagify import salt.ext.six as six
return fnd
return fnd
pass
data = {'changed': False, 'files': {'changed': []}, 'backend': 'roots'}
new_mtime_map = salt.fileserver.generate_mtime_map(__opts__['file_roots'])
data['changed'] = salt.fileserver.diff_mtime_map(old_mtime_map, new_mtime_map)
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
try: os.unlink(cache_path) except OSError: pass return file_hash(load, fnd)
pass
return []
from __future__ import absolute_import import errno import fnmatch import logging import os import re import time
import salt.loader import salt.utils import salt.utils.locales
import salt.ext.six as six
time.sleep(1) if not os.path.isfile(dest): _unlock_cache(lk_fn) return False
wait_lock(w_lock, list_cache, 15 * 60)
cache_stat = os.stat(list_cache) age = time.time() - cache_stat.st_mtime
age = opts.get('fileserver_list_cache_time', 30) + 1
refresh_cache = True break
log.info( 'Failed to get mtime on {0}, ' 'dangling symlink ?'.format(file_path)) continue
if sorted(map1) != sorted(map2): #log.debug('diff_mtime_map: the maps are different') return True
#log.debug('diff_mtime_map: the maps are the same') return False
path = salt.utils.url.unescape(path)
continue
import os import logging
import salt.fileserver import salt.utils import salt.utils.url
__virtualname__ = 'minion'
pass
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
#def file_list_emptydirs(load):
from __future__ import absolute_import import datetime import os import time import pickle import logging
import salt.fileserver as fs import salt.modules import salt.utils import salt.utils.s3 as s3
import salt.ext.six as six from salt.ext.six.moves import filter from salt.ext.six.moves.urllib.parse import quote as _quote
metadata = _init() return list(metadata.keys())
_get_file_from_s3(metadata, saltenv, bucket, file_path, cached_file_path)
_get_file_from_s3(metadata, saltenv, fnd['bucket'], path, cached_file_path)
cached_file_path = _get_cached_file_name( fnd['bucket'], load['saltenv'], fnd['path'])
for dirs in six.itervalues(_find_dirs(metadata[saltenv])): dirs = _trim_env_off_path(dirs, saltenv, trim_slash=True) ret += [_f for _f in dirs if _f]
metadata = None try: if os.path.getmtime(cache_file) > exp: metadata = _read_buckets_cache_file(cache_file) except OSError: pass
metadata = _refresh_buckets_cache_file(cache_file)
return os.path.join(__opts__['cachedir'], 's3cache')
if not os.path.exists(os.path.dirname(file_path)): os.makedirs(os.path.dirname(file_path))
def __get_s3_meta(bucket, key=key, keyid=keyid): return s3.query( key=key, keyid=keyid, kms_keyid=keyid, bucket=bucket, service_url=service_url, verify_ssl=verify_ssl, location=location, return_bin=False)
for saltenv, buckets in six.iteritems(_get_buckets()): bucket_files = {} for bucket_name in buckets: s3_meta = __get_s3_meta(bucket_name)
if not s3_meta: continue
bucket_files[bucket_name] = [k for k in s3_meta if 'Key' in k]
for bucket_name in _get_buckets(): s3_meta = __get_s3_meta(bucket_name)
if not s3_meta: continue
files = [k for k in s3_meta if 'Key' in k]
for saltenv in environments: env_files = [k for k in files if k['Key'].startswith(saltenv)]
if os.path.isfile(cache_file): os.remove(cache_file)
ret[bucket_name] += [k for k in filePaths if not k.endswith('/')]
item_meta['ETag'] = item_meta['ETag'].strip('"')
if os.path.isfile(cached_file_path): file_meta = _find_file_meta(metadata, bucket_name, saltenv, path) if file_meta: file_etag = file_meta['ETag']
if cached_md5 == file_md5: return
s3.query( key=key, keyid=keyid, kms_keyid=keyid, bucket=bucket_name, service_url=service_url, verify_ssl=verify_ssl, location=location, path=_quote(path), local_file=cached_file_path )
from __future__ import absolute_import import copy import errno import fnmatch import hashlib import logging import os import shutil from datetime import datetime from salt.exceptions import FileserverConfigError
import salt.ext.six as six HAS_SVN = False try: import pysvn HAS_SVN = True CLIENT = pysvn.Client() except ImportError: pass
import salt.utils import salt.utils.url import salt.fileserver from salt.utils.event import tagify
__virtualname__ = 'svn'
pass
try: shutil.rmtree(repo['lockfile']) except OSError as exc: _add_error(failed, repo, exc)
if six.text_type(remote) not in repo['url']: continue
if not fnmatch.fnmatch(repo['url'], six.text_type(remote)): continue
continue
ret.add('base')
continue
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
continue
continue
for key in ret: ret[key] = sorted(ret[key]) if save_cache: salt.fileserver.write_file_list_cache( __opts__, ret, list_cache, w_lock ) return ret.get(form, [])
from __future__ import absolute_import import os import os.path import logging import time
HAS_FCNTL = False
import salt.fileserver import salt.utils import salt.syspaths
return fnd
return fnd
pass
envs = __opts__.get('azurefs_envs', []) for env in envs: storage_conn = azure.get_storage_conn(opts=envs[env]) result = azure.list_blobs( storage_conn=storage_conn, container=env, )
for blob in result: file_name = os.path.join(base_dir, blob)
comps = file_name.split('/') file_path = '/'.join(comps[:-1]) if not os.path.exists(file_path): os.makedirs(file_path)
azure.get_blob( storage_conn=storage_conn, container=env, name=blob, local_path=file_name, )
if not path or not os.path.isfile(path): return ret
ret['hash_type'] = __opts__['hash_type']
from __future__ import print_function from __future__ import absolute_import import os
file_root = os.path.abspath(self.options.file_root) self.config['file_roots'] = {'base': _expand_glob_path([file_root])}
pillar_root = os.path.abspath(self.options.pillar_root) self.config['pillar_roots'] = {'base': _expand_glob_path([pillar_root])}
states_dir = os.path.abspath(self.options.states_dir) self.config['states_dirs'] = [states_dir]
self.setup_logfile_logger() verify_log(self.config)
from __future__ import absolute_import import os import warnings from salt.utils.verify import verify_log
warnings.filterwarnings(
warnings.filterwarnings( 'ignore', 'With-statements now directly support multiple context managers', DeprecationWarning )
warnings.filterwarnings( 'ignore', '^Module backports was already imported from (.*), but (.*) is being added to sys.path$', UserWarning )
import salt.log.setup
from salt.utils import migrations from salt.utils import kinds
log = salt.log.setup.logging.getLogger(__name__)
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
import salt.master self.master = salt.master.Master(self.config)
import salt.daemons.flo self.master = salt.daemons.flo.IofloMaster(self.config)
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
if self.check_running(): self.action_log_info('An instance is already running. Exiting') self.shutdown(1)
self.config['id'] = self.values.proxyid
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
from __future__ import absolute_import, print_function
pass
import salt.ext.six as six
from salt.exceptions import ( SaltClientError, CommandNotFoundError, CommandExecutionError, SaltInvocationError, )
ttype = 'zeromq'
try: self.minion = salt.minion.SMinion(opts) except SaltClientError as exc: raise SystemExit(str(exc))
pass
return ret
self.process = MultiprocessingProcess(target=raet_minion_run, kwargs={'cleanup_protecteds': [self.stack.ha], }) self.process.start() self._wait_caller(opts)
from __future__ import absolute_import, print_function import math import time import copy from datetime import datetime, timedelta
import salt.client import salt.output import salt.exceptions from salt.utils import print_cli
import salt.ext.six as six from salt.ext.six.moves import range
bwait = self.opts.get('batch_wait', 0) wait = []
if queue in minion_tracker: minion_tracker[queue]['active'] = False
for minion in minion_tracker[queue]['minions']: if minion not in parts: parts[minion] = {} parts[minion]['ret'] = {}
ret[minion] = data yield {minion: data}
from __future__ import absolute_import, print_function import os import logging
import salt.client.netapi import salt.utils.parsers as parsers from salt.utils.verify import check_user, verify_files, verify_log
current_umask = os.umask(0o027) verify_files([logfile], self.config['user']) os.umask(current_umask)
from __future__ import print_function from __future__ import absolute_import
self.setup_logfile_logger() verify_log(self.config) profiling_enabled = self.options.profiling_enabled
from __future__ import print_function from __future__ import absolute_import
from __future__ import absolute_import
import salt.spm import salt.utils.parsers as parsers from salt.utils.verify import verify_log
from __future__ import print_function from __future__ import absolute_import import os import sys
import salt.client from salt.utils import parsers, print_cli from salt.utils.verify import verify_log import salt.output
self.setup_logfile_logger() verify_log(self.config)
from __future__ import absolute_import, print_function import os import sys
import salt.ext.six as six
self.setup_logfile_logger() verify_log(self.config)
skip_perm_errors = self.options.eauth != ''
if retcodes.count(0) < len(retcodes): sys.stderr.write('ERROR: Minions returned with non-zero exit code\n') sys.exit(11)
sys.exit(1)
import contextlib import errno import logging import os import shutil import subprocess import time
import salt.utils import salt.modules.selinux from salt.exceptions import CommandExecutionError, FileLockError, MinionError
from salt.ext import six
fh_ = os.open(lock_fn, open_flags)
with os.fdopen(fh_, 'w'): pass log.trace('Write lock %s obtained', lock_fn) obtained_lock = True yield break
from __future__ import absolute_import import sys import copy import types
import salt.loader
from __future__ import absolute_import
import os import urlparse
from salt.ext.six.moves.urllib.parse import urlparse
import salt.fileclient import salt.utils.url
from __future__ import absolute_import import json import pprint import logging from os import path from functools import wraps
import salt.ext.six as six from jinja2 import BaseLoader, Markup, TemplateNotFound, nodes from jinja2.environment import TemplateModule from jinja2.ext import Extension from jinja2.exceptions import TemplateRuntimeError import jinja2 import yaml
import salt import salt.utils import salt.utils.url import salt.fileclient from salt.utils.odict import OrderedDict
if '..' in template: log.warning( 'Discarded template path \'{0}\', relative paths are ' 'prohibited'.format(template) ) raise TemplateNotFound(template)
continue
raise TemplateNotFound(template)
output.append('\'{0}\': \'{1}\''.format(key, value))
output.append('\'{0}\': {1!s}'.format(key, value))
output.append('\'{0}\': {1!r}'.format(key, value))
{% load_yaml as var1 %} foo: it works {% endload %} {% load_yaml as var2 %} bar: for real {% endload %}
{% from "doc1.sls" import var1, var2 as local2 %} {{ var1.foo }} {{ local2.bar }}
from __future__ import absolute_import import os import threading
import salt.utils import salt.payload
pass
try: os.remove(path) except IOError: pass return None
return None
try: os.remove(path) except IOError: pass return None
from __future__ import absolute_import import re
yield val
import salt.utils from salt.exceptions import SaltException
@property def buffered(self): return self.__buffered
def __iter__(self): return self
def __enter__(self): return self
from __future__ import absolute_import from collections import Callable
import salt.ext.six as six
import collections
try: from salt.ext.six.moves._thread import get_ident as _get_ident except ImportError: from salt.ext.six.moves._dummy_thread import get_ident as _get_ident
if key not in self: root = self.__root last = root[0] last[1] = root[0] = self.__map[key] = [last, root, key] dict_setitem(self, key, value)
dict_delitem(self, key) link_prev, link_next, key = self.__map.pop(key) link_prev[1] = link_next link_next[0] = link_prev
from __future__ import absolute_import, print_function import os import re import time import logging try: import msgpack HAS_MSGPACK = True except ImportError: HAS_MSGPACK = False
import salt.config import salt.payload import salt.utils.dictupdate
self._write()
if __name__ == '__main__':
import logging
try: import requests
import salt.utils import salt.utils.aws import salt.utils.xmlutil as xml from salt._compat import ElementTree as ET
if not key: key = salt.utils.aws.IROLE_CODE
log.debug(' Response content: {0}'.format(response))
if return_bin: return response
from __future__ import absolute_import import copy import contextlib
import salt.ext.six as six
PER_REMOTE_ONLY = ('name',) SYMLINK_RECURSE_DEPTH = 100
AUTH_PROVIDERS = ('pygit2',) AUTH_PARAMS = ('user', 'password', 'pubkey', 'privkey', 'passphrase', 'insecure_auth')
try: import git import gitdb HAS_GITPYTHON = True except ImportError: HAS_GITPYTHON = False
if not isinstance(err, ImportError): log.error('Import pygit2 failed: {0}'.format(err))
GITPYTHON_MINVER = '0.3' PYGIT2_MINVER = '0.20.3' LIBGIT2_MINVER = '0.20.0' DULWICH_MINVER = (0, 9, 4)
per_remote_only = {} for param in PER_REMOTE_ONLY: if param in per_remote_conf: per_remote_only[param] = per_remote_conf.pop(param)
if 'root' not in repo_conf: repo_conf['root'] = ''
for key, val in six.iteritems(repo_conf): setattr(self, key, val)
self.mountpoint = ''
pass
try: shutil.rmtree(lock_file) except OSError as exc: _add_error(failed, exc)
return self._fetch()
os.write(fh_, str(os.getpid()))
pid = 0
head_sha = None
raise GitLockError( exc.errno, 'Checkout lock exists for {0} remote \'{1}\'' .format(self.role, self.id) )
self.repo.git.tag('-d', ref.name[10:])
self.repo = git.Repo.init(self.cachedir) new = True
pass
return files, symlinks
return None, None
break
if not self.env_is_exposed(tgt_env): return None
log.error( 'Unable to get SHA of HEAD for %s remote \'%s\'', self.role, self.id ) return None
self.repo.checkout(checkout_ref) if branch: self.repo.reset(oid, pygit2.GIT_RESET_HARD)
raise GitLockError( exc.errno, 'Checkout lock exists for {0} remote \'{1}\'' .format(self.role, self.id) )
oid = self.repo.lookup_reference(remote_ref).get_object().id if local_ref not in refs: self.repo.create_reference(local_ref, oid)
return self.check_root()
tag_sha = tag_obj.target.hex
log.error( 'Unable to resolve %s from %s remote \'%s\' ' 'to either an annotated or non-annotated tag', tag_ref, self.role, self.id ) return None
return self.check_root()
remote_refs.append( line.split()[-1].replace(b'refs/heads/', b'refs/remotes/origin/') )
continue
self.repo = pygit2.init_repository(self.cachedir) new = True
pass
continue
received_objects = fetch_results['received_objects']
received_objects = fetch_results.received_objects
continue
return files, symlinks
oid = tree[self.root].oid tree = self.repo[oid]
return None, None
link_tgt = self.repo[tree[path].oid].data path = os.path.normpath( os.path.join(os.path.dirname(path), link_tgt) )
if not self.env_is_exposed(tgt_env): return None try: commit = self.repo.revparse_single(tgt_ref) except (KeyError, TypeError): pass else: return commit.tree return None
return True
return True
transport = 'ssh' address = self.url
return True
return True
continue
log.warning( '{0} remote \'{1}\' is an empty repository and will ' 'be skipped.'.format(self.role, self.id) ) return False
for ref in self.get_env_refs(refs_post): self.repo[ref] = refs_post[ref] for ref in refs_pre: if ref not in refs_post: del self.repo[ref] return True
continue
return None, None
break
if not self.env_is_exposed(tgt_env): return None try: int(tgt_ref, 16) except ValueError: return None
return None
pass
self.url = 'git+' + self.url
self.repo = dulwich.repo.Repo.init(self.cachedir) new = True
for parent in path.split(os.path.sep): try: tree = self.repo.get_object(tree[parent][1]) except (KeyError, TypeError): return None return tree
try: if not fnmatch.fnmatch(repo.url, remote): continue except TypeError: if not fnmatch.fnmatch(repo.url, six.text_type(remote)): continue
try: if not fnmatch.fnmatch(repo.url, remote): continue except TypeError: if not fnmatch.fnmatch(repo.url, six.text_type(remote)): continue
data = {'changed': False, 'backend': 'gitfs'}
pygit2ver = distutils.version.LooseVersion(pygit2.__version__) pygit2_minver = distutils.version.LooseVersion(PYGIT2_MINVER)
os.remove(destdir) os.makedirs(destdir)
os.remove(hashdir) os.makedirs(hashdir)
return fnd
return []
GitBase.__init__(self, opts, valid_providers=('gitpython', 'pygit2'))
GitBase.__init__(self, opts, valid_providers=('gitpython', 'pygit2'), cache_root=winrepo_dir)
sls[ks_opts['lang']['lang']] = {'locale': ['system']}
sls[ks_opts['keyboard']['xlayouts']] = {'keyboard': ['system']}
if 'selinux' in ks_opts.keys(): for mode in ks_opts['selinux']: sls[mode] = {'selinux': ['mode']}
if 'nobase' not in ks_opts['packages']['options']: sls['base'] = {'pkg_group': ['installed']}
from __future__ import absolute_import import os import sys import stat import codecs import shutil import hashlib import socket import tempfile import time import subprocess import multiprocessing import logging import pipes import msgpack import traceback import copy import re import uuid
try: import pwd except ImportError: if not sys.platform.lower().startswith('win'): raise
import salt.cloud from salt.exceptions import ( SaltCloudConfigError, SaltCloudException, SaltCloudSystemExit, SaltCloudExecutionTimeout, SaltCloudExecutionFailure, SaltCloudPasswordError )
import salt.ext.six as six
log = logging.getLogger(__name__)
with salt.utils.fopen(path, 'r') as fp_: return fp_.read()
return __render_script(os_, vm_, opts, minion)
return __render_script('{0}.sh'.format(os_), vm_, opts, minion)
return ''
if keysize < 2048: keysize = 2048 tdir = tempfile.mkdtemp()
minion = { 'master': 'salt', 'log_level': 'info', 'hash_type': 'sha256', }
minion.setdefault('grains', {}).update( salt.config.get_cloud_config_value( 'grains', vm_, opts, default={}, search_global=True ) ) return minion
master = copy.deepcopy(salt.config.DEFAULT_MASTER_OPTS) master.update( log_level='info', log_level_logfile='info' )
master.update( salt.config.get_cloud_config_value( 'master', vm_, opts, default={}, search_global=True ) ) return master
if 'pub_key' not in vm_ and 'priv_key' not in vm_: log.debug('Generating keys for \'{0[name]}\''.format(vm_))
if 'gateway' in vm_: deploy_kwargs.update({'gateway': vm_['gateway']})
usernames = [x for x in usernames if x] initial = usernames[:]
test_ssh_host = host test_ssh_port = port
sock.shutdown(socket.SHUT_RDWR) sock.close() break
log.debug( 'Gateway {0} on port {1} is reachable.'.format( test_ssh_host, test_ssh_port ) )
kwargs = {'hostname': host, 'creds': creds}
raise DeprecationWarning( '`salt.utils.cloud.deploy_script now only accepts ' 'dictionaries for it\'s `minion_conf` parameter. ' 'Loading YAML...' )
raise DeprecationWarning( '`salt.utils.cloud.deploy_script now only accepts ' 'dictionaries for it\'s `master_conf` parameter. ' 'Loading from YAML ...' )
for minion_id, minion_key in six.iteritems(preseed_minion_keys): rpath = os.path.join( preseed_minion_keys_tempdir, minion_id ) ssh_file(opts, rpath, minion_key, ssh_kwargs)
time.sleep(0.025)
return 1
'-oStrictHostKeyChecking=no', '-oUserKnownHostsFile=/dev/null', '-oControlPath=none'
'-oStrictHostKeyChecking=no', '-oUserKnownHostsFile=/dev/null', '-oControlPath=none'
ssh_args.extend([ '-oPasswordAuthentication=no', '-oChallengeResponseAuthentication=no', '-oPubkeyAuthentication=yes', '-oIdentitiesOnly=yes', '-oKbdInteractiveAuthentication=no', '-oIdentityFile={0}'.format(kwargs['key_filename']) ])
return 1
ssh_args.extend(['-t', '-t'])
'-oStrictHostKeyChecking={0}'.format(host_key_checking), '-oUserKnownHostsFile={0}'.format(known_hosts_file), '-oControlPath=none'
if ip.startswith('fe80:'): return False return True
return False
return False
return False
else: script_content = url script_name = '{0}.sh'.format( hashlib.sha1(script_content).hexdigest() )
builtin_deploy_dir = os.path.join( os.path.dirname(__file__), 'deploy' )
deploy_d_from_conf_file = os.path.join( os.path.dirname(config['conf_file']), 'cloud.deploy.d' )
deploy_d_from_syspaths = os.path.join( syspaths.CONFIG_DIR, 'cloud.deploy.d' )
deploy_scripts_search_paths = [] for entry in config.get('deploy_scripts_search_path', []): if entry.startswith(builtin_deploy_dir): continue
deploy_scripts_search_paths.append((entry, True))
if deploy_d_from_conf_file not in deploy_scripts_search_paths: deploy_scripts_search_paths.append( (deploy_d_from_conf_file, True) ) if deploy_d_from_syspaths not in deploy_scripts_search_paths: deploy_scripts_search_paths.append( (deploy_d_from_syspaths, True) )
if entry in finished: continue else: finished.append(entry)
missing_node_cache(prov_dir, nodes, provider, opts)
u'\xa0': u' ', u'\u2013': u'-',
raise exc
raise RuntimeError('Invalid password provided.')
import re import inspect
import salt.ext.six as six
return arg
if (isinstance(original_arg, six.string_types) and not original_arg.startswith('{')): return original_arg else: return arg
return original_arg
return original_arg
salt.utils.compat.pack_dunder(__name__)
from __future__ import absolute_import import hashlib import logging import sys
try: import boto import boto.exception logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import absolute_import import atexit import logging import time
from salt.exceptions import SaltSystemExit import salt.modules.cmdmod import salt.utils
try: from pyVim.connect import GetSi, SmartConnect, Disconnect from pyVmomi import vim, vmodl HAS_PYVMOMI = True except ImportError: HAS_PYVMOMI = False
log = logging.getLogger(__name__)
if port is None: port = 443 if protocol is None: protocol = 'https'
esx_cmd += ' -s {0} -u {1} -p \'{2}\' ' \ '--protocol={3} --portnumber={4} {5}'.format(host, user, pwd, protocol, port, cmd)
if not container_ref: container_ref = service_instance.content.rootFolder
obj_view = service_instance.content.viewManager.CreateContainerView( container_ref, [obj_type], True)
traversal_spec = vmodl.query.PropertyCollector.TraversalSpec( name='traverseEntities', path='view', skip=False, type=vim.view.ContainerView )
property_spec = vmodl.query.PropertyCollector.PropertySpec( type=obj_type, all=True if not property_list else False, pathSet=property_list )
obj_spec = vmodl.query.PropertyCollector.ObjectSpec( obj=obj_view, skip=True, selectSet=[traversal_spec] )
filter_spec = vmodl.query.PropertyCollector.FilterSpec( objectSet=[obj_spec], propSet=[property_spec], reportMissingObjectsInResults=False )
content = service_instance.content.propertyCollector.RetrieveContents([filter_spec])
obj_view.Destroy()
object_list = get_mors_with_properties(service_instance, object_type, property_list=[property_name], container_ref=container_ref)
content = get_content(service_instance, object_type, property_list=property_list, container_ref=container_ref)
import json import logging import time import pprint from salt.ext.six.moves import range import salt.ext.six as six import salt.utils try: import requests
return str(key)
log.error('Failed to read region from instance metadata. Giving up.')
log.error('Failed to read metadata. Giving up on IAM credentials.')
from __future__ import absolute_import import warnings
import yaml from yaml.nodes import MappingNode, SequenceNode from yaml.constructor import ConstructorError try: yaml.Loader = yaml.CLoader yaml.Dumper = yaml.CDumper except Exception: pass
if node.value == '': node.value = '0'
existing_nodes = [name_node.value for name_node, value_node in node.value] mergeable_items = [x for x in merge if x[0].value not in existing_nodes]
from __future__ import absolute_import import os import fnmatch import re import logging
import salt.payload import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import CommandExecutionError import salt.auth.ldap import salt.ext.six as six
import salt.ext.six as six if six.PY3: import ipaddress else: import salt.ext.ipaddress as ipaddress HAS_RANGE = False try:
if expanded_nodegroup or not first_call: return ret else: log.debug('No nested nodegroups detected. ' 'Using original nodegroup definition: {0}' .format(nodegroups[nodegroup])) return nodegroups[nodegroup]
tgt = ipaddress.ip_address(tgt)
tgt = ipaddress.ip_network(tgt)
log.error('Detected nodegroup expansion failure of "{0}"'.format(word)) return []
log.error( 'Unrecognized target engine "{0}" for' ' target expression "{1}"'.format( target_info['engine'], word, ) ) return []
results.extend([')' for item in unmatched])
allowed_minions = set()
if len(minions - allowed_minions_from_auth_list) > 0: return False
if mismatch: return False
if self.match_check(ind, fun): return True
continue
continue
from __future__ import absolute_import import re import sys
resource = url.split('salt://', 1)[-1]
import os import sys import time import errno import signal import select import logging
from win32file import ReadFile, WriteFile from win32pipe import PeekNamedPipe import msvcrt import win32api import win32con import win32process
import salt.utils from salt.ext.six import string_types from salt.log.setup import LOG_LEVELS
pass
rows=None, cols=None,
log_stdin=None, log_stdin_level='debug', log_stdout=None, log_stdout_level='debug', log_stderr=None, log_stderr_level='debug',
stream_stdout=None, stream_stderr=None, ):
_cleanup()
self.pid = None self.stdin = None self.stdout = None self.stderr = None
self.status = None self.__irix_hack = 'irix' in sys.platform.lower()
try: self._spawn()
def _translate_newlines(self, data): if data is None or not data: return return data.replace('\r\n', os.linesep)
def __enter__(self): return self
if self.isalive(): self.wait()
if mswindows: def _execute(self): raise NotImplementedError
ecode = win32process.GetExitCodeProcess(self._handle) if ecode == win32con.STILL_ACTIVE: raise self.exitstatus = ecode
else: def _spawn(self): self.pid, self.child_fd, self.child_fde = self.__fork_ptys()
self.stdin = sys.stdin.fileno() self.stdout = sys.stdout.fileno() self.stderr = sys.stderr.fileno()
self.child_fd = self.stdin
max_fd = resource.getrlimit(resource.RLIMIT_NOFILE) try: os.closerange(pty.STDERR_FILENO + 1, max_fd[0]) except OSError: pass
self.closed = False self.terminated = False
os.close(stdout_parent_fd) os.close(stderr_parent_fd) salt.utils.reinit_crypto()
child_name = os.ttyname(stdout_child_fd) try: tty_fd = os.open('/dev/tty', os.O_RDWR | os.O_NOCTTY) if tty_fd >= 0: os.close(tty_fd)
pass
os.setsid()
pass
os.dup2(stdout_child_fd, pty.STDIN_FILENO) os.dup2(stdout_child_fd, pty.STDOUT_FILENO) os.dup2(stderr_child_fd, pty.STDERR_FILENO)
salt.utils.reinit_crypto() os.close(stdout_child_fd) os.close(stderr_child_fd)
if self.child_fd: fd_flags = fcntl.fcntl(self.child_fd, fcntl.F_GETFL) if self.child_fde: fde_flags = fcntl.fcntl(self.child_fde, fcntl.F_GETFL)
rlist, _, _ = select.select(rfds, [], [], 0)
if self.child_fde in rlist: try: stderr = self._translate_newlines( salt.utils.to_str( os.read(self.child_fde, maxsize) ) )
if self.child_fd in rlist: try: stdout = self._translate_newlines( salt.utils.to_str( os.read(self.child_fd, maxsize) ) )
return stdout, stderr
return 24, 80
waitpid_options = 0
time.sleep(0.1) if not self.isalive(): return True else: return False
return
if self.isalive() and _ACTIVE is not None: _ACTIVE.append(self)
import salt.ext.six as six
import salt.utils import salt.defaults.exitcodes from salt.utils.filebuffer import BufferedReader
log = logging.getLogger(__name__)
continue
self.criteria = criteria[_REQUIRES_PATH] + \ criteria[_REQUIRES_STAT] + \ criteria[_REQUIRES_CONTENTS]
from __future__ import absolute_import
import fnmatch import glob import logging
def __setstate__(self, state): self._is_child = True Reactor.__init__( self, state['opts'], log_queue=state['log_queue'])
for name in res: res[name]['__sls__'] = fn_
client_cache = None event_user = 'Reactor'
from __future__ import absolute_import import io
import yaml import salt.ext.six as six
import os
return True
return False
return False
parent_dir = os.path.dirname(path)
return False
return os.access(parent_dir, os.W_OK)
return True
return False
import re import socket
from salt.ext.six import string_types import salt.utils
if salt.utils.is_windows():
try: socket.inet_pton(address_family, ip) except socket.error: return False
try: mask = int(mask) except ValueError: return False else: if not 1 <= mask <= int(mask_max): return False
import re import logging from salt.ext.six import string_types
from __future__ import absolute_import from __future__ import print_function
from __future__ import absolute_import import logging import collections import salt.exceptions
raise salt.exceptions.CommandExecutionError(lazy_obj.missing_fun_string(fun))
return bool(self._dict or not self.loaded)
return self.__nonzero__()
self._dict = {}
self.loaded = False
if not self.loaded: self._load_all() return len(self._dict)
import os import logging import smtplib from email.utils import formatdate
from __future__ import absolute_import, division, print_function import contextlib import copy import collections import datetime
from salt.ext import six
from salt.ext.six.moves import range from salt.ext.six.moves import zip from salt.ext.six.moves import map from stat import S_IMODE
try: import Crypto.Random HAS_CRYPTO = True except ImportError: HAS_CRYPTO = False
HAS_FCNTL = False
HAS_GRP = False
HAS_PWD = False
return False
if use in colors: for color in colors: if color == 'ENDC': continue colors[color] = colors[use]
if line > num_template_lines: return template
buf = [to_str(i) if isinstance(i, six.text_type) else i for i in buf]
user_name = 'SYSTEM'
reinit_crypto() sys.exit(salt.defaults.exitcodes.EX_OK)
os.chdir('/') os.setsid() os.umask(18)
return (os.access(exe, os.X_OK) and (os.path.isfile(exe) or os.path.islink(exe)))
return exe
log.error(err)
os.makedirs(fn_)
stamp = time.strftime('%a_%b_%d_%H-%M-%S_%Y')
parts = [os.path.normpath(p) for p in parts]
finger += '{0}:'.format(pre[ind])
data = data.copy()
pass
for key, value in six.iteritems(data): if key in expected_extra_kws: continue ret['kwargs'][key] = value
return ret
extra = {} for key, value in six.iteritems(data): if key in expected_extra_kws: continue extra[key] = copy.deepcopy(value)
warn_until( 'Carbon', 'It\'s time to start raising `SaltInvocationError` instead of ' 'returning warnings', _dont_call_warnings=True )
ret.setdefault('context', {}).update(extra)
try: with fopen(fp_, 'rb') as fp2_: block = fp2_.read(blocksize) except IOError: return False
return False
return True
try:
return True
return True
return default
for embedded in (x for x in data if isinstance(x, dict)): try: data = embedded[each] embed_match = True break except KeyError: pass if not embed_match: return default
is_proxy = False try: if 'salt-proxy' in main.__file__: is_proxy = True except AttributeError: pass return is_proxy
if include_pat and not exclude_pat: ret = retchk_include elif exclude_pat and not include_pat: ret = retchk_exclude elif include_pat and exclude_pat: ret = retchk_include and retchk_exclude else: ret = True
elif result is _empty and isinstance(state_result, dict) and ret: ret = check_state_result(state_result, recurse=True)
try: value = int(value) except (ValueError, TypeError): pass try: value = float(value) except (ValueError, TypeError): pass
os.chmod(path, stat.S_IWUSR) func(path)
if status.st_ino != 0: node = (status.st_dev, status.st_ino) if node in _seen: return _seen.add(node)
for chunk in iter(lambda: ifile.read(chunk_size), b''): hash_obj.update(chunk) return hash_obj.hexdigest()
return True
try: if isinstance(date, six.string_types): try: if HAS_TIMELIB: return timelib.strtodatetime(to_bytes(date)) except ValueError: pass
if date.isdigit(): date = int(date) else: date = float(date)
stacklevel = 2
stacklevel = 3
if not isinstance(cmp_result, numbers.Integral): log.error('The version comparison function did not return an ' 'integer/long.') return False
if cmp_result < -1: cmp_result = -1 elif cmp_result > 1: cmp_result = 1
ret[key] = {'old': '', 'new': new[key]}
ret[key] = {'new': '', 'old': old[key]}
ret[key] = {'old': old[key], 'new': new[key]}
continue
raise ValueError
if len(nontext) / len(data) > 0.30: return True return False
return []
log.trace('Trying pysss.getgrouplist for \'{0}\''.format(user)) try:
try: default_group = grp.getgrgid(pwd.getpwnam(user).pw_gid).gr_name ugroups.remove(default_group) except KeyError: pass
return {}
return []
if not path: raise ValueError('no path specified')
i = len(os.path.commonprefix([start_list, path_list]))
from __future__ import absolute_import import logging import inspect
HAS_LIBS = False try: import azure HAS_LIBS = True except ImportError: pass
import salt.ext.six as six from salt.exceptions import SaltSystemExit
import sys import time import binascii from datetime import datetime import hashlib import hmac import logging import salt.config import re
import salt.utils.xmlutil as xml from salt._compat import ElementTree as ET
try: import requests
from salt.ext.six.moves import map, range, zip from salt.ext.six.moves.urllib.parse import urlencode, urlparse
global __AccessKeyId__, __SecretAccessKey__, __Token__, __Expiration__
if __Expiration__ != '': timenow = datetime.utcnow() timestamp = timenow.strftime('%Y-%m-%dT%H:%M:%SZ') if timestamp < __Expiration__: return __AccessKeyId__, __SecretAccessKey__, __Token__
access_key_id, secret_access_key, token = creds(provider)
if token != '': params_with_headers['SecurityToken'] = token
now = time.mktime(datetime.utcnow().timetuple())
if role_arn is None: access_key_id, secret_access_key, token = creds(prov_dict) else: access_key_id, secret_access_key, token = assumed_creds(prov_dict, role_arn, location=location)
if not payload_hash: payload_hash = hashlib.sha256(data).hexdigest()
canonical_request = '\n'.join(( method, uri, querystring, canonical_headers, signed_headers, payload_hash ))
signing_key = _sig_key( secret_access_key, datestamp, location, product )
signature = hmac.new( signing_key, string_to_sign.encode('utf-8'), hashlib.sha256).hexdigest()
authorization_header = ( '{0} Credential={1}/{2}, SignedHeaders={3}, Signature={4}' ).format( algorithm, access_key_id, credential_scope, signed_headers, signature, )
if token != '': new_headers['X-Amz-Security-Token'] = token
if __Location__ != '': return __Location__
result = requests.get( "http://169.254.169.254/latest/dynamic/instance-identity/document", proxies={'http': ''}, timeout=AWS_METADATA_TIMEOUT, )
__Location__ = 'do-not-get-from-metadata' return None
from __future__ import absolute_import
import jinja2 import yaml import msgpack import salt.ext.six as six import tornado
try: import certifi HAS_CERTIFI = True except ImportError: HAS_CERTIFI = False
HAS_MARKUPSAFE = False
from backports import ssl_match_hostname HAS_SSL_MATCH_HOSTNAME = True
try: from salt.ext import ssl_match_hostname HAS_SSL_MATCH_HOSTNAME = True except ImportError: HAS_SSL_MATCH_HOSTNAME = False
import salt import salt.utils import salt.exceptions
tops.append(os.path.dirname(xml.__file__))
raise salt.exceptions.SaltSystemExit( 'The minimum required python version to run salt-ssh is "2.6".' )
tempdir = tempfile.mkdtemp() egg = zipfile.ZipFile(top_dirname) egg.extractall(tempdir) top = os.path.join(tempdir, base) os.chdir(tempdir)
tfp.add(base, arcname=os.path.join('py{0}'.format(py_ver), base)) continue
raise salt.exceptions.SaltSystemExit( 'The minimum required python version to run salt-ssh is "2.6".' )
tempdir = tempfile.mkdtemp() egg = zipfile.ZipFile(top_dirname) egg.extractall(tempdir) top = os.path.join(tempdir, base) os.chdir(tempdir)
tfp.add(base, arcname=os.path.join('py{0}'.format(py_ver), base)) continue
import logging import os import re
from .vt import Terminal, TerminalException
raise TerminalException('Password authentication failed')
break
ret_stdout = [] ret_stderr = [] while self.conn.has_unread_data: stdout, stderr = self.conn.recv()
from __future__ import absolute_import import os import logging import json import salt.utils.http from salt.exceptions import CommandExecutionError
import salt.utils
try: import ntsecuritycon import psutil import pywintypes import win32api import win32net import win32security HAS_WIN32 = True except ImportError: HAS_WIN32 = False
groups = [name]
user_name = 'SYSTEM'
security_descriptor = win32security.GetFileSecurity( path, win32security.OWNER_SECURITY_INFORMATION) owner_sid = security_descriptor.GetSecurityDescriptorOwner()
memcached.host: 127.0.0.1 memcached.port: 11211
my_memcached_config: memcached.host: 127.0.0.1 memcached.port: 11211
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import integer_types
try: import memcache HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import
comps = name.split('}') name = comps[1]
if not isinstance(xmldict[name], list): xmldict[name] = [xmldict[name]] xmldict[name].append(to_dict(item))
from __future__ import absolute_import, print_function import os import sys import types import signal import getpass import logging import optparse import traceback import yaml from functools import partial
import salt.ext.six as six
_mixin_prio_ = 0
for func in dir(base): if not func.startswith('process_'): continue
continue
self.explicit = True return optparse.Option.take_action(self, action, dest, *args, **kwargs)
_mixin_prio_ = 100
_setup_mp_logging_listener_ = False
log.setup_temp_logger( getattr(self.options, 'log_level', 'error') )
process_option_funcs = [] for option_key in options.__dict__: process_option_func = getattr( self, 'process_{0}'.format(option_key), None ) if process_option_func is not None: process_option_funcs.append(process_option_func)
return options, args
log.shutdown_multiprocessing_logging_listener()
self._mixin_after_parsed_funcs.append(self.__merge_config_with_cli)
for option in self.option_list: if option.dest is None: continue
default = self.defaults.get(option.dest) value = getattr(self.options, option.dest, default)
if value is not None: self.config[option.dest] = value
self.config[option.dest] = value
setattr(self.options, option.dest, self.config[option.dest])
self.options.saltfile = os.environ.get('SALT_SALTFILE', None)
return
self.options.saltfile = os.path.abspath(self.options.saltfile)
logging.getLogger(__name__).info( 'Loading Saltfile from \'{0}\''.format(self.options.saltfile) )
return
return
cli_config = saltfile_config[self.get_prog_name()]
for option in self.option_list: if option.dest is None: continue
continue
default = self.defaults.get(option.dest) value = getattr(self.options, option.dest, default) if value != default: continue
setattr(self.options, option.dest, cli_config[option.dest]) option.explicit = True
for group in self.option_groups: for option in group.option_list: if option.dest is None: continue
continue
default = self.defaults.get(option.dest) value = getattr(self.options, option.dest, default) if value != default: continue
for key in cli_config: setattr(self.options, key, cli_config[key])
sys.stderr.write( 'WARNING: CONFIG \'{0}\' directory does not exist.\n'.format( self.options.config_dir ) )
self.options.config_dir = os.path.abspath(self.options.config_dir)
raise RuntimeError( 'Please set {0}._default_logging_logfile_'.format( self.__class__.__name__ ) )
self._mixin_after_parsed_funcs.append(self.__setup_extended_logging) self._mixin_after_parsed_funcs.append(self._setup_mp_logging_listener) self._mixin_after_parsed_funcs.append(self.__setup_console_logger)
self.options.log_file = self.config.get(cli_setting_name)
self.options.log_file = self.config.get( self._logfile_config_setting_name_ )
self.options.log_file = self._default_logging_logfile_
self.options.log_file_level = self.config.get(cli_setting_name)
self.options.log_file_level = self.config.get( self._logfile_loglevel_config_setting_name_ )
self.options.log_level = self._default_logging_level_
self.config.pop(self._logfile_loglevel_config_setting_name_)
self._loglevel_config_setting_name_, self.config['log_level']
self.config.pop(cli_log_path)
self.config.pop(self._logfile_config_setting_name_)
cli_log_path, self.config.get( self._logfile_config_setting_name_, self._default_logging_logfile_ )
self.config.pop(cli_log_file_fmt)
self.config.pop('log_fmt_logfile', None)
self.config.pop(cli_log_file_datefmt)
self.config.pop('log_datefmt_logfile', None)
self.config.pop('log_datefmt_console', None)
if getattr(self.options, 'daemon', False) is True: return
self.config.pop(cli_log_datefmt)
self.config.pop('log_datefmt_console', None)
if self.check_pidfile(): os.unlink(self.config['pidfile'])
log.shutdown_multiprocessing_logging_listener(daemonizing=True)
salt.utils.daemonize()
self._setup_mp_logging_listener()
if salt.utils.is_windows(): from salt.utils.win_functions import get_parent_pid ppid = get_parent_pid() else: ppid = os.getppid()
def _install_signal_handlers(self): signal.signal(signal.SIGTERM, self._handle_signals) signal.signal(signal.SIGINT, self._handle_signals)
pass
ofh.write('')
)
_config_filename_ = 'master' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _setup_mp_logging_listener_ = True
_config_filename_ = 'minion' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'minion') _setup_mp_logging_listener_ = True
_config_filename_ = 'proxy' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'proxy')
_config_filename_ = 'master' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _setup_mp_logging_listener_ = True
_config_filename_ = 'master'
sys.stdout.write('Invalid options passed. Please try -h for '
_config_filename_ = 'master'
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _loglevel_config_setting_name_ = 'cli_salt_cp_log_file'
if len(self.args) <= 1: self.print_help() self.exit(salt.defaults.exitcodes.EX_USAGE)
_config_filename_ = 'master'
_skip_console_logging_config_ = True _logfile_config_setting_name_ = 'key_logfile' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'key')
process_config_dir._mixin_prio_ = ConfigDirMixIn._mixin_prio_
keys_config['key_logfile'] = os.devnull keys_config['pki_dir'] = self.options.gen_keys_dir
self.config['loglevel'] = 'info'
_config_filename_ = 'minion'
_default_logging_level_ = 'info' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'minion')
_config_filename_ = 'master'
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'master') _loglevel_config_setting_name_ = 'cli_salt_run_log_file'
_config_filename_ = 'master'
_default_logging_level_ = 'warning' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'ssh') _loglevel_config_setting_name_ = 'cli_salt_run_log_file'
_config_filename_ = 'cloud'
_default_logging_level_ = 'info' _logfile_config_setting_name_ = 'log_file' _loglevel_config_setting_name_ = 'log_level_logfile' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'cloud')
from salt.cloud import libcloudfuncs libcloudfuncs.check_libcloud_version()
_config_filename_ = 'spm' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'spm')
_config_filename_ = 'master' _default_logging_logfile_ = os.path.join(syspaths.LOGS_DIR, 'api')
APPL_KINDS = OrderedDict([('master', 0), ('minion', 1), ('syndic', 2), ('caller', 3)])
from __future__ import absolute_import import os import logging import signal import tempfile from threading import Thread, Event
import salt.ext.six as six try: import zmq HAS_ZMQ = True except ImportError: HAS_ZMQ = False
grains = {} pillars = {}
grains, pillars = self._get_cached_minion_data(*minion_ids)
os.remove(os.path.join(data_file))
os.remove(os.path.join(mine_file))
socket = context.socket(zmq.PUB) socket.setsockopt(zmq.LINGER, 100) socket.bind('ipc://' + self.timer_sock)
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
self.opts = opts
self.minions = []
self.timer_stop = Event() self.timer = CacheTimer(self.opts, self.timer_stop) self.timer.start() self.running = True
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
self.cleanup() if self.running: self.running = False self.timer_stop.set() self.timer.join()
creq_in = context.socket(zmq.REP) creq_in.setsockopt(zmq.LINGER, 100) creq_in.bind('ipc://' + self.cache_sock)
serial = salt.payload.Serial(self.opts.get('serial', ''))
signal.signal(signal.SIGINT, self.signal_handler)
self.secure()
if isinstance(msg, str): if msg == 'minions': reply = serial.dumps(self.minions) creq_in.send(reply)
if socks.get(cupd_in) == zmq.POLLIN: new_c_data = serial.loads(cupd_in.recv()) #cupd_in.send(serial.dumps('ACK'))
if not isinstance(new_c_data, list): log.error('ConCache Worker returned unusable result') del new_c_data continue
if socks.get(timer_in) == zmq.POLLIN: sec_event = serial.loads(timer_in.recv())
if int(sec_event % 30) == 0: cw = CacheWorker(self.opts) cw.start()
if __name__ == '__main__':
from __future__ import absolute_import import collections import logging
from salt._compat import subprocess
QUERYFORMAT = '%{NAME}_|-%{EPOCH}_|-%{VERSION}_|-%{RELEASE}_|-%{ARCH}_|-%{REPOID}'
except ValueError: return None
from salt.ext.six.moves.urllib.parse import urljoin as _urljoin import salt.ext.six.moves.http_client from salt.version import __version__ import salt.utils.http
from __future__ import print_function from __future__ import absolute_import from os.path import splitext, abspath from sys import modules
import win32serviceutil import win32service import win32event import win32api
def start(self): pass
def stop(self): pass
from sys import executable module_path = executable
import collections
from __future__ import absolute_import, print_function import sys import inspect import textwrap import functools
import salt.utils.args from salt.utils.odict import OrderedDict
#import yaml import salt.ext.six as six
def _failing_new(*args, **kwargs): raise TypeError('Can\'t create another NullSentinel instance')
attrs['__config__'] = True attrs['__flatten__'] = False attrs['__config_name__'] = None
instance.__flatten__ = True
instance.__allow_additional_items__ = True
title = None description = None _items = _sections = _order = None __flatten__ = False __allow_additional_items__ = False
properties[name] = serialized_section
if config.__flatten__ is True: serialized_config = config.serialize() cls.after_items_update.append(serialized_config) skip_order = True else: properties[item_name] = config.serialize()
required.append(item_name)
if item_name is not None: if item_name not in ordering: ordering.append(item_name) else: if name not in ordering: ordering.append(name)
serialized['required'] = required
serialized['x-ordering'] = ordering
__type__ = None __format__ = None _attributes = None __flatten__ = False
continue
if self.__serialize_attr_aliases__ and argname in self.__serialize_attr_aliases__: argname = self.__serialize_attr_aliases__[argname] serialized[argname] = argvalue
return self.items.serialize()
class PortItem(IntegerItem):
import os import logging
try: import win32con import win32api import win32process import win32security import win32pipe import win32event import win32profile import msvcrt import ctypes from ctypes import wintypes HAS_WIN32 = True except ImportError: HAS_WIN32 = False
import salt.utils
log = logging.getLogger(__name__)
kernel32 = ctypes.WinDLL('kernel32') advapi32 = ctypes.WinDLL('advapi32')
_win(kernel32.WaitForSingleObject, DWORD_IDV,
_win(kernel32.GetStdHandle, HANDLE_IHV,
_win(kernel32.CloseHandle, wintypes.BOOL,
_win(kernel32.SetHandleInformation, wintypes.BOOL,
_win(kernel32.DuplicateHandle, wintypes.BOOL,
_win(kernel32.GetCurrentProcess, wintypes.HANDLE)
_win(kernel32.GetExitCodeProcess, wintypes.BOOL,
_win(kernel32.CreatePipe, wintypes.BOOL,
_win(advapi32.CreateProcessWithLogonW, wintypes.BOOL,
token = win32security.LogonUser(username, domain, password, win32con.LOGON32_LOGON_INTERACTIVE, win32con.LOGON32_PROVIDER_DEFAULT)
security_attributes = win32security.SECURITY_ATTRIBUTES() security_attributes.bInheritHandle = 1
stdin_read, stdin_write = win32pipe.CreatePipe(security_attributes, 0) stdin_read = make_inheritable(stdin_read)
startup_info = win32process.STARTUPINFO() startup_info.dwFlags = win32con.STARTF_USESTDHANDLES startup_info.hStdInput = stdin_read startup_info.hStdOutput = stdout_write startup_info.hStdError = stderr_write
user_environment = win32profile.CreateEnvironmentBlock(token, False)
cmd = 'cmd /c {0}'.format(cmd)
procArgs = (None, cmd, security_attributes, security_attributes, 1, 0, user_environment, None, startup_info)
ret = {'pid': PId}
if win32event.WaitForSingleObject(hProcess, win32event.INFINITE) == win32con.WAIT_OBJECT_0: exitcode = win32process.GetExitCodeProcess(hProcess) ret['retcode'] = exitcode
win32api.CloseHandle(hProcess)
if win32api.GetUserName() == 'SYSTEM': return runas_system(cmd, username, password)
c2pread, c2pwrite = CreatePipe(inherit_read=False, inherit_write=True) errread, errwrite = CreatePipe(inherit_read=False, inherit_write=True)
stdin = kernel32.GetStdHandle(STD_INPUT_HANDLE) dupin = DuplicateHandle(srchandle=stdin, inherit=True)
startup_info = STARTUPINFO(dwFlags=win32con.STARTF_USESTDHANDLES, hStdInput=dupin, hStdOutput=c2pwrite, hStdError=errwrite)
cmd = 'cmd /c {0}'.format(cmd)
process_info = CreateProcessWithLogonW(username=username, domain=domain, password=password, logonflags=LOGON_WITH_PROFILE, commandline=cmd, startupinfo=startup_info, currentdirectory=cwd)
ret = {'pid': process_info.dwProcessId}
kernel32.CloseHandle(process_info.hProcess)
import logging
self.process.kill()
return StateRequisite(requisite, self.module, id_)
for attr in REQUISITES: if attr in kwargs: try: iter(kwargs[attr]) except TypeError: kwargs[attr] = [kwargs[attr]] self.kwargs = kwargs
for attr in REQUISITES: if attr in kwargs: kwargs[attr] = [ req() if isinstance(req, StateRequisite) else req for req in kwargs[attr] ]
return [ {k: kwargs[k]} for k in sorted(six.iterkeys(kwargs)) ]
for item in cls.__dict__: if item[0] == '_': continue
if not inspect.isclass(filt): continue
grain = getattr(filt, '__grain__', 'os_family') if grain not in match_groups: match_groups[grain] = OrderedDict([])
if hasattr(filt, '__match__'): match = filt.__match__ else: match = item
from __future__ import absolute_import import re import salt.ext.six as six
from __future__ import absolute_import import os import tempfile import sys import errno import time import random import shutil import salt.ext.six as six
_CreateTransaction = ctypes.windll.ktmw32.CreateTransaction _CommitTransaction = ctypes.windll.ktmw32.CommitTransaction _MoveFileTransacted = ctypes.windll.kernel32.MoveFileTransactedW _CloseHandle = ctypes.windll.kernel32.CloseHandle CAN_RENAME_OPEN_FILE = True
changes[namespace] = { 'new': config, 'old': update_config, }
changes[namespace] = { 'new': config, 'old': update_config, } return config
changes[namespace] = { 'new': config, 'old': update_config, }
changes[namespace] = { 'new': config, 'old': update_config, } return config
from __future__ import absolute_import import os import re import socket import logging from string import ascii_letters, digits
import salt.ext.six as six
try: import wmi import salt.utils.winapi except ImportError: pass
import salt.utils from salt._compat import subprocess, ipaddress
if ' ' in e: first = 1 else: first = -1
if e in punish: second = punish.index(e) else: second = -1
third = e.count(':')
fifth = -(e.count('.'))
sixth = -(len(e))
hosts = [] for name in h: name = name.strip() if len(name) > 0: hosts.append(name)
hosts = list(set(hosts)) return hosts
for addr in salt.utils.network.ip_addrs(): addr = ipaddress.ip_address(addr) if addr.is_loopback: continue possible_ids.append(str(addr))
if len(possible_ids) == 0: return 'noname'
addr = {'address': val.rstrip('(Preferred)'), 'prefixlen': None} iface['inet6'].append(addr)
iface['up'] = (val != 'Media disconnected')
if 'COMMAND' in chunks[1]:
if 'COMMAND' in chunks[1]:
log.warning('"lsof" returncode = 1, likely no active TCP sessions.') return remotes
from __future__ import absolute_import import inspect import logging import time from functools import wraps from collections import defaultdict
import salt.utils import salt.utils.args from salt.exceptions import CommandNotFoundError, CommandExecutionError from salt.version import SaltStackVersion, __saltstack_version__ from salt.log import LOG_LEVELS
import salt.ext.six as six
dependency_dict = defaultdict(lambda: defaultdict(set))
if frame: try: func_name = frame.f_globals['__func_alias__'][func.__name__] except (AttributeError, KeyError): func_name = func.__name__
if mod_key not in functions: continue
log.trace('{0} already removed, skipping'.format(mod_key)) continue
from __future__ import absolute_import import logging
import salt.minion import salt.utils.verify import salt.utils.jid from salt.utils.event import tagify
if not opts['job_cache'] or opts.get('ext_job_cache'): return
import logging from sys import stdout from os import makedirs from os.path import dirname, isdir from errno import EEXIST
import salt.utils
log = logging.getLogger(__name__)
HAS_SWIFT = False try: from swiftclient import client
from __future__ import absolute_import import pyrax
from __future__ import absolute_import
try: import pyrax from salt.utils.openstack.pyrax.authenticate import Authenticate from salt.utils.openstack.pyrax.queues import RackspaceQueues
import logging log = logging.getLogger(__name__)
import pyrax import pyrax.exceptions
from salt.utils.openstack.pyrax import authenticate
if self.conn.queue_exists(qname): return True return False
if not self.conn.queue_exists(qname): return {} for queue in self.conn.list(): if queue.name == qname: return queue
from __future__ import absolute_import, with_statement from distutils.version import LooseVersion import time import inspect import logging
import salt.utils from salt.exceptions import SaltCloudSystemExit
log = logging.getLogger(__name__)
NOVACLIENT_MINVER = '2.6.1'
class KwargsStruct(object): def __init__(self, **entries): self.__dict__.update(entries)
self.kwargs['os_auth_url'] = auth_url
self.kwargs['version'] = str(kwargs.get('version', 2))
from __future__ import absolute_import, with_statement import logging
import salt.ext.six as six HAS_NEUTRON = False try: from neutronclient.v2_0 import client from neutronclient.shell import NeutronShell
from salt import exceptions
log = logging.getLogger(__name__)
import os import sys import time import errno import select import logging import tempfile import subprocess
self.max_size_in_mem = kwargs.pop('max_size_in_mem', 512000)
#)
try: import exceptions except ImportError: pass
import salt.exceptions import salt.utils.event
from __future__ import absolute_import import logging from copy import copy
from salt.utils.odict import OrderedDict
import salt.ext.six as six
response = copy(obj_a)
response = copy(obj_b)
import os import time import fnmatch import hashlib import logging import datetime from collections import MutableMapping from multiprocessing.util import Finalize
import salt.ext.six as six import tornado.ioloop import tornado.iostream
SUB_EVENT = set([ 'state.highstate', 'state.sls', ])
TAGS = {
self.connect_pub()
cls.cache_regex = salt.utils.cache.CacheRegex(prepend='^')
self.cpub = True
self.cpush = True
wait = None
run_once = True
if not self.cpub and not self.connect_pub(timeout=wait): break
dump_data = self.serial.dumps(data, use_bin_type=True)
self.subscriber.read_async(event_handler)
try: self.destroy()
try: os.makedirs(minion_sock_dir, 0o755) except OSError as exc: log.error('Could not create SOCK_DIR: {0}'.format(exc)) if minion_sock_dir == default_minion_sock_dir: raise
raise
except Exception: log.critical('Unexpected error while polling minion events', exc_info=True) return None
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
Finalize(self, self.close, exitpriority=15)
except Exception: log.critical('Unexpected error while polling master events', exc_info=True) return None
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
if self.event_queue: self.flush_events() self.stop = True super(EventReturn, self)._handle_signals(signum, sigframe)
if log.level <= logging.DEBUG: log.debug('Event data that caused an exception: {0}'.format( self.event_queue))
from __future__ import absolute_import import time
import logging import os import shutil
import salt.fileclient import salt.utils.url
from salt.ext import six
while True: emptydirs = _list_emptydirs(mod_dir) if not emptydirs: break for emptydir in emptydirs: touched = True shutil.rmtree(emptydir, ignore_errors=True)
from __future__ import absolute_import, with_statement import os import time import signal import datetime import itertools import threading import logging import errno import random
import yaml import salt.ext.six as six
try: import dateutil.parser as dateutil_parser _WHEN_SUPPORTED = True _RANGE_SUPPORTED = True except ImportError: _WHEN_SUPPORTED = False _RANGE_SUPPORTED = False
self.loop_interval = six.MAXSIZE clean_proc_dir(opts)
if name in self.opts['schedule']: del self.opts['schedule'][name] schedule = self.opts['schedule']
if name in self.intervals: del self.intervals[name]
for job in data.keys(): if 'enabled' not in data[job]: data[job]['enabled'] = True
proc.start()
self.intervals = {}
evt = salt.utils.event.get_event('minion', opts=self.opts, listen=False) evt.fire_event({'complete': True}, tag='/salt/minion/minion_schedule_saved')
self.functions = salt.loader.minion_mods(self.opts) self.returners = salt.loader.returners(self.opts, self.functions)
log_setup.setup_multiprocessing_logging()
with salt.utils.fopen(proc_fn, 'w+b') as fp_: fp_.write(salt.payload.Serial(self.opts).dumps(ret))
if 'retcode' in self.functions.pack['__context__']: ret['retcode'] = self.functions.pack['__context__']['retcode']
pass
raise
exit(salt.defaults.exitcodes.EX_GENERIC)
schedule_keys = set(data.keys())
when = _when[0]
if '_when' in data and data['_when'] != when: data['_when_run'] = True data['_when'] = when seconds = when - now
if seconds < 0: continue
if '_when' not in data: data['_when'] = when
if when > data['_when']: data['_when'] = when data['_when_run'] = True
if seconds < 0: continue
if '_when' not in data: data['_when'] = when
if when > data['_when']: data['_when'] = when data['_when_run'] = True
if 'run_on_start' in data: if data['run_on_start']: run = True else: self.intervals[job] = int(time.time()) else: run = True
functions = self.functions self.functions = {} returners = self.returners self.returners = {}
proc.start()
self.functions = functions self.returners = returners
if salt.utils.is_windows(): fp_.close() try: os.unlink(fn_) continue except OSError: continue
if salt.utils.is_windows(): fp_.close() try: os.unlink(fn_) except OSError: pass
from __future__ import absolute_import
from salt.exceptions import SaltSystemExit
try: import zmq HAS_ZMQ = True except ImportError: HAS_ZMQ = False
import os import re import sys import stat import errno import socket import logging
if sys.platform.startswith('win'): import win32file else: import resource
from salt.log import is_console_configured from salt.exceptions import SaltClientError, SaltSystemExit, \ CommandExecutionError import salt.defaults.exitcodes import salt.utils
return True
match = re.match(r'^(\d+)\.(\d+)(?:\.(\d+))?', ver)
if point and point.isdigit(): point = int(point)
if os.getuid() == 0: os.chown(dir_, uid, gid) os.umask(cumask)
zmq_version()
if 'HOME' in os.environ: os.environ['HOME'] = pwuser.pw_dir
out = [head] (head, tail) = os.path.split(head)
out.insert(0, head) (head, tail) = os.path.split(head)
if user != current_user: msg += ' Try running as user {0}.'.format(user) else: msg += ' Please give {0} read permissions.'.format(user)
if skip_perm_errors: return raise SaltClientError(msg)
mof_s = mof_h = win32file._getmaxstdio()
return
msg += 'salt-master will crash pretty soon! ' level = logging.CRITICAL
level = logging.CRITICAL
elif (accepted_count * 4) >= mof_s: level = logging.INFO
path = dirs[0] while os.path.basename(path) not in ['salt', 'salt-tests-tmpdir']: path, base = os.path.split(path)
if not os.path.isdir(path): os.makedirs(path)
zmq_version()
import os.path import shutil
import salt.syspaths as syspaths
return
salt.utils.compat.pack_dunder(__name__)
from __future__ import absolute_import import hashlib import logging import sys
try: import boto import boto3 import boto.exception import boto3.session
logging.getLogger('boto3').setLevel(logging.CRITICAL) HAS_BOTO = True
import sys import os
from __future__ import absolute_import import re import string import random
try:
import crypt HAS_CRYPT = True
from salt.exceptions import SaltInvocationError
import codecs import os import imp import logging import tempfile import traceback import sys
import jinja2 import jinja2.ext
if 'salt' in kws: kws['salt'] = AliasedLoader(kws['salt'])
kws.update(context) context = kws assert 'opts' in context assert 'saltenv' in context
return dict(result=True, data=tmplsrc)
exc_info_on_loglevel=logging.DEBUG
output = os.linesep.join(output.splitlines())
return dict(result=True, data=outf.name)
tmplstr = tmplstr.decode(SLS_ENCODING)
loader = jinja2.FileSystemLoader( context, os.path.dirname(tmplpath))
raise SaltRenderError( 'Jinja variable {0}{1}'.format( exc, out), buf=tmplstr)
if newline: output += '\n'
from mako.lookup import TemplateLookup lookup = TemplateLookup(directories=[os.path.dirname(tmplpath)])
import gzip
from __future__ import absolute_import import collections
import copy import logging import salt.ext.six as six from salt.serializers.yamlex import merge_recursive as _yamlex_merge_recursive
for k in upd: dest[k] = upd[k]
from __future__ import absolute_import, print_function import logging
import salt.utils.http
from __future__ import absolute_import import sys
from salt.utils.winservice import Service, instart import salt import salt.defaults.exitcodes
import win32serviceutil import win32service import winerror
import copy import threading import collections from contextlib import contextmanager
if hasattr(func, 'im_func'): func = func.__func__
func_globals = func.__globals__ injected_func_globals = [] overridden_func_globals = {} for override in overrides: if override in func_globals: overridden_func_globals[override] = func_globals[override] else: injected_func_globals.append(override)
func_globals.update(overrides)
yield
func_globals.update(overridden_func_globals)
for injected in injected_func_globals: del func_globals[injected]
self._state = threading.local() self._state.data = None self.global_data = {}
for k, v in six.iteritems(self.parent.global_data): if k not in self._data: self._data[k] = copy.deepcopy(v)
from __future__ import absolute_import import json import salt.utils.http import logging
import salt.ext.six as six
from __future__ import absolute_import import logging import time
step = min(step or 1, timeout) * BLUR_FACTOR
step = min(step, max_time - time.time()) * BLUR_FACTOR
from __future__ import absolute_import import logging import os import subprocess
from salt.exceptions import SaltInvocationError import salt.utils
from __future__ import absolute_import import json import logging import os.path import pprint import socket import urllib import yaml
import tornado.httputil import tornado.simple_httpclient from tornado.httpclient import HTTPClient
url_full = tornado.httputil.url_concat(url, params)
log_url = sanitize_url(url_full, hide_fields)
sess_cookies = None
req_kwargs['prefetch'] = False
header_callback('HTTP/1.0 {0} MESSAGE'.format(result.status_code)) streaming_callback(result.content) return { 'handle': result, }
req_kwargs = {}
'/etc/ssl/certs/ca-certificates.crt', '/etc/pki/tls/certs/ca-bundle.crt', '/etc/pki/tls/certs/ca-bundle.trust.crt', '/etc/ssl/certs/ca-bundle.crt', '/var/lib/ca-certificates/ca-bundle.pem', '/etc/ssl/cert.pem',
cookies.append(cookie)
if 'expires' in cookie: cookie['expires'] = salt.ext.six.moves.http_cookiejar.http2time(cookie['expires'])
from __future__ import absolute_import from __future__ import print_function
from __future__ import absolute_import, with_statement import copy import os import sys import time import errno import types import signal import logging import threading import contextlib import subprocess import multiprocessing import multiprocessing.util
import salt.defaults.exitcodes import salt.utils import salt.log.setup import salt.defaults.exitcodes from salt.log.mixins import NewStyleClassMixIn
import salt.ext.six as six
HAS_PSUTIL = False try: import psutil HAS_PSUTIL = True except ImportError: pass
pass
return
if num_threads is None: num_threads = multiprocessing.cpu_count() self.num_threads = num_threads
self._job_queue = queue.Queue(queue_size)
for _ in range(num_threads): thread = threading.Thread(target=self._thread_target) thread.daemon = True thread.start() self._workers.append(thread)
try: try: func, args, kwargs = self._job_queue.get(timeout=1)
continue
self._process_map = {}
self._pid = os.getpid() self._sigterm_handler = signal.getsignal(signal.SIGTERM) self._restart_processes = True
if type(MultiprocessingProcess) is type(tgt) and ( issubclass(tgt, MultiprocessingProcess)): need_log_queue = True else: need_log_queue = False
self._process_map[pid]['Process'].join(1)
raise
self.check_children()
time.sleep(10)
if exc.errno != errno.EINTR: raise break
try: del self._process_map[pid] except KeyError: pass
instance._original_run = instance.run instance.run = instance._run return instance
salt.log.setup.set_multiprocessing_logging_queue(self.log_queue)
super(MultiprocessingProcess, self).__init__(*args, **kwargs)
def __setstate__(self, state): self._is_child = True args = state['args'] kwargs = state['kwargs'] self.__init__(*args, **kwargs)
del self._args_for_getstate del self._kwargs_for_getstate return {'args': args, 'kwargs': kwargs}
raise
raise
self.__setup_signals()
yield
for signum in old_signals: signal.signal(signum, old_signals[signum])
import logging import subprocess import os
import salt.utils import salt.utils.timed_subprocess import salt.grains.extra from salt.exceptions import CommandExecutionError, SaltInvocationError,\ TimedProcTimeoutError
log = logging.getLogger(__name__)
from __future__ import absolute_import import glob import sys import os
import salt.utils
from ctypes import cdll, c_char_p, c_int, c_void_p, pointer, create_string_buffer from ctypes.util import find_library
lib = glob.glob(os.path.join( '/opt/local/lib', 'libcrypto.so*')) lib = lib[0] if len(lib) > 0 else None
RSA_X931_PADDING = 5
from __future__ import absolute_import from uuid import uuid4 as _uuid
from salt.utils.odict import OrderedDict from salt.utils import warn_until from salt.state import HighState
import salt.ext.six as six
try: return self.get_all_decls()[id] except KeyError: self.get_all_decls()[id] = s = StateDeclaration(id) self.decls.append(s) return s
raise PyDslError( 'An error occurred while running highstate: {0}'.format( '; '.join(result) ) )
self.require_index = None
etcd.host: 127.0.0.1 etcd.port: 4001
my_etcd_config: etcd.host: 127.0.0.1 etcd.port: 4001
import logging
from salt.exceptions import CommandExecutionError
try: import etcd from urllib3.exceptions import ReadTimeoutError, MaxRetryError HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
log.error("etcd: failed to perform 'watch' operation on key {0} due to connection error".format(key)) return {}
log.error("etcd: Could not connect") raise etcd.EtcdConnectionFailed("Could not connect to etcd server")
log.error("etcd: {0}".format(err)) raise
log.error("etcd: error. python-etcd does not fully support python 2.6, no error information available") raise
future = async.async_method()
with current_ioloop(self.io_loop): ret = attr(*args, **kwargs) if isinstance(ret, tornado.concurrent.Future): ret = self._block_future(ret) return ret
self.async.close()
del self.async
import os import time import logging
import salt.utils
from __future__ import absolute_import import re import os import logging
import salt.utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
return re.split(r'\s+', out['stdout'])[1][:-1] == 'inode/blockdevice'
import logging import pythoncom import threading
import logging import re
HAS_LIBS = False try: import vboxapi
reload(vboxapi) _virtualboxManager = vboxapi.VirtualBoxManager(None, None)
start_time = time.time() timeout_in_seconds = timeout / 1000 max_time = start_time + timeout_in_seconds
args = (machine, session) progress = wait_for(_start_machine, timeout=timeout_in_seconds, func_args=args) if not progress: progress = machine.launchVMProcess(session, "", "")
time_left = max_time - time.time() progress.waitForCompletion(time_left * 1000)
time_left = max_time - time.time() vb_wait_for_session_state(session, timeout=time_left) log.info("Started machine %s", name)
if "memorySize" in machine: del machine["memorySize"] return machine
from __future__ import absolute_import import os import logging import time from collections import MutableMapping
import salt.ext.six as six
continue
continue
return None
sls[ps_opts['d-i']['languagechooser']['language-name-fb']['argument']] = { 'locale': ['system'] }
sls[ps_opts['d-i']['kbd-chooser']['method']['argument']] = { 'keyboard': ['system'] }
import os import sys import time import signal import tempfile import traceback import inspect
import salt.utils
enable_sig_handler('SIGINFO', _handle_sigusr1)
import psutil
def boot_time(): return psutil.BOOT_TIME
if psutil.version_info < (1, 0, 1): net_io_counters = psutil.network_io_counters()
def cpu_affinity(self, *args, **kwargs): if args or kwargs: return self.set_cpu_affinity(*args, **kwargs) else: return self.get_cpu_affinity()
if os.name == 'nt': socket.inet_pton = inet_pton socket.inet_ntop = inet_ntop
parts = dn.split(r'.') leftmost = parts[0] remainder = parts[1:]
raise CertificateError( "too many wildcards in certificate DNS name: " + repr(dn))
if not wildcards: return dn.lower() == hostname.lower()
for frag in remainder: pats.append(re.escape(frag))
PY2 = sys.version_info[0] == 2 PY3 = sys.version_info[0] == 3
MAXSIZE = int((1 << 31) - 1)
delattr(obj.__class__, self.name)
_moved_attributes = []
return sys.modules[fullname]
int2byte = operator.methodcaller("to_bytes", 1, "big")
from itertools import imap as map range = xrange
def long_range(start, end): while start < end: yield start start += 1
bytes = bytearray
_builtin_isinstance = isinstance
if hasattr(int, 'bit_length'): _int_bit_length = lambda i: i.bit_length() else: _int_bit_length = lambda i: len(bin(abs(i))) - 2
return bits
ips = sorted(set(ips)) nets = sorted(set(nets))
try: ip_int = self._ip_int_from_string(ip_str) except AddressValueError: self._report_invalid_netmask(ip_str)
try: return self._prefix_from_ip_int(ip_int) except ValueError: pass
ip_int ^= self._ALL_ONES try: return self._prefix_from_ip_int(ip_int) except ValueError: self._report_invalid_netmask(ip_str)
def __add__(self, other): if not isinstance(other, int): return NotImplemented return self.__class__(int(self) + other)
msg = '%200s has no associated address class' % (type(self),) raise NotImplementedError(msg)
other = other.__class__('%s/%s' % (other.network_address, other.prefixlen))
raise AssertionError('Error performing exclusion: ' 's1: %s s2: %s other: %s' % (s1, s2, other))
raise AssertionError('Error performing exclusion: ' 's1: %s s2: %s other: %s' % (s1, s2, other))
_ALL_ONES = (2**IPV4LENGTH) - 1 _DECIMAL_DIGITS = frozenset('0123456789')
_valid_mask_octets = frozenset((255, 254, 252, 248, 240, 224, 192, 128, 0))
return False
if isinstance(address, int): self._check_int_address(address) self._ip = address return
if isinstance(address, bytes): self._check_packed_address(address, 4) self._ip = _int_from_bytes(address, 'big') return
addr_str = str(address) self._ip = self._ip_int_from_string(addr_str)
return False
return False
_address_class = IPv4Address
if isinstance(address, bytes): self.network_address = IPv4Address(address) self._prefixlen = self._max_prefixlen self.netmask = IPv4Address(self._ALL_ONES) #fixme: address/network test here return
addr = _split_optional_netmask(address) self.network_address = IPv4Address(self._ip_int_from_string(addr[0]))
self._prefixlen = self._prefix_from_prefix_string(addr[1])
self._prefixlen = self._prefix_from_ip_string(addr[1])
_min_parts = 3 if len(parts) < _min_parts: msg = "At least %d parts expected in %r" % (_min_parts, ip_str) raise AddressValueError(msg)
_max_parts = self._HEXTET_COUNT + 1 if len(parts) > _max_parts: msg = "At most %d colons permitted in %r" % (_max_parts-1, ip_str) raise AddressValueError(msg)
doublecolon_start = index
best_doublecolon_len = doublecolon_len best_doublecolon_start = doublecolon_start
if best_doublecolon_end == len(hextets): hextets += [''] hextets[best_doublecolon_start:best_doublecolon_end] = [''] if best_doublecolon_start == 0: hextets = [''] + hextets
if isinstance(address, int): self._check_int_address(address) self._ip = address return
if isinstance(address, bytes): self._check_packed_address(address, 16) self._ip = _int_from_bytes(address, 'big') return
addr_str = str(address) self._ip = self._ip_int_from_string(addr_str)
return False
return False
_address_class = IPv6Address
if isinstance(address, int): self.network_address = IPv6Address(address) self._prefixlen = self._max_prefixlen self.netmask = IPv6Address(self._ALL_ONES) return
if isinstance(address, bytes): self.network_address = IPv6Address(address) self._prefixlen = self._max_prefixlen self.netmask = IPv6Address(self._ALL_ONES) return
addr = _split_optional_netmask(address)
self._prefixlen = self._prefix_from_prefix_string(addr[1])
from __future__ import absolute_import, print_function, with_statement import os import re import sys import copy import time import types import signal import fnmatch import logging import threading import traceback import contextlib import multiprocessing from random import randint, shuffle from stat import S_IMODE
import salt.ext.six as six if six.PY3: import ipaddress else: import salt.ext.ipaddress as ipaddress from salt.ext.six.moves import range
ret['master'] = ip_port[0]
ret['master'] = ip_port[0] ret['master_port'] = ip_port[1]
os.makedirs(fn_, **mode)
uid = kwargs.pop('uid', -1) gid = kwargs.pop('gid', -1)
_args.append(arg)
_kwargs.update(string_kwarg)
for key, val in six.iteritems(string_kwarg): invalid_kwargs.append('{0}={1}'.format(key, val))
for key, val in six.iteritems(data): _kwargs['__pub_{0}'.format(key)] = val
if minion.schedule.loop_interval < loop_interval: loop_interval = minion.schedule.loop_interval log.debug( 'Overriding loop_interval because of scheduled jobs.' )
if isinstance(opts['master'], list): conn = False local_masters = copy.copy(opts['master']) last_exc = None
if 'master_list' not in opts: opts['master_list'] = local_masters
self.connected = False msg = ('No master could be reached or all masters ' 'denied the minions connection attempt.') log.error(msg)
opts['grains'] = salt.loader.grains(opts) super(SMinion, self).__init__(opts)
MINION_CONNECT_TIMEOUT = 5
self.minions = self._spawn_minions()
self.io_loop.start()
super(Minion, self).__init__(opts) self.timeout = timeout self.safe = safe
if HAS_ZMQ: try: zmq_version_info = zmq.zmq_version_info() except AttributeError: zmq_version_info = tuple(
if not salt.utils.is_proxy(): self.opts['grains'] = salt.loader.grains(opts)
if not salt.utils.is_proxy(): self.io_loop.spawn_callback(salt.engines.start_engines, self.opts, self.process_manager)
if signal.getsignal(signal.SIGINT) is signal.SIG_DFL: signal.signal(signal.SIGINT, self._handle_signals)
signal.signal(signal.SIGINT, self._handle_signals)
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children() exit(0)
if hasattr(self, 'proxy'): proxy = self.proxy else: proxy = None
if modules_max_memory is True: resource.setrlimit(resource.RLIMIT_AS, old_mem_limit)
process.start()
salt.log.setup.shutdown_multiprocessing_logging()
salt.log.setup.setup_multiprocessing_logging()
pass
salt.utils.minion.cache_jobs(self.opts, load['jid'], ret)
log.error('Pillar data could not be refreshed. ' 'One or more masters may be down!')
try: master, self.pub_channel = yield self.eval_master( opts=self.opts, failed=True) except SaltClientError: pass
multiprocessing.active_children()
self.event_publisher = salt.utils.event.AsyncEventPublisher( self.opts, self.handle_event, io_loop=self.io_loop, )
enable_sigusr1_handler()
salt.utils.enable_ctrl_logoff_handler()
for periodic_cb in six.itervalues(self.periodic_callbacks): periodic_cb.start()
if 'tgt' not in load or 'jid' not in load or 'fun' not in load \ or 'arg' not in load: return False
if 'tgt_type' not in data: data['tgt_type'] = 'glob' kwargs = {}
self.pub_channel.on_recv(self._process_cmd_socket)
self._reset_event_aggregation() self.local.event.set_event_handler(self._process_event)
self.forward_events = tornado.ioloop.PeriodicCallback(self._forward_events, self.opts['syndic_event_forward_timeout'] * 1000, io_loop=self.io_loop) self.forward_events.start()
self._fire_master_syndic_start()
enable_sigusr1_handler()
self.local = salt.client.get_local_client( self.opts['_minion_conf_file'], io_loop=self.io_loop)
self.pub_channel.on_recv(self._process_cmd_socket)
return
jdict['__master_id__'] = event['data']['master_id']
if 'retcode' not in event['data']: self.raw_events.append(event)
master, self.pub_channel = yield self.eval_master(opts=self.opts)
super(Syndic, self).destroy() if hasattr(self, 'local'): del self.local
SYNDIC_CONNECT_TIMEOUT = 5 SYNDIC_EVENT_TIMEOUT = 5
self.syndic_mode = self.opts.get('syndic_mode', 'sync') self.syndic_failover = self.opts.get('syndic_failover', 'random')
syndic.tune_in_no_block() log.info('Syndic successfully connected to {0}'.format(opts['master'])) break
if self._syndics[master].done():
return False
continue
return False
self.local = salt.client.get_local_client( self.opts['_minion_conf_file'], io_loop=self.io_loop) self.local.event.subscribe('')
self._reset_event_aggregation() self.local.event.set_event_handler(self._process_event)
self.forward_events = tornado.ioloop.PeriodicCallback(self._forward_events, self.opts['syndic_event_forward_timeout'] * 1000, io_loop=self.io_loop) self.forward_events.start()
enable_sigusr1_handler()
return
jdict['__master_id__'] = master
return False
for member in val: if fnmatch.fnmatch(str(member).lower(), comps[1].lower()): return True return False
tgt = ipaddress.ip_address(tgt)
tgt = ipaddress.ip_network(tgt)
log.error('Detected nodegroup expansion failure of "{0}"'.format(word)) return False
log.error('Unrecognized target engine "{0}" for' ' target expression "{1}"'.format( target_info['engine'], word, ) ) return False
results.append(str(self.glob_match(word)))
self.functions, self.returners, self.function_errors, self.executors = self._load_modules()
self.functions['saltutil.sync_all'](saltenv='base')
self.proxy = salt.loader.proxy(self.opts)
if self.opts['add_proxymodule_to_opts']: self.opts['proxymodule'] = self.proxy
self.io_loop.spawn_callback(salt.engines.start_engines, self.opts, self.process_manager, proxy=self.proxy)
self.functions['saltutil.sync_grains'](saltenv='base') self.grains_cache = self.opts['grains']
from __future__ import absolute_import, print_function import logging
import salt.exceptions import salt.loader import salt.minion import salt.utils.args import salt.utils.event from salt.client import mixins from salt.output import display_output from salt.utils.lazy import verify_fun
async_pub = self._gen_async_pub() ret = self._proc_function(self.opts['fun'], low, user, async_pub['tag'], async_pub['jid'],
from __future__ import absolute_import import os import struct
import salt.utils
if not isinstance(config, dict): return False, ('Configuration for btmp beacon must ' 'be a list of dictionaries.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
from __future__ import absolute_import import time
import salt.utils import salt.utils.vt
if not isinstance(config, dict): return False, ('Configuration for sh beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
import salt.utils
try: from pyroute2.ipdb import IPDB HAS_PYROUTE2 = True except ImportError: HAS_PYROUTE2 = False
from __future__ import absolute_import import logging import copy import re
import salt.loader import salt.utils import salt.utils.minion from salt.ext.six.moves import map
if isinstance(config[mod], dict): del config[mod]['enabled'] else: self._remove_list_item(config[mod], 'enabled')
self.opts['beacons'][name]['enabled'] = enabled_value
from __future__ import absolute_import import logging
from __future__ import absolute_import
if not isinstance(config, dict): return False, ('Configuration for service beacon must be a dictionary.') return True, 'Valid beacon configuration'
if config[service] is None: defaults = { 'oncleanshutdown': False, 'emitatstartup': True, 'onchangeonly': False } config[service] = defaults
from __future__ import absolute_import
from __future__ import absolute_import import collections import fnmatch import os
import salt.ext.six
if notifier.check_events(1): notifier.read_events() notifier.process_events() queue = __context__['inotify.queue'] while queue: event = queue.popleft()
path = event.path while path != '/': if path in config: break path = os.path.dirname(path)
current = set() for wd in wm.watches: current.add(wm.watches[wd].path)
return ret
from __future__ import absolute_import
import salt.utils import salt.utils.locales import salt.utils.cloud import salt.ext.six
try: import systemd.journal HAS_SYSTEMD = True except ImportError: HAS_SYSTEMD = False
__context__['systemd.journald'].seek_tail() __context__['systemd.journald'].get_previous() return __context__['systemd.journald']
sub = salt.utils.cloud.simple_types_filter(cur) sub.update({'tag': name}) ret.append(sub)
from __future__ import absolute_import import logging import re
import salt.utils
try: import psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
if not isinstance(config, dict): return False, ('Configuration for memusage ' 'beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import os import struct
import salt.utils
if not isinstance(config, dict): return False, ('Configuration for wtmp beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
import salt.utils
from __future__ import absolute_import import logging
import salt.utils.http
__proxyenabled__ = ['*']
from __future__ import absolute_import import logging
try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
if not isinstance(config, dict): return False, ('Configuration for ps beacon must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging
try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
from __future__ import absolute_import import logging import re
try: import psutil HAS_PSUTIL = True except ImportError: HAS_PSUTIL = False
if not isinstance(config, dict): return False, ('Configuration for diskusage beacon ' 'must be a dictionary.') return True, 'Valid beacon configuration'
log.error('{0} is not a valid mount point, skipping.'.format(mount)) continue
from __future__ import absolute_import import logging
try: from twilio.rest import TwilioRestClient HAS_TWILIO = True except ImportError: HAS_TWILIO = False
if not isinstance(config, dict): return False, ('Configuration for twilio_txt_msg beacon ' 'must be a dictionary.') return True, 'Valid beacon configuration'
from __future__ import absolute_import import logging import os
import salt.utils
from salt.ext.six.moves import zip
if 'emitatstartup' not in config: config['emitatstartup'] = True if 'onchangeonly' not in config: config['onchangeonly'] = False
from __future__ import absolute_import, print_function, with_statement import signal import logging import weakref import traceback import collections
import tornado.stack_context
kwargs_keys = list(kwargs)
for kwargs_key in kwargs_keys: if kwargs_key.startswith('__pub_'): pub_data[kwargs_key] = kwargs.pop(kwargs_key)
if kwarg: kwarg['__kwarg__'] = True arglist.append(kwarg)
completed_funcs = []
if kwargs: salt.utils.warn_until( 'Carbon', 'kwargs must be passed inside the low under "kwargs"' )
with tornado.stack_context.StackContext(self.functions.context_dict.clone): data['return'] = self.functions[fun](*args, **kwargs) data['success'] = True
log.info('Runner completed: {0}'.format(data['jid'])) del event del namespaced_event return data['return']
salt.log.setup.shutdown_multiprocessing_logging()
salt.log.setup.setup_multiprocessing_logging()
low['__jid__'] = jid low['__user__'] = user low['__tag__'] = tag
proc.start()
if self.opts.get('quiet', False): return
if suffix in ('new',): return
import salt.ext.six as six try: import zmq HAS_ZMQ = True except ImportError: HAS_ZMQ = False
HAS_RANGE = False try: import seco.range HAS_RANGE = True except ImportError: pass
import salt.config opts = salt.config.client_config(c_path)
key_user = key_user.replace('\\', '_')
salt.utils.verify.check_path_traversal(self.opts['cachedir'], key_user, self.skip_perm_errors)
return ''
return self.opts['timeout']
raise SaltClientError( 'The salt master could not be contacted. Is master running?' )
raise SaltClientError(general_exception)
minions = set(minions)
event_iter = self.get_event_iter_returns(jid, minions, timeout=timeout)
ret = self.get_cache_returns(jid) if ret != {}: found.update(set(ret)) yield ret
if len(found.intersection(minions)) >= len(minions): raise StopIteration()
minion_timeouts = {}
if id_ not in minion_timeouts: minion_timeouts[id_] = time.time() + timeout
for raw in jinfo_iter: if raw is None: break
open_jids.add(jinfo['jid'])
if raw['data']['return'] == {}: continue
if block: time.sleep(0.01) else: yield
if open_jids: for jid in open_jids: self.event.unsubscribe(jid)
event_iter = self.get_event_iter_returns(jid, minions, timeout=timeout)
if len(set(ret).intersection(minions)) >= len(minions): return ret
if len(set(ret).intersection(minions)) >= len(minions): return ret
return ret
connected_minions = None return_count = 0
if expr_form == 'range' and HAS_RANGE: tgt = self._convert_range_to_list(tgt) expr_form = 'list'
if kwargs: payload_kwargs['kwargs'] = kwargs
if self.opts['order_masters']: payload_kwargs['to'] = timeout
if not self.event.connect_pub(timeout=timeout): raise SaltReqTimeoutError() payload = channel.send(payload_kwargs, timeout=timeout)
key = self.__read_master_key() if key == self.key: return payload self.key = key payload_kwargs['key'] = self.key payload = channel.send(payload_kwargs)
del channel
if listen and not self.event.connect_pub(timeout=timeout): raise SaltReqTimeoutError() payload = yield channel.send(payload_kwargs, timeout=timeout)
del channel
if hasattr(self, 'event'): del self.event
import os import time import logging
from raet import raeting, nacling from raet.lane.stacking import LaneStack from raet.lane.yarding import RemoteYard import salt.config import salt.client import salt.utils import salt.syspaths as syspaths from salt.utils import kinds
import os
import salt.config import salt.auth import salt.client import salt.runner import salt.wheel import salt.utils import salt.syspaths as syspaths from salt.utils.event import tagify from salt.exceptions import EauthAuthenticationError
funparts = cmd.get('fun', '').split('.')
from __future__ import absolute_import, print_function import base64 import copy import getpass import json import logging import multiprocessing import subprocess import hashlib import tarfile import os import re import sys import time import yaml import uuid import tempfile import binascii import sys
import salt.ext.six as six
DEFAULT_THIN_DIR = '/var/tmp/.%%USER%%_%%FQDNUUID%%_salt'
RSTR_RE = r'(?:^|\r?\n)' + RSTR + '(?:\r?\n|$)'
shim_file += "c"
stdout, stderr, retcode = single.shell.copy_id()
if len(running) >= self.opts.get('ssh_max_procs', 25) or len(self.targets) >= len(running): time.sleep(0.1)
argv = self.opts['argv']
self.returners['{0}.save_load'.format(self.opts['master_job_cache'])](jid, job_load)
argv = self.opts['argv']
final_exit = 1
opts_pkg['id'] = self.id
if isinstance(mine_args, dict): self.args = [] self.kwargs = mine_args elif isinstance(mine_args, list): self.args = mine_args self.kwargs = {}
with tempfile.NamedTemporaryFile(mode='w', prefix='shim_', delete=False) as shim_tmp_file: shim_tmp_file.write(cmd_str)
target_shim_file = '.{0}'.format(binascii.hexlify(os.urandom(6))) self.shell.send(shim_tmp_file.name, target_shim_file)
try: os.remove(shim_tmp_file.name) except IOError: pass
ret = self.shell.exec_cmd('/bin/sh \'$HOME/{0}\''.format(target_shim_file))
self.shell.exec_cmd('rm \'$HOME/{0}\''.format(target_shim_file))
return 'ERROR: Failure deploying thin, undefined state: {0}'.format(stdout), stderr, retcode
while re.search(RSTR_RE, stdout): stdout = re.split(RSTR_RE, stdout, 1)[1].strip()
while re.search(RSTR_RE, stderr): stderr = re.split(RSTR_RE, stderr, 1)[1].strip()
return None
return 'Undefined SHIM state'
return None
import collections
import salt.pillar import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM
items = raw data = items
from __future__ import absolute_import import copy import logging
import salt.client.ssh import salt.runner
ssh = salt.client.ssh.SSH(opts)
rets = {} for ret in ssh.run_iter(): rets.update(ret)
runner = salt.runner.RunnerClient(__opts__['__master_opts__']) return runner.cmd(fun, arg)
from __future__ import absolute_import import collections import math
import salt.utils import salt.utils.dictupdate from salt.exceptions import SaltException
import salt.ext.six as six
__grains__ = {}
from __future__ import absolute_import import json import copy
import salt.loader import salt.utils import salt.client.ssh
import salt.ext.six as six
cmd = '{0}.{1}'.format(self.cmd_prefix, cmd)
raise KeyError('Cannot assign to module key {0} in the ' 'FunctionWrapper'.format(cmd))
cmd = '{0}.{1}'.format(self.cmd_prefix, cmd)
self.aliases[cmd] = value
from __future__ import absolute_import import re import os
import salt.utils import salt.syspaths as syspaths
import salt.ext.six as six
from __future__ import absolute_import import copy
import salt.client.ssh
ssh = salt.client.ssh.SSH(opts)
rets = {} for ret in ssh.run_iter(mine=True): rets.update(ret)
import os import copy import json import logging
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
if errors: return errors return high_data
comps = fun.split('.') if len(comps) < 2: __context__['retcode'] = 1 return 'Invalid function passed'
kwargs.update({'state': comps[0], 'fun': comps[1], '__id__': name, 'name': name})
if salt.utils.test_mode(test=test, **kwargs): opts['test'] = True else: opts['test'] = __opts__.get('test', None)
__pillar__.update(kwargs.get('pillar', {}))
st_ = salt.client.ssh.state.SSHState(__opts__, __pillar__)
err = st_.verify_data(kwargs) if err: __context__['retcode'] = 1 return err
chunks = [kwargs]
trans_tar = salt.client.ssh.state.prep_trans_tar( __context__['fileclient'], chunks, file_refs, __pillar__, id_=st_kwargs['id_'])
trans_tar_sum = salt.utils.get_hash(trans_tar, __opts__['hash_type'])
cmd = 'state.pkg {0}/salt_state.tgz test={1} pkg_sum={2} hash_type={3}'.format( __opts__['thin_dir'], test, trans_tar_sum, __opts__['hash_type'])
single = salt.client.ssh.Single( __opts__, cmd, fsclient=__context__['fileclient'], minion_opts=__salt__.minion_opts, **st_kwargs)
single.shell.send( trans_tar, '{0}/salt_state.tgz'.format(__opts__['thin_dir']))
stdout, stderr, _ = single.cmd_block()
try: os.remove(trans_tar) except (OSError, IOError): pass
return stdout
import salt.client.ssh import logging import os from salt.exceptions import CommandExecutionError
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
EX_THIN_DEPLOY = 11 EX_THIN_CHECKSUM = 12 EX_MOD_DEPLOY = 13 EX_SCP_NOT_FOUND = 14 EX_CANTCREAT = 73
#%%OPTS
sys.stdout.write("{0}\ndeploy\n".format(OPTIONS.delimiter)) sys.exit(EX_THIN_DEPLOY)
for chunk in iter(lambda: ifile.read(chunk_size), b''): hash_obj.update(chunk) return hash_obj.hexdigest()
if len(ARGS) == 1: argv_prepared = ARGS[0].split() else: argv_prepared = ARGS
from __future__ import absolute_import import os import copy import logging
import salt.config import salt.syspaths as syspaths
import os import tarfile import tempfile import json import shutil from contextlib import closing
import salt.client.ssh.shell import salt.client.ssh import salt.utils import salt.utils.thin import salt.utils.url import salt.roster import salt.state import salt.loader import salt.minion
cwd = os.getcwd()
import re import os import json import time import logging import subprocess
import salt.defaults.exitcodes import salt.utils import salt.utils.nb_popen import salt.utils.vt
RSTR = '_edbc7885e4f9aac9b83b35999b68d015148caf467b78fa39c05f669c0ff89878' RSTR_RE = re.compile(r'(?:^|\r?\n)' + RSTR + r'(?:\r?\n|$)')
send_password = False
return '', 'Password authentication failed', 254
from __future__ import absolute_import import signal import logging
import salt.loader import salt.utils.process
if signal.getsignal(signal.SIGINT) is signal.SIG_DFL: signal.signal(signal.SIGINT, self._handle_signals)
signal.signal(signal.SIGINT, self._handle_signals)
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
from __future__ import absolute_import import os import imp import sys import salt import time import logging import inspect import tempfile import functools from collections import MutableMapping from zipimport import zipimporter
import salt.modules.cmdmod
import salt.ext.six as six try: import pkg_resources HAS_PKG_RESOURCES = True except ImportError: HAS_PKG_RESOURCES = False
pyximport = None
if name not in loader.file_mapping: return {}
try:
self.suffix_map = {}
for (suffix, mode, kind) in SUFFIXES: self.suffix_map[suffix] = (suffix, mode, kind) suffix_order.append(suffix)
self.suffix_map['.pyx'] = tuple()
self.file_mapping = salt.utils.odict.OrderedDict()
if ext not in self.suffix_map:
if ext == '': subfiles = os.listdir(fpath) for suffix in suffix_order: if '' == suffix:
self.file_mapping[f_noext] = (fpath, ext)
if hasattr(self, 'opts'): self.refresh_file_mapping() self.initial_load = False
if mod_name in self.file_mapping: yield mod_name
for k in self.file_mapping: if mod_name in k: yield k
for k in self.file_mapping: if mod_name not in k: yield k
for submodule in submodules: if submodule.__name__.startswith(mod.__name__ + '.'): reload(submodule) self._reload_submodules(submodule)
for p_name, p_value in six.iteritems(self.pack): setattr(mod, p_name, p_value)
if virtual_ret is not True: self.missing_modules[module_name] = virtual_err self.missing_modules[name] = virtual_err return False
if module_name in self.loaded_modules: mod_dict = self.loaded_modules[module_name] else: mod_dict = self.mod_dict_class()
continue
if full_funcname not in self._dict: self._dict[full_funcname] = func if funcname not in mod_dict: setattr(mod_dict, funcname, func) mod_dict[funcname] = func self._apply_outputter(func, mod)
if self._load_module(name) and key in self._dict: return True
if virtual is not True and module_name != virtual: log.trace('Loaded {0} as virtual {1}'.format( module_name, virtual ))
elif virtual is True and virtualname != module_name: if virtualname is not True: module_name = virtualname
log.debug( 'KeyError when loading {0}'.format(module_name), exc_info=True )
log.error( 'Failed to read the virtual function for ' '{0}: {1}'.format( self.tag, module_name ), exc_info=True ) return (False, module_name, error_reason)
from __future__ import absolute_import
import salt.utils
import salt.ext.six as six
import salt.output.highstate
from numbers import Number
import salt.output from salt.ext.six import string_types from salt.utils import get_colors import salt.utils.locales
import pprint
ret += '{0}\n'.format(pprint.pformat(data))
from __future__ import absolute_import
try: import progressbar HAS_PROGRESSBAR = True except ImportError: HAS_PROGRESSBAR = False
from __future__ import print_function from __future__ import absolute_import import os import sys import errno import logging import traceback from salt.ext.six import string_types
import salt.loader import salt.utils from salt.utils import print_cli import salt.ext.six as six
try: progress_outputter = salt.loader.outputters(opts)[out]
if exc.errno != errno.EPIPE: raise exc
out = opts['output']
if out != 'grains': log.error('Invalid outputter {0} specified, fall back to nested'.format(out)) return outputters['nested']
import yaml
from salt.utils.yamldumper import OrderedDumper
__virtualname__ = 'yaml'
params.update(default_flow_style=False)
params.update(default_flow_style=False, indent=__opts__['output_indent'])
__virtualname__ = 'quiet'
from __future__ import absolute_import
import salt.utils.locales
import salt.utils
import pprint
__virtualname__ = 'pprint'
from __future__ import absolute_import import pprint import textwrap
import salt.utils import salt.output from salt.utils.locales import sdecode
import salt.ext.six as six
nchanges = 1 hstrs.append((u'{0} {1}{2[ENDC]}' .format(hcolor, data, colors)))
if __opts__.get('state_output_diff', False) and \ ret['result'] and not schanged: continue
if not __opts__.get('state_verbose', False) and \ ret['result'] and not schanged: continue
msg = _format_terse(tcolor, comps, ret, colors, tabular) hstrs.append(msg) continue
if ret['result'] is not False: msg = _format_terse(tcolor, comps, ret, colors, tabular) hstrs.append(msg) continue
if ret['result'] and not schanged: msg = _format_terse(tcolor, comps, ret, colors, tabular) hstrs.append(msg) continue
comment = str(ret['comment']) comment = comment.strip().replace( u'\n', u'\n' + u' ' * 14)
'colors': colors
import json import logging
__virtualname__ = 'json'
return json.dumps({})
import salt.utils import salt.output from salt.utils.locales import sdecode
from __future__ import absolute_import
import salt.ext.six as six
from __future__ import absolute_import
import salt.ext.six as six
from __future__ import absolute_import import os import subprocess
import salt.utils.locales
import os.path import msgpack
import salt.loader import salt.utils import salt.utils.cloud import salt.utils.validate.net from salt import syspaths
from __future__ import absolute_import import os.path
import msgpack
import salt.loader import salt.utils import salt.utils.cloud import salt.utils.validate.net import salt.config from salt import syspaths from salt.ext.six import string_types
HAS_RANGE = False try: import seco.range HAS_RANGE = True except ImportError: log.error('Unable to load range library')
tgt_func = { 'range': target_range, 'glob': target_range, }
import salt.loader import salt.syspaths
from __future__ import absolute_import import socket
ports = list(map(int, str(ports).split(',')))
from __future__ import absolute_import import socket import logging
import salt.utils.network from salt._compat import ipaddress
ports = list(map(int, str(ports).split(',')))
import fnmatch import re
HAS_RANGE = False try: import seco.range HAS_RANGE = True except ImportError: pass
import salt.loader from salt.template import compile_template from salt.ext.six import string_types from salt.roster import get_roster_file
from __future__ import absolute_import import os import re import fnmatch import json import subprocess
import salt.utils from salt.roster import get_roster_file
import salt.ext.six as six
import logging import copy
try: import confidant.client import confidant.formatter HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
import logging
import yaml
log = logging.getLogger(__name__)
import logging
try: import salt.utils.etcd_util HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
path %= { 'minion_id': minion_id }
import logging
try: import consul HAS_CONSUL = True except ImportError: HAS_CONSUL = False
log = logging.getLogger(__name__)
path %= { 'minion_id': minion_id, 'role': role }
if value is None: return ret
pillar_value = yaml.load(value)
sorted_mappings = sorted(mappings, key=lambda m: (-len(m[0]), m[0]))
from __future__ import absolute_import import logging import os import time import pickle from copy import deepcopy
import salt.ext.six as six from salt.ext.six.moves import filter from salt.ext.six.moves.urllib.parse import quote as _quote
from salt.pillar import Pillar import salt.utils import salt.utils.s3 as s3
log = logging.getLogger(__name__)
opts['ext_pillar'] = [x for x in opts['ext_pillar'] if 's3' not in x]
if os.path.isfile(cache_file): cache_file_mtime = os.path.getmtime(cache_file) else: cache_file_mtime = 0
if not os.path.exists(os.path.dirname(file_path)): os.makedirs(os.path.dirname(file_path))
def __get_pillar_files_from_s3_meta(s3_meta): return [k for k in s3_meta if 'Key' in k]
log.debug('Single environment per bucket mode')
if s3_meta: bucket_files[bucket] = __get_pillar_files_from_s3_meta(s3_meta)
log.debug('Multiple environment per bucket mode') s3_meta = __get_s3_meta()
if s3_meta: files = __get_pillar_files_from_s3_meta(s3_meta) environments = __get_pillar_environments(files)
for saltenv in environments: env_files = [k for k in files if k['Key'].startswith(saltenv)]
if os.path.isfile(cache_file): os.remove(cache_file)
filePaths = [k['Key'] for k in data] ret[bucket] += [k for k in filePaths if not k.endswith('/')]
log.debug("Cached file: path={0}, md5={1}, etag={2}".format(cached_file_path, cached_md5, file_md5)) if cached_md5 == file_md5: return
s3.query( key=creds.key, keyid=creds.keyid, kms_keyid=creds.kms_keyid, bucket=bucket, service_url=creds.service_url, path=_quote(path), local_file=cached_file_path, verify_ssl=creds.verify_ssl, location=creds.location )
import logging import re
try: import pymongo HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
log = logging.getLogger(__name__)
if re_pattern: minion_id = re.sub(re_pattern, re_replace, minion_id)
result['_id'] = str(result['_id'])
log.debug( 'ext_pillar.mongo: no document found in collection {0}'.format( collection ) ) return {}
import logging
from salt.serializers.yamlex import deserialize
log = logging.getLogger(__name__)
stack_k = stack[k] stack[k] = _cleanup(v) v = stack_k
import logging
import salt.utils.virt
log = logging.getLogger(__name__)
import logging
from salt.utils.odict import OrderedDict from salt.ext.six.moves import range from salt.ext import six
log = logging.getLogger(__name__)
def __virtual__(): return False
qbuffer = []
qbuffer.extend([[None, s] for s in args])
klist = list(kwargs.keys()) klist.sort() qbuffer.extend([[k, kwargs[k]] for k in klist])
if root: self.result[root] = self.focus = {} else: self.focus = self.result
if self.depth == self.num_fields - 1:
from __future__ import absolute_import import copy import os import collections import logging import tornado.gen
import salt.ext.six as six
raise SaltClientError(msg)
self.opts = opts self.grains = grains self.minion_id = minion_id self.ext = ext self.functions = functions self.pillar = pillar self.pillarenv = pillarenv
self.cache = salt.utils.cache.CacheFactory.factory( self.opts['pillar_cache_backend'], self.opts['pillar_cache_ttl'], minion_cache_path=self._minion_cache_path(minion_id))
fresh_pillar = self.fetch_pillar() self.cache[self.minion_id] = {self.saltenv: fresh_pillar} log.debug('Pillar cache miss for minion {0}'.format(self.minion_id))
for saltenv, targets in six.iteritems(top): sorted_targets = sorted(targets, key=lambda target: orders[saltenv][target]) for target in sorted_targets: sorted_top[saltenv][target] = targets[target] return sorted_top
return None, mods, errors
mopts['file_roots'] = self.actual_file_roots mopts['saltversion'] = __version__ pillar['master'] = mopts
from __future__ import absolute_import import re import logging
try: import boto.ec2 import boto.utils import boto.exception HAS_BOTO = True except ImportError: HAS_BOTO = False
log = logging.getLogger(__name__)
(instance_id, region) = _get_instance_info()
from __future__ import absolute_import
from salt.exceptions import SaltInvocationError from salt.utils.reclass import ( prepend_reclass_source_path, filter_out_source_path_option, set_inventory_base_uri_default )
import salt.ext.six as six
__virtualname__ = 'reclass'
opts = next(six.itervalues(pillar)) prepend_reclass_source_path(opts) break
from reclass.adapters.salt import ext_pillar as reclass_ext_pillar from reclass.errors import ReclassException
filter_out_source_path_option(kwargs)
set_inventory_base_uri_default(__opts__, kwargs)
return reclass_ext_pillar(minion_id, pillar, **kwargs)
from __future__ import absolute_import import logging import salt.utils.vault
from contextlib import contextmanager import logging
from salt.pillar.sql_base import SqlBaseExtPillar
log = logging.getLogger(__name__)
try: import MySQLdb HAS_MYSQL = True except ImportError: HAS_MYSQL = False
import logging
log = logging.getLogger(__name__)
__virtualname__ = 'foreman'
if api != 2: log.error('Foreman API v2 is supported only, please specify' 'version 2 in your Salt master config') raise Exception
import logging
log = logging.getLogger(__name__)
import logging
import yaml
log = logging.getLogger(__name__)
import logging
log = logging.getLogger(__name__)
__virtualname__ = 'varstack'
import fnmatch import logging import os
import salt.utils import salt.utils.dictupdate import salt.utils.minions
log = logging.getLogger(__name__)
for dir_name in dir_names: pillar_node[dir_name] = {}
del pillar
return ngroup_pillar
import logging import json
log = logging.getLogger(__name__)
# Copyright (C) 2014 Floris Bruynooghe <flub@devork.be>
from __future__ import absolute_import import copy import hashlib import logging import os
import salt.pillar
try: import hglib except ImportError: hglib = None
__opts__ = {}
from contextlib import contextmanager import logging import sqlite3
import salt.utils from salt.pillar.sql_base import SqlBaseExtPillar
log = logging.getLogger(__name__)
_opts = __opts__.get('sqlite3', {})
from __future__ import print_function from __future__ import absolute_import import os import logging
from salt.exceptions import SaltInvocationError
import yaml from jinja2 import Environment, FileSystemLoader try:
log = logging.getLogger(__name__)
import json
return globals()[function](minion_id, pillar, **kwargs)
if not key_data: return {}
if isinstance(data, dict) and not pillar_key: return data elif not pillar_key: return {'redis_pillar': data} else: return {pillar_key: data}
from __future__ import absolute_import
from salt.minion import Matcher
import salt.ext.six as six
import os import subprocess
import salt.utils
gen_hyper_keys(minion_id)
from __future__ import absolute_import import logging
import salt.utils
import yaml import salt.ext.six as six
log = logging.getLogger(__name__)
import logging
log = logging.getLogger(__name__)
from __future__ import absolute_import import logging
try: import salt.utils.openstack.neutron as suoneu HAS_NEUTRON = True except NameError as exc: HAS_NEUTRON = False
log = logging.getLogger(__name__)
from copy import deepcopy import logging import os import hashlib
HAS_SVN = False try: import pysvn HAS_SVN = True CLIENT = pysvn.Client() except ImportError: pass
from salt.pillar import Pillar
log = logging.getLogger(__name__)
__virtualname__ = 'svn'
options = repo_string.strip().split() branch = options[0] repo_location = options[1] root = ''
branch = (branch == 'trunk' and 'base' or branch)
env: /path/to/virtualenv/
my_application.clients:
Client:
name: shortname
filter: {'kw': 'args'}
fields: - field_1 - field_2
sys.path.insert(0, os.path.join( virtualenv.path_locations(env)[1], 'site-packages'))
sys.path.append(project_path)
if name_field not in model: raise salt.exceptions.SaltException( "Name '{0}' not found in returned fields.".format( name_field))
import logging
log = logging.getLogger(__name__)
#pepa_delimiter: ..
#pepa_grains:
#pepa_pillars:
#log_level: debug
from __future__ import absolute_import, print_function
import logging import sys import glob import yaml import jinja2 import re from os.path import isfile, join
import salt.ext.six as six
import salt.utils
log = None if __name__ == '__main__':
__opts__ = { 'pepa_roots': { 'base': '/srv/salt' }, 'pepa_delimiter': '..', 'pepa_validate': False }
inp = {} inp['default'] = 'default' inp['hostname'] = minion_id
output = inp output['pepa_templates'] = [] immutable = {}
with salt.utils.fopen(args.config) as fh_: __opts__.update(yaml.load(fh_.read()))
__grains__ = {} if 'pepa_grains' in __opts__: __grains__ = __opts__['pepa_grains'] if args.grains: __grains__.update(yaml.load(args.grains))
__pillar__ = {} if 'pepa_pillar' in __opts__: __pillar__ = __opts__['pepa_pillar'] if args.pillar: __pillar__.update(yaml.load(args.pillar))
if args.validate: __opts__['pepa_validate'] = True
from contextlib import contextmanager import logging
from salt.pillar.sql_base import SqlBaseExtPillar
log = logging.getLogger(__name__)
try: from pysqlcipher import dbapi2 as sqlcipher HAS_SQLCIPHER = True except ImportError: HAS_SQLCIPHER = False
import copy import logging import hashlib import os
import salt.utils.gitfs import salt.utils.dictupdate from salt.exceptions import FileserverConfigError from salt.pillar import Pillar
import salt.ext.six as six try: import git HAS_GITPYTHON = True except ImportError: HAS_GITPYTHON = False
log = logging.getLogger(__name__)
__virtualname__ = 'git'
return False
try: salt.utils.gitfs.GitPillar(__opts__) return __virtualname__ except FileserverConfigError: pass
pillar.fetch_remotes()
pillar_roots = [pillar_dir] pillar_roots.extend([x for x in all_dirs if x != pillar_dir]) opts['pillar_roots'] = {env: pillar_roots}
self.working_dir = rp_
pass
options = repo_string.strip().split() branch_env = options[0] repo_location = options[1] root = ''
cfg_branch, _, environment = branch_env.partition(':')
pillar_dir = os.path.normpath(os.path.join(gitpil.working_dir, root))
if __opts__['pillar_roots'].get(branch, []) == [pillar_dir]: return {}
from __future__ import absolute_import, print_function import re import sys import platform
if sys.version_info[0] == 3: MAX_SIZE = sys.maxsize string_types = (str,) else: MAX_SIZE = sys.maxint string_types = (basestring,) from itertools import imap as map
return 0 < self.major < 2014
return method(self.noc_info, other.noc_info)
if self.rc > 0 and other.rc <= 0: noc_info = list(self.noc_info) noc_info[3] = -1 return method(tuple(noc_info), other.noc_info)
def __discover_version(saltstack_version): import os import subprocess
return saltstack_version
return saltstack_version
kwargs['close_fds'] = True
saltstack_version.sha = out.strip() saltstack_version.noc = -1
raise
__saltstack_version__ = __get_version(__saltstack_version__) del __get_version
__version_info__ = __saltstack_version__.info __version__ = __saltstack_version__.string
from __future__ import absolute_import, print_function import os import sys import time import logging import threading import traceback from random import randint
from salt import cloud, defaults
if os.getuid() == 0 and not salt.utils.is_windows(): os.kill(parent_pid, 0)
log.error('Minion process encountered exception: {0}'.format(exc)) os._exit(salt.defaults.exitcodes.EX_GENERIC)
os.kill(pid, signum)
minion = salt.cli.daemons.Minion() minion.start() break
signal.signal(signal.SIGINT, prev_sigint_handler) signal.signal(signal.SIGTERM, prev_sigterm_handler)
time.sleep(2 + randint(1, 10)) rlogger = logging.getLogger() for handler in rlogger.handlers: rlogger.removeHandler(handler) logging.basicConfig()
os.kill(parent_pid, 0)
os._exit(999)
queue.put(random_delay)
has_saltcloud = False
from __future__ import absolute_import import warnings
warnings.filterwarnings(
warnings.filterwarnings( 'ignore', 'With-statements now directly support multiple context managers', DeprecationWarning )
warnings.filterwarnings( 'ignore', '^Module backports was already imported from (.*), but (.*) is being added to sys.path$', UserWarning )
del locale if not encoding: encoding = sys.getdefaultencoding() or 'ascii'
if sys.version_info[0] < 3: import __builtin__ as builtins else:
setattr(builtins, '__salt_system_encoding__', encoding)
del sys del builtins del encoding
del __define_global_system_encoding_variable__
from __future__ import absolute_import import os import re import sys import time import types import socket import logging import logging.handlers import traceback import multiprocessing
import salt.ext.six as six
PROFILE = logging.PROFILE = 15 TRACE = logging.TRACE = 5 GARBAGE = logging.GARBAGE = 1 QUIET = logging.QUIET = 1000
from salt.textformat import TextFormat from salt.log.handlers import (TemporaryLoggingHandler, StreamHandler, SysLogHandler, FileHandler, WatchedFileHandler, QueueHandler) from salt.log.mixins import LoggingMixInMeta, NewStyleClassMixIn
SORTED_LEVEL_NAMES = [ l[0] for l in sorted(six.iteritems(LOG_LEVELS), key=lambda x: x[1]) ]
LOGGING_LOGGER_CLASS = logging.getLoggerClass()
LOGGING_NULL_HANDLER = TemporaryLoggingHandler(logging.WARNING)
LOGGING_TEMP_HANDLER = StreamHandler(sys.stderr)
LOGGING_STORE_HANDLER = TemporaryLoggingHandler()
self.bracketname = '[%-17s]' % self.name self.bracketlevel = '[%-8s]' % self.levelname self.bracketprocess = '[%5s]' % self.process
handler.release() return instance
handler.release() return instance
handler.release() return instance
pass
if logging.getLoggerClass() is not SaltLoggingClass:
logging.root.setLevel(GARBAGE)
logging.root.addHandler(LOGGING_NULL_HANDLER)
logging.root.addHandler(LOGGING_STORE_HANDLER)
continue
break
__remove_null_logging_handler()
__remove_temp_logging_handler()
continue
break
if not log_format: log_format = '[%(levelname)-8s] %(message)s' if not date_format: date_format = '%H:%M:%S'
__remove_temp_logging_handler()
syslog_opts['address'] = os.sep.join( parsed_log_path.path.split(os.sep)[:-1] )
raise RuntimeError( 'The syslog facility \'{0}\' is not known'.format( facility_name ) )
syslog_opts.pop('socktype', None)
handler = SysLogHandler(**syslog_opts)
handler = WatchedFileHandler(log_path, mode='a', encoding='utf-8', delay=0)
return
return
import salt.loader
initial_handlers = logging.root.handlers[:]
providers = salt.loader.log_handlers(opts)
additional_handlers = []
initial_handlers_count = len(logging.root.handlers)
handlers = [handlers]
__remove_queue_logging_handler()
__remove_null_logging_handler()
return __MP_LOGGING_QUEUE
return
return
__MP_LOGGING_CONFIGURED = True
__remove_null_logging_handler() __remove_queue_logging_handler()
return
logging.root.removeHandler(__MP_LOGGING_QUEUE_HANDLER) __MP_LOGGING_QUEUE_HANDLER = None __MP_LOGGING_CONFIGURED = False
return
pass
__MP_LOGGING_QUEUE_PROCESS.terminate()
setup_temp_logger() setup_extended_logging(opts)
break
return
LOGGING_NULL_HANDLER = None break
return
LOGGING_STORE_HANDLER = None break
return
__remove_null_logging_handler()
LOGGING_TEMP_HANDLER = None break
logging.captureWarnings(True)
if is_mp_logging_listener_configured(): shutdown_multiprocessing_logging_listener()
sys.excepthook = __global_logging_exception_handler
import sys import logging
if self.level > exc_info_on_loglevel: return formatted_record
if not record.exc_info_on_loglevel_instance and not exc_info_on_loglevel_formatted: return formatted_record
if formatted_record[-1:] != '\n': formatted_record += '\n'
from salt.log.setup import ( LOG_LEVELS, SORTED_LEVEL_NAMES, is_console_configured, is_logfile_configured, is_logging_configured, is_temp_logging_configured, setup_temp_logger, setup_console_logger, setup_logfile_logger, set_logger_level, )
from __future__ import absolute_import import socket import logging
from salt.log.mixins import NewStyleClassMixIn from salt.log.setup import LOG_LEVELS
try: from log4mongo.handlers import MongoHandler, MongoFormatter HAS_MONGO = True except ImportError: HAS_MONGO = False
from __future__ import absolute_import, print_function import logging import logging.handlers import time import datetime import socket import threading
from salt.log.setup import LOG_LEVELS from salt.log.mixins import NewStyleClassMixIn import salt.utils.network
import salt.ext.six as six
import msgpack if msgpack.loads(msgpack.dumps([1, 2, 3]), use_list=True) is None: raise ImportError
__virtualname__ = 'fluent'
__opts__.get( 'log_level', 'error' )
self._close()
if self.pendings: self.pendings += bytes_ bytes_ = self.pendings
self._reconnect()
self.socket.sendall(bytes_)
self.pendings = None
import sys import atexit import logging import threading import logging.handlers
from salt.log.mixins import NewStyleClassMixIn, ExcInfoOnLogLevelFormatMixIn
self.__messages.pop(0)
continue
from __future__ import absolute_import import os import json import logging import logging.handlers import datetime
from salt.log.setup import LOG_LEVELS from salt.log.mixins import NewStyleClassMixIn import salt.utils.network
import salt.ext.six as six try: import zmq except ImportError: pass
__virtualname__ = 'logstash'
__opts__.get( 'log_level', 'error' )
__opts__.get( 'log_level', 'error' )
import logging
import salt.loader from salt.log import LOG_LEVELS
try: import raven from raven.handlers.logging import SentryHandler HAS_RAVEN = True except ImportError: HAS_RAVEN = False
__virtualname__ = 'sentry'
options.update({ 'site': get_config_value('site'),
'name': get_config_value('name'),
'exclude_paths': get_config_value('exclude_paths', ()),
'include_paths': get_config_value('include_paths', ()),
'list_max_length': get_config_value('list_max_length'),
'string_max_length': get_config_value('string_max_length'),
'auto_log_stacks': get_config_value('auto_log_stacks'),
'timeout': get_config_value('timeout', 1),
'processors': get_config_value('processors'),
'dsn': dsn
import logging
pass
import salt.ext.six as six
class Channel(object): @staticmethod def factory(opts, **kwargs): ttype = 'zeromq'
from __future__ import absolute_import import logging import socket import msgpack import weakref import time
import tornado import tornado.gen import tornado.netutil import tornado.concurrent from tornado.ioloop import IOLoop from tornado.iostream import IOStream
import salt.transport.client import salt.transport.frame import salt.ext.six as six
def future_with_timeout_callback(future): if future._future_with_timeout is not None: future._future_with_timeout._done_callback(future)
self._future._future_with_timeout = self if self._future.done(): future_with_timeout_callback(self._future)
self._future._future_with_timeout = None self.set_exception(tornado.ioloop.TimeoutError())
self.sock = None self.io_loop = io_loop or IOLoop.current() self._closing = False
instance_map = weakref.WeakKeyDictionary()
key = str(socket_path)
new_client.__singleton_init__(io_loop=io_loop, socket_path=socket_path) loop_instance_map[key] = new_client
pass
import tornado.ioloop
import salt.config import salt.transport.ipc
ipc_client.connect()
import tornado.ioloop
import salt.transport.ipc import salt.config
ipc_server.start(ipc_server_socket_path)
io_loop.start()
def print_to_console(payload): print(payload)
self.sock = None self.io_loop = io_loop or IOLoop.current() self._closing = False self.streams = set()
import tornado.ioloop
import salt.config import salt.transport.ipc
io_loop = tornado.ioloop.IOLoop()
io_loop.run_sync(ipc_subscriber.connect)
timeout = None
break
ret = None
self.io_loop.spawn_callback(self.io_loop.stop)
if self._read_sync_future is not None: self._read_sync_future.exc_info() if self._read_stream_future is not None: self._read_stream_future.exc_info()
from __future__ import absolute_import import msgpack import salt.ext.six as six
from __future__ import absolute_import
ttype = 'zeromq'
ttype = 'zeromq'
if ttype == 'zeromq': import salt.transport.zeromq return salt.transport.zeromq.ZeroMQPubServerChannel(opts, **kwargs)
from __future__ import absolute_import, print_function import logging
import salt.utils from salt.transport.client import ReqChannel
ret = { 'data': None, 'dest': None, }
from __future__ import absolute_import import os import copy import errno import signal import hashlib import logging import weakref from random import randint
import tornado import tornado.gen import tornado.concurrent
import salt.ext.six as six from Crypto.Cipher import PKCS1_OAEP
instance_map = weakref.WeakKeyDictionary()
def __init__(self, opts, **kwargs): pass
def __singleton_init__(self, opts, **kwargs): self.opts = dict(opts) self.ttype = 'zeromq'
self.crypt = kwargs.get('crypt', 'aes')
self.auth = salt.crypt.AsyncAuth(self.opts, io_loop=self._io_loop)
yield self.auth.authenticate()
yield self.auth.authenticate() ret = yield self.message_client.send( self._package_load(self.auth.crypticle.dumps(load)), timeout=timeout, tries=tries, )
yield self.auth.authenticate()
ret = yield _do_transfer()
yield self.auth.authenticate() ret = yield _do_transfer()
self._socket.setsockopt(zmq.IPV4ONLY, 0)
import threading self._w_monitor = ZeroMQSocketMonitor(self._socket) t = threading.Thread(target=self._w_monitor.start_poll) t.start()
stream.send('Server-side exception handling payload')
log.info('Starting the Salt Publisher on {0}'.format(pub_uri)) pub_sock.bind(pub_uri)
if load['tgt_type'] == 'list': int_payload['topic_lst'] = load['tgt']
int_payload['topic_lst'] = match_ids
self._init_socket()
self.send_future_map = {}
if hasattr(zmq, 'RECONNECT_IVL_MAX'): self.socket.setsockopt( zmq.RECONNECT_IVL_MAX, 5000 )
del self.send_queue[0] continue
self.io_loop.remove_timeout(timeout)
message = self.serial.dumps(message)
self.send_future_map[message] = future
from __future__ import absolute_import import logging import msgpack import socket import os import weakref import time import traceback
import tornado import tornado.tcpserver import tornado.gen import tornado.concurrent import tornado.tcpclient import tornado.netutil
if six.PY2: import urlparse else: import urllib.parse as urlparse
from Crypto.Cipher import PKCS1_OAEP
def __setstate__(self, state): self._is_child = True self.__init__( state['opts'], state['socket_queue'], log_queue=state['log_queue'] )
connection, address = self._socket.accept() self.socket_queue.put((connection, address), True, None)
if tornado.util.errno_from_exception(e) == errno.ECONNABORTED: continue raise
instance_map = weakref.WeakKeyDictionary()
new_obj = object.__new__(cls) new_obj.__singleton_init__(opts, **kwargs) loop_instance_map[key] = new_obj
def __init__(self, opts, **kwargs): pass
def __singleton_init__(self, opts, **kwargs): self.opts = dict(opts)
self.crypt = kwargs.get('crypt', 'aes')
raise SaltClientError('Connection to master lost')
stream.write('Server-side exception handling payload') stream.close()
self.io_loop.spawn_callback( self._handle_connection, client_socket, address)
if self.connect_callback is not None: def handle_future(future): response = future.result() self.io_loop.add_callback(self.connect_callback, response) future.add_done_callback(handle_future)
if self._connecting_future.done(): self._connecting_future = self.connect() yield self._connecting_future
if self._connecting_future.done(): self._connecting_future = self.connect() yield self._connecting_future
raise Exception('Unable to find available messageid')
self.send_future_map[message_id] = future
self._read_until_future.exc_info()
self.presence_events = True
return
return
continue
f = client.stream.write(payload) self.io_loop.add_future(f, lambda f: True)
if self.io_loop is None: self.io_loop = tornado.ioloop.IOLoop.current()
try: self.io_loop.start() except (KeyboardInterrupt, SystemExit): salt.log.setup.shutdown_multiprocessing_logging()
if load['tgt_type'] == 'list': int_payload['topic_lst'] = load['tgt'] pub_sock.send(int_payload)
from __future__ import absolute_import
from salt.utils.async import SyncWrapper
sync = SyncWrapper(AsyncReqChannel.factory, (opts,), kwargs) return sync
_resolver_configured = False
ttype = 'zeromq'
ttype = 'zeromq'
if ttype == 'zeromq': import salt.transport.zeromq return salt.transport.zeromq.AsyncZeroMQPubChannel(opts, **kwargs)
import salt.transport.ipc return salt.transport.ipc.IPCMessageClient(opts, **kwargs)
from __future__ import absolute_import import multiprocessing import ctypes import logging import os import hashlib import shutil import binascii
import salt.crypt import salt.payload import salt.master import salt.transport.frame import salt.utils.event import salt.ext.six as six from salt.utils.cache import CacheCli
import tornado.gen from Crypto.Cipher import PKCS1_OAEP from Crypto.PublicKey import RSA
auto_reject = self.auto_key.check_autoreject(load['id']) auto_sign = self.auto_key.check_autosign(load['id'])
pass
key_path = None
if self.cache_cli: self.cache_cli.put_cache([load['id']])
pass
pass
from __future__ import absolute_import, with_statement import copy import ctypes import os import re import sys import time import errno import signal import stat import logging import multiprocessing import tempfile import traceback
from Crypto.PublicKey import RSA import salt.ext.six as six from salt.ext.six.moves import range
HAS_RESOURCE = False
try:
self.loop_interval = int(self.opts['loop_interval']) self.rotate = int(time.time())
def __setstate__(self, state): self._is_child = True self.__init__(state['opts'], log_queue=state['log_queue'])
self.presence_events = True
self._post_fork_init()
last = int(time.time()) salt.daemons.masterapi.clean_fsbackend(self.opts) salt.daemons.masterapi.clean_pub_auth(self.opts)
log.debug('Pinging all connected minions ' 'due to key rotation') salt.utils.master.ping_all_connected_minions(self.opts)
if self.schedule.loop_interval < self.loop_interval: self.loop_interval = self.schedule.loop_interval
self.event.fire_event(data, tagify('present', 'presence'), timeout=3) old_present.clear() old_present.update(present)
try: fileserver.init() except FileserverConfigError as exc: critical_errors.append('{0}'.format(exc))
salt.utils.gitfs.GitPillar(new_opts)
def run_reqserver(self, **kwargs): secrets = kwargs.pop('secrets', None) if secrets is not None: SMaster.secrets = secrets
with default_signals(signal.SIGINT, signal.SIGTERM):
log.info('Creating master maintenance process') self.process_manager.add_process(Maintenance, args=(self.opts,))
self.process_manager.add_process(self.run_reqserver, kwargs=kwargs, name='ReqServer')
if signal.getsignal(signal.SIGINT) is signal.SIG_DFL: signal.signal(signal.SIGINT, self._handle_signals)
signal.signal(signal.SIGINT, self._handle_signals)
self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
def __setstate__(self, state): self._is_child = True self.__init__(state['hopts'], log_queue=state['log_queue'])
self.key = key
os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
if hasattr(self, 'process_manager'): self.process_manager.stop_restarting() self.process_manager.send_signal_to_processes(signum) self.process_manager.kill_children()
if HAS_ZMQ: zmq.eventloop.ioloop.install() self.io_loop = LOOP_CLASS() for req_channel in self.req_channels:
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return False
return False
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return {}
self.masterapi._minion_event(load) self._handle_minion_event(load)
log.warning('Authentication failure of type "eauth" occurred.') return ''
if not token or token['eauth'] not in self.opts['external_auth']: log.warning('Authentication failure of type "token" occurred.') return ''
if clear_load['fun'] != 'saltutil.find_job': log.warning( 'Authentication failure of type "token" occurred.' ) return ''
log.warning( 'Authentication failure of type "eauth" occurred.' ) return ''
group_auth_match = False for group_config in group_perm_keys: group_config = group_config.rstrip('%') for group in groups: if group == group_config: group_auth_match = True
#[group for groups in ['external_auth'][extra['eauth']]]):
if not self.loadauth.time_auth(extra): log.warning( 'Authentication failure of type "eauth" occurred.' ) return ''
if clear_load['fun'] != 'saltutil.find_job': log.warning( 'Authentication failure of type "eauth" occurred.' ) return ''
self._send_pub(payload)
passed_jid = clear_load['jid'] if clear_load.get('jid') else None nocache = extra.get('nocache', False)
self.event.fire_event(new_job_load, tagify([clear_load['jid'], 'new'], 'job'))
if isinstance(exc, zmq.ZMQError) and exc.errno == errno.EINTR: return
from __future__ import absolute_import import logging
from salt.exceptions import SaltSystemExit
__proxyenabled__ = ['cisconso']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
__virtualname__ = 'cisconso'
ret = client.get_datastore(DatastoreType.RUNNING) GRAINS_CACHE.update(ret) return GRAINS_CACHE
from __future__ import absolute_import import salt.ext.six.moves.http_client as http_client
import logging import time import json from salt.exceptions import (CommandExecutionError, MinionError)
return True
return True
state = devices[str(dev_id)]['state']['on'] and Const.LAMP_OFF or Const.LAMP_ON
import json import logging
from salt.utils.vt_helper import SSHConnection from salt.utils.vt import TerminalException
__proxyenabled__ = ['ssh_sample']
log = logging.getLogger(__file__)
out, err = DETAILS['server'].sendline(cmd)
DETAILS['grains_cache'] = parse(out)
out, err = DETAILS['server'].sendline('pkg_list\n')
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
out, err = DETAILS['server'].sendline(cmd)
return parse(out)
import logging import salt.utils import salt.utils.http
__proxyenabled__ = ['fx2']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
from __future__ import absolute_import import logging
from salt.exceptions import SaltSystemExit
__proxyenabled__ = ['esxi']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
__virtualname__ = 'esxi'
try: username, password = find_credentials(host) except SaltSystemExit as err: log.critical('Error: {0}'.format(err)) return False
ret = __salt__['vsphere.system_info'](host=host, username=user, password=password)
continue
raise SaltSystemExit('Cannot complete login due to an incorrect user name or password.')
import logging log = logging.getLogger(__file__)
import napalm
NETWORK_DEVICE['UP'] = True
from __future__ import absolute_import from __future__ import print_function import logging
try: HAS_JUNOS = True import jnpr.junos import jnpr.junos.utils import jnpr.junos.utils.config import jnpr.junos.utils.sw except ImportError: HAS_JUNOS = False
__virtualname__ = 'junos'
crypt_salt = secure_password(8, use_random=False)
import logging import salt.utils.http
__proxyenabled__ = ['rest_sample']
GRAINS_CACHE = {} DETAILS = {}
log = logging.getLogger(__file__)
DETAILS['url'] = opts['proxy']['url']
if not DETAILS['url'].endswith('/'): DETAILS['url'] += '/'
from __future__ import absolute_import, print_function import os import sys import copy import time import hmac import base64 import hashlib import logging import stat import traceback import binascii import weakref import getpass
import salt.ext.six as six
pass
return priv
pass
if opts['master_sign_pubkey']:
def __setstate__(self, state): self.__init__(state['opts'])
instance_map = weakref.WeakKeyDictionary()
creds_map = {}
io_loop = io_loop or tornado.ioloop.IOLoop.current() if io_loop not in AsyncAuth.instance_map: AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary() loop_instance_map = AsyncAuth.instance_map[io_loop]
new_auth = object.__new__(cls) new_auth.__singleton_init__(opts, io_loop=io_loop) loop_instance_map[key] = new_auth
def __init__(self, opts, io_loop=None): pass
continue
user = self.opts.get('user', 'root') salt.utils.verify.check_path_traversal(self.opts['pki_dir'], user)
if 'pub_sig' in payload and self.opts['verify_master_pubkey_sign']: return True elif 'pub_sig' not in payload and not self.opts['verify_master_pubkey_sign']: return True
log.error('The master key has changed, the salt master could ' 'have been subverted, verify salt master\'s public ' 'key') return ''
instances = weakref.WeakValueDictionary()
def __init__(self, opts, io_loop=None): super(SAuth, self).__init__(opts, io_loop=io_loop)
if not data.startswith(self.PICKLE_PAD): return {} load = self.serial.loads(data[len(self.PICKLE_PAD):], raw=raw) return load
from __future__ import absolute_import
from salt.utils.schema import (Schema, IPv4Item, ) from salt.config.schemas.common import (MinionDefaultInclude, IncludeConfig )
__allow_additional_items__ = True
from __future__ import absolute_import
from salt.utils.schema import (Schema, StringItem, IntegerItem, SecretItem, PortItem, BooleanItem, RequirementsItem, DictItem, AnyOfItem ) from salt.config.schemas.minion import MinionConfiguration
pattern=r'^((\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})|([A-Za-z0-9][A-Za-z0-9\.\-]{1,255}))$', min_length=1, required=True)
from __future__ import absolute_import
from salt.utils.schema import (Schema, StringItem, ArrayItem, OneOfItem)
from __future__ import absolute_import from __future__ import generators import os import re import sys import glob import time import codecs import logging from copy import deepcopy import types
import yaml try: yaml.Loader = yaml.CLoader yaml.Dumper = yaml.CDumper except Exception: pass
import salt.ext.six as six from salt.ext.six import string_types, text_type from salt.ext.six.moves.urllib.parse import urlparse
_DFLT_IPC_MODE = 'tcp' _MASTER_TRIES = -1
'master': (string_types, list),
'master_port': int,
'master_uri_format': str,
'master_finger': str,
'master_shuffle': bool,
'master_alive_interval': int,
'master_failback': bool,
'master_failback_interval': int,
'master_sign_key_name': str,
'master_sign_pubkey': bool,
'verify_master_pubkey_sign': bool,
'always_verify_signature': bool,
'master_pubkey_signature': str,
'master_use_pubkey_signature': bool,
'syndic_finger': str,
'user': str,
'root_dir': str,
'pki_dir': str,
'id': str,
'cachedir': str,
'cache_jobs': bool,
'conf_file': str,
'sock_dir': str,
'backup_mode': str,
'renderer': str,
'renderer_whitelist': list,
'renderer_blacklist': list,
'failhard': bool,
'autoload_dynamic_modules': bool,
'environment': str,
'pillarenv': str,
'state_top': str,
'startup_states': str,
'sls_list': list,
'top_file': str,
'file_client': str,
'use_master_when_local': bool,
'file_roots': dict,
'pillar_roots': dict,
'hash_type': str,
'disable_modules': list,
'disable_returners': list,
'whitelist_modules': list,
'module_dirs': list,
'returner_dirs': list,
'states_dirs': list,
'grains_dirs': list,
'render_dirs': list,
'outputter_dirs': list,
'utils_dirs': list,
'providers': dict,
'clean_dynamic_modules': bool,
'open_mode': bool,
'multiprocessing': bool,
'mine_enabled': bool,
'mine_return_job': bool,
'mine_interval': int,
'ipc_mode': str,
'ipv6': bool,
'file_buffer_size': int,
'tcp_pub_port': int,
'tcp_pull_port': int,
'tcp_master_pub_port': int,
'tcp_master_pull_port': int,
'tcp_master_publish_pull': int,
'tcp_master_workers': int,
'log_file': str,
'log_level': str,
'log_level_logfile': str,
'log_datefmt': str,
'log_datefmt_logfile': str,
'log_fmt_console': str,
'log_fmt_logfile': (tuple, str),
'log_granular_levels': dict,
'max_event_size': int,
'test': bool,
'cython_enable': bool,
'enable_zip_modules': bool,
'show_timeout': bool,
'show_jid': bool,
'state_verbose': bool,
'state_output': str,
'state_output_diff': bool,
'state_auto_order': bool,
'state_events': bool,
'acceptance_wait_time': float,
'acceptance_wait_time_max': float,
'rejected_retry': bool,
'loop_interval': float,
'verify_env': bool,
'grains': dict,
'permissive_pki_access': bool,
'default_include': str,
'update_url': (bool, string_types),
'update_restart_services': list,
'retry_dns': float,
'recon_max': float,
'recon_default': float,
'recon_randomize': bool,
'event_return': str,
'event_return_queue': int,
'event_return_whitelist': list,
'event_return_blacklist': list,
'event_match_type': str,
'pidfile': str,
'range_server': str,
'tcp_keepalive': bool,
'tcp_keepalive_idle': float,
'tcp_keepalive_cnt': float,
'tcp_keepalive_intvl': float,
'interface': str,
'publish_port': int,
'pub_hwm': int,
'salt_event_pub_hwm': int, 'event_publisher_pub_hwm': int,
'worker_threads': int,
'ret_port': int,
'keep_jobs': int,
'master_roots': dict,
'ext_pillar': list,
'pillar_version': int,
'pillar_opts': bool,
'pillar_cache': bool,
'pillar_cache_ttl': int,
'pillar_cache_backend': str,
'pillar_source_merging_strategy': str,
'pillar_merge_lists': bool,
'top_file_merging_strategy': str,
'env_order': list,
'default_top': str,
'max_open_files': int,
'auto_accept': bool, 'autosign_timeout': int,
'master_tops': dict,
'order_masters': bool,
'job_cache': bool,
'ext_job_cache': str,
'master_job_cache': str,
'job_cache_store_endtime': bool,
'publish_session': int,
'reactor': list,
'reactor_refresh_interval': int,
'reactor_worker_threads': int,
'reactor_worker_hwm': int,
'engines': list,
'search_index_interval': int,
'nodegroups': dict,
'ssh_list_nodegroups': dict,
'key_logfile': str,
'winrepo_source_dir': str,
'modules_max_memory': int,
'grains_refresh_every': int,
'enable_lspci': bool,
'syndic_wait': int,
'jinja_lstrip_blocks': bool,
'jinja_trim_blocks': bool,
'minion_id_caching': bool,
'sign_pub_messages': bool,
'keysize': int,
'transport': str,
'gather_job_timeout': int,
'auth_timeout': int,
'auth_tries': int,
'auth_safemode': bool,
'random_reauth_delay': int,
'syndic_event_forward_timeout': float,
'syndic_max_event_process_time': float,
'syndic_jid_forward_cache_hwm': int,
'ioflo_verbose': int,
'ioflo_realtime': bool,
'ioflo_console_logdir': str,
'ping_interval': int,
'cli_summary': bool,
'max_minions': int,
'zmq_filtering': bool,
'con_cache': bool, 'rotate_aes_key': bool,
'cache_sreqs': bool,
'cmd_safe': bool,
'dummy_publisher': bool,
'rest_timeout': int,
'sudo_user': str,
'http_request_timeout': float,
'http_max_body': int,
'bootstrap_delay': int,
'minion_restart_command': list,
'pub_ret': bool,
'salt_event_pub_hwm': 2000, 'event_publisher_pub_hwm': 1000, 'event_match_type': 'startswith', 'minion_restart_command': [], 'pub_ret': True,
'pidfile': '/var/run/salt-api.pid', 'logfile': '/var/log/salt/api', 'rest_timeout': 300,
return valid_type.__name__
return {}
env_path = os.environ.get(env_var, path) if not env_path or not os.path.isfile(env_path): env_path = path if path != default_path: env_path = path
return {}
'syndic_master_port', opts.get( 'master_port', minion_defaults.get( 'master_port', DEFAULT_MINION_OPTS['master_port'] ) )
master_config_path = _absolute_path(master_config_path, config_dir)
providers_config_path = overrides['providers_config']
providers_config_path = _absolute_path(providers_config_path, config_dir)
profiles_config_path = overrides['profiles_config']
profiles_config_path = _absolute_path(profiles_config_path, config_dir)
deploy_scripts_search_path = overrides.get( 'deploy_scripts_search_path', defaults.get('deploy_scripts_search_path', 'cloud.deploy.d') ) if isinstance(deploy_scripts_search_path, string_types): deploy_scripts_search_path = [deploy_scripts_search_path]
deploy_scripts_search_path[idx] = entry continue
deploy_scripts_search_path.pop(idx)
overrides.update( deploy_scripts_search_path=tuple(deploy_scripts_search_path) )
master_config.update(overrides) overrides = master_config
'providers_config', os.path.join(salt.syspaths.CONFIG_DIR, 'cloud.providers')
'profiles_config', os.path.join(salt.syspaths.CONFIG_DIR, 'cloud.profiles')
opts = apply_cloud_config(overrides, defaults)
providers_config = opts['providers']
providers_config = cloud_providers_config(providers_config_path)
opts['providers'] = providers_config
if profiles_config is None: profiles_config = vm_profiles_config(profiles_config_path, providers_config) opts['profiles'] = profiles_config
apply_sdb(opts)
return opts
alias, driver = driver.split(':')
alias, driver = driver.split(':')
config = old_to_new(config)
if 'provider' in provider_config: provider_config['driver'] = provider_config.pop('provider')
opts['providers'][lprovider] = {} opts['providers'][lprovider][lprovider] = provider_config
vms[profile] = extended
for name, settings in six.iteritems(config.copy()): if '.' in name: log.warning( 'Please switch to the new providers configuration syntax' )
config = old_to_new(config)
for prov_name, prov_settings in six.iteritems(config.pop('providers')): config[prov_name] = prov_settings break
if 'provider' in details: details['driver'] = details.pop('provider')
providers[provider_alias][driver]['profiles'] = {}
details['driver'] = provider
details['extends'] = extends keep_looping = True
keep_looping = False for alias, entries in six.iteritems(providers.copy()): for driver, details in six.iteritems(entries):
continue
keep_looping = True continue
value = default
value = deepcopy(opts[name])
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
return provider_details
return False
required_keys = ['provider'] alias, driver = provider.split(':')
non_image_drivers = ['nova', 'virtualbox']
if driver == 'linode' and profile_key.get('clonefrom', False): non_image_drivers.append('linode') non_size_drivers.append('linode')
if driver == 'vmware' and profile_key.get('image', True): non_image_drivers.append('vmware')
for item in list(required_keys): if item in provider_key: required_keys.remove(item)
if vm_: for item in list(required_keys): if item in vm_: required_keys.remove(item)
id_cache = os.path.join(root_dir, config_dir.lstrip(os.path.sep), 'minion_id')
using_ip_for_id = False if not opts.get('id'): opts['id'], using_ip_for_id = get_id( opts, cache_minion_id=cache_minion_id)
if not using_ip_for_id and 'append_domain' in opts: opts['id'] = _append_domain(opts)
opts['open_mode'] = opts['open_mode'] is True
opts['utils_dirs'] = ( opts.get('utils_dirs') or [os.path.join(opts['extension_modules'], 'utils')] )
insert_system_path(opts, opts['utils_dirs'])
prepend_root_dirs = [ 'pki_dir', 'cachedir', 'sock_dir', 'extension_modules', 'pidfile', ]
for config_key in ('log_file', 'key_logfile'): if urlparse(opts.get(config_key, '')).scheme == '': prepend_root_dirs.append(config_key)
if 'beacons' not in opts: opts['beacons'] = {}
if 'schedule' not in opts: opts['schedule'] = {}
opts['hash_type'] = opts['hash_type'].lower()
if not using_ip_for_id and 'append_domain' in opts: opts['id'] = _append_domain(opts) if append_master: opts['id'] += '_master'
for config_key in ('log_file', 'key_logfile'): log_setting = opts.get(config_key, '') if log_setting is None: continue
opts['open_mode'] = opts['open_mode'] is True opts['auto_accept'] = opts['auto_accept'] is True opts['file_roots'] = _validate_file_roots(opts)
if isinstance(opts['file_ignore_regex'], str): ignore_regex = [opts['file_ignore_regex']] elif isinstance(opts['file_ignore_regex'], list): ignore_regex = opts['file_ignore_regex']
re.compile(regex) opts['file_ignore_regex'].append(regex)
if isinstance(opts['file_ignore_glob'], str): opts['file_ignore_glob'] = [opts['file_ignore_glob']]
opts['hash_type'] = opts['hash_type'].lower()
_validate_opts(opts) return opts
defaults = DEFAULT_MASTER_OPTS defaults.update(DEFAULT_API_OPTS)
defaults = DEFAULT_MASTER_OPTS defaults.update(DEFAULT_SPM_OPTS)
prepend_root_dirs = [ 'formula_path', 'pillar_path', 'reactor_path', 'spm_cache_dir', 'spm_build_dir' ]
for config_key in ('spm_logfile',): log_setting = opts.get(config_key, '') if log_setting is None: continue
from __future__ import absolute_import import time import logging
bitmask = 0xffffffff h = 0
salt --async '*' splay.splay pkg.install cowsay version=3.03-8.el6
from __future__ import absolute_import import json try:
from salt.executors import ModuleExecutorBase import salt.utils import salt.syspaths
from __future__ import absolute_import import sys import os.path
if 'SETUP_DIRNAME' in globals():
if __PLATFORM.startswith('win'): ROOT_DIR = r'c:\salt' else: ROOT_DIR = '/'
import time import os import codecs import logging
import salt.utils from salt.utils.odict import OrderedDict from salt._compat import string_io from salt.ext.six import string_types
ret = {}
input_data = ifile.read() if not input_data.strip(): log.error('Template is nothing but whitespace: {0}'.format(template)) return ret
render_pipe = template_shebang(template, renderers, default, blacklist, whitelist, input_data)
time.sleep(0.01) ret = render(input_data, saltenv, sls, **render_kwargs)
pass
render_pipe = check_render_pipe_str(line.strip()[2:], renderers, blacklist, whitelist)
OLD_STYLE_RENDERERS = {}
from StringIO import StringIO
import salt.utils.templates from salt.exceptions import SaltRenderError
{% from 'lib.sls' import pythonpkg with context %}
/etc/redis/redis.conf: file.managed: - source: salt://redis.conf - template: jinja - context: bind: 127.0.0.1
{% set port = 6379 %}
{% from 'lib.sls' import port with context %} port {{ port }} bind {{ bind }}
{{ salt['cmd.run']('whoami') }} {{ salt.cmd.run('whoami') }}
from __future__ import absolute_import import logging
from salt.exceptions import SaltRenderError import salt.utils.templates
import salt.ext.six as six
mod_dict[mod] = lambda: None
from StringIO import StringIO
from salt.exceptions import SaltRenderError import salt.utils.templates
try: import hjson as hjson HAS_LIBS = True except ImportError: HAS_LIBS = False
from salt.ext.six import string_types
import logging import warnings from yaml.scanner import ScannerError from yaml.parser import ParserError from yaml.constructor import ConstructorError
data = data.encode('utf-8')
from __future__ import absolute_import
import logging import warnings
import salt.utils.url from salt.serializers.yamlex import deserialize
import os
from salt.exceptions import SaltRenderError import salt.utils.templates
try: from genshi.template import MarkupTemplate from genshi.template import NewTextTemplate from genshi.template import OldTextTemplate HAS_LIBS = True except ImportError: HAS_LIBS = False
from salt.ext.six import string_types
import dson import logging
from salt.ext import six
Pkg.installed("nginx", require=Pkg("some-other-package"))
from __future__ import absolute_import import logging import os import re
from salt.ext.six import exec_ import salt.utils import salt.loader from salt.fileclient import get_file_client from salt.utils.pyobjects import Registry, StateFactory, SaltObject, Map import salt.ext.six as six
_globals = {}
_globals['include'] = Registry.include _globals['extend'] = Registry.make_extend
Map.__salt__ = __salt__ _globals['Map'] = Map
'__salt__': __salt__, '__pillar__': __pillar__, '__grains__': __grains__
if not salt_data: return _globals
client = get_file_client(__opts__)
imports = None
final_template, final_locals = process_template(template, _globals) _globals.update(final_locals)
Registry.enabled = True
exec_(final_template, _globals)
import salt.utils json = salt.utils.import_json()
from salt.ext.six import string_types
apache2.service.running() \\ .require(apache2.pkg, pkg='libapache2-mod-wsgi') \\ .watch(file='/etc/apache2/httpd.conf')
apache2.service.require(state('libapache2-mod-wsgi').pkg, pkg='apache2') \\ .watch(file='/etc/apache2/httpd.conf')
apache2.service.running()
_, mod = include('a-non-pydsl-sls', 'a-pydsl-sls')
mod = include('a-pydsl-sls')
mod.myfunc(1, 2, "three")
s.cmd.run('echo at render time', cwd='/') s.file.managed('target.txt', source='salt://source.txt')
extend(state('.start').stateconf.require(stateconf='xxx::goal'))
extend(state('.goal').stateconf.require_in(stateconf='yyy::start'))
mod.__deepcopy__ = lambda x: mod
from __future__ import absolute_import import os import re import logging from subprocess import Popen, PIPE
import salt.utils import salt.syspaths from salt.exceptions import SaltRenderError
import salt.ext.six as six
from __future__ import absolute_import
import msgpack
from salt.ext.six import string_types
from __future__ import absolute_import import logging import re import getopt import copy from os import path as ospath
import salt.utils from salt.exceptions import SaltRenderError
import salt.ext.six as six
data = copy.deepcopy(high) try: rewrite_single_shorthand_state_decl(data) rewrite_sls_includes_excludes(data, sls, saltenv)
extract_state_confs(data)
match = re.search(__opts__['stateconf_end_marker'], sls_templ) if match: process_sls_data(sls_templ[:match.start()], extract=True)
if STATE_CONF: tmplctx = STATE_CONF.copy() if tmplctx: prefix = sls + '::'
data = process_sls_data(sls_templ, tmplctx)
def add_implicit_requires(data):
if prev_state[0] is not None: try: next(nvlist(args, ['require']))[2].insert(0, dict([prev_state]))
continue
class Bunch(dict): def __getattr__(self, name): return self[name]
try: from Cheetah.Template import Template HAS_LIBS = True except ImportError: HAS_LIBS = False
from salt.ext.six import string_types
import logging try: import json5 as json HAS_JSON5 = True except ImportError: HAS_JSON5 = False
from salt.ext.six import string_types
__virtualname__ = 'json5'
from __future__ import absolute_import import os
import salt.minion import salt.loader import salt.utils
import salt.ext.six as six
import os
import salt.search import salt.ext.six as six
HAS_WHOOSH = False try: import whoosh.index import whoosh.fields import whoosh.store import whoosh.qparser HAS_WHOOSH = True except ImportError: pass
__virtualname__ = 'whoosh'
from __future__ import absolute_import import sys import types import subprocess
from salt.ext.six import binary_type, string_types, text_type from salt.ext.six.moves import cStringIO, StringIO
import xml.etree.cElementTree as ElementTree
import xml.etree.ElementTree as ElementTree
import elementtree.cElementTree as ElementTree
import elementtree.ElementTree as ElementTree
PY3 = sys.version_info[0] == 3
from __future__ import absolute_import
from __future__ import absolute_import import salt.runner
from __future__ import absolute_import import os import time import logging import traceback
import salt.state import salt.payload from salt.exceptions import SaltRenderError
from __future__ import absolute_import
import salt.client
from __future__ import absolute_import import salt.wheel
from __future__ import absolute_import import os import json
import salt.utils
from __future__ import absolute_import, division import fnmatch
import logging
import salt.utils import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_records': 'list', }
__virtualname__ = 'fmadm'
header = [field for field in output[0].lower().split(" ") if field] del output[0]
fault = OrderedDict() for field in header: fault[field] = entry[header.index(field)]
header = [field for field in output[0].lower().split(" ") if field] del output[0]
for entry in output: entry = [item for item in entry.split(" ") if item] entry = entry[0:3] + [" ".join(entry[3:])]
component = OrderedDict() for field in header: component[field] = entry[header.index(field)]
keyed_result = OrderedDict() for component in result: keyed_result[component['module']] = component del keyed_result[component['module']]['module']
if line.startswith('-'): if summary and summary_data and fault_data: result.update(_merge_data(summary_data, fault_data))
continue
if not summary: summary.append(line) continue
if summary and not summary_data: summary.append(line) summary_data = _parse_fmdump("\n".join(summary))[0] continue
result.update(_merge_data(summary_data, fault_data))
import salt.utils from salt.ext.six import string_types from salt.exceptions import CommandExecutionError import logging
__virtualname__ = 'user'
if purge: try: sid = getUserSid(name) win32profile.DeleteProfile(sid) except pywintypes.error as exc: (number, context, message) = exc
current_info = info(name) if not current_info: raise CommandExecutionError('User \'{0}\' does not exist'.format(name))
new_info = info(new_name) if new_info: raise CommandExecutionError( 'User \'{0}\' already exists'.format(new_name) )
pythoncom.CoInitialize() c = wmi.WMI(find_classes=0)
try: user = c.Win32_UserAccount(Name=name)[0] except IndexError: raise CommandExecutionError('User \'{0}\' does not exist'.format(name))
result = user.Rename(new_name)[0]
import os import re import glob import hashlib import tempfile import logging
import salt.utils
__valid_configs = { 'user': [ 'tomcat-manager.user', 'tomcat-manager:user' ], 'passwd': [ 'tomcat-manager.passwd', 'tomcat-manager:passwd' ] }
auth = _auth(url) if auth is False: ret['res'] = False ret['msg'] = 'missing username and password settings (grain/pillar)' return ret
_install_opener(auth)
ret['msg'] = _urlopen(url, timeout=timeout).read().splitlines()
ret['msg'] = _urlopen(url6, timeout=timeout).read().splitlines()
deployed = _wget('deploy', opts, url, timeout=timeout) res = '\n'.join(deployed['msg'])
if cache: __salt__['file.remove'](tfile)
import logging
import salt.utils.systemd import salt.utils.odict as odict
log = logging.getLogger(__name__)
__virtualname__ = 'service'
if service[1]: if include_enabled: enabled_services.update({service[0]: sorted(service[1].split())}) continue if include_disabled: disabled_services.update({service[0]: []})
from __future__ import absolute_import import logging import os import datetime
import salt.utils from salt.exceptions import CommandExecutionError
import yaml import salt.ext.six as six from salt.ext.six.moves import range log = logging.getLogger(__name__)
args = args and list(args) or []
if self.subcmd == 'apply': self.subcmd_args = [args[0]] del args[0]
args.extend([ 'test' ])
self.args = args
for line in output.splitlines(): if not line: continue fact, value = _format_fact(line) if not fact: continue ret[fact] = value return ret
from __future__ import absolute_import import os
import salt.utils
__func_alias__ = { 'list_': 'list' }
__virtualname__ = 'autoruns'
import salt.utils.sdb
import os import glob import logging import time
from salt.exceptions import CommandExecutionError import salt.utils
__func_alias__ = { 'reload_': 'reload' }
VALID_SERVICE_DIRS = [ '/service', '/var/service', '/etc/service', ] SERVICE_DIR = None for service_dir in VALID_SERVICE_DIRS: if os.path.exists(service_dir): SERVICE_DIR = service_dir break
AVAIL_SVR_DIRS = []
__virtualname__ = 'service'
return False
ret = ava.difference(ena)
ret = ava.union(ena)
return name in _get_svc_list(name, 'ENABLED')
return name not in _get_svc_list(name, 'ENABLED')
if not available(name): return False
alias = get_svc_alias() if name in alias: log.error('This service is aliased, enable its alias instead') return False
svc_realpath = _get_svc_path(name)[0] down_file = os.path.join(svc_realpath, 'down')
try: os.symlink(svc_realpath, _service_path(name))
log.error('Unable to create symlink {0}'.format(down_file)) if not start: os.unlink(down_file) return False
if retcode_sv != 0: os.unlink(os.path.join([_service_path(name), name])) return False return True
if not enabled(name): return False
svc_realpath = _get_svc_path(name)[0] down_file = os.path.join(svc_realpath, 'down')
from __future__ import absolute_import, print_function import json import logging
from salt.exceptions import SaltInvocationError import salt.utils.http
import salt.utils
import logging import re import socket
return False
return [x for x in cmd['stdout'].split('\n') if check_ip(x)]
return [x for x in cmd['stdout'].split('\n') if check_ip(x)]
return SPF(domain, 'TXT', nameserver)
return SPF(sections[1][9:], 'SPF', nameserver)
continue
a = A aaaa = AAAA ns = NS spf = SPF mx = MX
if salt.utils.is_proxy() and 'proxy' in __opts__: return True return (False, 'The marathon execution module cannot be loaded: this only works in proxy minions.')
import os try: import spwd HAS_SPWD = True except ImportError: HAS_SPWD = False try: import pwd except ImportError:
import salt.utils from salt.exceptions import CommandExecutionError try: import salt.utils.pycrypto HAS_CRYPT = True except ImportError: HAS_CRYPT = False
__virtualname__ = 'shadow'
return ret
import copy import logging import json import os
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
log = logging.getLogger(__name__)
__virtualname__ = 'dsc'
cmd = 'Install-Module -name "{0}" -Force'.format(name) no_ret = _pshell(cmd) return name in list_modules()
cmd = 'Uninstall-Module "{0}"'.format(name) no_ret = _pshell(cmd) return name not in list_modules()
if not os.path.exists(path): error = '"{0} not found.'.format(path) log.error(error) raise CommandExecutionError(error)
config = os.path.splitext(os.path.basename(path))[0]
cmd = '{0} '.format(path) cmd += '| Select-Object -Property FullName, Extension, Exists, ' \ '@{Name="LastWriteTime";Expression={Get-Date ($_.LastWriteTime) -Format g}}'
if ret.get('Exists'): log.info('DSC Compile Config: {0}'.format(ret)) return ret
if ret.get('Exists'): log.info('DSC Compile Config: {0}'.format(ret)) return ret
if not os.path.exists(config): error = '{0} not found.'.format(config) log.error(error) raise CommandExecutionError(error)
_pshell(cmd)
from __future__ import absolute_import import logging import re
import salt.ext.six as six
from salt.exceptions import SaltInvocationError import salt.utils
__virtualname__ = 'win_smtp_server'
for obj in objs: name = str(obj.Name).replace(prefix, '', 1) ret[name] = str(obj.LogModuleId)
settings = _normalize_server_settings(**settings)
new_settings = get_server_setting(settings=settings.keys(), server=server) failed_settings = dict()
for key in log_format_types: if str(format_id) == log_format_types[key]: return key _LOG.warning('Unable to determine log format.') return None
if not addresses: addresses = dict() _LOG.debug('Empty %s specified.', setting)
for address in addresses: formatted_addresses.append('{0}, {1}'.format(address.strip(), addresses[address].strip()))
if set(formatted_addresses) == set(current_addresses): _LOG.debug('%s already contains the provided addresses.', setting) return True
current_grant_by_default = _get_wmi_setting('IIsIPSecuritySetting', 'GrantByDefault', server)
import logging
import salt.utils.mac_utils from salt.exceptions import CommandExecutionError
import salt.utils
#pylint: disable=E0602
from __future__ import absolute_import import logging
import salt.utils.boto3 import salt.utils.compat import salt.utils
import os import stat import string import logging
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'partition'
__func_alias__ = { 'set_': 'set', 'list_': 'list', }
mode = 'partitions'
import logging import os import plistlib import re
import salt.utils import salt.utils.decorators as decorators import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'service'
true_path = os.path.realpath(file_path) if not os.path.exists(true_path): continue
with salt.utils.fopen(file_path): plist = plistlib.readPlist(true_path)
return services[name]
return service
return service
return None
import os import logging
import salt.utils
import os import sys import time
import salt.utils import salt.key
import salt.ext.six as six
__func_alias__ = { 'list_': 'list' }
pki_dir = pki_dir.replace('minion', 'master')
if transport in ('zeromq', 'tcp'): key_dirs = _check_minions_directories(pki_dir) else: key_dirs = _check_minions_directories_raetkey(pki_dir)
continue
ret['retcode'] = int(not __salt__['ps.kill_pid'](pid))
import logging
from salt.exceptions import CommandExecutionError import salt.utils
__virtualname__ = 'virt'
#pylint: disable=E0602
import logging import json import salt.ext.six as six
try: import boto import boto.sqs logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import absolute_import import os import re
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError
import salt.ext.six as six
import functools import glob import json import logging import os import shutil import subprocess import sys import time import traceback import base64 from salt.utils import vt
try: import pwd except ImportError: pass
__virtualname__ = 'cmd'
log = logging.getLogger(__name__)
if __pub_jid and python_shell is None: return True elif __opts__.get('cmd_safe', True) is False and python_shell is None: return True
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
if not cwd: cwd = os.path.expanduser('~{0}'.format('' if not runas else runas))
if not os.access(cwd, os.R_OK): cwd = '/' if salt.utils.is_windows(): cwd = os.tempnam()[:3]
cwd = str(cwd)
stack = traceback.extract_stack(limit=2)
(cmd, cwd) = _render_cmd(cmd, cwd, template, saltenv, pillarenv, pillar_override)
if python_shell: cmd = 'chcp 437 > nul & ' + cmd
if kwargs['shell'] is True: kwargs['executable'] = shell kwargs['close_fds'] = True
ret['retcode'] = 1 return ret
err = ''
pass
with salt.utils.fopen(jid_file, 'w+b') as fn_: fn_.write(serial.dumps(jid_dict))
pass
sh_ = '/bin/sh' if os.path.isfile(os.path.join(root, 'bin/bash')): sh_ = '/bin/bash'
return None
cmd = '{0} | ConvertTo-Json -Depth 32'.format(cmd)
saltenv=saltenv, pillarenv=kwargs.get('pillarenv'), pillar_override=kwargs.get('pillar'), )
import json import logging
import salt.utils from salt.exceptions import CommandExecutionError
__func_alias__ = { 'list_': 'list' }
stdout = json.loads(result['stdout']) return stdout != {}
stdout = json.loads(result['stdout']) return stdout != {}
__virtualname__ = 'system'
import salt.utils from salt.ext.six import string_types from salt.exceptions import CommandExecutionError
from __future__ import absolute_import try: import pwd except ImportError: pass
import salt.ext.six as six import salt.utils from salt.exceptions import SaltInvocationError
__virtualname__ = 'shadow'
import os import re import stat import tempfile
import salt.utils from salt.utils import which as _which from salt.exceptions import SaltInvocationError
newaliases = _which('newaliases') if newaliases is not None: __salt__['cmd.run'](newaliases)
import os import logging
import salt.utils
__salt__['file.write']('{0}-make.conf'.format(os.path.join(cdir, jname)), 'WITH_PKGNG=yes')
_check_config_exists()
if is_jail(name): return '{0} already exists'.format(name)
make_pkgng_aware(name)
if is_jail(name): return 'Created jail {0}'.format(name)
if is_jail(name): return 'Looks like there was an issue deleteing jail \ {0}'.format(name)
return 'Looks like jail {0} has not been created'.format(name)
import inspect import logging import sys
import salt.minion import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.ext.six import string_types
from __future__ import absolute_import import logging
import salt.utils
from __future__ import absolute_import import re
import salt.utils
result = __salt__['cmd.retcode']('tuned-adm off') if int(result) != 0: return False return True
import copy import os import re import logging from salt.ext import six try:
import salt.utils import salt.utils.itertools from salt.utils.decorators import which as _which
__virtualname__ = 'pkg'
if refresh: refresh_db()
if len(names) == 1: return ret[names[0]] return ret
available_version = latest_version
ret[pkg] = {'old': oldstate, 'new': state} return ret
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
if repo['uri'] not in repos: repos[repo['uri']] = [repo]
refresh_db() return ret
import logging
import salt.utils import salt.utils.s3
from __future__ import absolute_import
import datetime import json import logging import os import re import traceback import shutil import types
from salt.modules import cmdmod from salt.exceptions import CommandExecutionError, SaltInvocationError import salt.utils import salt.utils.odict
import salt.ext.six as six
__virtualname__ = 'docker'
kwargs['timeout'] = timeout
kwargs['version'] = 'auto'
kwargs['base_url'] = os.environ.get('DOCKER_HOST')
if inspect: for container in containers: container_id = container.get('Id') if container_id: inspect = _get_container_infos(container_id) container['detail'] = inspect.copy()
byte = response.read(4096) fic.write(byte)
_valid(status, comment='Kill signal \'{0}\' successfully' ' sent to the container \'{1}\''.format(signal, container), id_=container)
valid_states = [ 'Download complete', 'Already exists', ]
import salt.utils import socket
import logging import time
import logging
import salt.ext.six as six if six.PY3: import ipaddress else: import salt.ext.ipaddress as ipaddress
def long_range(start, end): while start < end: yield start start += 1
import salt.utils
log = logging.getLogger(__name__)
for item in _CREATE_OPTIONS_REQUIRED[set_type]: if item not in kwargs: return 'Error: {0} is a required argument'.format(item)
if 'family' in _CREATE_OPTIONS[set_type]: cmd = '{0} family {1}'.format(cmd, ipset_family)
cmd = '{0} add -exist {1} {2}'.format(_ipset_cmd(), set, cmd) out = __salt__['cmd.run'](cmd, python_shell=False)
return False
return False
return False
if ':' in item: key, value = item.split(':', 1) setinfo[key] = value[1:]
from __future__ import absolute_import, generators, with_statement import time import logging import salt import os import os.path
import salt.utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
__func_alias__ = { 'reload_': 'reload' }
_current_statement = None _current_option = None _current_parameter = None _current_parameter_value = None
for i in params: if _is_simple_type(i): _current_parameter = SimpleParameter(i) else: _current_parameter = TypedParameter() _parse_typed_parameter(i) _current_option.add_parameter(_current_parameter)
return full_version[:3]
version_line_index = 0 version_column_index = 1 line = lines[version_line_index].split()[version_column_index] return _format_return_data(0, stdout=line)
import collections
import os import yaml import salt.ext.six as six
import salt.pillar import salt.utils from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import CommandExecutionError
if args: return item(*args)
data = salt.utils.alias_function(items, 'data')
fetch = get
import logging
import salt.utils import salt.utils.decorators as decorators
__func_alias__ = { 'list_nictags': 'list' }
__virtualname__ = 'nictagadm'
import logging
import salt.utils
import salt.utils import logging import re
__virtualname__ = 'timezone'
return False
return 'localtime'
return False
from __future__ import absolute_import import logging import pipes
import salt.utils
from __future__ import absolute_import import os import logging import copy
try: import salt.cloud HAS_SALTCLOUD = True except ImportError: HAS_SALTCLOUD = False
import salt.ext.six as six
info = next(six.itervalues(next(six.itervalues(next(six.itervalues(info))))))
from __future__ import absolute_import import fnmatch import logging import os import pprint
import yaml import salt.ext.six as six
import salt.utils from salt.exceptions import SaltInvocationError
srcinfo.append(__salt__['cp.cache_file'](pkg_src, saltenv))
if len(ret) == 1 and not pkg_glob: try: return next(six.itervalues(ret)) except StopIteration: return '' return ret
import os import re import logging
log = logging.getLogger(__name__)
uninstall_python(python, runas=runas) return False
import time import logging
import salt.crypt import salt.payload import salt.transport import salt.utils.args from salt.exceptions import SaltReqTimeoutError
if len(returned_minions) < 1: return {} end_loop = True
from __future__ import absolute_import import re import logging
import sys import contextlib import os from salt.ext.six.moves import range from salt.ext.six.moves import map
HAS_IMPORTLIB = False
from salt.exceptions import CommandExecutionError import salt.utils import salt.modules.cmdmod
__virtualname__ = 'virt'
xapi_uri = 'httpu:///var/run/xend/xen-api.sock'
except Exception: return __salt__['cmd.run']( '{0} vcpu-pin {1} {2} {3}'.format(_get_xtool(), vm_, vcpu, cpus), python_shell=False)
return False
return 'xenstore' in __salt__['cmd.run'](__grains__['ps'])
cputime_percent = (1.0e-7 * cputime / host_cpus) / vcpus
from __future__ import absolute_import, print_function import errno import logging import os import shutil import tempfile import time import re import traceback import functools
import salt.utils from salt.exceptions import SaltInvocationError
cmd = 'rpm --import {0}'.format(pkg_pub_key_file) __salt__['cmd.run'](cmd, runas=runas, use_vt=True)
proc.sendline(phrase)
time.sleep(0.5)
urllib3_logger = logging.getLogger('urllib3') urllib3_logger.setLevel(logging.WARNING)
dev['hash'] = all_devices['hash'] log.info('Found device %s in Zenoss', device) return dev
from __future__ import absolute_import import re import logging
import salt.utils
from __future__ import absolute_import, print_function import os import copy import math import random import logging import operator import collections import json from functools import reduce
import salt.utils.compat from salt.utils.odict import OrderedDict import yaml import salt.ext.six as six
import salt.utils import salt.utils.dictupdate from salt.defaults import DEFAULT_TARGET_DELIM from salt.exceptions import SaltException
__grains__ = {}
__outputter__ = { 'items': 'nested', 'item': 'nested', 'setval': 'nested', }
_infinitedict = lambda: collections.defaultdict(_infinitedict)
_new_value_type = 'simple' if isinstance(val, dict): _new_value_type = 'complex' elif isinstance(val, list): _new_value_type = 'complex'
fetch = get
import logging
import salt.utils try: import wmi except ImportError: pass
interface = interface.split('\\') interface = ''.join(interface)
if servers is False: return False
try: if servers[index - 1] == ip: return True except IndexError: pass
if ip in servers: rm_dns(ip, interface)
interface = interface.split('\\') interface = ''.join(interface)
import os import re import logging
import salt.utils
from __future__ import absolute_import import json import logging
if not channel.startswith('#'): channel = '#{0}'.format(channel)
result = salt.utils.slack.query(function='message', api_key=api_key, method='POST', header_dict={'Content-Type': 'application/x-www-form-urlencoded'}, data=_urlencode(parameters), opts=__opts__)
database: image: mongo:3.0 command: mongod --smallfiles --quiet --logpath=/dev/null '
from __future__ import print_function from __future__ import absolute_import import re import json from salt.utils.odict import OrderedDict from salt.utils import fopen as _fopen
if not hasattr(value, 'iteritems'): self._uncomment_if_commented(key)
import re import logging
import salt.utils
import salt.ext.six as six
__virtualname__ = 'pecl'
import os
import salt.utils
__virtualname__ = 'pam'
from __future__ import absolute_import import copy import logging import os import re
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError, MinionError
import salt.ext.six as six
__virtualname__ = 'pkg'
return False
if refresh: refresh_db()
continue
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg, ver = re.split('[; ]', line, 1)[0].rsplit('-', 1)
repo = kwargs.get('repo', '') if not fromrepo and repo: fromrepo = repo
return {}
import logging import socket import time
import salt.utils import salt.utils.network import salt.utils.validate.net from salt.exceptions import ( CommandExecutionError, SaltInvocationError ) from salt.ext.six.moves import range
log = logging.getLogger(__name__)
__virtualname__ = 'ip'
import binascii import hashlib import logging import os import re import subprocess
import salt.utils import salt.utils.files import salt.utils.decorators as decorators from salt.exceptions import ( SaltInvocationError, CommandExecutionError, ) from salt.ext.six.moves import range
if enc in ['e', 'ecdsa']: return 'ecdsa-sha2-nistp256' return enc
continue
search = re.search(linere, line) if not search: continue
continue
options = opts.split(',')
if 'Key not removed' in rval: return 'Key not removed' elif 'Key removed' in rval: return 'Key removed' else: return 'Key not present'
full = _get_config_file(user, config)
if not os.path.isfile(full): return 'Authorized keys file {0} not present'.format(full)
with salt.utils.fopen(full, 'r') as _fh: for line in _fh: if line.startswith('#'): lines.append(line) continue
search = re.search(linere, line) if not search: continue
lines.append(line) continue
if pkey == key: continue
with salt.utils.fopen(full, 'w') as _fh: _fh.writelines(lines)
rcon = salt.utils.which('restorecon') if rcon: cmd = [rcon, fconfig] subprocess.call(cmd)
need_dash_t = ('CentOS-5',)
rm_known_host(user, hostname, config=config)
ssh_dir = os.path.dirname(full) if user: uinfo = __salt__['user.info'](user)
if user: os.chown(ssh_dir, uinfo['uid'], uinfo['gid']) os.chmod(ssh_dir, 0o700)
user = [user]
continue
userKeys += ['id_rsa.pub', 'id_dsa.pub', 'id_ecdsa.pub', 'id_ed25519.pub']
userKeys += ['id_rsa', 'id_dsa', 'id_ecdsa', 'id_ed25519']
keyname = key fn_ = '{0}/.ssh/{1}'.format(userinfo['home'], key)
_keys = {} for key in keys: if keys[key]: _keys[key] = keys[key] return _keys
salt.utils.files.process_read_exception(exc, key)
from __future__ import absolute_import, print_function import datetime import copy import textwrap import difflib import logging import tempfile import os import pipes import time import shutil import re import random
import salt import salt.utils.odict import salt.utils import salt.utils.dictupdate import salt.utils.network from salt.exceptions import CommandExecutionError, SaltInvocationError import salt.utils.cloud import salt.config
import salt.ext.six as six
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list', 'ls_': 'ls' }
lxc_init_interface['clone_from'] = _cloud_get(clone_from, None) if lxc_init_interface['clone_from'] is not None: break
overrides = salt.utils.clean_kwargs(**copy.deepcopy(kwargs)) profile_match = salt.utils.dictupdate.update( copy.deepcopy(profile_match), overrides ) return profile_match
nic_opts = {}
if nic and isinstance(nic, (six.string_types, dict)): nicp = get_network_profile(nic) else: nicp = {} if DEFAULT_NIC not in nicp: nicp[DEFAULT_NIC] = {}
gateway_set = True
for idx in ['lxc.cgroup.memory.limit_in_bytes']: if not default_data.get(idx): self._filter_data(idx)
with salt.utils.fopen(self.path, 'w') as fic: fic.write(content) fic.flush()
ntf = tempfile.NamedTemporaryFile() ntf.write(self.as_string()) ntf.flush() return ntf
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
for param in ('path', 'image', 'vgname', 'template'): kwargs.pop(param, None)
changes_dict = {'init': []} changes = changes_dict.get('init')
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
download_template_deps = ('dist', 'release', 'arch')
if kw_overrides_match is None: return profile_match return kw_overrides_match
if backing in ('aufs', 'dir', 'overlayfs', 'btrfs'): lvname = vgname = None
return _after_ignition_network_profile(cmd, ret, name, network_profile, path, nic_opts)
if kw_overrides_match is None: return profile_match return kw_overrides_match
return _after_ignition_network_profile(cmd, ret, name, network_profile, path, nic_opts)
return start(name, path=path)
unfreeze(name, path=path)
remove = salt.utils.alias_function(destroy, 'remove')
if not _exists: _exists = name in ls_(cache=False, path=path) return _exists
ret['size'] = size.splitlines()[-1].split()[1]
ret['changes'] = {}
if result['retcode'] in (0, 2): __context__[k] = ret = not result['retcode']
time.sleep(5)
if ret: run(name, 'touch \'{0}\''.format(SEED_MARKER), path=path, python_shell=False)
__context__['cmd.run_chroot.func'] = __salt__['cmd.run'] ret = __salt__['cmd.run_chroot'](rootfs, cmd, stdin=stdin, python_shell=python_shell, output_loglevel=output_loglevel, ignore_retcode=ignore_retcode)
return None
if kw_overrides_match is _marker: return profile_match return kw_overrides_match
import logging
from salt.ext import six
try: import win32com.client import pythoncom
import salt.utils
search_string = '' search_params = []
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session')
wua_searcher = wua_session.CreateUpdateSearcher()
if categories is None: category_match = True else: for category in update.Categories: if category.Name in categories: category_match = True
if severities is None: severity_match = True else: if update.MsrcSeverity in severities: severity_match = True
results['Total'] += 1
if not update.IsDownloaded and not update.IsInstalled: results['Available'] += 1
if update.IsDownloaded and not update.IsInstalled: results['Downloaded'] += 1
if update.IsInstalled: results['Installed'] += 1
for category in update.Categories: if category.Name in results['Categories']: results['Categories'][category.Name] += 1 else: results['Categories'][category.Name] = 1
if update.MsrcSeverity: if update.MsrcSeverity in results['Severity']: results['Severity'][update.MsrcSeverity] += 1 else: results['Severity'][update.MsrcSeverity] = 1
results[guid]['Severity'] = str(update.MsrcSeverity)
results[guid]['NeedsReboot'] = str(update.RebootRequired)
rb = {0: 'Never Requires Reboot', 1: 'Always Requires Reboot', 2: 'Can Require Reboot'} results[guid]['RebootBehavior'] = rb[update.InstallationBehavior.RebootBehavior]
results[guid]['Categories'] = [] for category in update.Categories: results[guid]['Categories'].append(category.Name)
salt '*' win_wua.list_update 12345678-abcd-1234-abcd-1234567890ab
salt '*' win_wua.list_update KB3030298
salt '*' win_wua.list_update 'Microsoft Camera Codec Pack'
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session')
wua_searcher = wua_session.CreateUpdateSearcher()
wua_found = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
search_string = 'UpdateID=\'{0}\''.format(name)
if found_using_guid: for update in wua_search_result.Updates: wua_found.Add(update) else: for update in wua_search_result.Updates: if name in update.Title: wua_found.Add(update)
salt '*' win_wua.list_updates
salt '*' win_wua.list_updates categories=['Critical Updates','Drivers']
salt '*' win_wua.list_updates categories=['Security Updates'] severities=['Critical']
salt '*' win_wua.list_updates severities=['Critical']
salt '*' win_wua.list_updates summary=True
salt '*' win_wua.list_updates categories=['Feature Packs','Windows 8.1'] summary=True
updates = _wua_search(software_updates=software, driver_updates=drivers, skip_installed=not installed)
updates = _filter_list_by_category(updates=updates, categories=categories)
if not updates: return 'No updates found. Check software and drivers parameters. One must be true.'
if guid is None: return "No GUID Specified"
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session') wua_session.ClientApplicationID = 'Salt: Install Update'
wua_searcher = wua_session.CreateUpdateSearcher() wua_download_list = win32com.client.Dispatch('Microsoft.Update.UpdateColl') wua_downloader = wua_session.CreateUpdateDownloader()
if wua_download_list.Count == 0: log.debug('No updates to download') ret['Success'] = False ret['Message'] = 'No updates to download' return ret
log.debug('Downloading...') wua_downloader.Updates = wua_download_list
if guid is None: return 'No GUID Specified'
pythoncom.CoInitialize()
wua_session = win32com.client.Dispatch('Microsoft.Update.Session') wua_session.ClientApplicationID = 'Salt: Install Update'
if wua_download_list.Count == 0: log.debug('No updates to download') else: log.debug('Downloading...') wua_downloader.Updates = wua_download_list
for update in wua_search_result.Updates: if update.IsDownloaded: log.debug(u'To be installed: {0}'.format(update.Title)) wua_install_list.Add(update)
log.debug('No updates to install') ret['Success'] = False ret['Message'] = 'No Updates to install' return ret
try: result = wua_installer.Install()
ret['Success'] = False ret['Result'] = format(error)
pythoncom.CoInitialize()
obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')
obj_au_settings = obj_au.Settings
obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')
obj_sm.ClientApplicationID = "My App"
try: obj_sm.AddService2('7971f918-a847-4430-9279-4a52d1efe18d', 7, '') ret['msupdate'] = msupdate except Exception as error:
ret['Comment'] = "Failed with failure code: {0}".format(exc[5]) ret['Success'] = False
if _get_msupdate_status(): try: obj_sm.RemoveService('7971f918-a847-4430-9279-4a52d1efe18d') ret['msupdate'] = msupdate except Exception as error:
ret['Comment'] = "Failed with failure code: {0}".format(exc[5]) ret['Success'] = False
pythoncom.CoInitialize()
obj_au = win32com.client.Dispatch('Microsoft.Update.AutoUpdate')
obj_au_settings = obj_au.Settings
obj_sm = win32com.client.Dispatch('Microsoft.Update.ServiceManager')
col_services = obj_sm.Services
for service in col_services: if service.name == 'Microsoft Update': return True
pythoncom.CoInitialize()
obj_sys = win32com.client.Dispatch('Microsoft.Update.SystemInfo')
import logging import re
from salt.utils.decorators import depends import salt.utils
from __future__ import absolute_import import logging
import salt.utils
import os
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError
__virtualname__ = 'grub'
return '/boot/grub/menu.lst'
from __future__ import absolute_import, generators, print_function, with_statement import re import logging
import salt.utils
url += '?auto' try: response = _urlopen(url, timeout=timeout).read().splitlines() except URLError: return 'error'
for line in response: splt = line.split(':', 1) splt[0] = splt[0].strip() splt[1] = splt[1].strip()
return ret
from __future__ import absolute_import import logging
import salt.utils
log = logging.getLogger(__name__)
mnt_image = salt.utils.alias_function(mount_image, 'mnt_image')
import logging
from salt.ext.six import string_types from salt.exceptions import get_error_message as _get_error_message
try: import pymongo HAS_MONGODB = True except ImportError: HAS_MONGODB = False
from __future__ import absolute_import import os import re import logging
import salt.utils from salt.utils import which as _which from salt.exceptions import CommandNotFoundError, CommandExecutionError
import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'mount'
if salt.utils.is_windows(): return (False, 'The mount module cannot be loaded: not a POSIX-like system.') return True
compatibility_keys = ('device', 'name', 'fstype', 'opts', 'dump', 'pass')
return True
if isinstance(opts, list): opts = ','.join(opts)
entry = _fstab_entry(**entry_args) try: criteria = entry.pick(match_on)
if not os.path.isfile(config): raise CommandExecutionError('Bad config file "{0}"'.format(config))
ret = 'present' if entry.match(line): lines.append(line) else: ret = 'change' lines.append(str(entry))
if ret is None: lines.append(str(entry)) ret = 'new'
ofile.writelines(lines)
__salt__['cmd.run']('automount -cv') return True
if isinstance(opts, list): opts = ','.join(opts) lines = [] change = False present = False automaster_file = "/etc/auto_master"
lines.append(line) continue
lines.append(line) continue
lines.append(line) continue
ofile.writelines(lines)
return 'present'
newline = ( '{0}\t{1}\t{2}\n'.format( name, type_opts, device_fmt)
ofile.writelines(lines)
continue
continue
continue
if 'defaults' in opts and __grains__['os'] in ['MacOS', 'Darwin']: opts = None
if not cmd_path: return False elif not _which('ldd'): raise CommandNotFoundError('ldd')
from __future__ import absolute_import import logging import salt.utils
__virtualname__ = 'service'
try: import netaddr HAS_NETADDR = True except ImportError as e: HAS_NETADDR = False
from __future__ import absolute_import import sqlite3 import os
self.init_queries.append("CREATE TABLE inspector_ignored (path CHAR(4096))") self.init_queries.append("CREATE TABLE inspector_allowed (path CHAR(4096))")
from __future__ import absolute_import, print_function import os import sys from subprocess import Popen, PIPE, STDOUT
from salt.modules.inspectlib.dbhandle import DBHandle from salt.modules.inspectlib.exceptions import (InspectorSnapshotException) import salt.utils from salt.utils import fsutils from salt.utils import reinit_crypto
data = dict()
if not data[pkg_name]: data.pop(pkg_name)
try: os.kill(int(open(pidfile).read().strip()), 0) sys.exit(1) except Exception as ex: pass
try: if os.fork() > 0: reinit_crypto() sys.exit(0) else: reinit_crypto() except OSError as ex: sys.exit(1)
from __future__ import absolute_import import os import time import logging
import salt.utils.network from salt.modules.inspectlib.dbhandle import DBHandle from salt.modules.inspectlib.exceptions import (InspectorQueryException, SIException)
return users
if os_family == 'suse': PATTERNS = 'pkg.list_installed_patterns' elif os_family == 'redhat': PATTERNS = 'pkg.group_list' else: PATTERNS = None
if 'packages' not in excludes: data['packages'] = __salt__['pkg.list_pkgs']()
if 'repositories' not in excludes: repos = __salt__['pkg.list_repos']() if repos: data['repositories'] = repos
import salt.utils
(status, ring, pending, node) = line.split()
import salt.utils import salt.utils.decorators as decorators from salt.ext import six from salt.exceptions import CommandExecutionError from salt.utils import locales
__virtualname__ = 'user'
break
break
pass
return True
if __grains__['os_family'] not in ('Debian',): return False
log.debug( 'While the userdel exited with code 12, this is a known bug on ' 'debian based distributions. See http://goo.gl/HH3FzT' ) return True
gecos_field = data.pw_gecos.split(',', 3) while len(gecos_field) < 4: gecos_field.append('')
import os import logging
import salt.utils
__func_alias__ = { 'set_': 'set' }
if value == 'True': new_line = key elif value == 'False': new_line = '' else: new_line = '{0} {1}'.format(key, value)
import hashlib import random
import salt.utils.pycrypto from salt.exceptions import SaltInvocationError
__virtualname__ = 'random'
#pylint: disable=E0602
from __future__ import absolute_import import logging
import salt.utils.boto3 import salt.utils.compat import salt.utils
if IdentityPoolName is not None and IdentityPoolName != request_params.get('IdentityPoolName'): request_params['IdentityPoolName'] = IdentityPoolName
current_val = request_params.pop('DeveloperProviderName', None) if current_val is None and DeveloperProviderName is not None: request_params['DeveloperProviderName'] = DeveloperProviderName
from __future__ import absolute_import import logging
__virtualname__ = 'group'
else: retcode = 0
from __future__ import absolute_import import difflib import os import yaml
ret = {} ret['result'] = False ret['comment'] = 'Event module not available. Beacon add failed.' return ret
ret['comment'] = 'Event module not available. Beacon add failed.'
ret['comment'] = 'Event module not available. Beacon add failed.'
ret['comment'] = 'Event module not available. Beacon add failed.'
ret['comment'] = 'Event module not available. Beacons enable job failed.'
ret['comment'] = 'Event module not available. Beacons enable job failed.'
ret['comment'] = 'Event module not available. Beacon enable job failed.'
ret['comment'] = 'Event module not available. Beacon disable job failed.'
from __future__ import absolute_import import os import re
import salt.utils from salt._compat import subprocess
__virtualname__ = 'jail'
continue
import salt.utils
if module not in get_modules(): log.error('Module {0} not available'.format(module)) return False
if self._session_expired: raise ForceRetryError("Retry on session loss at top")
if self.cancelled: raise CancelledError("Semaphore cancelled")
children = self.client.get_children(self.path, self._watch_lease_change)
if len(children) < self.max_leases: self.client.create(self.create_path, self.data, ephemeral=self.ephemeral_lease)
if self.client.exists(self.create_path): self.is_acquired = True else: self.is_acquired = False
return self.is_acquired
if force: SEMAPHORE_MAP[path].assured_path = True
if zk_hosts is not None and path not in SEMAPHORE_MAP: zk = _get_zk_conn(zk_hosts) SEMAPHORE_MAP[path] = _Semaphore(zk, path, identifier, max_leases=max_concurrency, ephemeral_lease=ephemeral_lease)
import logging
from salt.exceptions import SaltInvocationError
try: from celery import Celery from celery.exceptions import TimeoutError HAS_CELERY = True except ImportError: HAS_CELERY = False
import glob import shutil import logging import os
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import string_types
__virtualname__ = 'virtualenv'
if clear is True: cmd.append('--clear') if system_site_packages is True: cmd.append('--system-site-packages')
cmd.append(path)
ret = __salt__['cmd.run_all'](cmd, runas=user, python_shell=False) if ret['retcode'] != 0: return ret
if (pip or distribute) and not os.path.exists(venv_setuptools): _install_script( 'https://bitbucket.org/pypa/setuptools/raw/default/ez_setup.py', path, venv_python, user, saltenv=saltenv, use_vt=use_vt )
for fpath in glob.glob(os.path.join(path, 'distribute-*.tar.gz*')): os.unlink(fpath)
return ret
from __future__ import absolute_import import salt.utils from datetime import datetime import logging import time
try: import pythoncom import win32com.client HAS_DEPENDENCIES = True except ImportError: HAS_DEPENDENCIES = False from salt.ext.six.moves import range
__virtualname__ = 'task'
TASK_ACTION_EXEC = 0 TASK_ACTION_COM_HANDLER = 5 TASK_ACTION_SEND_EMAIL = 6 TASK_ACTION_SHOW_MESSAGE = 7
TASK_COMPATIBILITY_AT = 0 TASK_COMPATIBILITY_V1 = 1 TASK_COMPATIBILITY_V2 = 2 TASK_COMPATIBILITY_V3 = 3
TASK_VALIDATE_ONLY = 0x1 TASK_CREATE = 0x2 TASK_UPDATE = 0x4 TASK_CREATE_OR_UPDATE = 0x6 TASK_DISABLE = 0x8 TASK_DONT_ADD_PRINCIPAL_ACE = 0x10 TASK_IGNORE_REGISTRATION_TRIGGERS = 0x20
TASK_INSTANCES_PARALLEL = 0 TASK_INSTANCES_QUEUE = 1 TASK_INSTANCES_IGNORE_NEW = 2 TASK_INSTANCES_STOP_EXISTING = 3
TASK_LOGON_NONE = 0 TASK_LOGON_PASSWORD = 1 TASK_LOGON_S4U = 2 TASK_LOGON_INTERACTIVE_TOKEN = 3 TASK_LOGON_GROUP = 4 TASK_LOGON_SERVICE_ACCOUNT = 5 TASK_LOGON_INTERACTIVE_TOKEN_OR_PASSWORD = 6
TASK_RUNLEVEL_LUA = 0 TASK_RUNLEVEL_HIGHEST = 1
TASK_STATE_UNKNOWN = 0 TASK_STATE_DISABLED = 1 TASK_STATE_QUEUED = 2 TASK_STATE_READY = 3 TASK_STATE_RUNNING = 4
TASK_TRIGGER_EVENT = 0 TASK_TRIGGER_TIME = 1 TASK_TRIGGER_DAILY = 2 TASK_TRIGGER_WEEKLY = 3 TASK_TRIGGER_MONTHLY = 4 TASK_TRIGGER_MONTHLYDOW = 5 TASK_TRIGGER_IDLE = 6 TASK_TRIGGER_REGISTRATION = 7 TASK_TRIGGER_BOOT = 8 TASK_TRIGGER_LOGON = 9 TASK_TRIGGER_SESSION_STATE_CHANGE = 11
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) tasks = task_folder.GetTasks(0)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) folders = task_folder.GetFolders(0)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition triggers = task_definition.Triggers
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition actions = task_definition.Actions
if name in list_tasks(location) and not force: return '{0} already exists'.format(name)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_definition = task_service.NewTask(0)
edit_task(task_definition=task_definition, user_name=user_name, password=password)
add_action(task_definition=task_definition, **kwargs)
add_trigger(task_definition=task_definition, **kwargs)
task_folder = task_service.GetFolder(location)
_save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=password, logon_type=task_definition.Principal.LogonType)
if name in list_tasks(location): return True else: return False
if name in list_tasks(location): return '{0} already exists'.format(name)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
if xml_path: xml_text = xml_path
task_folder = task_service.GetFolder(location)
try: task_folder.RegisterTask(name, xml_text, TASK_CREATE, user_name, password, logon_type)
if name in list_tasks(location): return True else: return False
if name in list_folders(location): return '{0} already exists'.format(name)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_folder.CreateFolder(name)
if name in list_folders(location): return True else: return False
save_definition = False if kwargs.get('task_definition', False): task_definition = kwargs.get('task_definition') else: save_definition = True
if not name: return 'Required parameter "name" not passed'
if name in list_tasks(location):
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_definition = task_folder.GetTask(name).Definition
return '{0} not found'.format(name)
if save_definition: task_definition.RegistrationInfo.Author = 'Salt Minion' task_definition.RegistrationInfo.Source = "Salt Minion Daemon"
if enabled is not None: task_definition.Settings.Enabled = enabled if hidden is not None: task_definition.Settings.Hidden = hidden
if run_if_idle is not None: task_definition.Settings.RunOnlyIfIdle = run_if_idle
if ac_only is not None: task_definition.Settings.DisallowStartIfOnBatteries = ac_only if stop_if_on_batteries is not None: task_definition.Settings.StopIfGoingOnBatteries = stop_if_on_batteries if wake_to_run is not None: task_definition.Settings.WakeToRun = wake_to_run
if save_definition: return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=user_name, password=password, logon_type=task_definition.Principal.LogonType)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
if name not in list_tasks(location): return True else: return False
if name not in list_folders(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_folder.DeleteFolder(name, 0)
if name not in list_folders(location): return True else: return False
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if task.State == TASK_STATE_RUNNING: return 'Task already running'
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task = task_folder.GetTask(name)
if not name: return 'Required parameter "name" not passed'
if name in list_tasks(location):
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_definition = task_folder.GetTask(name).Definition
return '{0} not found'.format(name)
if kwargs.get('server', False): task_action.Server = kwargs.get('server') else: return 'Required parameter "server" not found'
if save_definition: return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition actions = task_definition.Actions
return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
if start_date: date_format = _get_date_time_format(start_date) if date_format: dt_obj = datetime.strptime(start_date, date_format) else: return 'Invalid start_date' else: dt_obj = datetime.now()
if not name: return 'Required parameter "name" not passed'
if name in list_tasks(location):
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location)
task_definition = task_folder.GetTask(name).Definition
return '{0} not found'.format(name)
trigger = task_definition.Triggers.Create(trigger_types[trigger_type])
elif trigger_types[trigger_type] == TASK_TRIGGER_DAILY: trigger.Id = 'Daily_ID1' trigger.DaysInterval = kwargs.get('days_interval', 1)
elif trigger_types[trigger_type] == TASK_TRIGGER_IDLE: trigger.Id = 'OnIdle_ID1'
elif trigger_types[trigger_type] == TASK_TRIGGER_REGISTRATION: trigger.Id = 'OnTaskCreation_ID1'
elif trigger_types[trigger_type] == TASK_TRIGGER_BOOT: trigger.Id = 'OnBoot_ID1'
elif trigger_types[trigger_type] == TASK_TRIGGER_LOGON: trigger.Id = 'OnLogon_ID1'
if save_definition: return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
if name not in list_tasks(location): return '{0} not found in {1}'.format(name, location)
pythoncom.CoInitialize() task_service = win32com.client.Dispatch("Schedule.Service") task_service.Connect()
task_folder = task_service.GetFolder(location) task_definition = task_folder.GetTask(name).Definition triggers = task_definition.Triggers
return _save_task_definition(name=name, task_folder=task_folder, task_definition=task_definition, user_name=task_definition.Principal.UserID, password=None, logon_type=task_definition.Principal.LogonType)
#pylint: disable=E0602
import logging
exists = conn.describe_stacks(name) log.debug('Stack {0} exists.'.format(name)) return True
return conn.validate_template(template_body, template_url)
proxyfile = '/etc/salt/proxy' status_file, msg_new, msg_old = _proxy_conf_file(proxyfile, test) changes_new.extend(msg_new) changes_old.extend(msg_old) status_proc = False
import copy import os import logging
import salt.utils from salt.exceptions import CommandExecutionError, MinionError
__virtualname__ = 'pkg'
fd_, adminfile = salt.utils.mkstemp(prefix="salt-", close_fd=False)
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
salt 'global_zone' pkg.install sources='[{"SMClgcc346": "/var/spool/pkg/gcc-3.4.6-sol10-sparc-local.pkg"}]' current_zone_only=True
salt '*' pkg.install sources='[{"<pkg name>": "salt://pkgs/<pkg filename>"}]' instance="overwrite"
if kwargs.get('current_zone_only') == 'True': cmd_prefix += '-G '
out = __salt__['cmd.run_all'](cmd, output_loglevel='trace', python_shell=False)
if 'admin_source' not in kwargs: os.unlink(adminfile)
fd_, adminfile = salt.utils.mkstemp(prefix="salt-", close_fd=False)
cmd = ['/usr/sbin/pkgrm', '-n', '-a', adminfile] + targets out = __salt__['cmd.run_all'](cmd, python_shell=False, output_loglevel='trace')
if 'admin_source' not in kwargs: os.unlink(adminfile)
import os import random
import salt.utils from salt.ext.six.moves import range
if old == '*': return True
line = line[10:] commented_cron_job = True
if SALT_CRON_IDENTIFIER in comment_line: parts = comment_line.split(SALT_CRON_IDENTIFIER) comment_line = parts[0].rstrip() if len(parts[1]) > 1: identifier = parts[1][1:]
ls = salt.utils.alias_function(list_tab, 'ls')
return comdat['stderr']
daymonth_max = 28
return comdat['stderr']
return comdat['stderr']
rm_ = ind
return comdat['stderr']
return comdat['stderr']
return comdat['stderr']
import logging
import salt.utils
import logging import os import os.path import re
import salt.utils from salt.exceptions import CommandExecutionError
__func_alias__ = { 'reload_': 'reload' }
BINS = frozenset(('svc', 'supervise', 'svok')) return all(salt.utils.which(b) for b in BINS)
from __future__ import absolute_import import json import logging import os import yaml
continue
if ':' in key: namespace, key = key.split(':', 1) else: namespace, key = key, None
defaults = _load(namespace)
if key: return salt.utils.traverse_dict_and_list(defaults, key, default) else: return defaults
import os
from __future__ import absolute_import import salt.utils import time import logging from salt.exceptions import CommandExecutionError
try: import win32security import win32service import win32serviceutil import pywintypes HAS_WIN32_MODS = True except ImportError: HAS_WIN32_MODS = False
__virtualname__ = 'service'
if name in get_all(): raise CommandExecutionError('Service Already Exists: {0}'.format(name))
bin_path = bin_path.strip('"') if exe_args is not None: bin_path = '{0} {1}'.format(bin_path, exe_args)
handle_scm = win32service.OpenSCManager( None, None, win32service.SC_MANAGER_ALL_ACCESS)
handle_svc = win32service.CreateService(handle_scm, name, display_name, win32service.SERVICE_ALL_ACCESS, service_type, start_type, error_control, bin_path, load_order_group, 0, dependencies, account_name, account_password)
if start_type == 2: win32service.ChangeServiceConfig2( handle_svc, win32service.SERVICE_CONFIG_DELAYED_AUTO_START_INFO, start_delayed)
import salt.utils
from __future__ import absolute_import import logging
from salt.exceptions import SaltInvocationError import salt.utils
__virtualname__ = 'win_snmp'
for service, bitmask in sorted_types: if current_bitmask > 0: remaining_bitmask = current_bitmask - bitmask
services = sorted(set(services))
vdata = sum(_SERVICE_TYPES[service] for service in services)
new_settings = get_agent_settings() failed_settings = dict()
for current_value in current_values: permissions = str() for permission_name in _PERMISSION_TYPES: if current_value['vdata'] == _PERMISSION_TYPES[permission_name]: permissions = permission_name break ret[current_value['vname']] = permissions
for vname in values: if vname not in current_communities: __salt__['reg.set_value'](_HKEY, _COMMUNITIES_KEY, vname, values[vname], 'REG_DWORD')
new_communities = get_community_names() failed_communities = dict()
from __future__ import absolute_import import copy import re import os import logging
import salt.config import salt.utils try: import salt.utils.cloud HAS_CLOUD = True except ImportError: HAS_CLOUD = False
import salt.ext.six as six
mode = str(mode)
return '0{0}'.format(ret)
from __future__ import absolute_import from distutils.version import LooseVersion import re import logging
import salt.utils from salt.exceptions import CommandExecutionError
import re import logging
import salt.utils
from __future__ import absolute_import try: import iptc IPTC_IMPORTED = True except ImportError: IPTC_IMPORTED = False
from salt.exceptions import CommandExecutionError import salt.utils
from __future__ import absolute_import, print_function import yaml import logging
import salt.utils.http import salt.ext.six as six from salt._compat import ElementTree as ET
from __future__ import absolute_import import os
import salt.utils import salt.utils.odict as odict
import salt.ext.six as six
return dict(_list_hosts())
for addr in hosts: if host in hosts[addr]: return addr return ''
if not alias.strip(): line_to_add = ''
if lines and not lines[-1].endswith(os.linesep): lines[-1] += os.linesep line = line_to_add lines.append(line)
lines[ind] = ''
lines[ind] = newline + os.linesep
ofile.write(line.strip() + os.linesep)
import salt.loader
import logging
import salt.utils import salt.utils.decorators as decorators
__func_alias__ = { 'list_': 'list', 'get_': 'get', 'put_': 'put', 'delete_': 'delete', }
__virtualname__ = 'mdata'
__virtualname__ = 'shadow'
import logging
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import integer_types
try: import memcache HAS_MEMCACHE = True except ImportError: HAS_MEMCACHE = False
__func_alias__ = { 'set_': 'set' }
import logging
from salt.ext.six.moves import shlex_quote as _cmd_quote
import salt.utils.validate.net from salt.exceptions import CommandExecutionError
__virtualname__ = 'bluetooth'
import logging log = logging.getLogger(__file__)
from napalm import get_network_driver HAS_NAPALM = True
from __future__ import absolute_import try: import grp except ImportError: pass
import salt.utils import salt.utils.itertools from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.modules.mac_user import _dscl, _flush_dscl_cache
__virtualname__ = 'group'
gid_list = _list_gids() if str(gid) in gid_list: raise CommandExecutionError( 'gid \'{0}\' already exists'.format(gid) )
else: retcode = 0
grinfo = next(iter(x for x in grp.getgrall() if x.gr_name == name))
import re import logging
import salt.utils.itertools from salt.exceptions import CommandExecutionError
import salt.utils
if gem_bin is None: if __salt__['rvm.is_installed'](runas=runas): return __salt__['rvm.do'](ruby, cmdline, runas=runas)
import logging import os.path import re import tempfile
import salt.utils from salt.exceptions import CommandExecutionError, CommandNotFoundError
try: choc_path = _find_chocolatey(__context__, __salt__) except CommandExecutionError: choc_path = None if choc_path and not force: return 'Chocolatey found at {0}'.format(choc_path)
net4_url = 'http://download.microsoft.com/download/1/B/E/1BE39E79-7E39-46A3-96FF-047F95396215/dotNetFx40_Full_setup.exe'
ps_path = 'C:\\Windows\\SYSTEM32\\WindowsPowerShell\\v1.0\\powershell.exe'
import os import stat
import salt.utils from salt.ext.six import string_types
if '.' in val: val = float(val) else: val = int(val) data[plugin][key] = val
from __future__ import absolute_import
from salt.ext.six import string_types, text_type from salt.ext.six.moves import range from salt.ext.six.moves.urllib.request import urlopen as _urlopen
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'buildout'
return _merge_statuses([boot_ret, buildout_ret])
import salt.utils import logging import salt.utils.mac_utils from salt.exceptions import CommandExecutionError
if not salt.utils.is_darwin(): return False, 'Not Darwin'
cmd = "dscl . -create /Users/{0} Password '*'".format(name) salt.utils.mac_utils.execute_return_success(cmd)
import os import logging
__func_alias__ = { 'set_': 'set' }
#pylint: disable=E0602
import logging import json import yaml
def ordered_dict_presenter(dumper, data): return dumper.represent_dict(list(data.items()))
from __future__ import absolute_import import logging
HAS_LIBS = False try: from servicenow_rest.api import Client
from __future__ import absolute_import import os
import salt.utils import salt.exceptions
import salt.ext.six as six
__virtualname__ = 'django'
from __future__ import absolute_import import logging import yaml import urllib
import salt.ext.six as six HAS_LIBS = False try: import splunklib.client import requests HAS_LIBS = True except ImportError: pass
from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_': 'list' }
try: search = client.saved_searches[name] except KeyError: pass return search
def ordered_dict_presenter(dumper, data): return dumper.represent_dict(six.iteritems(data)) yaml.add_representer( OrderedDict, ordered_dict_presenter, Dumper=yaml.dumper.SafeDumper)
import logging import os.path import os
import jinja2 import jinja2.exceptions
import salt.utils import salt.utils.templates import salt.utils.validate.net import salt.ext.six as six
log = logging.getLogger(__name__)
JINJA = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, 'rh_ip') ) )
__virtualname__ = 'ip'
retain_settings = opts.get('retain_settings', False) result = current if retain_settings else {}
lines = contents.read().splitlines() try: lines.remove('') except ValueError: pass return lines
if iface_type not in ['slave']: return __salt__['cmd.run']('ifdown {0}'.format(iface)) return None
if iface_type not in ['slave']: return __salt__['cmd.run']('ifup {0}'.format(iface)) return None
current_network_settings = _parse_rh_config(_RH_NETWORK_FILE)
_write_file_network(network, _RH_NETWORK_FILE)
from __future__ import absolute_import import time import logging import re import sys import shlex
import salt.utils
import salt.ext.six as six
import MySQLdb import MySQLdb.cursors import MySQLdb.converters from MySQLdb.constants import FIELD_TYPE, FLAG HAS_MYSQLDB = True
import pymysql pymysql.install_as_MySQLdb() import MySQLdb import MySQLdb.cursors import MySQLdb.converters from MySQLdb.constants import FIELD_TYPE, FLAG HAS_MYSQLDB = True
HAS_MYSQLDB = False
qry = 'CHECK TABLE {0}.{1}'.format(s_name, s_table) _execute(cur, qry) results = cur.fetchall() log.debug(results) return results
qry = 'REPAIR TABLE {0}.{1}'.format(s_name, s_table) _execute(cur, qry) results = cur.fetchall() log.debug(results) return results
qry = 'OPTIMIZE TABLE {0}.{1}'.format(s_name, s_table) _execute(cur, qry) results = cur.fetchall() log.debug(results) return results
if 'connection_default_file' in kwargs: get_opts = False else: get_opts = True
database += token try: if exploded_grant[position_tracker + 1] == '.': phrase = 'tables' except IndexError: break
if exploded_grant[position_tracker + 1] == '@': phrase = 'pre-host'
return -1
return -2
if db_exists(name, **connection_args): log.info('DB \'{0}\' already exists'.format(name)) return False
if not db_exists(name, **connection_args): log.info('DB \'{0}\' does not exist'.format(name)) return False
def __grant_normalize(grant): if grant == 'ALL': grant = 'ALL PRIVILEGES'
for opt in ssl_option: key = next(six.iterkeys(opt))
new_ssl_option.append("{0} '{1}'".format(normal_key, opt[key].replace("'", '')))
dbc = quote_identifier(dbc, for_grants=(table is '*'))
s_database = quote_identifier(dbc, for_grants=(table is '*'))
s_database = dbc
if len(rtnv) == 0: rtnv.append([])
if len(rtnv) == 0: rtnv.append([])
from __future__ import absolute_import import os import logging
import salt.utils
python_shell = False if '*.' in cmd: python_shell = True
temp_dir = __salt__['temp.dir'](prefix='pkg-')
cmd = 'xar -x -f {0} {1}'.format(pkg, ' '.join(files)) __salt__['cmd.run'](cmd, cwd=temp_dir, output_loglevel='quiet')
for f in files: i = _get_pkg_id_from_pkginfo(os.path.join(temp_dir, f)) if len(i): package_ids.extend(i)
__salt__['file.remove'](temp_dir)
cmd = 'find {0} -name *.pkg'.format(base_path) out = __salt__['cmd.run'](cmd, python_shell=True)
python_shell = False if '*.' in cmd: python_shell = True
from salt import utils
import logging log = logging.getLogger(__name__)
import salt.utils
import json
import salt.utils
from __future__ import absolute_import import logging
import salt.utils import salt.utils.locales
self.categories = categories
self.foundCategories = []
self.download_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.install_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.win_downloader = self.update_session.CreateUpdateDownloader() self.win_downloader.Updates = self.download_collection
self.win_installer = self.update_session.CreateUpdateInstaller() self.win_installer.Updates = self.install_collection
self.download_results = None
self.install_results = None
self.search_results = None
for update in self.search_results.Updates: if update.InstallationBehavior.CanRequestUserInput: log.debug(U'Skipped update {0} - requests user input'.format(update.title)) continue
if self.skipDownloaded and update.IsDownloaded: log.debug(u'Skipped update {0} - already downloaded'.format(update.title)) continue
self.foundCategories = _gather_update_categories(self.download_collection) log.debug('found categories: {0}'.format(str(self.foundCategories))) return True
log.debug('generated search string: {0}'.format(search_string)) return self.Search(search_string)
log.debug('blugger has {0} updates in it'.format(self.install_collection.Count)) if self.install_collection.Count == 0: return {}
updates.append('{0}: {1}'.format( self.install_results.GetUpdateResult(i).ResultCode, self.install_collection.Item(i).Title))
for i, update in enumerate(updates): results['update {0}'.format(i)] = update
return [update['Title'] for update in updates_verbose]
comment += 'Search was done without error.\n'
salt '*' win_update.list_updates
salt '*' win_update.list_updates fields="['Title', 'Description']"
salt '*' win_update.list_updates categories="['Critical Updates']" verbose=True
salt '*' win_update.download_updates
salt '*' win_update.download_updates categories="['Critical Updates']"
comment, passed, retries = _search(quidditch, retries) if not passed: return (comment, str(passed))
comment, passed, retries = _download(quidditch, retries) if not passed: return (comment, str(passed))
salt '*' win_update.install_updates
salt '*' win_update.install_updates categories="['Critical Updates']"
comment, passed, retries = _search(quidditch, retries) if not passed: return (comment, str(passed))
comment, passed, retries = _download(quidditch, retries) if not passed: return (comment, str(passed))
comment, passed, retries = _install(quidditch, retries) if not passed: return (comment, str(passed))
from __future__ import absolute_import import logging
import salt.utils import salt.ext.six as six import salt.utils.event from salt._compat import subprocess from salt.utils.network import host_to_ip as _host_to_ip
__virtualname__ = 'status'
cmd = list2cmdline(['wmic', 'cpu']) info = __salt__['cmd.run'](cmd).split('\r\n')
column = info[0].index('LoadPercentage')
end = info[1].index(' ', column+1)
return int(info[1][column:end])
cmd = list2cmdline(['wmic', 'os', 'get', 'lastbootuptime']) outs = __salt__['cmd.run'](cmd)
stats_line = '' stats_line = outs.split('\r\n')[1]
startup_time = stats_line[:14] startup_time = time.strptime(startup_time, '%Y%m%d%H%M%S') startup_time = datetime.datetime(*startup_time[:6])
uptime = datetime.datetime.now() - startup_time
owner['user'] = 'SYSTEM' owner['user_domain'] = 'NT AUTHORITY'
port = 4505 master_ip = None
if master is not None: tmp_ip = _host_to_ip(master) if tmp_ip is not None: master_ip = tmp_ip
#_connarg('connection_useSSL', 'useSSL')
from __future__ import absolute_import import copy import logging import re import os.path
import salt.utils import salt.utils.itertools from salt.exceptions import CommandExecutionError, MinionError
import salt.ext.six as six
__virtualname__ = 'pkg'
if refresh: refresh_db()
for name in names: ret[name] = '' cmd = ['pacman', '-Sp', '--needed', '--print-format', '%n %v'] cmd.extend(names)
if name in names: ret[name] = version_num
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg_params = {name: version_num}
prefix = prefix or '=' targets.append('{0}{1}{2}'.format(param, prefix, verstr))
from __future__ import absolute_import import os import re import logging import glob
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError, MinionError
__virtualname__ = 'pkg'
return False
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg, ver = line.split(None)[1].rsplit('-', 1)
if refresh: refresh_db()
if refresh: refresh_db()
ret = {} for name in names: ret[name] = ''
if len(names) == 1: return ret[names[0]] return ret
available_version = latest_version
targets = [x for x in pkg_params if x in old] if not targets: return {}
regex = re.compile(r'\s*repository\s*=\s*'+repo+r'/?\s*(#.*)?$')
from __future__ import absolute_import import logging import datetime import os
import salt.utils
import logging import struct
import logging import re
import salt.utils from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS from salt.exceptions import ( CommandExecutionError )
log = logging.getLogger(__name__)
after_jump = []
rule = rule.strip()
_fh.writelines(rules)
return 'Error: table_type hook and priority required'
if not position: position = get_rule_handle(table, chain, rule, family)
import os import logging
import salt.utils
from __future__ import absolute_import
import bz2 import copy
from salt.exceptions import CommandExecutionError, SaltInvocationError
import salt.ext.six as six
try: import docker import docker.utils HAS_DOCKER_PY = True except ImportError: HAS_DOCKER_PY = False
log = logging.getLogger(__name__)
MIN_DOCKER = (1, 4, 0) MIN_DOCKER_PY = (1, 4, 0)
CLIENT_TIMEOUT = 60
STOP_TIMEOUT = 10
__virtualname__ = 'dockerng'
post = None
client_kwargs['base_url'] = os.environ.get('DOCKER_HOST')
client_kwargs['version'] = 'auto'
if timeout is not None and __context__['docker.client'].timeout != timeout: __context__['docker.client'].timeout = timeout
return None
__context__[contextkey] = 'docker-exec'
if from_config is not None: __context__[contextkey] = from_config return from_config
log.warning( 'Assuming tag \'{0}\' for repo \'{1}\'' .format(default_tag, image) ) r_tag = default_tag
raise CommandExecutionError( 'Error {0}: {1}'.format(exc.response.status_code, exc.explanation) )
raise
raise CommandExecutionError( 'Error {0}: {1}'.format(exc.response.status_code, exc.explanation) )
raise
data['Image'] = '{0}:{1}'.format(repo_name, repo_tag) data['Id'] = status
already_pushed = data.setdefault('Layers', {}).setdefault( 'Already_Pushed', []) already_pushed.append(item['id'])
pushed = data.setdefault('Layers', {}).setdefault( 'Pushed', []) pushed.append(item['id'])
return
try: kwargs['command'] = salt.utils.shlex_split(kwargs['command']) except AttributeError: pass
return
kwargs['user'] = str(kwargs['user']) return
return
return
return
return
try: kwargs['entrypoint'] = salt.utils.shlex_split(kwargs['entrypoint']) except AttributeError: pass
return
return
return
return
return
return
link_name = '/' + link_name
return
return
kwargs['volumes_from'] = str(kwargs['volumes_from'])
return
return
log.info( 'Assuming network_mode \'{0}\' is a network.'.format( kwargs['network_mode']) )
return
return
continue
raise SaltInvocationError(kwarg + ' cannot be None')
validator = kwarg validation_arg = ()
_locals[key](*validation_arg)
for key in list(__context__): try: if key.startswith('validation.docker.'): __context__.pop(key) except AttributeError: pass
image_id = inspect_image(name)['Id']
if verbose: for img_id in ret: ret[img_id]['Info'] = inspect_image(img_id)
mappings = inspect_container(name).get('NetworkSettings', {}).get( 'Ports', {}) if not mappings: return {}
if kwargs.get('verbose', False): for c_id in ret: ret[c_id]['Info'] = inspect_container(c_id)
columns = {} for idx, col_name in enumerate(response['Titles']): columns[idx] = col_name
ret = [] for process in response['Processes']: cur_proc = {} for idx, val in enumerate(process): cur_proc[columns[idx]] = val ret.append(cur_proc) return ret
inspect_image(image)
cp = salt.utils.alias_function(copy_from, 'cp')
out = salt.utils.fopen(path, 'wb')
data = compressor.flush() if data: out.write(data)
if kwargs.get(push, False): ret['Push'] = __salt__['cp.push'](path)
out = salt.utils.fopen(path, 'wb')
data = compressor.flush() if data: out.write(data)
os.remove(saved_path)
if kwargs.get(push, False): ret['Push'] = __salt__['cp.push'](path)
return response
return response
return response
return response
return response
return response
return response
return response
return response
return response
return response
return {'result': ignore_already_stopped, 'comment': 'Container \'{0}\' absent'.format(name)}
post = None
from __future__ import absolute_import import functools import logging import os.path import os import re import time
import jinja2 import jinja2.exceptions import salt.ext.six as six
import salt.utils import salt.utils.templates import salt.utils.validate.net import salt.utils.odict
log = logging.getLogger(__name__)
JINJA = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, 'debian_ip') ) )
__virtualname__ = 'ip'
'address': __ipv4_quad, 'netmask': __ipv4_netmask, 'broadcast': __ipv4_quad, 'metric': __int,
if os.path.exists(_DEB_NETWORK_DIR): interface_files += ['{0}/{1}'.format(_DEB_NETWORK_DIR, dir) for dir in os.listdir(_DEB_NETWORK_DIR)]
if line.lstrip().startswith('#') or line.isspace(): continue if line.startswith('iface'): sline = line.split()
if iface_name not in adapters: adapters[iface_name] = salt.utils.odict.OrderedDict()
if 'data' not in adapters[iface_name]: adapters[iface_name]['data'] = salt.utils.odict.OrderedDict()
elif line[0].isspace(): sline = line.split()
iface_data['inet']['ethtool_keys'] = sorted(ethtool)
iface_data['inet6'] = {} iface_data['inet6']['addrfam'] = 'inet6'
if optname == 'proto' and valuestr == 'none': valuestr = 'static'
_optname = _optname.replace('-', '_') iface_data[addrfam][_optname] = value
return [item + '\n' for item in ifcfg.split('\n')]
adapters = _parse_interfaces() adapters[iface] = data
return saved_ifcfg.split('\n')
return filename
__salt__['kmod.load']('bonding')
__salt__['pkg.install']('ifenslave-2.6')
return [item + '\n' for item in ifcfg]
if iface_type not in ['slave', 'source']: return __salt__['cmd.run'](['ifdown', iface]) return None
return [item + '\n' for item in ifcfg.split('\n')]
if iface_type not in ('slave', 'source'): return __salt__['cmd.run'](['ifup', iface]) return None
current_network_settings = _parse_current_network_settings()
opts = _parse_network_settings(settings, current_network_settings)
_write_file_network(network, _DEB_NETWORKING_FILE, True)
if not found_domain: new_contents.insert(0, 'domain {0}\n' . format(domainname))
if not ('test' in settings and settings['test']): _write_file_network(new_resolv, _DEB_RESOLV_FILE)
from __future__ import absolute_import import os import shutil
import salt.utils
relative_path = parts.repo or "gentoo"
tmp.sort(cmp=lambda x, y: cmp(x.lstrip('-'), y.lstrip('-'))) return tmp
new_flags.sort(cmp=lambda x, y: cmp(x.lstrip('-'), y.lstrip('-')))
if has_wildcard: match_list = set(atom) else: match_list = set(_porttree().dbapi.xmatch("match-all", atom))
dirty_flags = _porttree().dbapi.aux_get(cpv, ["IUSE"])[0].split() return list(set(dirty_flags))
_porttree().dbapi.settings.reset() _porttree().dbapi.settings.lock() return use, use_expand_hidden, usemask, useforce
import os import re
from salt.exceptions import CommandExecutionError
#pylint: disable=E0602
import logging import time import salt.ext.six as six
try: import boto import boto.elasticache import boto.utils logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
log = logger.getLogger(__name__)
apiserver_url = "http://127.0.0.1:8080"
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
ret = _get_labels(node, apiserver_url)
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
labels = _get_labels(node, apiserver_url)
ret['comment'] = "Label {0} already set".format(name)
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
node = _guess_node_id(node) apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
_create_namespace(name, apiserver_url) ret['changes'] = name ret['comment'] = "Namespace {0} created".format(name)
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
ret = _get_namespaces(apiserver_url, namespace) return ret
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
if not decode: ret = _get_secrets(namespace, name, apiserver_url) else: ret = _decode_secrets(_get_secrets(namespace, name, apiserver_url)) return ret
apiserver_url = _guess_apiserver(apiserver_url) if apiserver_url is None: return False
import logging log = logging.getLogger(__name__)
from salt.ext import six
from napalm import get_network_driver HAS_NAPALM = True
return proxy_output
import os import tempfile import hashlib import logging
import salt.utils
pass
from __future__ import absolute_import import os import re import logging
import salt.utils from salt.exceptions import SaltInvocationError
import salt.ext.six as six
log = logging.getLogger(__name__)
if s is None: ret = salt.utils.shlex_split('') else: ret = salt.utils.shlex_split(s)
uninstall_ruby(ruby, runas=runas) return False
raise SaltInvocationError('Command must be specified')
raise SaltInvocationError('Command must be specified')
import logging import os import re
import salt.utils
__virtualname__ = 'debconf'
from __future__ import absolute_import, print_function import errno import logging import os import tempfile import shutil
import salt.utils from salt.exceptions import SaltInvocationError, CommandExecutionError
if isinstance(sources, str): sources = sources.split(',') for src in sources: _get_src(tree_base, src, saltenv)
for dsc in dscs: afile = os.path.basename(dsc) adist = os.path.join(dest_dir, afile) shutil.copy(dsc, adist)
gpg_info_file = '{0}/gpg-agent-info-salt'.format(gnupghome) with salt.utils.fopen(gpg_info_file, 'r') as fow: gpg_raw_info = fow.readlines()
import logging
import salt.utils
import os import uuid import pprint import logging try:
import salt.utils import salt.utils.yast import salt.utils.preseed import salt.utils.kickstart import salt.syspaths from salt.exceptions import SaltInvocationError
import copy import logging import re
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkg'
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
delete = salt.utils.alias_function(remove, 'delete') purge = salt.utils.alias_function(remove, 'purge')
import logging import os.path
import salt.utils from salt.exceptions import ( CommandExecutionError, CommandNotFoundError, SaltInvocationError )
__func_alias__ = { 'list_': 'list' }
if not _valid_composer(composer): raise CommandNotFoundError( '\'composer.{0}\' is not available. Couldn\'t find \'{1}\'.' .format(action, composer) )
if directory is None and action != 'selfupdate': raise SaltInvocationError( 'The \'directory\' argument is required for composer.{0}'.format(action) )
cmd = [composer, action, '--no-interaction', '--no-ansi']
if php is not None: cmd = [php] + cmd
if directory is not None: cmd.extend(['--working-dir', directory])
if quiet is True: cmd.append('--quiet')
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
from __future__ import absolute_import import json import logging as logger
try: import requests import requests.exceptions HAS_LIBS = True except ImportError: HAS_LIBS = False
import salt.ext.six as six
import salt.utils import salt.output import salt.exceptions
log = logger.getLogger(__name__)
__virtualname__ = 'bigip'
if item_kind is None: items.append(value) else: items.append({'kind': item_kind, 'name': value})
if isinstance(member, dict):
else:
from __future__ import absolute_import import logging
import salt.ext.six.moves.http_client from salt.exceptions import CommandExecutionError
my-minion: - esxi-1.example.com - esxi-2.example.com
<vcenter-password> esxi_hosts='[esxi-1.example.com, esxi-2.example.com]'
from __future__ import absolute_import import datetime import logging
import salt.ext.six as six import salt.utils import salt.utils.vmware import salt.utils.http from salt.utils import dictupdate from salt.exceptions import CommandExecutionError
try: from pyVmomi import vim, vmodl HAS_PYVMOMI = True except ImportError: HAS_PYVMOMI = False
salt '*' vsphere.esxcli_cmd my.esxi.host root bad-password \ 'system coredump network get'
salt '*' vsphere.get_coredump_network_config my.esxi.host root bad-password
salt '*' vsphere.get_coredump_network_config my.vcenter.location root bad-password \ esxi_hosts='[esxi-1.host.com, esxi-2.host.com]'
ret.update({esxi_host: {'Coredump Config': _format_coredump_stdout(response)}})
salt '*' vsphere.coredump_network_enable my.esxi.host root bad-password True
salt '*' vsphere.set_coredump_network_config my.esxi.host root bad-password 'dump_ip.host.com'
ret.update({esxi_host: response})
salt '*' vsphere.get_firewall_status my.esxi.host root bad-password
ret.update({esxi_host: _format_firewall_stdout(response)})
salt '*' vsphere.enable_firewall_ruleset my.esxi.host root bad-password True 'syslog'
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) ret.update({host: response})
salt '*' vsphere.syslog_service_reload my.esxi.host root bad-password
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) ret.update({host: response})
salt '*' vsphere.set_syslog_config my.esxi.host root bad-password \ loghost ssl://localhost:5432,tcp://10.1.0.1:1514
if firewall and syslog_config == 'loghost': if esxi_hosts: if not isinstance(esxi_hosts, list): raise CommandExecutionError('\'esxi_hosts\' must be a list.')
if esxi_hosts: if not isinstance(esxi_hosts, list): raise CommandExecutionError('\'esxi_hosts\' must be a list.')
if ret.get(esxi_host) is None: ret.update({esxi_host: {}}) ret[esxi_host].update(response)
salt '*' vsphere.get_syslog_config my.esxi.host root bad-password
ret.update({esxi_host: _format_syslog_config(response)})
response = salt.utils.vmware.esxcli(host, username, password, cmd, protocol=protocol, port=port) ret.update({host: _format_syslog_config(response)})
salt '*' vsphere.reset_syslog_config my.esxi.host root bad-password \ syslog_config='logdir,loghost'
response_dict = _reset_syslog_config_params(host, username, password, cmd, resets, valid_resets, protocol=protocol, port=port) ret.update({host: response_dict})
salt '*' vsphere.get_host_datetime my.esxi.host root bad-password
salt '*' vsphere.get_ntp_config my.esxi.host root bad-password
salt '*' vsphere.get_service_policy my.esxi.host root bad-password 'ssh'
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
salt '*' vsphere.get_service_running my.esxi.host root bad-password 'ssh'
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
salt '*' vsphere.get_vmotion_enabled my.esxi.host root bad-password
salt '*' vsphere.get_vsan_enabled my.esxi.host root bad-password
salt '*' vsphere.get_vsan_eligible_disks my.esxi.host root bad-password
salt '*' vsphere.list_ssds my.esxi.host root bad-password
salt '*' vsphere.list_non_ssds my.esxi.host root bad-password
salt '*' vsphere.ntp_configure my.esxi.host root bad-password '[192.174.1.100, 192.174.1.200]'
ntp_config = vim.HostNtpConfig(server=ntp_servers)
date_config = vim.HostDateTimeConfig(ntpConfig=ntp_config)
salt '*' vsphere.service_start my.esxi.host root bad-password 'ntpd'
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
salt '*' vsphere.service_stop my.esxi.host root bad-password 'ssh'
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
salt '*' vsphere.service_restart my.esxi.host root bad-password 'ntpd'
if service_name == 'SSH' or service_name == 'ssh': temp_service_name = 'TSM-SSH' else: temp_service_name = service_name
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
salt '*' vsphere.set_service_policy my.esxi.host root bad-password 'ntpd' 'automatic'
if service_name not in valid_services: ret.update({host_name: {'Error': '{0} is not a valid service name.'.format(service_name)}}) return ret
for service in services: service_key = None
if service.key == service_name: service_key = service.key elif service_name == 'ssh' or service_name == 'SSH': if service.key == 'TSM-SSH': service_key = 'TSM-SSH'
salt '*' vsphere.update_date_time my.esxi.host root bad-password
account_manager = salt.utils.vmware.get_inventory(service_instance).accountManager
user_account = vim.host.LocalAccountManager.AccountSpecification() user_account.id = username user_account.password = new_password
salt '*' vsphere.vmotion_disable my.esxi.host root bad-password
salt '*' vsphere.vmotion_enable my.esxi.host root bad-password
salt '*' vsphere.vsan_add_disks my.esxi.host root bad-password
disk_names = [] for disk in eligible: disk_names.append(disk.canonicalName) ret.update({host_name: {'Disks Added': disk_names}})
ret.update({host_name: {'Disks Added': eligible}})
ret.update({host_name: {'Error': error}})
ret.update({host_name: {'Disks Added': 'No new VSAN-eligible disks were found to add.'}})
salt '*' vsphere.vsan_disable my.esxi.host root bad-password
vsan_config = vim.vsan.host.ConfigInfo() vsan_config.enabled = False
salt '*' vsphere.vsan_enable my.esxi.host root bad-password
vsan_config = vim.vsan.host.ConfigInfo() vsan_config.enabled = True
if host_name: host_ref = search_index.FindByDnsName(dnsName=host_name, vmSearch=False) else: host_ref = search_index.FindByDnsName(dnsName=host, vmSearch=False)
if host_ref is None: host_ref = search_index.FindByIp(ip=host, vmSearch=False)
suitable_disks = [] query = vsan_system.QueryDisksForVsan() for item in query: if item.state == 'eligible': suitable_disks.append(item)
disks = _get_host_ssds(host_ref) + _get_host_non_ssds(host_ref)
matching = [] for disk in disks: for suitable_disk in suitable_disks: if disk.canonicalName == suitable_disk.disk.canonicalName: matching.append(disk)
ret[host_name] = {}
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import SaltException
@decorators.memoize def __detect_os(): return salt.utils.which('ipvsadm')
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if out['retcode']: ret = out['stderr'].strip() else: ret = out['stdout'].strip()
if out['retcode']: ret = out['stderr'].strip() else: ret = True return ret
if not kwargs: cmd += ' '
if not kwargs: cmd += ' '
#pylint: disable=E0602
from __future__ import absolute_import import logging
import salt.utils.compat import salt.utils from salt.ext.six import string_types
#pylint: disable=E0602
from __future__ import absolute_import import logging import json
import salt.utils.boto3 import salt.utils.compat import salt.utils from salt.ext.six import string_types
import fnmatch import logging
import salt.loader import salt.runner import salt.state import salt.utils import salt.utils.schema as S from salt.utils.doc import strip_rst as _strip_rst from salt.ext.six.moves import zip
import salt.ext.six as six
__virtualname__ = 'sys'
target_mod = module + '.' if not module.endswith('.') else module
target_mod = module + '.' if not module.endswith('.') else module
target_mod = module + '.' if not module.endswith('.') else module
target_mod = module + '.' if not module.endswith('.') else module
return sorted(__salt__)
module = module + '.' if not module.endswith('.') else module
return True
return sorted(st_.states)
module = module + '.' if not module.endswith('.') else module
return sorted(run_.functions)
module = module + '.' if not module.endswith('.') else module
return sorted(returners_)
module = module + '.' if not module.endswith('.') else module
))
))
import copy import logging import os
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkg'
info = salt.utils.alias_function(version, 'info')
update = salt.utils.alias_function(refresh_db, 'update')
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
return True
opts = ''.join([opt for opt in opts if opt in 'AfIMq']) targets = pkg_params
pkg_params = {name: kwargs.get('version')}
return ' '.join(cmd)
if pkg[0].find("/") > 0: origin = pkg[0] pkg = [k for k, v in old.iteritems() if v['origin'] == origin][0]
delete = salt.utils.alias_function(remove, 'delete') purge = salt.utils.alias_function(remove, 'purge')
import base64 import hashlib import hmac import StringIO
import salt.exceptions import salt.ext.six as six import salt.utils
import re import os import logging
import salt.utils from salt.exceptions import CommandExecutionError
__func_alias__ = { 'list_': 'list' }
from __future__ import absolute_import import json import logging import os import tempfile
import salt.ext.six as six
if params: endpoint = 'resources'
for key, val in six.iteritems(params): params[key] = str(val)
from __future__ import absolute_import
__virtualname__ = 'service'
if __grains__['kernelrelease'] == "5.9": return (False, 'The smf execution module failed to load: SMF not available on Solaris 9.') return __virtualname__
clear_cmd = '/usr/sbin/svcadm clear {0}'.format(name) __salt__['cmd.retcode'](clear_cmd, python_shell=False) return not __salt__['cmd.retcode'](cmd, python_shell=False)
return start(name)
return start(name)
return _get_enabled_disabled("true")
return _get_enabled_disabled("false")
import yaml import json
import salt.utils.pagerduty from salt.ext.six import string_types
list_maintenance_windows = salt.utils.alias_function(list_windows, 'list_maintenance_windows')
list_escalation_policies = salt.utils.alias_function(list_policies, 'list_escalation_policies')
from __future__ import absolute_import try: import pwd except ImportError: pass import logging import time
import salt.utils import salt.utils.decorators as decorators from salt.utils.locales import sdecode as _sdecode from salt.exceptions import CommandExecutionError, SaltInvocationError
__virtualname__ = 'user'
if createhome: __salt__['file.mkdir'](home, user=uid, group=gid)
time.sleep(1) if groups: chgroups(name, groups) return True
if force: log.warn('force option is unsupported on MacOS, ignoring')
if remove: __salt__['file.remove'](info(name)['home'])
chgroups(name, ()) return _dscl(['/Users/{0}'.format(name)], ctype='delete')['retcode'] == 0
time.sleep(1) return info(name).get('uid') == uid
time.sleep(1) return info(name).get('gid') == gid
time.sleep(1) return info(name).get('shell') == shell
time.sleep(1) return info(name).get('home') == home
ctype='create'
time.sleep(1)
time.sleep(1) return info(new_name).get('RecordName') == new_name
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
if not HAS_REQUESTS: return False return __virtualname__
auth = _auth(profile=profile)
return {'err_code': response.status_code, 'err_msg': json.loads(response.text).get('err', '')}
alert_ids = get_alert_config(deployment_id, api_key=api_key, profile=profile)
from __future__ import absolute_import import re import logging
from salt.exceptions import SaltInvocationError from salt.utils import dictdiffer
import salt.ext.six as six
from __future__ import absolute_import import json import logging import os
from salt.exceptions import SaltInvocationError import salt.utils
__virtualname__ = 'win_iis'
current_bindings = list_bindings(site)
current_name = None
certs = _list_certs()
current_cert_bindings = list_cert_bindings(site)
cmd_ret = _srvmgr(func=str().join(pscmd_validate), as_json=True)
for setting in settings: settings[setting] = str(settings[setting])
try: complex(settings[setting]) value = settings[setting] except ValueError: value = "'{0}'".format(settings[setting])
new_settings = get_container_setting(name=name, container=container, settings=settings.keys()) failed_settings = dict()
if not os.path.isdir(sourcepath): _LOG.error('Path is not present: %s', sourcepath) return False
if not os.path.isdir(sourcepath): _LOG.error('Path is not present: %s', sourcepath) return False
import salt.utils import hashlib import datetime import socket import salt.utils.network import salt.utils.validate.net
try:
__virtualname__ = 'network'
addresses.append(line.strip()) continue
hwaddr = salt.utils.alias_function(hw_addr, 'hwaddr')
import logging
import salt.utils
__virtualname__ = 'pkg'
comps[1] = comps[1].lstrip('"').rstrip('"')
if iface_type not in ['slave']: return __salt__['cmd.run']('ip link set {0} down'.format(iface)) return None
if iface_type not in ['slave']: return __salt__['cmd.run']('ip link set {0} up'.format(iface)) return None
import logging import json
keystone.auth_key: 203802934809284k2j34lkj2l3kj43k
keystone.auth_key: 203802934809284k2j34lkj2l3kj43k
keystone.auth_key: 303802934809284k2j34lkj2l3kj43k
import logging
import salt.utils.openstack.swift as suos
log = logging.getLogger(__name__)
from __future__ import absolute_import import logging
from salt.exceptions import SaltInvocationError
#pylint: disable=E0602
from __future__ import absolute_import import logging import time import json from boto.ec2.blockdevicemapping import BlockDeviceMapping, BlockDeviceType
import salt.utils import salt.utils.compat import salt.ext.six as six from salt.exceptions import SaltInvocationError, CommandExecutionError
try: import boto import boto.ec2 HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import print_function from __future__ import absolute_import import copy import logging
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'pkg' log = logging.getLogger(__name__)
lines = __salt__['cmd.run_stdout']("/bin/pkg list -Huv").splitlines() for line in lines: upgrades[_ips_get_pkgname(line)] = _ips_get_pkgversion(line) return upgrades
old = list_pkgs()
cmd = ['pkg', 'update', '-v', '--accept'] out = __salt__['cmd.run_all'](cmd, output_loglevel='trace', python_shell=False)
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
for line in lines: name = _ips_get_pkgname(line) version = _ips_get_pkgversion(line) __salt__['pkg_resource.add_pkg'](ret, name, version)
available_version = salt.utils.alias_function(latest_version, 'available_version')
return name
return name
return {}
old = list_pkgs()
cmd += '{0}'.format(pkg2inst)
__context__.pop('pkg.list_pkgs', None) new = list_pkgs() ret = salt.utils.compare_dicts(old, new)
if test: return 'Test succeeded.'
old = list_pkgs()
cmd = '/bin/pkg uninstall -v {0}'.format(pkg2rm) out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')
__context__.pop('pkg.list_pkgs', None) new = list_pkgs() ret = salt.utils.compare_dicts(old, new)
import logging import salt.utils
__virtualname__ = 'pkg'
#pylint: disable=E0602
from __future__ import absolute_import import logging import json
import salt.utils.boto3 import salt.utils.compat import salt.utils from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
time.sleep((2 ** (RoleRetries - retry)) + (random.randint(0, 1000) / 1000)) continue
time.sleep((2 ** (RoleRetries - retry)) + (random.randint(0, 1000) / 1000)) continue
#pylint: disable=E0602
import logging import re
try: import boto import boto.ec2 logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
from __future__ import absolute_import import logging import string import json import datetime
import salt.utils.boto3 import salt.utils.compat
from __future__ import absolute_import
__virtualname__ = 'influxdb'
if hasattr(client, 'create_user'): client.create_user(name, passwd) return True
if database: return client.add_database_user(name, passwd) return client.add_cluster_admin(name, passwd)
import logging import os import subprocess import re import collections import decimal
from salt.ext import six from salt.ext.six.moves import zip
import salt.utils import salt.utils.decorators as decorators from salt.utils.decorators import depends from salt.exceptions import CommandExecutionError
if not comps: continue
if len(stats) < h_len: h_len = len(stats) dev_stats[disk].append(stats)
if h_len < len(dev_header): sys_header = dev_header[h_len:] dev_header = dev_header[0:h_len]
from __future__ import absolute_import import re
#import salt.ext.six as six
from salt.exceptions import ( #CommandExecutionError, SaltInvocationError ) from salt.utils import warn_until
_version_ary = __version__.split('.') CUR_VER = SaltStackVersion(_version_ary[0], _version_ary[1]) BORON = SaltStackVersion.from_name('Boron')
HAS_GLANCE = False try: from glanceclient import client from glanceclient import exc HAS_GLANCE = True except ImportError: pass
HAS_KEYSTONE = False try: from keystoneclient.v2_0 import client as kstone #import keystoneclient.apiclient.exceptions as kstone_exc HAS_KEYSTONE = True except ImportError: pass
g_endpoint_url = re.sub('/v2', '', g_endpoint_url['internalurl'])
raise SaltInvocationError('Only can use keystone admin token ' + 'with Glance API v1')
return ret
import salt.utils import salt.utils.mac_utils from salt.exceptions import SaltInvocationError
import logging
import salt.utils
__virtualname__ = 'group'
retcode = __salt__['cmd.retcode']('pw groupmod {0} -m {1}'.format( name, username), python_shell=False)
retcode = __salt__['cmd.retcode']('pw groupmod {0} -d {1}'.format( name, username), python_shell=False)
import glob import logging import os import stat
import salt.utils
__virtualname__ = 'service'
if 'unknown' in out: return '3' else: return out.split()[1]
return bool(os.stat( os.path.join('/etc/init.d', name)).st_mode & stat.S_IXUSR)
result = _chkconfig_is_enabled(name, runlevel) if result: return True
return [x for x in _services if _service_is_sysv(x)]
import re import logging
import salt.utils
new_conf.append(new_line)
new_conf.append(_format_master(**line))
new_conf.append(line)
new_conf.append(new_line)
#========================================================================== #smtp inet n - n - - smtpd if private == 'y': private = '-'
return pairs, conf_list
continue
import copy import logging import json
import salt.utils
log = logging.getLogger(__name__)
__virtualname__ = 'psget'
flags = [('Name', name)]
flags = [('Name', name)]
cmd = 'Uninstall-Module "{0}"'.format(name) no_ret = _pshell(cmd) return name not in list_modules()
flags = [('Name', name)]
cmd = 'Get-PSRepository "{0}"'.format(name) no_ret = _pshell(cmd) return name not in list_modules()
import logging import hmac import base64 import subprocess
HAS_LIBS = False try: import splunklib.client from splunklib.client import AuthenticationError from splunklib.binding import HTTPError HAS_LIBS = True except ImportError: pass
return True
kwargs = {} roles = [role.name for role in user.role_entities]
__salt__['file.remove'](tmpfilename)
import logging
try: import elasticsearch logging.getLogger('elasticsearch').setLevel(logging.CRITICAL) HAS_ELASTICSEARCH = True except ImportError: HAS_ELASTICSEARCH = False
import os import re import sys import uuid import string
import salt.utils from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS from salt.exceptions import SaltException from salt.ext import six
return '/etc/sysconfig/scripts/SuSEfirewall2-custom'
kwargs.pop('name', None) kwargs.pop('state', None)
after_jump = [] after_jump_arguments = (
'to-port',
add_arg('-m', '--match', dest='match', action='append')
import os import datetime try: import spwd except ImportError: pass
import salt.utils from salt.exceptions import CommandExecutionError try: import salt.utils.pycrypto HAS_CRYPT = True except ImportError: HAS_CRYPT = False
import logging import salt.utils
from __future__ import absolute_import import logging
from salt.ext.six.moves.urllib.parse import urlencode as _urlencode
from salt.exceptions import SaltInvocationError import salt.utils.pushover
import re
import salt.utils from salt import utils, exceptions
import salt.utils
__virtualname__ = 'group'
from __future__ import absolute_import import os import re import uuid import logging
import salt.utils import salt.utils.fsutils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
image_path = "{0}/ext2_saved".format(mountpoint) orig_fstype = ret['before']['type']
import os import errno import logging import re import string
import salt.utils import salt.utils.itertools from salt.exceptions import SaltInvocationError, CommandExecutionError
return 'UTC'
from __future__ import absolute_import import datetime import hashlib import logging import re import os import socket
import salt.utils import salt.utils.decorators as decorators import salt.utils.network import salt.utils.validate.net from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if salt.utils.is_windows(): return (False, 'The network execution module cannot be loaded on Windows: use win_network instead.') return True
continue
local_addr = ''.join(x for x in local_addr if x not in '[]')
if salt.utils.is_sunos(): traceroute_version = [0, 0, 0] else: cmd2 = 'traceroute --version' out2 = __salt__['cmd.run'](cmd2) try:
hwaddr = salt.utils.alias_function(hw_addr, 'hwaddr')
host_c = salt.utils.fopen('/etc/hosts', 'r').readlines()
host[host.index(o_hostname.split('.')[0])] = hostname.split('.')[0]
if __grains__['os_family'] == 'RedHat': network_c = salt.utils.fopen('/etc/sysconfig/network', 'r').readlines()
if parts[0].endswith('sh:'): out = ' '.join(parts[1:]) ret['comment'] = out
import salt.utils
import os import re import plistlib from distutils.version import LooseVersion
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandExecutionError
import salt.ext.six as six
__virtualname__ = 'service'
true_path = os.path.realpath(file_path) if not os.path.exists(true_path): continue
with salt.utils.fopen(file_path): plist = plistlib.readPlist(true_path)
return services[name]
return service
return service
raise CommandExecutionError('Service not found: {0}'.format(name))
return_stdout = kwargs.pop('return_stdout', False)
cmd = ['launchctl', sub_cmd] cmd.extend(args)
kwargs['python_shell'] = False ret = __salt__['cmd.run_all'](cmd, **kwargs)
service = _get_service(name) label = service['plist']['Label']
return launchctl('list', label, return_stdout=True, output_loglevel='trace', runas=runas)
return launchctl('list', return_stdout=True, output_loglevel='trace', runas=runas)
service = _get_service(name) label = service['plist']['Label']
return launchctl('enable', 'system/{0}'.format(label), runas=runas)
service = _get_service(name) label = service['plist']['Label']
return launchctl('disable', 'system/{0}'.format(label), runas=runas)
service = _get_service(name) path = service['file_path']
return launchctl('load', path, runas=runas)
service = _get_service(name) path = service['file_path']
return launchctl('unload', path, runas=runas)
if enabled(name): stop(name, runas=runas) start(name, runas=runas)
if sig: return __salt__['status.pid'](sig)
try: list_(name=name, runas=runas) return True except CommandExecutionError: return False
return not enabled(name, runas=runas)
enabled = get_enabled(runas=runas)
available = list(_available_services().keys())
return sorted(set(enabled + available))
stdout = list_(runas=runas) service_lines = [line for line in stdout.splitlines()]
enabled = [] for line in service_lines: if line.startswith('PID'): continue
#pylint: disable=E0602
import logging
import salt.utils.compat
import logging import re import os from salt.ext.six.moves import map
try: import win32gui import win32con HAS_WIN32 = True except ImportError: HAS_WIN32 = False
import salt.utils
log = logging.getLogger(__name__)
return list(map(_normalize_dir, ret))
salt '*' win_path.add 'c:\\python27' 0
if index < 0: index = len(sysPath) + index + 1 if index > len(sysPath): index = len(sysPath)
try: currIndex = sysPath.index(path) if currIndex != index: sysPath.pop(currIndex) else: return True except ValueError: pass
if regedit: return rehash() else: return False
import json import logging import random import string
import salt.utils import salt.utils.itertools import salt.ext.six as six from salt.exceptions import SaltInvocationError from salt.ext.six.moves import range from salt.exceptions import CommandExecutionError
data_rows = _strip_listing_to_done(cmdoutput.splitlines())
clear_pw = True password = ''.join(random.SystemRandom().choice( string.ascii_uppercase + string.digits) for x in range(15))
res2 = clear_password(name, runas)
delete_user(name, runas) msg = 'Error' return _format_response(res2, msg)
import logging import os import re import datetime
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
__virtualname__ = 'lowpkg'
dselect_pkg_avail = _get_pkg_ds_avail()
from __future__ import absolute_import import salt.utils
__virtualname__ = 'service'
import salt.utils
import os import logging
import os import ast import logging
import salt.utils import salt.payload
import salt.ext.six as six
import json
import salt.utils
from __future__ import absolute_import import os import logging
import salt.utils from salt.exceptions import CommandExecutionError from salt.ext.six import string_types
try:
__virtualname__ = 'win_dacl'
from __future__ import absolute_import import os import glob import tempfile import time import logging
import salt.utils import salt.crypt
import salt.ext.six as six
log = logging.getLogger(__name__)
import os
import salt.utils
from __future__ import absolute_import import copy import logging import re
import salt.utils import salt.utils.mac_utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
available_version = salt.utils.alias_function(latest_version, 'available_version')
if pkgs is None: version_num = kwargs.get('version') variant_spec = kwargs.get('variant') spec = None
import logging
import salt.utils
__virtualname__ = 'ntp'
import json import logging
import salt.utils import salt.modules.cmdmod from salt.exceptions import CommandExecutionError
__func_alias__ = { 'list_': 'list' }
npm_output = result['stdout'] or result['stderr'] try: return json.loads(npm_output) except ValueError: pass
while not lines[0].startswith('{') and not lines[0].startswith('['): lines = lines[1:]
return npm_output
if pkg: pkg = _cmd_quote(pkg)
if pkg: pkg = _cmd_quote(pkg)
if result['retcode'] != 0 and result['stderr']: raise CommandExecutionError(result['stderr'])
#pylint: disable=E0602
from __future__ import absolute_import import logging import socket
subnets = conn.get_all_subnets(subnet_ids=subnets)
from __future__ import absolute_import import logging import json
from salt.exceptions import CommandExecutionError from salt.ext import six
from cassandra.cluster import Cluster from cassandra.cluster import NoHostAvailable from cassandra.connection import ConnectionException, ConnectionShutdown from cassandra.auth import PlainTextAuthProvider from cassandra.query import dict_factory HAS_DRIVER = True
if not isinstance(value, six.text_type): if not isinstance(value, (set, list, dict)): value = str(value) values[key] = value
replication_map = { 'class': replication_strategy }
import logging
import salt.utils
import logging import glob import re
from salt.ext.six.moves import shlex_quote as _cmd_quote
import salt.utils.systemd
__virtualname__ = 'service'
if service != 'README': ret.add(service)
from __future__ import absolute_import import os
from salt.ext.six import string_types
import salt.utils from salt.exceptions import CommandExecutionError, CommandNotFoundError
return True
if isinstance(val, string_types): if val.lower() == 'true': val = True elif val.lower() == 'false': val = False ret[key] = val
from __future__ import absolute_import import time import datetime
from salt.exceptions import SaltInvocationError, CommandExecutionError
import salt.ext.six as six try: import salt.utils.psutil_compat as psutil
return psutil.TOTAL_PHYMEM
return psutil.NUM_CPUS
b_time = int(psutil.boot_time())
try:
#pylint: disable=E0602
from __future__ import absolute_import import logging import json import yaml
import salt.utils.compat import salt.utils.odict as odict import salt.utils.boto
from salt.ext.six import string_types
conn.get_instance_profile(name) return True
conn.create_instance_profile(name) log.info('Created {0} instance profile.'.format(name))
_policy = _policy.get_role_policy_response.policy_document _policy = _unquote(_policy) _policy = json.loads(_policy, object_pairs_hook=odict.OrderedDict) return _policy
arn = ret['get_user_response']['get_user_result']['user']['arn']
def ordered_dict_presenter(dumper, data): return dumper.represent_dict(data.items())
from __future__ import absolute_import import logging import os import time import re
import salt.utils
return list(os.walk('/sys/fs/bcache/'))[0][1][0]
return os.path.basename(_bcsys(dev, 'cache'))
cache = uuid()
if bucket_size is None: bucket_size = _size_map(_fssys('bucket_size'))
cache = uuid() if cache: if not force: log.error('BCache cache {0} is already on the system'.format(cache)) return False cache = _bdev()
if cache: if not stop(): return False elif not _wipe(cache): return False
if not _wipe(dev): return False
if not _run_all(cmd, 'error', 'Error creating bcache partitions on {0}: {{0}}'.format(dev)): return False dev = '{0}2'.format(dev)
cmd = 'make-bcache --cache /dev/{0} --block {1} --wipe-bcache'.format(dev, block_size)
if bucket_size: cmd += ' --bucket {0}'.format(bucket_size)
updates = dict([(key, val) for key, val in kwargs.items() if not key.startswith('__')])
if statii[dev]['cache'] == cuuid: count += 1
result['uuid'] = uuid() base_attr = ['block_size', 'bucket_size', 'cache_available_percent', 'cache_replacement_policy', 'congested']
result.update(_sysfs_parse(_bcpath(dev), base_attr, stats, config, internals)) result.update(_sysfs_parse(_fspath(), base_attr, stats, config, internals))
back_uuid = uuid(dev) if back_uuid is not None: result['cache'] = back_uuid
if superblock: result['superblock'] = super_(dev)
return os.path.join('/sys/block/', dev)
intfs = __salt__['sysfs.interfaces'](path)
del intfs['w']
bintflist = [intf for iflist in bintf.values() for intf in iflist] result.update(__salt__['sysfs.read'](bintflist, path))
for boolkey in ('running', 'writeback_running', 'congested'): if boolkey in result: result[boolkey] = bool(result[boolkey])
block_sizes = ('hw_sector_size', 'minimum_io_size', 'physical_block_size', 'logical_block_size') discard_sizes = ('discard_max_bytes', 'discard_max_hw_bytes', )
cmd += ' seek={0}'.format((size/1024**2) - blocks) endres += _run_all(cmd, 'warn', wipe_failmsg)
import ctypes import string
import salt.ext.six as six import salt.utils
__virtualname__ = 'disk'
import logging import re import os HAS_DBUS = False try: import dbus HAS_DBUS = True except ImportError: pass
import salt.utils import salt.utils.locales import salt.utils.systemd import salt.ext.six as six from salt.exceptions import CommandExecutionError
__virtualname__ = 'locale'
cmd = 'grep "^LANG=" /etc/default/locale'
update_locale = salt.utils.which('update-locale') if update_locale is None: raise CommandExecutionError( 'Cannot set locale: "update-locale" was not found.')
__salt__['file.replace']( '/etc/default/locale', '^LANG=.*', 'LANG="{0}"'.format(locale), append_if_not_found=True )
if not locale_info['charmap'] and not on_ubuntu: locale_info['charmap'] = locale_info['codeset'] locale = salt.utils.locales.join_locale(locale_info)
import logging import json from lxml import etree
log = logging.getLogger(__name__)
__virtualname__ = 'junos'
import logging
import salt.utils
from __future__ import absolute_import import json import logging
import logging
import salt.utils import salt.utils.decorators as decorators
import json
import salt.utils HAS_CLOUD = False try:
__virtualname__ = 'saltcloud'
import salt.utils as utils
import os import logging
import salt.ext.six as six
to_unset = [key for key in os.environ if key not in environ] for key in to_unset: ret[key] = setval(key, False, false_unsets, permanent=permanent)
import logging import time from datetime import datetime
try: import pythoncom import wmi import win32net import win32api import win32con import pywintypes from ctypes import windll HAS_WIN32NET_MODS = True except ImportError: HAS_WIN32NET_MODS = False
import salt.utils import salt.utils.locales from salt.modules.reg import read_value
log = logging.getLogger(__name__)
__virtualname__ = 'system'
system_info = win32net.NetServerGetInfo(None, 101)
if desc: system_info['comment'] = desc.decode('utf-8') else: return False
if isinstance(account_ou, str): account_ou = account_ou.split('\\') account_ou = ''.join(account_ou)
dt_obj = salt.utils.date_cast(newtime)
return set_system_date_time(hours=int(dt_obj.strftime('%H')), minutes=int(dt_obj.strftime('%M')), seconds=int(dt_obj.strftime('%S')))
time_tuple = (years, months, days, hours, minutes, seconds, 0)
dt_obj = salt.utils.date_cast(newdate)
return set_system_date_time(years=int(dt_obj.strftime('%Y')), months=int(dt_obj.strftime('%m')), days=int(dt_obj.strftime('%d')))
checks = (get_pending_update, get_pending_file_rename, get_pending_servermanager, get_pending_component_servicing, get_pending_computer_name, get_pending_domain_join)
import salt.utils import salt.ext.six as six
return True
import errno import logging
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
import os import sys import time import traceback import random
import salt import salt.utils import salt.version import salt.loader import salt.ext.six as six from salt.utils.decorators import depends
__func_alias__ = { 'true_': 'true', 'false_': 'false' }
for argument in args: ret['args'].append(str(type(argument)))
for key, val in six.iteritems(kwargs): ret['kwargs'][key] = str(type(val))
from __future__ import absolute_import import logging
import salt.ext.six as six
HAS_KEYSTONE = False try: from keystoneclient.v2_0 import client import keystoneclient.exceptions HAS_KEYSTONE = True except ImportError: pass
def get(key, default=None): return connection_args.get('connection_' + key, __salt__['config.get'](prefix + key, default))
if insecure: kwargs['insecure'] = True
return ret
import salt.utils import salt.utils.decorators as decorators
@decorators.memoize def __detect_os(): return salt.utils.which('nginx')
from __future__ import absolute_import import logging import re
from salt.exceptions import CommandExecutionError import salt.utils
cmd = '--{0}-{1}={2} --permanent'.format(action, _type, name)
import collections import logging import os import sys import traceback
import salt.crypt import salt.utils.event import salt.payload import salt.transport import salt.ext.six as six
log.warning('Local mode detected. Event with tag {0} will NOT be sent.'.format(tag)) return False
if 'master_uri' not in __opts__: __opts__['master_uri'] = 'tcp://{ip}:{port}'.format( ip=salt.utils.ip_bracket(__opts__['interface']),
if isinstance(data, collections.Mapping): data_dict.update(data)
import os import glob
__virtualname__ = 'service'
import logging import salt.utils
from __future__ import absolute_import import logging import re import pprint import time
from salt.exceptions import CommandExecutionError
import salt.ext.six as six
import salt.utils import salt.utils.fsutils from salt.exceptions import CommandExecutionError from salt.exceptions import get_error_message as _get_error_message
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
return salt.utils.is_proxy() and 'proxy' in __opts__
import time import logging
import salt.payload import salt.transport import salt.utils.args from salt.exceptions import SaltReqTimeoutError
import salt.ext.six as six from salt.ext.six.moves import range, zip from salt.ext.six.moves.urllib.parse import urlparse as _urlparse
if salt.utils.is_windows(): return (False, 'The file execution module cannot be loaded: only available on non-Windows systems - use win_file instead.') return True
return os.path.join(__salt__['config.get']('cachedir'), 'file_backup')
gid = group_to_gid(gid)
return ''
return gid
return uid
return os.lchown(path, uid, gid)
path = os.path.expanduser(path)
path = os.path.expanduser(path)
path = os.path.expanduser(path)
#after = _sed_esc(after, escape_all) limit = _sed_esc(limit, escape_all)
path = os.path.realpath(os.path.expanduser(path))
if not os.path.isfile(path): raise SaltInvocationError('File not found: {0}'.format(path))
if not salt.utils.istextfile(path): raise SaltInvocationError( 'Cannot perform string replacements on a binary file: {0}'.format(path))
if not found: return False
try: temp_file = _mkstemp_copy(path=path, preserve_inode=False) except (OSError, IOError) as exc: raise CommandExecutionError("Exception: {0}".format(exc))
return ''.join(difflib.unified_diff(orig_file, new_file))
if before is None and after is None and not match: match = content
if (idx < len(lines) and _starts_till(lines[idx + 1], cnd) < 0) or idx + 1 == len(lines): out.append(cnd)
has_changes = False
repl = str(repl)
if nrepl > 0: found = True has_changes = True if pattern != repl else has_changes
if re.search('^{0}$'.format(re.escape(content)), r_data, flags=flags_num): found = True
try: temp_file = _mkstemp_copy(path=path, preserve_inode=preserve_inode) except (OSError, IOError) as exc: raise CommandExecutionError("Exception: {0}".format(exc))
if 0 != len(new_file): if not new_file[-1].endswith('\n'): new_file[-1] += '\n' new_file.append(not_found_content + '\n')
temp_file = _mkstemp_copy(path=path, preserve_inode=preserve_inode)
try: fh_ = salt.utils.atomicfile.atomic_open(path, 'w') for line in new_file: fh_.write(line) finally: fh_.close()
in_block = True
in_block = False
if content and content[-1] == '\n': content = content[:-1]
for cline in content.split('\n'): new_file.append(cline + '\n')
old_content += line result = None
raise CommandExecutionError( 'Unterminated marked block. End of file reached before marker_end.' )
new_file.insert(0, marker_end + '\n') new_file.insert(0, content) new_file.insert(0, marker_start + '\n') done = True
if backup is not False: shutil.copy2(path, '{0}{1}'.format(path, backup))
try: fh_ = salt.utils.atomicfile.atomic_open(path, 'w') for line in new_file: fh_.write(line) finally: fh_.close()
check_perms(path, None, perms['user'], perms['group'], perms['mode'])
return replace(path, pattern, '', flags=flags, bufsize=bufsize, dry_run=True, search_only=True, show_changes=False, ignore_if_missing=ignore_if_missing)
if '-N' not in cmd and '--forward' not in cmd: cmd.append('--forward')
if not has_rejectfile_option: cmd.append('--reject-file=-')
pstat = os.lstat(path)
return ret
raise CommandExecutionError( 'none of the specified sources were found' )
sfn = '' source_sum = {}
if source and not (not follow_symlinks and os.path.islink(real_name)): name_sum = get_hash(real_name, source_sum['hash_type']) else: name_sum = None
ret['changes'].pop('diff', None) return _error(ret, 'Parent directory not present')
if mode: current_umask = os.umask(0o77)
makedirs_perms(directory, user, group, mode)
dirname = os.path.normpath(os.path.dirname(path))
directories_to_create.reverse() for directory_to_create in directories_to_create: log.debug('Creating directory: %s', directory_to_create) mkdir(directory_to_create, user=user, group=group, mode=mode)
if exc.errno != errno.EEXIST: raise
return False
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
return False
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
return False
if exc.errno != errno.EEXIST: raise else: ret['comment'] = 'File {0} exists and cannot be overwritten'.format(name)
src_dir = parent_dir.replace(':', '_')
bkdir = os.path.join(bkroot, src_dir)
strpfmt = '{0}_%a_%b_%d_%H-%M-%S_%f_%Y'.format(basename)
continue
bkdir = os.path.join(bkroot, parent_dir[1:])
continue
pids = {} procfs = os.listdir('/proc/') for pfile in procfs: try: pids[int(pfile)] = [] except ValueError: pass
files = {} for pid in pids: ppath = '/proc/{0}'.format(pid) try: tids = os.listdir('{0}/task'.format(ppath)) except OSError: continue
fd_ = []
#except:
for fdpath in fd_: try: name = os.path.realpath(fdpath) os.stat(name) except OSError: continue
files[name].append(pid) files[name] = sorted(set(files[name]))
from __future__ import absolute_import import datetime
import salt.utils import salt.utils.itertools from salt.exceptions import SaltInvocationError
import salt.ext.six as six
return line
query = 'CREATE DATABASE "{0}"'.format(name)
ret = _psql_prepare_and_run(['-c', query], user=user, host=host, port=port, maintenance_db=maintenance_db, password=password, runas=runas) return ret['retcode'] == 0
ret = _psql_prepare_and_run(['-c', query], user=user, host=host, port=port, maintenance_db=maintenance_db, password=password, runas=runas) return ret['retcode'] == 0
_x = lambda s: s if return_password else ''
if not bool(role): log.info( '{0} \'{1}\' could not be found'.format(typ_.capitalize(), name) ) return False
sub_cmd = 'DROP ROLE "{0}"'.format(name) _psql_prepare_and_run( ['-c', sub_cmd], runas=runas, host=host, user=user, port=port, maintenance_db=maintenance_db, password=password)
cmdret = _psql_prepare_and_run(['-f', sqlfile.name], user=user, runas=runas, host=host, port=port, password=password, maintenance_db=dbname) return cmdret
sub_cmd = 'DROP SCHEMA "{0}"'.format(name) _psql_prepare_and_run( ['-c', sub_cmd], runas=user, maintenance_db=dbname, host=db_host, user=db_user, port=db_port, password=db_password)
from __future__ import absolute_import import datetime import os import re import fnmatch import collections import copy import time
import salt.ext.six as six
__func_alias__ = { 'time_': 'time' }
get_version = { 'Linux': linux_cpustats, 'FreeBSD': freebsd_cpustats, }
get_version = { 'Linux': linux_cpuinfo, 'FreeBSD': freebsd_cpuinfo, }
get_version = { 'Linux': linux_diskstats, 'FreeBSD': freebsd_diskstats, }
fstypes.add('*')
selected.add(arg)
fstypes.add(arg)
get_version = { 'Linux': linux_vmstats, 'FreeBSD': freebsd_vmstats, }
get_version = { 'Linux': linux_netstats, 'FreeBSD': freebsd_netstats, }
get_version = { 'Linux': linux_netdev, 'FreeBSD': freebsd_netdev, }
get_version = { 'Linux': linux_version, 'FreeBSD': lambda: __salt__['cmd.run']('sysctl -n kern.version'), }
port = 4505 master_ip = None
if master is not None: tmp_ip = _host_to_ip(master) if tmp_ip is not None: master_ip = tmp_ip
from __future__ import absolute_import import logging
import salt.utils
import salt.ext.six as six
if not __execute_cmd('config -g cfgUserAdmin -o \ cfgUserAdminUserName -i {0} {1}'.format(uid, username)): delete_user(username, uid) return False
if not set_permissions(username, permissions, uid): log.warning('unable to set user permissions') delete_user(username, uid) return False
if not change_password(username, password, uid): log.warning('unable to set user password') delete_user(username, uid) return False
if not __execute_cmd('config -g cfgUserAdmin -o \ cfgUserAdminEnable -i {0} 1'.format(uid)): delete_user(username, uid) return False
if uid is None: user = list_users() uid = user[username]['index']
for i in permissions.split(','): perm = i.strip()
from __future__ import absolute_import import copy import logging try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False
import salt.ext.six as six
import salt.utils from salt.exceptions import CommandExecutionError from salt.utils import locales
__virtualname__ = 'user'
from __future__ import absolute_import import copy as pycopy import difflib import os import yaml
import salt.utils import salt.utils.odict
import salt.ext.six as six
salt '*' schedule.list show_all=True
salt '*' schedule.list show_disabled=False
if job.startswith('__') and not show_all: del schedule[job] continue
if 'enabled' not in schedule[job]: schedule[job]['enabled'] = True
if not show_disabled and not schedule[job]['enabled']: del schedule[job] continue
if schedule[job]['_seconds'] > 0: schedule[job]['seconds'] = schedule[job]['_seconds'] elif 'seconds' in schedule[job]: del schedule[job]['seconds']
del schedule[job]['_seconds']
ret['comment'] = 'Event module not available. Schedule add failed.' ret['result'] = True
ret['comment'] = 'Event module not available. Schedule add failed.'
if 'enabled' not in kwargs: schedule[name]['enabled'] = True
ret['comment'] = 'Event module not available. Schedule add failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
ret['comment'] = 'Event module not available. Schedule save failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
ret['comment'] = 'Event module not available. Schedule enable job failed.'
errors = [] minions = [] for minion in response: minions.append(minion) if not response[minion]: errors.append(minion)
errors = [] minions = [] for minion in response: minions.append(minion) if not response[minion]: errors.append(minion)
try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
def __virtual__(): if not HAS_SQLITE3: return (False, 'The sqlite3 execution module failed to load: the sqlite3 python library is not available.') return True
import re import logging import shlex import yaml
import salt.utils from salt.utils.locales import sdecode as _sdecode from salt.exceptions import SaltInvocationError
import salt.ext.six as six
GUID_REGEX = re.compile(r'{?([0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})}?', re.I)
cmd = ['prlctl', sub_cmd] if args: cmd.extend(_normalize_args(args))
return __salt__['cmd.run'](cmd, runas=runas)
if args is None: args = [] else: args = _normalize_args(args)
return prlctl('list', args, runas=runas)
args = [_sdecode(name)] if kill: args.append('--kill')
return prlctl('stop', args, runas=runas)
args = [_sdecode(name)] args.extend(_normalize_args(command))
return prlctl('exec', args, runas=runas)
info = prlctl('snapshot-list', [name, '--id', snap_id], runas=runas)
if not len(info): raise SaltInvocationError( u'No snapshots for VM "{0}" have ID "{1}"'.format(name, snap_id) )
name = _sdecode(name) snap_name = _sdecode(snap_name)
info = prlctl('snapshot-list', name, runas=runas)
snap_ids = _find_guids(info)
named_ids = [] for snap_id in snap_ids: if snapshot_id_to_name(name, snap_id, runas=runas) == snap_name: named_ids.append(snap_id)
if re.match(GUID_REGEX, snap_name): return snap_name.strip('{}') else: return snapshot_name_to_id(name, snap_name, strict=True, runas=runas)
name = _sdecode(name) if snap_name: snap_name = _validate_snap_name(name, snap_name, runas=runas)
args = [name] if tree: args.append('--tree') if snap_name: args.extend(['--id', snap_name])
res = prlctl('snapshot-list', args, runas=runas)
if names: snap_ids = _find_guids(res)
else: return res
name = _sdecode(name) if snap_name: snap_name = _sdecode(snap_name)
args = [name] if snap_name: args.extend(['--name', snap_name]) if desc: args.extend(['--description', desc])
return prlctl('snapshot', args, runas=runas)
name = _sdecode(name) snap_name = _validate_snap_name(name, snap_name, runas=runas)
args = [name, '--id', snap_name]
return prlctl('snapshot-delete', args, runas=runas)
name = _sdecode(name) snap_name = _validate_snap_name(name, snap_name, runas=runas)
args = [name, '--id', snap_name]
return prlctl('snapshot-switch', args, runas=runas)
import logging import socket import json from distutils.version import LooseVersion
import salt.utils
__virtualname__ = 'zabbix'
for key in kwargs.keys(): if not key.startswith('_'): params.setdefault(key, kwargs[key])
if not isinstance(usrgrps, list): usrgrps = [usrgrps] for usrgrp in usrgrps: params['usrgrps'].append({"usrgrpid": usrgrp})
for k, v in result_json.items(): if isinstance(v, list): result_json[k] += next_page_results[k]
resource = None for field in identifier_fields: if field in data: resource = get_resource(resource_name, data[field], identifier_fields, profile, subdomain, api_key) if resource is not None: break
del __context__['pagerduty_util.resource_cache'][resource_name] return _query(method='POST', action=resource_name, data=data, profile=profile, subdomain=subdomain, api_key=api_key)
import os import shutil import logging import tempfile
import salt.crypt import salt.utils import salt.utils.cloud import salt.config import salt.syspaths import uuid
log = logging.getLogger(__name__)
__func_alias__ = { 'apply_': 'apply' }
pass
import hashlib import logging import os.path import random import signal
import salt.utils from salt.ext.six.moves import range
if hasher == 'sha256': h = hashlib.sha256(password) elif hasher == 'md5': h = hashlib.md5(password) else: return NotImplemented
h.update(r['Salt']) r['Hash'] = h.hexdigest()
#pylint: disable=E0602
from __future__ import absolute_import import logging import json
import salt.utils.boto3 import salt.utils.compat import salt.utils from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
import copy import os import re import logging import json
from salt.modules.cmdmod import _parse_env import salt.utils from salt.exceptions import ( CommandExecutionError, MinionError, SaltInvocationError )
try: import apt.cache import apt.debfile from aptsources import sourceslist HAS_APT = True except ImportError: HAS_APT = False
LP_SRC_FORMAT = 'deb http://ppa.launchpad.net/{0}/{1}/ubuntu {2} main' LP_PVT_SRC_FORMAT = 'deb https://{0}private-ppa.launchpad.net/{1}/{2}/ubuntu' \ ' {3} main'
__virtualname__ = 'pkg'
os.environ.update(DPKG_ENV_VARS)
for name in names: ret[name] = '' pkgs = list_pkgs(versions_as_list=True) repo = ['-o', 'APT::Default-Release={0}'.format(fromrepo)] \ if fromrepo else None
if refresh: refresh_db()
if name in all_virt and name not in pkgs: candidate = '1' else: candidate = ''
if not any( (salt.utils.compare_versions(ver1=x, oper='>=', ver2=candidate, cmp_func=version_cmp) for x in installed) ): ret[name] = candidate
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
ident = re.sub(r' \[.+B\]$', '', ident) ret[ident] = True
if not _latest_version == _version: _refresh_db = True
if not _latest_version == _version: _refresh_db = True
_refresh_db = True
repo = kwargs.get('repo', '') if not fromrepo and repo: fromrepo = repo
version_num = kwargs['version']
if pkg_type == 'repository': pkgstr = '{0}={1}'.format(pkgname, version_num) else: pkgstr = pkgpath
cmd.insert(-1, '--force-yes')
rexp = re.compile('(?m)^Conf '
apt_pkg.init_system()
pass
if (is_ppa and repo_type == 'deb' and source.type == 'deb-src' and source.uri == repo_uri and source.dist == repo_dist):
refresh_db() return ret
from __future__ import absolute_import import copy import fnmatch import itertools import logging import os import re import string
from salt.ext import six from salt.ext.six.moves import zip
import salt.utils import salt.utils.itertools import salt.utils.decorators as decorators import salt.utils.pkg.rpm from salt.exceptions import ( CommandExecutionError, MinionError, SaltInvocationError )
__virtualname__ = 'pkg'
value = value.rstrip('-')
value = value.lstrip('@')
pkginfo = salt.utils.pkg.rpm.pkginfo(**cur) cur = {} if pkginfo is not None: yield pkginfo
if repo and not fromrepo: fromrepo = repo
conf = { 'reposdir': ['/etc/yum/repos.d', '/etc/yum.repos.d'], }
fn = None paths = ('/etc/yum/yum.conf', '/etc/yum.conf') for path in paths: if os.path.exists(path): fn = path break
conf[opt] = [x.strip() for x in cp.get('main', opt).split(',')]
if isinstance(basedir, six.string_types): basedir = [x.strip() for x in basedir.split(',')]
if not basedir: basedir = _get_yum_config_value('reposdir')
if refresh: refresh_db(**kwargs)
break
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
repos = tuple( x for x, y in six.iteritems(list_repos()) if str(y.get('enabled', '1')) == '1' )
cmd.extend(args)
for pkgname in ret[reponame]: sorted_versions = sorted( [_LooseVersion(x) for x in ret[reponame][pkgname]], reverse=True ) ret[reponame][pkgname] = [x.vstring for x in sorted_versions]
list_updates = salt.utils.alias_function(list_upgrades, 'list_updates')
for key, value in pkg_nfo.items(): if key == 'source_rpm': t_nfo['source'] = value else: t_nfo[key] = value
pkg_params = {name: version_num}
current_locks = list_holds(full=_yum() == 'yum')
if key is None: continue
break
target_found = True
for p_type in pkgtypes: ret[p_type].update(set(expanded[p_type]))
pkgs = [x for x in targets if x not in list_pkgs()] if not pkgs: return {}
repofile = '' for repo in repos: if repo == name: repofile = repos[repo]['file']
filerepos = _parse_repo_file(repofile)[1] return filerepos[name]
basedirs = _normalize_basedir(basedir) repos = list_repos(basedirs)
repofile = '' for arepo in repos: if arepo == repo: repofile = repos[arepo]['file']
onlyrepo = True for arepo in six.iterkeys(repos): if arepo == repo: continue if repos[arepo]['file'] == repofile: onlyrepo = False
if onlyrepo: os.remove(repofile) return 'File {0} containing repo {1} has been removed'.format( repofile, repo)
repo_opts = dict( (x, kwargs[x]) for x in kwargs if not x.startswith('__') and x not in ('saltenv',) )
todelete = [] for key in repo_opts: if repo_opts[key] != 0 and not repo_opts[key]: del repo_opts[key] todelete.append(key)
if 'enabled' not in repo_opts: repo_opts['enabled'] = int(str(repo_opts.pop('disabled', False)).lower() != 'true')
if 'mirrorlist' in repo_opts: todelete.append('baseurl') elif 'baseurl' in repo_opts: todelete.append('mirrorlist')
if 'name' in todelete: raise SaltInvocationError('The repo name cannot be deleted')
repos = {} basedirs = _normalize_basedir(basedir) repos = list_repos(basedirs)
repofile = repos[repo]['file'] header, filerepos = _parse_repo_file(repofile)
for key in todelete: if key in six.iterkeys(filerepos[repo].copy()): del filerepos[repo][key]
from __future__ import absolute_import import copy import logging import re import os import time import datetime
import salt.utils from salt.exceptions import ( CommandExecutionError, MinionError)
__virtualname__ = 'pkg'
if not salt.utils.which('zypper'): return (False, "Module zypper: zypper package manager not found") return __virtualname__
self.__xml = False self.__no_lock = False self.__no_raise = False self.__refresh = False
if self.__called: self._reset() self.__called = False
if self.__no_lock: self.__no_lock = not self.__refresh
list_updates = salt.utils.alias_function(list_upgrades, 'list_updates')
if kwargs.get('refresh', True): refresh_db()
if len(names) == 1 and len(ret): return ret[names[0]]
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
for alias in repos_cfg.sections(): repo_meta = _get_repo_info(alias, repos_cfg=repos_cfg)
new_url = _urlparse(url) if not new_url.path:
__zypper__.xml.call('ar', url, repo)
cmd_opt = []
if not added and not cmd_opt: raise CommandExecutionError( 'Specified arguments did not result in modification of repo' )
pkg_params = {name: version_num}
from __future__ import absolute_import import logging import os import re import datetime
import salt.utils import salt.utils.itertools import salt.utils.decorators as decorators import salt.utils.pkg.rpm from salt.ext.six.moves import zip from salt.ext import six
from salt.exceptions import CommandExecutionError, SaltInvocationError
__virtualname__ = 'lowpkg'
cmd.extend(packages)
msg = 'Failed to verify package(s)' if out['stderr']: msg += ': {0}'.format(out['stderr']) raise CommandExecutionError(msg)
if ret['retcode'] > 1: del ret['stdout'] return ret elif not ret['retcode']: return data
cmd.extend(packages)
from __future__ import absolute_import import os import logging
import salt.ext.six as six
import salt.utils
__virtualname__ = 'service'
if krel[0] > 5 or (krel[0] == 5 and krel[1] > 0): if not os.path.exists('/usr/sbin/rcctl'): return __virtualname__
with salt.utils.fopen('/etc/rc', 'r') as handle: lines = handle.readlines()
line = line[len(match.group(1)):] for daemon in start_daemon_parameter_regex.findall(line): daemons_flags[daemon] = True
if available(service): services.append(service)
from __future__ import absolute_import import logging import re
import salt.utils
import stat import os import logging
from __future__ import absolute_import, print_function import logging import os
import salt.output import salt.utils import salt.loader import salt.template from salt.exceptions import CommandExecutionError, SaltRenderError
from salt.runners.winrepo import ( genrepo as _genrepo, update_git_repos as _update_git_repos, PER_REMOTE_OVERRIDES ) from salt.ext import six try: import msgpack except ImportError:
__virtualname__ = 'winrepo'
repo = _get_local_repo_dir(saltenv)
repo = repo.split('\\') definition = name.split('.') repo.extend(definition)
sls_file = '{0}.sls'.format(os.sep.join(repo)) if not os.path.exists(sls_file):
sls_file = '{0}\\init.sls'.format(os.sep.join(repo)) if not os.path.exists(sls_file):
return 'Software definition {0} not found'.format(name)
renderers = salt.loader.render(__opts__, __salt__) config = {}
try: config = salt.template.compile_template( sls_file, renderers, __opts__['renderer'], __opts__['renderer_blacklist'], __opts__['renderer_whitelist'])
from __future__ import absolute_import, print_function import json import logging
import salt.utils.http
__func_alias__ = { 'list_': 'list' }
import os
import salt.utils
return salt.utils.pem_finger(os.path.join(__opts__['pki_dir'], 'minion.pub'), sum_type=__opts__.get('hash_type', 'md5'))
return salt.utils.pem_finger(os.path.join(__opts__['pki_dir'], 'minion_master.pub'), sum_type=__opts__.get('hash_type', 'md5'))
import logging import json
import salt.utils import salt.utils.decorators as decorators
__func_alias__ = { 'list_installed': 'list', 'update_installed': 'update', 'import_image': 'import' }
__virtualname__ = 'imgadm'
continue
result = [] for image in res['stdout'].splitlines(): image = [var for var in image.split(" ") if var] result.append(image[2])
import logging
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import import logging import os import stat
import salt.ext.six as six
import salt.utils
from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.ext.six import string_types, integer_types import salt.utils
__func_alias__ = { 'zip_': 'zip' }
os.setegid(uinfo['gid']) os.seteuid(uinfo['uid'])
if runas: os.seteuid(euid) os.setegid(egid) if exc is not None: raise CommandExecutionError( 'Exception encountered creating zipfile: {0}'.format(exc) )
os.setegid(uinfo['gid']) os.seteuid(uinfo['uid'])
cleaned_files = [] with contextlib.closing(zipfile.ZipFile(zip_file, "r")) as zfile: files = zfile.namelist()
if info.external_attr == 2716663808: source = zfile.read(target) os.symlink(source, os.path.join(dest, target)) continue
if runas: os.seteuid(euid) os.setegid(egid) if exc is not None: raise CommandExecutionError( 'Exception encountered unpacking zipfile: {0}'.format(exc) )
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
count = 100 if not isinstance(trim_output, bool): count = trim_output
SUPPORTED_BSD_LIKE = ['FreeBSD', 'NetBSD', 'OpenBSD']
if line.startswith('bridge name'): continue vals = line.split() if not vals: continue
if len(vals) > 1: brname = vals[0]
import re import os
import salt.utils import salt.utils.mac_utils from salt.exceptions import CommandExecutionError, SaltInvocationError
rexp = re.compile('(?m)^ [*|-] ' r'([^ ].*)[\r\n].*\(([^\)]+)')
rexp = re.compile('(?m)^ [*] ' r'([^ ].*)[\r\n].*\(([^\)]+)')
rexp1 = re.compile('(?m)^ [*|-] ' r'([^ ].*)[\r\n].*restart*')
to_ignore = name.rsplit('-', 1)[0]
rexp = re.compile('(?m)^ ["]?' r'([^,|\s].*[^"|\n|,])[,|"]?')
cmd = ['softwareupdate', '--set-catalog', url]
cmd = ['softwareupdate', '--clear-catalog']
import logging import os import re
import salt.ext.six as six import salt.utils from salt.ext.six import string_types from salt.exceptions import CommandExecutionError import salt.utils.systemd import string
__virtualname__ = 'sysctl'
regex = re.compile(r'^{0}\s+=\s+{1}$'.format(re.escape(name), re.escape(value)))
if not os.path.isfile(config): try: with salt.utils.fopen(config, 'w+') as _fh:
comps = [i.strip() for i in line.split('=', 1)]
if isinstance(comps[1], string_types) and ' ' in comps[1]: comps[1] = re.sub(r'\s+', '\t', comps[1])
if isinstance(value, string_types) and ' ' in value: value = re.sub(r'\s+', '\t', value)
import logging
import salt.utils
__virtualname__ = 'group'
from __future__ import absolute_import import errno import functools import logging import os import re import shutil import time import tempfile
import salt.defaults.exitcodes import salt.utils import salt.utils.systemd from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext import six
if preserve_state \ and orig_state == 'stopped' \ and state(name) != 'stopped': stop(name)
time.sleep(5)
if ret: run(name, 'touch \'{0}\''.format(SEED_MARKER), python_shell=False)
list_ = salt.utils.alias_function(list_running, 'list_')
return start(name)
destroy = salt.utils.alias_function(remove, 'destroy')
if 'index' in kwargs: pull_opts.append('--dkr-index-url={0}'.format(kwargs['index']))
from __future__ import absolute_import import os import base64 import logging
import salt.utils
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
file_name = '{artifact_id}-{version}{classifier}.{packaging}'.format( artifact_id=artifact_id, version=version, packaging=packaging, classifier=__get_classifier_url(classifier))
artifact_metadata_url = '{artifactory_url}/{repository}/{group_url}/{artifact_id}/maven-metadata.xml'.format( artifactory_url=artifactory_url, repository=repository, group_url=group_url, artifact_id=artifact_id) log.debug('artifact_metadata_url=%s', artifact_metadata_url) return artifact_metadata_url
latest_version_url = '{artifactory_url}/api/search/latestVersion?g={group_url}&a={artifact_id}&repos={repository}'.format( artifactory_url=artifactory_url, repository=repository, group_url=group_url, artifact_id=artifact_id) log.debug('latest_version_url=%s', latest_version_url) return latest_version_url
import os import os.path import logging
import salt.utils
__func_alias__ = { 'list_': 'list' }
info.insert(2, '')
ret['error'] = 'This package does not seem to exist' return ret
import os import re import logging from salt.ext.six.moves import zip import salt.ext.six as six
HAS_AUGEAS = False try: from augeas import Augeas as _Augeas HAS_AUGEAS = True except ImportError: pass
import salt.utils from salt.exceptions import SaltInvocationError
__virtualname__ = 'augeas'
cmd, arg = command.split(' ', 1)
repo_name: my_repo
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError import salt.utils.http
HAS_LIBS = False try: import github import github.PaginatedList import github.NamedUser from github.GithubException import UnknownObjectException
return True
if issue.get('pull_request'): continue issue_id = issue.get('id') if output == 'full': ret[issue_id] = issue else: ret[issue_id] = _format_issue(issue)
return result['dict']
next_page = False continue
page_number = link_info.split('>')[0].split('&page=')[1]
next_page = False
import logging
import salt.utils
import logging from salt.serializers import json
import salt.utils.compat import salt.utils.odict as odict
from __future__ import absolute_import
from salt.ext.six.moves.urllib.parse import urljoin as _urljoin import salt.ext.six.moves.http_client
import salt.utils.http
__func_alias__ = { 'list_': 'list' }
if not key: query_params['recurse'] = 'True' function = 'kv/' else: function = 'kv/{0}'.format(key)
from __future__ import absolute_import import os import re import time import logging
import salt.utils from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if serialized.startswith("="): serialized = serialized[1:].strip()
return [tuple(items.split("=")) for items in opt]
if len(out) == 1 and 'restore status' in out[0].lower(): return {'restore_status': out[0]}
#pylint: disable=E0602
import logging
from salt.ext.six import string_types import salt.utils.odict as odict
if isinstance(instances, str) or isinstance(instances, six.text_type): instances = [instances] conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
if isinstance(instances, str) or isinstance(instances, six.text_type): instances = [instances] conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
import copy import logging import re
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkg'
if blocked and unsatisfied: ret['blocked'] = blocked
if refresh: refresh_db()
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
cmd = 'emerge-webrsync -q' if salt.utils.which('emerge-delta-webrsync'): cmd = 'emerge-delta-webrsync -q' return __salt__['cmd.retcode'](cmd, python_shell=False) == 0
cmd = 'emerge-webrsync -q' if salt.utils.which('emerge-delta-webrsync'): cmd = 'emerge-delta-webrsync -q' return __salt__['cmd.retcode'](cmd, python_shell=False) == 0
from __future__ import absolute_import import re import logging
import salt.utils
from __future__ import absolute_import import re import logging try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False
try:
__virtualname__ = 'gnome'
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import import json import os
import salt.utils
elif isinstance(value, six.string_types): if value.lower() == 'none': return None return value else: return None
data = slave if core is None else {core: {'data': slave}}
if not _is_master() and _get_none_or_value(host) is None: err = [ 'solr.pre_indexing_check can only be called by "master" minions'] return _get_return_dict(False, err)
import logging import re
import salt.utils
__virtualname__ = 'varnish'
break
#pylint: disable=E0602
from __future__ import absolute_import import datetime import logging import json import sys import email.mime.multipart
import salt.utils.odict as odict
from __future__ import absolute_import
from salt.exceptions import CommandExecutionError import salt.utils
from salt.exceptions import CommandExecutionError from salt.exceptions import SaltInvocationError import logging
import os import re
from __future__ import absolute_import import os import logging import hashlib import glob import random import ctypes import tempfile import yaml import re import datetime import ast
import salt.utils import salt.exceptions import salt.ext.six as six from salt.utils.odict import OrderedDict
try: import M2Crypto HAS_M2 = True except ImportError: HAS_M2 = False
text = _match.group(0) break
pem_body = ''.join(pem_body.split())
ret = pem_header+'\n' for i in range(0, len(pem_body), 64): ret += pem_body[i:i+64]+'\n' ret += pem_footer+'\n'
kwargs['public_key'] = get_public_key(kwargs['public_key']).replace('\n', '')
for ignore in list(_STATE_INTERNAL_KEYWORDS) + ['listen_in', 'preqrequired']: kwargs.pop(ignore, None)
kwargs.update(signing_policy)
cert.set_version(kwargs['version'] - 1)
if 'public_key' not in kwargs and 'csr' not in kwargs: kwargs['public_key'] = kwargs['signing_private_key']
extval = kwargs.get(extname) or kwargs.get(extlongname) or \ csrexts.get(extname) or csrexts.get(extlongname)
from __future__ import absolute_import import functools import copy import logging import os import pipes import time import traceback
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.utils import vt
return None
state = __salt__['{0}.state'.format(container_type)]
from __future__ import absolute_import from salt.ext.six.moves import zip
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
conn_args = {} for arg in ['host', 'port', 'db', 'password']: if arg in connection_args: conn_args[arg] = connection_args[arg]
server.ping()
server.ping()
from __future__ import absolute_import import copy import logging import time import traceback
import salt.crypt import salt.payload import salt.utils import salt.utils.network import salt.utils.event from salt.exceptions import SaltClientError
import salt.ext.six as six
time.sleep(0.5) return event_ret
if not m_data: return
pass
cmd = 'dockerng.ps' docker_hosts = get('*', cmd)
for containers in six.itervalues(docker_hosts): host = containers.pop('host') host_ips = []
import logging import sys import xml.etree.ElementTree as ET
import salt.utils import salt.utils.cloud as suc from salt.exceptions import SaltInvocationError
if isinstance(bricks, str): bricks = [bricks]
if device_vg and len(bricks) > 1: raise SaltInvocationError('Block device backend volume does not ' + 'support multiple bricks')
root = _gluster_xml('volume status {0}'.format(name)) if not _gluster_ok(root): return None
running = (volinfo[target]['status'] == '1')
log.error('Volume {0} must be stopped before deletion'.format(target)) return False
from __future__ import absolute_import from __future__ import unicode_literals import sys import logging
try:
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'reg'
return instr.encode('mbcs')
return instr
return obj
_, res = SendMessageTimeout(HWND_BROADCAST, WM_SETTINGCHANGE, 0, 0, SMTO_ABORTIFHUNG, 5000) return not bool(res)
registry = Registry() hkey = registry.hkeys[local_hive] key_path = local_key access_mask = registry.registry_32[use_32bit_registry]
key_list = [] key_list = _traverse_registry_tree(hkey, key_path, key_list, access_mask) key_list.append(r'{0}'.format(key_path))
from __future__ import absolute_import, print_function
import salt.utils
if old_value is not None: __salt__['file.sed'](makeconf, '^{0}=.*'.format(var), '')
if old_value is not None: __salt__['file.sed'](makeconf, value, '', limit=var)
value = value.replace('\\', '') if setval is None: return False return value in setval.split()
from __future__ import absolute_import, print_function import copy import fnmatch import json import logging import os import shutil import sys import tarfile import tempfile import time
import salt.config import salt.payload import salt.state import salt.utils import salt.utils.jid import salt.utils.url from salt.exceptions import SaltInvocationError
import salt.ext.six as six
__context__['retcode'] = 0
__salt__['cmd.run']('attrib -R "{0}"'.format(notify_path))
__salt__['cmd.run']('attrib -R "{0}"'.format(notify_path))
__opts__['test'] = orig_test return ret
if queue: _wait(kwargs.get('__pub_jid')) else: conflict = running(concurrent) if conflict: __context__['retcode'] = 1 return conflict
__opts__['environment'] = saltenv __opts__['pillarenv'] = pillarenv
__salt__['cmd.run'](['attrib', '-R', cache_file], python_shell=False)
__opts__['test'] = orig_test
pass
__opts__['test'] = orig_test return ret
__opts__['test'] = orig_test return ret
__opts__['test'] = orig_test if errors: __context__['retcode'] = 1 return errors return high_
__opts__['test'] = orig_test return ret
__salt__['saltutil.refresh_modules']()
__salt__['saltutil.refresh_modules']()
import os
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandNotFoundError
__virtualname__ = 'service'
from __future__ import absolute_import from json import JSONEncoder, loads
return loads(_MssqlEncoder().encode({'resultset': cur.fetchall()}))['resultset']
return (('Could not run the query', ), (str(e), ))
return len(tsql_query("SELECT database_id FROM sys.databases WHERE NAME='{0}'".format(database_name), **kwargs)) == 1
return len(tsql_query(query='sp_helprole "{0}"'.format(role), as_dict=True, **kwargs)) == 1
return len(tsql_query(query="SELECT name FROM sys.syslogins WHERE name='{0}'".format(login), **kwargs)) == 1
if 'database' not in kwargs: return False
return len(tsql_query(query="SELECT name FROM sysusers WHERE name='{0}'".format(username), **kwargs)) == 1
if 'database' not in kwargs: return False if user_exists(username, **kwargs): return False
from __future__ import absolute_import import copy import errno import glob import logging import os import re import shlex
import salt.utils.itertools import salt.utils.systemd from salt.exceptions import CommandExecutionError from salt.ext import six
__virtualname__ = 'service'
__context__[contextkey] = True
ret = _default_runlevel()
ret.update(set( [x for x in _get_sysv_services() if _sysv_enabled(x)] )) return sorted(ret)
ret.update(set( [x for x in _get_sysv_services() if not _sysv_enabled(x)] )) return sorted(ret)
return sorted(ret)
import os.path
import salt.utils import salt.ext.six as six from salt.exceptions import CommandExecutionError
__virtualname__ = 'lvm'
if not pvdisplay(device): cmd.append(device) elif not override: raise CommandExecutionError('Device "{0}" is already an LVM physical volume.'.format(device))
return True
for device in devices: if not pvdisplay(device): raise CommandExecutionError('Device "{0}" was not affected.'.format(device))
for device in devices: if pvdisplay(device): raise CommandExecutionError('Device "{0}" was not affected.'.format(device))
import logging log = logging.getLogger(__file__)
from napalm import get_network_driver HAS_NAPALM = True
import os import re import subprocess import sys
import salt.utils
try: for link in listdir: path = dirpath + link readlink = os.readlink(path) filenames = []
kernel_current = __salt__['cmd.run']('uname -a') for kernel in kernel_versions: if kernel in kernel_current: kernel_restart = False break
is_oneshot = True
from __future__ import absolute_import import logging import uuid import re
import salt.utils
import salt.modules.cmdmod
val = '\n'.join([v for v in val.split('\n') if not v.startswith('#')])
record = { 'handle': handle, 'description': dmi_raw.pop(0).strip(), 'type': int(htype) }
if not clean: dmi.append(record) continue
dmi_data = _dmi_data(dmi_raw, clean, fields) if len(dmi_data): record['data'] = dmi_data dmi.append(record) elif not clean: dmi.append(record)
val = _dmi_cast(key, line.strip(), clean) if val is not None: key_data[1].append(val)
except: pass
return False
return not re.search(r'manufacturer|to be filled|available|asset|^no(ne|t)', val, flags=re.IGNORECASE)
from __future__ import absolute_import
super(SendMsgBot, self).__init__(jid, password)
for handler in logging.root.handlers: handler.addFilter(SleekXMPPMUC())
import logging
import salt.utils import salt.modules.cmdmod import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_': 'list', }
return salt.utils.which('zfs')
man = salt.utils.which('man') if not man: return False
cmd = 'which zfs'
if properties: optlist = [] for prop in properties.keys():
cmd = '{0} {1}'.format(cmd, name)
res = __salt__['cmd.run_all'](cmd)
if res['retcode'] != 0: ret[name] = res['stderr'] if 'stderr' in res else res['stdout'] else: ret[name] = 'created'
if recursive:
if ltype: cmd = '{0} -t {1}'.format(cmd, ltype)
if recursive: cmd = '{0} -r'.format(cmd) if depth: cmd = '{0} -d {1}'.format(cmd, depth)
properties = properties.split(',')
if name: cmd = '{0} {1}'.format(cmd, name)
if properties: optlist = [] for prop in properties.keys():
if not _check_features(): ret['error'] = 'bookmarks are not supported' return ret
if not snapshot: ret['error'] = 'one or more snapshots must be specified'
if err == 'usage:': break ret[csnap][ctag] = res['stderr']
if not snapshot: ret['error'] = 'one or more snapshots must be specified'
if err == 'usage:': break ret[csnap][ctag] = res['stderr']
if not snapshot: ret['error'] = 'one or more snapshots must be specified'
if properties: optlist = [] for prop in properties.keys():
if err == 'usage:': break ret[csnap] = res['stderr']
if not dataset: ret['error'] = 'one or more snapshots must be specified'
for ds in dataset: for prop in properties.keys():
if depth: cmd = '{0} -d {1}'.format(cmd, depth) elif recursive: cmd = '{0} -r'.format(cmd)
fields = fields.split(',')
if source: cmd = '{0} -s {1}'.format(cmd, source)
if ltype: cmd = '{0} -t {1}'.format(cmd, ltype)
cmd = '{0} {1}'.format(cmd, properties)
cmd = '{0} {1}'.format(cmd, ' '.join(dataset))
from __future__ import absolute_import import os import stat import logging
import salt.ext.six as six
if key_name is None: key_name = _format_dict_key(args, plugin)
if isinstance(command, dict): plugin = next(six.iterkeys(command)) args = command[plugin] else: plugin = command args = ''
stat_f = os.path.join(PLUGINDIR, plugin) execute_bit = stat.S_IXUSR & os.stat(stat_f)[stat.ST_MODE] if execute_bit: ret.append(plugin)
import salt.utils.http
from __future__ import absolute_import import os
import salt.ext.six as six
keystone.region_name: 'RegionOne'
import logging
import salt.utils.openstack.nova as suon
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list' }
__virtualname__ = 'nova'
import os import re import shutil import logging
import salt.utils import tempfile import salt.utils.locales import salt.utils.url from salt.ext.six import string_types from salt.exceptions import CommandExecutionError, CommandNotFoundError
__func_alias__ = { 'list_': 'list' }
return False
cached_requirements = __salt__['cp.cache_file']( requirements, saltenv )
raise ValueError('Timeout cannot be a float')
pip_version = version(pip_bin)
if salt.utils.compare_versions(ver1=pip_version, oper='>=', ver2='1.4'): cmd.append('--pre')
if not (entry == '.' or entry.startswith(('file://', '/'))): match = egg_match.search(entry)
raise CommandExecutionError( 'You must specify an egg for this editable' )
raise ValueError('Timeout cannot be a float')
continue
continue
from napalm import get_network_driver HAS_NAPALM = True
import os
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
if not os.path.isfile(config): try: with salt.utils.fopen(config, 'w+') as _fh:
if apply_change is True: assign(name, value) return 'Updated and applied' return 'Updated'
from __future__ import absolute_import import logging
try: import salt.utils.openstack.neutron as suoneu HAS_NEUTRON = True except NameError as exc: HAS_NEUTRON = False
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list' }
from __future__ import absolute_import import logging import os import re
from salt.exceptions import CommandExecutionError import salt.utils
import salt.ext.six as six
cmd = __salt__['cmd.run_all']('racadm {0} {1}'.format(command, modswitch))
cmd = __salt__['cmd.run_all']('racadm {0} {1}'.format(command, modswitch))
if not __execute_cmd('config -g cfgUserAdmin -o ' 'cfgUserAdminUserName -i {0} {1}' .format(uid, username), host=host, admin_username=admin_username, admin_password=admin_password): delete_user(username, uid) return False
if not set_permissions(username, permissions, uid): log.warning('unable to set user permissions') delete_user(username, uid) return False
if not change_password(username, password, uid): log.warning('unable to set user password') delete_user(username, uid) return False
if not __execute_cmd('config -g cfgUserAdmin -o ' 'cfgUserAdminEnable -i {0} 1'.format(uid)): delete_user(username, uid) return False
if uid is None: user = list_users() uid = user[username]['index']
for i in permissions.split(','): perm = i.strip()
slot = str(slot) return slots[slot]['slotname']
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'acl'
vals['type'] = 'acl' if comps[0] == 'default': vals['type'] = 'default' comps.pop(0)
if comps[0] == 'user' and not comps[1]: comps[1] = user elif comps[0] == 'group' and not comps[1]: comps[1] = group vals[comps[0]] = comps[1]
import logging import re import os import bz2
import salt.utils from salt.exceptions import SaltInvocationError
__virtualname__ = 'cyg'
if not os.path.exists(cyg_cache_dir): os.mkdir(cyg_cache_dir) elif os.path.exists(cyg_setup_path): os.remove(cyg_setup_path)
if packages is not None: args.append('--packages {pkgs}'.format(pkgs=packages)) if not _check_cygwin_installed(cyg_arch): _run_silent_cygwin(cyg_arch=cyg_arch)
if not _check_cygwin_installed(cyg_arch): LOG.debug('Cygwin ({0}) not installed,\ could not update'.format(cyg_arch)) return False
from __future__ import absolute_import
self.o.maxtimeout = config['api_login_timeout'] self.o.wait_for_rsp(timeout=1)
from __future__ import absolute_import import logging
import json import salt.ext.six import salt.ext.six.moves.http_client from salt.ext.six.moves.urllib.parse import urljoin as _urljoin
import salt.utils.http
#pylint: disable=E0602
from __future__ import absolute_import import logging import time
import salt.ext.six as six
MAX_ATTEMPTS = 30 for i in range(MAX_ATTEMPTS): if exists( table_name, region, key, keyid, profile ): return True else:
MAX_ATTEMPTS = 30 for i in range(MAX_ATTEMPTS): if not exists(table_name, region, key, keyid, profile): return True else:
from __future__ import absolute_import, print_function import datetime import json import logging import time
from salt.exceptions import SaltInvocationError import salt.utils.http
log.error('Wrong type, skipping {0}'.format(kwarg))
days=5 \ CN='My Little CA' \ C=US \ ST=Utah \ L=Salt Lake City \ O=Saltstack \ emailAddress=pleasedontemail@example.com
Created Private Key: "/etc/pki/my_little/certs/www.example.com.key Created CSR for "www.example.com": "/etc/pki/my_little/certs/www.example.com.csr"
Created Certificate for "www.example.com": /etc/pki/my_little/certs/www.example.com.crt"
Created Private Key: "/etc/pki/my_little/certs//DBReplica_No.1.key." Created CSR for "DBReplica_No.1": "/etc/pki/my_little/certs/DBReplica_No.1.csr."
Created Certificate for "DBReplica_No.1": "/etc/pki/my_little/certs/DBReplica_No.1.crt"
cert_type=client
Created Certificate for "DBReplica_No.1": "/etc/pki/my_little/certs/DBReplica_No.1.crt"
cert_type=server
cert_type=server type_ext=True
Certificate "MasterDBReplica_No.2" already exists
cert_type=server type_ext=True
cert_type=server type_ext=True
cert_type=server cert_filename="something_completely_different"
import os import time import calendar import logging import math import binascii import salt.utils from salt._compat import string_types from salt.ext.six.moves import range as _range from datetime import datetime
return True
subject = '/'
key = OpenSSL.crypto.load_privatekey( OpenSSL.crypto.FILETYPE_PEM, fic2.read()) bits = key.bits()
cert = OpenSSL.crypto.X509() cert.set_version(2)
date_fmt = '%Y%m%d%H%M%SZ'
import copy
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six
__virtualname__ = 'pkgutil'
if salt.utils.is_true(kwargs.get('removed')): return {}
for name in names: ret[name] = ''
if refresh: refresh_db()
ret[name] = version_rev
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs, **kwargs)[0]
from __future__ import absolute_import
import salt.utils import salt.syspaths from salt.exceptions import SaltInvocationError
import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'gpg'
return salt.utils.which('gpg')
ret['message'] = 'Secret key for {0} deleted\n'.format(fingerprint)
with salt.utils.flopen(filename, 'rb') as _fp: _contents = _fp.read() result = gpg.encrypt(_contents, recipients, passphrase=gpg_passphrase, output=output)
import salt.utils import salt.utils.mac_utils from salt.exceptions import SaltInvocationError from salt.ext.six.moves import range
import logging import os
import salt.utils import salt.utils.decorators as decorators from salt.exceptions import CommandNotFoundError
__virtualname__ = 'service'
if __grains__['os'] == 'FreeBSD': return __virtualname__ return (False, 'The freebsdservice execution module cannot be loaded: only available on FreeBSD systems.')
return False
try: from gentoolkit.eclean import search, clean, cli, exclude as excludemod HAS_GENTOOLKIT = True except ImportError: pass
__virtualname__ = 'gentoolkit'
clean_me = search.findPackages(None, destructive=destructive, package_names=package_names, time_limit=time_limit, exclude=exclude, pkgdir=search.pkgdir)
import salt.utils import salt.exceptions
__func_alias__ = { 'set_': 'set' }
import time import logging
from salt.exceptions import CommandExecutionError
try: import ldap import ldap.modlist HAS_LDAP = True except ImportError: HAS_LDAP = False
__virtualname__ = 'ldap'
if HAS_LDAP: return __virtualname__ return (False, 'The ldapmod execution module cannot be loaded: ' 'ldap config not present.')
import fnmatch import os import re import logging
import salt.utils from salt.ext.six import string_types from salt.exceptions import SaltInvocationError, CommandExecutionError import salt.ext.six as six
__virtualname__ = 'ports'
ret = {name: {'old': old.get(name, ''), 'new': new.get(name, '')}}
pkg = next(iter(configuration)) conf_ptr = configuration[pkg]
from distutils.version import LooseVersion try:
import salt.utils
command = 'Add-WindowsFeature' management_tools = '' if LooseVersion(__grains__['osversion']) >= LooseVersion('6.2'): command = 'Install-WindowsFeature' management_tools = '-IncludeManagementTools'
import logging
try: import ethtool HAS_ETHTOOL = True except ImportError: HAS_ETHTOOL = False
__virtualname__ = 'ethtool'
from __future__ import absolute_import import os import logging import fnmatch
import salt.minion import salt.fileclient import salt.utils import salt.utils.url import salt.crypt import salt.transport from salt.exceptions import CommandExecutionError
import salt.ext.six as six
if template not in salt.utils.templates.TEMPLATE_REGISTRY: raise CommandExecutionError( 'Attempted to render file paths with unavailable engine ' '{0}'.format(template) )
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
__context__[contextkey] = result
saltenv = env
saltenv = env
saltenv = env
if path_cached: path_hash = hash_file(path) path_cached_hash = hash_file(path_cached)
return _client().cache_local_file(path)
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
saltenv = env
import copy import fnmatch import logging import os import signal import sys
try: import esky from esky import EskyVersionError HAS_ESKY = True except ImportError: HAS_ESKY = False from salt.ext import six from salt.ext.six.moves.urllib.error import URLError
try: salt_SIGKILL = signal.SIGKILL except AttributeError: salt_SIGKILL = signal.SIGTERM
ret = __salt__['event.fire']({}, 'module_refresh')
log.trace('refresh_modules waiting for module refresh to complete') eventer.get_event(tag='/salt/minion/minion_mod_complete', wait=30)
my-minion: arg: - 30 fun: test.sleep jid: 20160503150049487736 pid: 9601 ret: tgt: my-minion tgt_type: glob user: root
return
return
return signal_job(jid, salt_SIGKILL)
ret = [] for data in running(): ret.append(signal_job(data['jid'], salt_SIGKILL)) return ret
if expr_form == 'list' and len(tgt) == seen: break
__grains__ = grains
m.opts['grains'] = grains
import copy import re import logging
import salt.utils from salt.exceptions import CommandExecutionError, MinionError
__virtualname__ = 'pkg'
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
for name in names: ret[name] = ''
if len(names) == 1: return ret[names[0]] return ret
available_version = salt.utils.alias_function(latest_version, 'available_version')
import os import logging import re
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
log = logging.getLogger(__name__)
__func_alias__ = { 'list_': 'list' }
__virtualname__ = 'raid'
if not os.path.exists(device): msg = "Device {0} doesn't exist!" raise CommandExecutionError(msg.format(device))
if __grains__.get('os_family') == 'Debian': cfg_file = '/etc/mdadm/mdadm.conf' else: cfg_file = '/etc/mdadm.conf'
if isinstance(devices, str): devices = devices.split(',')
import os import stat import os.path import logging import struct
try: import win32api import win32file import win32security import win32con from pywintypes import error as pywinerror HAS_WINDOWS_MODULES = True except ImportError: HAS_WINDOWS_MODULES = False
import salt.utils
__virtualname__ = 'file'
if token_privileges[privilege] == privilege_attrs: log.debug( 'The requested privilege {0} is already in the ' 'requested state.'.format(privilege_name) ) return True
if exc.winerror == 1332: return '' else: raise
if exc.winerror == 1332: return '' else: raise
try: userSID, domainName, objectType = win32security.LookupAccountName(None, user) except pywinerror: err += 'User does not exist\n'
try: groupSID, domainName, objectType = win32security.LookupAccountName(None, pgroup) except pywinerror: err += 'Group does not exist\n'
win32security.SetNamedSecurityInfo( path, win32security.SE_FILE_OBJECT, win32security.OWNER_SECURITY_INFORMATION + win32security.GROUP_SECURITY_INFORMATION, userSID, groupSID, None, None )
win32security.SetNamedSecurityInfo( path, win32security.SE_FILE_OBJECT, win32security.OWNER_SECURITY_INFORMATION, userSID, None, None, None )
try: groupSID, domainName, objectType = win32security.LookupAccountName(None, group) except pywinerror: err += 'Group does not exist\n'
attributes = {}
intAttributes = win32file.GetFileAttributes(path)
if not os.path.exists(path): return 'File/Folder not found: {0}'.format(path)
if force: file_attributes = win32api.GetFileAttributes(path) win32api.SetFileAttributes(path, win32con.FILE_ATTRIBUTE_NORMAL)
os.remove(path)
os.rmdir(path)
remove(item, force)
os.rmdir(path)
win32api.SetFileAttributes(path, file_attributes)
if sys.getwindowsversion().major < 6: raise SaltInvocationError('Symlinks are only supported on Windows Vista or later.')
src = os.path.normpath(src) link = os.path.normpath(link)
reparse_data = _get_reparse_data(path)
if not reparse_data: return False
header_parser = struct.Struct('L') ReparseTag, = header_parser.unpack(reparse_data[:header_parser.size]) if not ReparseTag & 0xA000FFFF == 0xA000000C: return False else: return True
path = os.path.normpath(path)
data_parser = struct.Struct('LHHHHHHL') ReparseTag, ReparseDataLength, Reserved, SubstituteNameOffset, \ SubstituteNameLength, PrintNameOffset, \ PrintNameLength, Flags = data_parser.unpack(reparse_data[:data_parser.size])
target = win32file.GetLongPathName(target)
if exc.winerror == 2: return target raise
#pylint: disable=E0602
import logging
import salt.utils.compat import salt.utils.odict as odict from salt.exceptions import SaltInvocationError
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if ttl is None: ttl = 60 status = _zone.add_record(_type, name, _value, ttl, identifier) return _wait_for_sync(status.id, conn, wait_for_sync)
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
if retry_on_rate_limit and 'Throttling' == e.code: log.debug('Throttled by AWS API.') time.sleep(2) rate_limit_retries -= 1
import re
import salt.utils
__func_alias__ = { 'id_': 'id', 'reload_': 'reload', }
return True
from __future__ import absolute_import import os import re import sys import shutil import subprocess
import yaml import jinja2 import jinja2.exceptions from xml.dom import minidom import salt.ext.six as six
import salt.utils import salt.utils.files import salt.utils.templates import salt.utils.validate.net from salt.exceptions import CommandExecutionError, SaltInvocationError
JINJA = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.join(salt.utils.templates.TEMPLATE_DIRNAME, 'virt') ) )
config_data = __salt__['config.option']('virt.nic', {}).get( profile_name, None )
if isinstance(config_data, dict): append_dict_profile_to_interface_list(config_data)
attributes['source'] = attributes.pop(type_)
disk_name = next(six.iterkeys(diskp[0])) disk_type = diskp[0][disk_name]['format'] disk_file_name = '{0}.{1}'.format(disk_name, disk_type)
mode = (0o0777 ^ mask) & 0o0666 os.chmod(img_dest, mode)
pass
flags = libvirt.VIR_DOMAIN_MEM_MAXIMUM if config: flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
return ret1 == ret2 == 0
flags = libvirt.VIR_DOMAIN_VCPU_MAXIMUM if config: flags = flags | libvirt.VIR_DOMAIN_AFFECT_CONFIG
mem -= 256 for vm_ in list_domains(): dom = _get_domain(vm_) if dom.ID() > 0: mem -= dom.info()[2] / 1024 return mem
return dom.reset(0) == 0
return False
pass
return False
return False
return False
cputime_percent = (1.0e-7 * cputime / host_cpus) / vcpus
for vm_ in list_active_vms(): info[vm_] = _info(vm_)
if getattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_MIGRATABLE', False): flags += libvirt.VIR_CONNECT_BASELINE_CPU_MIGRATABLE else: raise ValueError
flags += libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES
with salt.utils.fopen('/usr/share/libvirt/cpu_map.xml', 'r') as cpu_map: cpu_map = minidom.parse(cpu_map)
import copy import logging import os import re from distutils.version import LooseVersion as _LooseVersion
import salt.utils import salt.utils.files import salt.utils.itertools import salt.utils.url from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.ext import six
value_regex = None
to_expand = '~' + str(user) if user else '~'
if not isinstance(identity, list): identity = [identity]
return ['--local']
return ['--file', _git_config(cwd, user)]
kwargs = salt.utils.clean_kwargs(**kwargs) format_ = kwargs.pop('format', None) if kwargs: salt.utils.invalid_kwargs(kwargs)
return True
return _git_run(command, cwd=cwd, runas=user, ignore_retcode=ignore_retcode, redirect_stderr=True)['stdout']
command.extend(['--', filename])
all_ = kwargs.pop('all', False)
if result['retcode'] == 1: return None ret = result['stdout'].splitlines() if all_: return ret else: try: return ret[-1] except IndexError: return ''
ref_key = 'new tags' \ if new_ref_type == 'tag' \ else 'new branches' ret.setdefault(ref_key, []).append(ref_name)
ret.setdefault('updated branches', {})[ref_name] = \ {'old': old_sha, 'new': new_sha}
ret.setdefault('updated tags', []).append(ref_name)
shared = str(shared).lower()
return False
return False
worktree_data = dict([(x, '') for x in tracked_data_points])
tags_found = _git_tag_points_at(cwd, wt_ptr['HEAD'], user) if tags_found: wt_ptr['tags'] = tags_found
break
salt.utils.files.process_read_exception(exc, path)
wt_loc = toplevel
if wt_detached: tags_found = _git_tag_points_at(cwd, wt_head, user) if tags_found: wt_ptr['tags'] = tags_found
action = str(action)
salt myminion git.submodule /path/to/repo/sub/repo init=True salt myminion git.submodule /path/to/repo/sub/repo update opts='--init'
salt myminion git.submodule /path/to/repo/sub/repo update opts='--rebase'
salt myminion git.submodule /path/to/repo/sub/repo add opts='https://mydomain.tld/repo.git'
log.error('Running \'git --version\' returned no stdout') __context__[contextkey] = 'unknown'
return _git_run(command, cwd=cwd, runas=user, ignore_retcode=ignore_retcode, redirect_stderr=True)['stdout']
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'sysctl'
max_primes = [500, 1000, 2500, 5000]
test_command = 'sysbench --test=cpu --cpu-max-prime={0} run' result = None ret_val = {}
thread_yields = [100, 200, 500, 1000] thread_locks = [2, 4, 8, 16]
test_command = 'sysbench --num-threads=64 --test=threads ' test_command += '--thread-yields={0} --thread-locks={1} run ' result = None ret_val = {}
test_command = 'sysbench --num-threads=250 --test=mutex ' test_command += '--mutex-num={0} --mutex-locks={1} --mutex-loops={2} run ' result = None ret_val = {}
memory_oper = ['read', 'write'] memory_scope = ['local', 'global']
test_command = 'sysbench --num-threads=64 --test=memory ' test_command += '--memory-oper={0} --memory-scope={1} ' test_command += '--memory-block-size=1K --memory-total-size=32G run ' result = None ret_val = {}
test_modes = ['seqwr', 'seqrewr', 'seqrd', 'rndrd', 'rndwr', 'rndrw']
test_command = 'sysbench --num-threads=16 --test=fileio ' test_command += '--file-num=32 --file-total-size=1G --file-test-mode={0} ' result = None ret_val = {}
for mode in test_modes: key = 'Mode: {0}'.format(mode)
run_command = (test_command + 'prepare').format(mode) __salt__['cmd.run'](run_command)
run_command = (test_command + 'run').format(mode) result = __salt__['cmd.run'](run_command) ret_val[key] = _parser(result)
run_command = (test_command + 'cleanup').format(mode) __salt__['cmd.run'](run_command)
from __future__ import absolute_import import logging
import logging
try:
log = logging.getLogger(__name__)
__func_alias__ = { 'get_': 'get', 'set_': 'set', 'rm_': 'rm', 'ls_': 'ls' }
from datetime import datetime
import salt.utils import salt.utils.mac_utils from salt.exceptions import SaltInvocationError
time_format = _get_date_time_format(time) dt_obj = datetime.strptime(time, time_format)
import contextlib import functools import glob import logging import os import re import tempfile
import copy import json import logging
import salt.utils from salt.exceptions import CommandExecutionError, MinionError import salt.ext.six as six from salt.ext.six.moves import zip
__virtualname__ = 'pkg'
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
return pkg_info['versions']['stable'] or pkg_info['versions']['devel']
available_version = salt.utils.alias_function(latest_version, 'available_version')
if taps: if not isinstance(taps, list): taps = [taps]
from __future__ import absolute_import import errno import os import locale import logging from distutils.version import LooseVersion
import salt.ext.six as six from salt.ext.six.moves.urllib.parse import urlparse as _urlparse try: import msgpack except ImportError: import msgpack_pure as msgpack
from salt.exceptions import (CommandExecutionError, SaltInvocationError, SaltRenderError) import salt.utils import salt.syspaths from salt.exceptions import MinionError
__virtualname__ = 'pkg'
ret = {} for name in names: ret[name] = ''
if salt.utils.is_true(kwargs.get('refresh', True)): refresh_db(saltenv)
for name in names: latest_installed = '0' latest_available = '0'
available_version = salt.utils.alias_function(latest_version, 'available_version')
if any([salt.utils.is_true(kwargs.get(x)) for x in ('removed', 'purge_desired')]): return {}
pkg_info = _get_package_info(key, saltenv=saltenv) if not pkg_info: continue for pkg_ver in pkg_info.keys(): if pkg_info[pkg_ver]['full_name'] == pkg_name: val = pkg_ver
reg_software.update({d_name: d_vers})
cached_files = __salt__['cp.cache_dir']( winrepo_source_dir, saltenv, include_pat='*.sls' ) genrepo(saltenv=saltenv) return cached_files
cached_hash_file = __salt__['cp.cache_file'](source_hash, saltenv)
items = source_hash.split('=', 1)
if not name and not pkgs: return 'Must pass a single package or a list of packages'
pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs, **kwargs)[0]
pkg_params = { name: { 'version': kwargs.get('version'), 'extra_install_flags': kwargs.get('extra_install_flags') } }
old = list_pkgs(saltenv=saltenv)
changed = [] latest = [] for pkg_name, options in six.iteritems(pkg_params):
pkginfo = _get_package_info(pkg_name, saltenv=saltenv)
if not pkginfo: log.error('Unable to locate package {0}'.format(pkg_name)) ret[pkg_name] = 'Unable to locate package {0}'.format(pkg_name) continue
version_num = '' if options: version_num = options.get('version', False)
if version_num == old.get(pkg_name) \ or (pkg_name in old and old[pkg_name] == 'Not Found'): ret[pkg_name] = {'current': version_num} continue
elif version_num not in pkginfo: log.error('Version {0} not found for package ' '{1}'.format(version_num, pkg_name)) ret[pkg_name] = {'not found': version_num} continue
installer = pkginfo[version_num].get('installer', False) cache_dir = pkginfo[version_num].get('cache_dir', False) cache_file = pkginfo[version_num].get('cache_file', False)
if not installer: log.error('No installer configured for version {0} of package ' '{1}'.format(version_num, pkg_name)) ret[pkg_name] = {'no installer': version_num} continue
if installer.startswith(('salt:', 'http:', 'https:', 'ftp:')):
if cache_file and cache_file.startswith('salt:'):
cached_file = __salt__['cp.is_cached'](cache_file, saltenv) if not cached_file: cached_file = __salt__['cp.cache_file'](cache_file, saltenv)
if not cached_file: log.error('Unable to cache {0}'.format(cache_file)) ret[pkg_name] = { 'failed to cache cache_file': cache_file } continue
cached_pkg = __salt__['cp.is_cached'](installer, saltenv) if not cached_pkg: cached_pkg = __salt__['cp.cache_file'](installer, saltenv)
if not cached_pkg: log.error('Unable to cache {0}'.format(installer)) ret[pkg_name] = {'unable to cache': installer} continue
cached_pkg = installer
cached_pkg = cached_pkg.replace('/', '\\') cache_path, _ = os.path.split(cached_pkg)
if pkginfo[version_num].get('use_scheduler', False):
new = list_pkgs(saltenv=saltenv)
if latest: for pkg_name in latest: if old.get(pkg_name, 'old') == new.get(pkg_name, 'new'): ret[pkg_name] = {'current': new[pkg_name]}
difference = salt.utils.compare_dicts(old, new)
ret.update(difference)
return {}
if not name and not pkgs: return 'Must pass a single package or a list of packages'
pkg_params = __salt__['pkg_resource.parse_targets'](name, pkgs, **kwargs)[0]
old = list_pkgs(saltenv=saltenv)
changed = [] for target in pkg_params:
pkginfo = _get_package_info(target, saltenv=saltenv)
if not pkginfo: log.error('Unable to locate package {0}'.format(name)) ret[target] = 'Unable to locate package {0}'.format(target) continue
if not version: version_num = _get_latest_pkg_version(pkginfo) else: version_num = version
uninstaller = pkginfo[version_num].get('uninstaller')
if not uninstaller: uninstaller = pkginfo[version_num].get('installer')
if not uninstaller: log.error('Error: No installer or uninstaller configured ' 'for package {0}'.format(name)) ret[target] = {'no uninstaller': version_num} continue
if uninstaller.startswith(('salt:', 'http:', 'https:', 'ftp:')):
cached_pkg = __salt__['cp.is_cached'](uninstaller) if not cached_pkg: cached_pkg = __salt__['cp.cache_file'](uninstaller)
if not cached_pkg: log.error('Unable to cache {0}'.format(uninstaller)) ret[target] = {'unable to cache': uninstaller} continue
cached_pkg = uninstaller
cached_pkg = cached_pkg.replace('/', '\\') cache_path, _ = os.path.split(cached_pkg)
expanded_cached_pkg = str(os.path.expandvars(cached_pkg))
if pkginfo[version_num].get('use_scheduler', False):
new = list_pkgs(saltenv=saltenv) tries = 0 difference = salt.utils.compare_dicts(old, new)
ret.update(difference)
from __future__ import absolute_import import os.path
import salt.utils import salt.utils.itertools import salt.utils.mac_utils from salt.exceptions import SaltInvocationError
__func_alias__ = {'list_': 'list'}
__virtualname__ = 'pkgutil'
#pylint: disable=E0602
import logging from salt.exceptions import SaltInvocationError from time import time, sleep
import salt.ext.six as six try: import boto import boto.rds2 logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
import os import re
import salt.utils
__virtualname__ = 'kmod'
ret.append('.'.join(comps[:comps.index('ko')]))
return [None]
import logging import os
import salt.utils from salt.ext.six.moves import range
log = logging.getLogger(__name__)
if old == '*': return True
comps = line.split() path = comps[0] mask = comps[1]
ls = salt.utils.alias_function(list_tab, 'ls')
mask = str(mask).upper()
for item in mask.split(','): if item not in _MASK_TYPES: return 'Invalid mask type: {0}' . format(item)
return comdat['stderr']
mask = str(mask).upper()
for item in mask.split(','): if item not in _MASK_TYPES: return 'Invalid mask type: {0}' . format(item)
return comdat['stderr']
#pylint: disable=E0602
try: #pylint: disable=unused-import import boto import boto.sns #pylint: enable=unused-import logging.getLogger('boto').setLevel(logging.CRITICAL) HAS_BOTO = True except ImportError: HAS_BOTO = False
import salt.utils import salt.ext.six as six
if item in ['constraint']: cmd += [item_type]
if item in ['constraint']: if not isinstance(extra_args, (list, tuple)) or '--full' not in extra_args: cmd += ['--full']
if item in ['constraint']: if isinstance(item_type, six.string_types): cmd += [item_type]
if item not in ['constraint']: cmd += [item_id] if isinstance(item_type, six.string_types): cmd += [item_type]
if item in ['constraint']: extra_args = extra_args + ['id={0}'.format(item_id)] cmd += extra_args
from __future__ import absolute_import, print_function import logging
import logging import json import os try:
import salt.utils import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
__func_alias__ = { 'list_vms': 'list' }
__virtualname__ = 'vmadm'
if res['stderr'].startswith('Successfully created VM'): return res['stderr'][24:]
vmcfg = {} kwargs = salt.utils.clean_kwargs(**kwargs) for k, v in kwargs.iteritems(): vmcfg[k] = v
vmcfg = {} kwargs = salt.utils.clean_kwargs(**kwargs) for k, v in kwargs.iteritems(): vmcfg[k] = v
from __future__ import absolute_import try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False import copy import logging
import salt.utils import salt.ext.six as six from salt.exceptions import CommandExecutionError
__virtualname__ = 'user'
from __future__ import absolute_import import logging import os import tempfile
import salt.utils import salt.utils.decorators as decorators
import salt.ext.six as six
import re import logging
import salt.utils from salt.ext import six
__virtualname__ = 'firewall'
import os import stat import logging
import salt.utils import salt.utils.decorators as decorators from salt.utils.odict import OrderedDict
man = salt.utils.which('man') if not man: return False
if line.startswith('pool') and line.endswith('write'): continue if line.endswith('bandwidth'): continue
for zp in res['stdout'].splitlines(): zp = zp.split("\t") zp_data = {}
for zp in res['stdout'].splitlines(): zp = zp.split("\t") zp_data = {}
if exists(zpool): ret[zpool] = 'storage pool already exists' return ret
res = __salt__['cmd.run_all'](cmd, python_shell=False)
if res['retcode'] != 0: ret[zpool] = res['stderr'] if 'stderr' in res else res['stdout'] else: ret[zpool] = 'created'
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
ret[zpool] = {}
for vdev in vdevs: if os.path.isfile(vdev): ret[vdev] = 'existed' else: dlist.append(vdev)
for vdev in vdevs: if not os.path.isfile(vdev): ret[vdev] = 'failed' else: if vdev not in ret: ret[vdev] = 'created' return ret
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
expand = kwargs.get('expand', False)
if not exists(zpool): ret[zpool] = 'storage pool does not exist' return ret
from __future__ import absolute_import, print_function import json import logging import time
import salt.utils.http
from __future__ import absolute_import
import salt.utils from salt.exceptions import CommandExecutionError
return None
import re import time import datetime
from salt.ext.six.moves import map from salt.exceptions import CommandNotFoundError
import salt.utils
BSD = ('OpenBSD', 'FreeBSD')
if __grains__['os_family'] == 'RedHat': output = _cmd('at', '-l') else: output = _cmd('atq')
if output == '': return {'jobs': jobs}
for line in output.splitlines(): job_tag = ''
output = _cmd('at', '-d', ' '.join(opts)) if output is None: return '\'at.atrm\' is not available.'
binary = salt.utils.which('at') if not binary: return '\'at.at\' is not available.'
output = _cmd('at', '-c', str(jobid))
from __future__ import absolute_import import logging import re
import salt.utils
if len(types) == 1: return ret[types[0]] else: for key in ret.keys(): if key not in types: del ret[key]
import logging import subprocess
from salt import utils
if _TRAFFICLINE: cmd = _traffic_line('-S') else: cmd = _traffic_ctl('server', 'stop')
if _TRAFFICLINE: cmd = _traffic_line('-U') else: cmd = _traffic_ctl('server', 'start')
import glob import os import re import itertools import fnmatch
import salt.utils import salt.modules.cmdmod import salt.utils.systemd
__virtualname__ = 'service'
for utmp in '/var/run/utmp', '/run/utmp': try: result[os.stat(utmp).st_mtime] = utmp except Exception: pass return result[sorted(result).pop()]
ret = _default_runlevel()
import salt.utils from salt.exceptions import CommandExecutionError
__virtualname__ = 'desktop'
import os
import yaml import logging
import salt.utils
return {'shell': os.environ.get('SHELL', '/bin/sh')}
import salt.utils
import glob import logging import re
import salt.utils import salt.utils.decorators as decorators
import salt.modules.cmdmod
from __future__ import absolute_import import logging
from salt.exceptions import SaltSystemExit import salt.utils import salt.modules.vsphere
ret = salt.modules.vsphere.system_info(host=host, username=user, password=password)
continue
raise SaltSystemExit('Cannot complete login due to an incorrect user name or password.')
from __future__ import absolute_import
import salt.utils import salt.modules.nxos
import logging
import salt.utils
from __future__ import absolute_import import itertools import os import json import socket import sys import re import platform import logging import locale import salt.exceptions
import salt.log import salt.utils import salt.utils.network
import salt.modules.cmdmod import salt.modules.smbios
import salt.ext.six as six
try:
known_vendors = ['nvidia', 'amd', 'ati', 'intel'] gpu_classes = ('vga compatible controller', '3d controller')
sysctl = salt.utils.which('sysctl') arch = salt.utils.which('arch') cmds = {}
grains = {} grains['cpu_flags'] = []
grains = {'mem_total': 0} if osdata['kernel'] == 'Linux': meminfo = '/proc/meminfo'
tot_bytes = win32api.GlobalMemoryStatusEx()['TotalPhys'] grains['mem_total'] = int(tot_bytes / (1024 ** 2))
grains = dict() if osdata['kernel'] != 'Windows': return grains
grains['virtual'] = 'kvm'
grains = {'virtual': 'physical'}
skip_cmds = ('AIX',)
if osdata['kernel'] in skip_cmds: _cmds = ()
if salt.utils.is_windows() or 'systemd-detect-virt' in cmd or 'prtdiag' in cmd: continue failed_commands.add(command)
break
if output: grains['virtual'] = output.lower() break
failed_commands.discard('lspci') failed_commands.discard('dmidecode')
grains['virtual_subtype'] = 'Xen Dom0'
grains['virtual_subtype'] = 'Xen HVM DomU'
grains['virtual_subtype'] = 'Xen PV DomU'
grains['virtual_subtype'] = 'Xen Dom0'
grains['virtual_subtype'] = 'Xen PV DomU'
(osfullname, _) = osinfo.Name.split('|', 1) osfullname = osfullname.strip()
data[match.group(1)] = re.sub(r'\\([$"\'\\`])', r'\1', match.group(2))
(grains['kernel'], grains['nodename'], grains['kernelrelease'], version, grains['cpuarch'], _) = platform.uname()
try:
grains['os'] = grains['osfullname'] = 'Solaris' grains['osrelease'] = ''
grains['osrelease'] = grains['kernelrelease'].split('-')[0]
grains['os_family'] = _OS_FAMILY_MAP.get(grains['os'], grains['os'])
osarch = sorted(archinfo, key=archinfo.get, reverse=True)
grains.update(_hw_data(grains))
grains.update(_virtual(grains)) grains.update(_ps(grains))
if grains['os_family'] == "RedHat": grains['osmajorrelease'] = grains['osrelease'].split('.', 1)[0]
grains['locale_info']['defaultlanguage'] = 'unknown' grains['locale_info']['defaultencoding'] = 'unknown'
global __FQDN__ grains = {}
ret = {} ifaces = _get_interfaces() for face in ifaces: if 'hwaddr' in ifaces[face]: ret[face] = ifaces[face]['hwaddr'] return {'hwaddr_interfaces': ret}
return {'path': os.environ.get('PATH', '').strip()}
return {'pythonversion': list(sys.version_info)}
return {'pythonpath': sys.path}
return {'pythonexecutable': sys.executable}
salt_path = os.path.abspath(os.path.join(__file__, os.path.pardir)) return {'saltpath': os.path.dirname(salt_path)}
from salt.version import __version__ return {'saltversion': __version__}
try: import zmq
from salt.version import __version_info__ return {'saltversioninfo': list(__version_info__)}
grains['manufacturer'] = sysinfo['Manufacturer'] grains['productname'] = sysinfo['Product'] grains['uuid'] = sysinfo['UUID']
return {'master': __opts__.get('master', '')}
import contextlib import logging import hashlib import os import shutil import ftplib from tornado.httputil import parse_response_start_line, HTTPInputError
if os.path.isfile(destdir): os.remove(destdir) os.makedirs(destdir)
if not path.endswith('/'): path = path + '/'
del dirs[:]
if not os.path.isabs(url_data.path): raise CommandExecutionError( 'Path \'{0}\' is not absolute'.format(url_data.path) ) return url_data.path
write_body = [False]
return
log.error( 'Failed to render template with error: {0}'.format( data['data'] ) ) return ''
dest = self._extrn_path(url, saltenv, cachedir=cachedir) makedirs = True
path = salt.utils.url.unescape(path)
dest2check = dest if not dest2check: rel_path = self._check_proto(path)
if os.path.isdir(dest): salt.utils.rm_rf(dest) fn_ = salt.utils.fopen(dest, 'wb+')
apikey: asdff7896asdh789 sharedsecret: saltybacon driver: gogrid
import pprint import logging import time import hashlib
import salt.config as config import salt.utils.cloud from salt.exceptions import SaltCloudSystemExit, SaltCloudException
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'gogrid', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
service_url: amazonaws.com
endpoint: myendpoint.example.com:1138/services/Cloud
ssh_gateway: gateway.example.com
ssh_gateway_port: 22
ssh_gateway_username: root
ssh_gateway_private_key: /path/to/key.pem
ssh_gateway_password: ExamplePasswordHere
userdata_file: /etc/salt/my-userdata-file
from __future__ import absolute_import import os import sys import stat import time import uuid import pprint import logging import yaml
import hmac import hashlib import binascii import datetime import base64 import msgpack import json import re import decimal
import salt.utils from salt import syspaths from salt._compat import ElementTree as ET import salt.utils.http as http import salt.utils.aws as aws import salt.loader from salt.template import compile_template
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudException, SaltCloudSystemExit, SaltCloudConfigError, SaltCloudExecutionTimeout, SaltCloudExecutionFailure )
try: import Crypto
log = logging.getLogger(__name__)
access_key_id, secret_access_key, token = aws.creds(provider)
t = datetime.datetime.utcnow()
result.status_code
if not isinstance(ssh_gateway, str): return None
ssh_gateway_config = {'ssh_gateway': ssh_gateway}
ssh_gateway_config['ssh_gateway_port'] = config.get_cloud_config_value( 'ssh_gateway_port', vm_, __opts__, default=None, search_global=False )
ssh_gateway_config['ssh_gateway_user'] = config.get_cloud_config_value( 'ssh_gateway_username', vm_, __opts__, default=None, search_global=False )
ssh_gateway_config['ssh_gateway_key'] = config.get_cloud_config_value( 'ssh_gateway_private_key', vm_, __opts__, default=None, search_global=False )
ssh_gateway_config['ssh_gateway_password'] = config.get_cloud_config_value( 'ssh_gateway_password', vm_, __opts__, default=None, search_global=False )
if avz not in zones: raise SaltCloudException( 'The specified availability zone isn\'t valid in this region: ' '{0}\n'.format( avz ) )
elif zones[avz] != 'available': raise SaltCloudException( 'The specified availability zone isn\'t currently available: ' '{0}\n'.format( avz ) )
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
_associate_eip_with_interface(eni_id, associate_public_ip, vm_=vm_)
param.update({key: str(data).lower()})
raise SaltCloudSystemExit( 'The request_instance action must be called with -a or --action.' )
spot_prefix = 'LaunchSpecification.'
spot_prefix = ''
exc_info_on_loglevel=logging.DEBUG
if not rd_data: err_msg = 'There was an error querying EC2 for the root device ' \ 'of image id {0}. Empty response.'.format(image_id) raise SaltCloudSystemExit(err_msg)
dev_index = dev_list.index(rd_name)
params[ '{0}BlockDeviceMapping.{1}.DeviceName'.format( spot_prefix, dev_index ) ] = rd_name
termination_key = '{0}BlockDeviceMapping.{1}.Ebs.DeleteOnTermination'.format(spot_prefix, dev_index) params[termination_key] = str(set_del_root_vol_on_destroy).lower()
if 'Ebs.VolumeType' not in ex_blockdevicemappings[dev_index]: type_key = '{0}BlockDeviceMapping.{1}.Ebs.VolumeType'.format(spot_prefix, dev_index) params[type_key] = rd_type
exc_info_on_loglevel=logging.DEBUG
if spot_config: sir_id = data[0]['spotInstanceRequestId']
return False
return False
log.info('Spot instance status: {0}'.format( data[0]['status']['message'] )) return None
log.error('Spot instance request resulted in state \'{0}\'. ' 'Nothing else we can do here.') return False
params = {'Action': 'CancelSpotInstanceRequests', 'SpotInstanceRequestId.1': sir_id} data = aws.query(params, location=location, provider=provider, opts=__opts__, sigver='4')
raise SaltCloudSystemExit( 'The query_instance action must be called with -a or --action.' )
time.sleep(1) continue
time.sleep(1) continue
return False
return False
destroy(vm_['name'])
raise SaltCloudSystemExit( 'The wait_for_instance action must be called with -a or --action.' )
time.sleep(60)
if not salt.utils.cloud.wait_for_port(ip_address, port=445, timeout=ssh_connect_timeout): raise SaltCloudSystemExit( 'Failed to connect to remote windows host' )
if not use_winrm:
else:
winrm_port = config.get_cloud_config_value( 'winrm_port', vm_, __opts__, default=5986 )
if not salt.utils.cloud.wait_for_port(ip_address, port=winrm_port, timeout=ssh_connect_timeout): raise SaltCloudSystemExit( 'Failed to connect to remote windows host (winrm)' )
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'ec2', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
vm_['private_key'] = key_filename
vm_['gateway'] = get_ssh_gateway_config(vm_)
if keyname(vm_) is None: raise SaltCloudSystemExit( 'The required \'keyname\' configuration setting is missing from the ' '\'ec2\' driver.' )
if isinstance(data, str): log.error('Error requesting instance: {0}'.format(data)) return {}
vm_['instance_id_list'] = [] for instance in data: vm_['instance_id_list'].append(instance['instanceId'])
queue_instances(vm_['instance_id_list'])
data = query_instance(vm_)
log.info('Created node {0}'.format(vm_['name']))
ret = instance.copy()
node = _get_node(instance_id=vm_['instance_id']) ret.update(node)
delvols_on_destroy = kwargs.get('del_all_vols_on_destroy', None)
if instance_id is None: return { 'Error': 'A valid instance_id or resource_id was not specified.' }
continue
continue
time.sleep(1) continue
time.sleep(0.5)
if not locations: locations = [get_location()]
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
kwargs['size'] = '10'
if 'encrypted' in kwargs and 'snapshot' not in kwargs: params['Encrypted'] = kwargs['encrypted']
if wait_to_finish: salt.utils.cloud.run_func_until_ret_arg(fun=describe_volumes, kwargs={'volume_id': volume_id}, fun_call=call, argument_being_watched='status', required_argument_response='available')
if wait_to_finish: salt.utils.cloud.run_func_until_ret_arg(fun=describe_snapshots, kwargs={'snapshot_id': snapshot_id}, fun_call=call, argument_being_watched='status', required_argument_response='completed')
if 'snapshot_ids' in kwargs: kwargs['snapshot_id'] = kwargs['snapshot_ids']
from __future__ import absolute_import import os import os.path import time import logging import pprint import base64 import salt.cache import salt.config as config import salt.utils.cloud from salt.exceptions import SaltCloudSystemExit
cache = None storconn = None compconn = None netconn = None webconn = None resconn = None
log = logging.getLogger(__name__)
if data is None: return {}
pass
source_image = VirtualHardDisk(uri=vm_['image']) img_ref = None if win_installer: os_type = 'Windows' else: os_type = 'Linux'
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'azure', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
from __future__ import absolute_import
import salt.utils.cloud import salt.config as config
from salt.utils.openstack import pyrax as suop
id: wFGEwgregeqw3435gDger key: GDE43t43REGTrkilg43934t34qT43t4dgegerGEgg location: cn-qingdao driver: aliyun
from __future__ import absolute_import import time import json import pprint import logging import hmac import uuid import sys import base64 from hashlib import sha1
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
optional = [ 'InstanceName', 'InternetChargeType', 'InternetMaxBandwidthIn', 'InternetMaxBandwidthOut', 'HostName', 'Password', 'SystemDisk.Category', ]
result = query(params) return result['InstanceId']
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'aliyun', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
return False
destroy(vm_['name'])
ret = salt.utils.cloud.bootstrap(vm_, __opts__) ret.update(data.__dict__)
stringToSign = 'GET&%2F&' + percent_encode(canonicalizedQueryString[1:])
if params: parameters.update(params)
signature = _compute_signature(parameters, access_key_secret) parameters['Signature'] = signature
time.sleep(0.5)
if 'Code' in items or len(items['Images']['Image']) == 0: raise SaltCloudNotFound('The specified image could not be found.')
from __future__ import absolute_import from random import randint from re import findall import pprint import logging import time import os.path import subprocess
import salt.utils import salt.utils.cloud import salt.utils.xmlutil import salt.utils.vmware from salt.exceptions import SaltCloudSystemExit
import salt.config as config
try: from pyVmomi import vim HAS_PYVMOMI = True except Exception: HAS_PYVMOMI = False
try: from requests.packages.urllib3 import disable_warnings disable_warnings() except Exception: pass
try: import six except ImportError: HAS_SIX = False
log = logging.getLogger(__name__)
if adapter_type: log.error("Cannot change type of '{0}' to '{1}'. Not changing type".format(network_adapter.deviceInfo.label, adapter_type)) edited_network_adapter = network_adapter
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.virtualSharing
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.physicalSharing
scsi_spec.device.sharedBus = vim.vm.device.VirtualSCSIController.Sharing.noSharing
ide_controllers[device.key] = len(device.device)
network_spec = _add_new_network_adapter_helper(network_adapter_label, network_name, adapter_type, switch_type, container_ref) adapter_mapping = _set_network_adapter_mapping(devices['network'][network_adapter_label]) device_specs.append(network_spec) nics_map.append(adapter_mapping)
scsi_controller_properties = devices['scsi'][scsi_controller_label] scsi_spec = _add_new_scsi_controller_helper(scsi_controller_label, scsi_controller_properties, bus_number) device_specs.append(scsi_spec) bus_number += 1
ide_spec = _add_new_ide_controller_helper(ide_controller_label, None, bus_number) device_specs.append(ide_spec) bus_number += 1
octets = ip_address.split('.') if len(octets) != 4: return False
for i, octet in enumerate(octets):
return False
first_octet, second_octet, third_octet, fourth_octet = octets
if first_octet < 1: return False elif first_octet > 223: return False elif first_octet == 127: return False
if first_octet == 169 and second_octet == 254: return False
for octet in (second_octet, third_octet, fourth_octet): if (octet < 0) or (octet > 255): return False return True
if snapshot.childSnapshotList: ret = _get_snapshots(snapshot.childSnapshotList, current_snapshot, snapshot_path) if current_snapshot: return ret snapshots.update(ret)
if vm.config.template: status = 'VMware tools cannot be updated on a template' return status
if vm.guest.toolsStatus == "toolsOk": status = 'VMware tools is already up to date' return status
if vm.summary.runtime.powerState != "poweredOn": status = 'VM must be powered on to upgrade tools' return status
if vm.guest.toolsStatus in ["toolsNotRunning", "toolsNotInstalled"]: status = 'VMware tools is either not running or not installed' return status
_get_si()
inv = salt.utils.vmware.get_inventory(_get_si())
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'vmware', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
container_ref = None if datacenter: datacenter_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.Datacenter, datacenter) container_ref = datacenter_ref if datacenter_ref else None
reloc_spec = vim.vm.RelocateSpec()
config_spec = vim.vm.ConfigSpec()
clone_spec = vim.vm.CloneSpec( template=template, location=reloc_spec, config=config_spec )
pod_spec = vim.storageDrs.PodSelectionSpec(storagePod=datastore_cluster_ref)
si = _get_si()
recommended_datastores = si.content.storageResourceManager.RecommendDatastores(storageSpec=storage_spec)
task = object_ref.Clone(folder_ref, vm_name, clone_spec) salt.utils.vmware.wait_for_task(task, vm_name, 'clone', 5, 'info')
exc_info_on_loglevel=logging.DEBUG
if not clone_type and power: task = new_vm_ref.PowerOn() salt.utils.vmware.wait_for_task(task, vm_name, 'power', 5, 'info')
datacenter_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.Datacenter, datacenter_name) if datacenter_ref: return {datacenter_name: 'datacenter already exists'}
si = _get_si()
cluster_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.ClusterComputeResource, cluster_name) if cluster_ref: return {cluster_name: 'cluster already exists'}
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
si = _get_si()
log.warning('You can only set either memdump or quiesce to True. Setting quiesce=False') quiesce = False
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
task = host_ref.Destroy_Task()
task = host_ref.parent.Destroy_Task()
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
datastore_cluster_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.StoragePod, datastore_cluster_name) if datastore_cluster_ref: return {datastore_cluster_name: 'datastore cluster already exists'}
exc_info_on_loglevel=logging.DEBUG
user: myuser@pam or myuser@pve password: mypassword url: hypervisor.domain.tld driver: proxmox verify_ssl: True
from __future__ import absolute_import import time import pprint import logging
import salt.ext.six as six import salt.utils
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
ret[name]['config'] = get_vmconfig( ret[name]['vmid'], ret[name]['node'], ret[name]['type'] )
nodes = query('get', 'nodes')
private_ips = [] public_ips = []
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'proxmox', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
if 'ip_address' in vm_: ip_address = str(vm_['ip_address']) elif 'public_ips' in data:
if not wait_for_created(data['upid'], timeout=300): return {'Error': 'Unable to create {0}, command timed out'.format(name)}
if not start(name, vmid, call='action'): log.error('Node {0} ({1}) failed to start!'.format(name, vmid)) raise SaltCloudExecutionFailure
log.error('Wrong VM type. Valid options are: qemu, openvz (proxmox3) or lxc (proxmox4)') raise SaltCloudExecutionFailure
vm_['host'] = config.get_cloud_config_value( 'default_host', get_configured_provider(), __opts__, search_global=False )
log.error('No host given to create this VM on') raise SaltCloudExecutionFailure
vmhost = vm_['host'] newnode['vmid'] = newid
newnode['hostname'] = vm_['name'] newnode['ostemplate'] = vm_['image']
for prop in 'cpus', 'disk', 'ip_address', 'nameserver', 'password', 'swap', 'poolid', 'storage':
newnode['hostname'] = vm_['name'] newnode['ostemplate'] = vm_['image']
if 'disk' in vm_: log.warning('The "disk" option is not supported for LXC hosts and was ignored')
if 'gw' in vm_: newnode['net0'] = newnode['net0'] + ',gw=' + vm_['gw']
for prop in 'acpi', 'cores', 'cpu', 'pool', 'storage', 'sata0', 'ostype', 'ide2', 'net0':
salt.utils.cloud.fire_event( 'event', 'requesting instance', 'salt/cloud/{0}/requesting'.format(vm_['name']), {'kwargs': newnode}, )
data = query('get', 'nodes/{0}/{1}/{2}/config'.format(node, node_type, vmid))
if get_vm_status(vmid=vmobj['vmid'])['status'] != 'stopped': stop(name, vmobj['vmid'], 'action')
if not wait_for_state(vmobj['vmid'], 'stopped'): return {'Error': 'Unable to stop {0}, command timed out'.format(name)}
time.sleep(1)
from __future__ import absolute_import import json import os import logging import copy import time from pprint import pformat
import salt.utils
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
import salt.ext.six as six
log = logging.getLogger(__name__)
return salt.runner.RunnerClient(_master_opts())
timeout = __FUN_TIMEOUT.get( fun,
profile = vm_.get( 'lxc_profile', vm_.get('container_profile', None))
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
from __future__ import absolute_import import logging
import salt.utils
import salt.utils.cloud import salt.config as config
log = logging.getLogger(__name__)
user: MYLOGIN apikey: JVkbSJDGHSDKUKSDJfhsdklfjgsjdkflhjlsdfffhgdgjkenrtuinv driver: softlayer
from __future__ import absolute_import import logging import time
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
try: import SoftLayer HAS_SLLIBS = True except ImportError: HAS_SLLIBS = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'softlayer', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
name = name.split('.')[0]
from __future__ import absolute_import import copy import logging import pprint import time import yaml
import salt.config as config from salt.exceptions import SaltCloudSystemExit import salt.utils.cloud
log = logging.getLogger(__name__)
if name not in nodes: return {} salt.utils.cloud.cache_node(nodes[name], __active_provider_name__, __opts__) return nodes[name]
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'azure', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
system_config.domain_join = None system_config.win_rm = None
cleanup_services = config.get_cloud_config_value( 'cleanup_services', get_configured_provider(), __opts__, search_global=False, default=False ) if cleanup_services: log.debug('Deleting service {0}'.format(service_name))
get_storage = show_storage
get_storage_keys = show_storage_keys
get_disk = show_disk
get_service_certificate = show_service_certificate
get_management_certificate = show_management_certificate
get_input_endpoint = show_input_endpoint
get_deployment = show_deployment
get_affinity_group = show_affinity_group
get_storage_container = show_storage_container
get_storage_container_metadata = show_storage_container_metadata
get_storage_container_acl = show_storage_container_acl
get_blob_service_properties = show_blob_service_properties
get_blob_properties = show_blob_properties
from __future__ import absolute_import import logging import os import pprint import time
import salt.config as config from salt.exceptions import ( SaltCloudConfigError, SaltCloudExecutionFailure, SaltCloudExecutionTimeout, SaltCloudNotFound, SaltCloudSystemExit ) from salt.utils import is_true
import salt.utils.cloud
try:
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'opennebula', vm_['profile']) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
return False
destroy(vm_['name'])
time.sleep(0.5)
password: letmein apikey: 901d3f579h23c8v73q9
password: USE_KEYRING
ignore_cidr: 192.168.50.0/24
from __future__ import absolute_import import os import logging import socket import pprint
try: from libcloud.compute.base import NodeState HAS_LIBCLOUD = True except ImportError: HAS_LIBCLOUD = False
import salt.utils
import salt.utils.cloud import salt.utils.pycrypto as sup import salt.config as config from salt.utils import namespaced_function from salt.exceptions import ( SaltCloudConfigError, SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: from netaddr import all_matching_cidrs HAS_NETADDR = True except ImportError: HAS_NETADDR = False
log = logging.getLogger(__name__)
return False
raise SaltCloudSystemExit( 'The request_instance action must be called with -a or --action.' )
avz = config.get_cloud_config_value( 'availability_zone', vm_, __opts__, default=None, search_global=False ) if avz is not None: kwargs['ex_availability_zone'] = avz
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'openstack', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
data, vm_ = request_instance(vm_)
vm_['instance_id'] = data.id
exc_info_on_loglevel=logging.DEBUG
return False
return
pass
destroy(vm_['name'])
user: MYLOGIN apikey: JVkbSJDGHSDKUKSDJfhsdklfjgsjdkflhjlsdfffhgdgjkenrtuinv driver: softlayer_hw
from __future__ import absolute_import import logging import time import decimal
import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudSystemExit
try: import SoftLayer HAS_SLLIBS = True except ImportError: HAS_SLLIBS = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'softlayer_hw', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
port_speed = config.get_cloud_config_value( 'port_speed', vm_, __opts__, default=273 ) kwargs['prices'].append({'id': port_speed})
bandwidth = config.get_cloud_config_value( 'bandwidth', vm_, __opts__, default=1800 ) kwargs['prices'].append({'id': bandwidth})
#response = conn.verifyOrder(kwargs)
exc_info_on_loglevel=logging.DEBUG
'ssh_connect_timeout', vm_, __opts__, 900
name = name.split('.')[0]
from __future__ import absolute_import import logging import pprint import re import time import datetime
import salt.utils.cloud
log = logging.getLogger(__name__)
LASTCALL = int(time.mktime(datetime.datetime.now().timetuple()))
response = _query('linode', 'boot', args={'LinodeID': linode_id, 'ConfigID': config_id})['DATA'] boot_job_id = response['JobID']
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'linode', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
datacenter_id = 2
update_linode(node_id, update_args={'Label': name}) log.debug('Set name for {0} - was linode{1}.'.format(name, node_id))
private_ip_assignment = get_private_ip(vm_) if private_ip_assignment: create_private_ip(node_id)
ssh_interface = _get_ssh_interface(vm_)
if ssh_interface == 'private_ips' and private_ip_assignment is False: create_private_ip(node_id) private_ip_assignment = True
config_id = create_config(kwargs={'name': name, 'linode_id': node_id, 'root_disk_id': root_disk_id, 'swap_disk_id': swap_disk_id})['ConfigID']
boot(kwargs={'linode_id': node_id, 'config_id': config_id, 'check_running': False})
if ssh_interface == 'private_ips': vm_['ssh_host'] = data['private_ips'][0] else: vm_['ssh_host'] = data['public_ips'][0]
vm_['password'] = get_password(vm_)
ret = salt.utils.cloud.bootstrap(vm_, __opts__)
kernel_id = 138
access_key: 0e604a2c-aea6-4081-acb2-e1d1258ef95c token: be8fd96b-04eb-4d39-b6ba-a9edbcf17f12 driver: scaleway
from __future__ import absolute_import import json import logging import pprint import time
from salt.ext.six.moves import range import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
ret = {} for node in items['servers']: ret[node['name']] = {} for item in node: value = node[item] ret[node['name']][item] = value return ret
if server_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'scaleway', server_['profile'], vm_=server_) is False: return False
if 'provider' in server_: server_['driver'] = server_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
destroy(server_['name'])
if request.status_code == 204: return True
time.sleep(0.5)
pip install https://pysphere.googlecode.com/files/pysphere-0.1.8.zip
import pprint import logging import time
import salt.utils.cloud import salt.utils.xmlutil from salt.exceptions import SaltCloudSystemExit from salt.utils import warn_until
import salt.config as config
HAS_LIBS = False try: from pysphere import VIServer, MORTypes, VIException HAS_LIBS = True
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'vsphere', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
ret = show_instance(name=vm_['name'], call='action')
ret['deploy_kwargs'] = deploy_kwargs
username: user@example.com password: secretpassword datacenter_id: <UUID> ssh_private_key: /path/to/private.key ssh_public_key: /path/to/public.key
from __future__ import absolute_import import logging import os import pprint import time
import salt.utils import salt.config as config from salt.exceptions import ( SaltCloudConfigError, SaltCloudNotFound, SaltCloudExecutionFailure, SaltCloudExecutionTimeout, SaltCloudSystemExit )
import salt.utils.cloud
log = logging.getLogger(__name__)
exc_info_on_loglevel=logging.DEBUG
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'profitbricks', vm_['profile']) is False: return False
vm_size = override_size(vm_)
ssh_keys = get_public_keys(vm_)
server = Server( name=vm_['name'], ram=vm_size['ram'], cores=vm_size['cores'], create_volumes=[volume] )
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
from __future__ import absolute_import import logging import socket import pprint
import salt.utils
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured( __opts__, __active_provider_name__ or 'dimensiondata', vm_['profile']) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
exc_info_on_loglevel=logging.DEBUG
from __future__ import absolute_import import pprint import logging
import salt.config as config
try: from libcloud.compute.drivers.cloudstack import CloudStackNetwork import libcloud.security libcloud.security.CA_CERTS_PATH.append('/etc/ssl/certs/YaST-CA.pem') HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
log.warning('Cannot get projects, you may need to update libcloud to 0.15 or later') return False
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'cloudstack', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info=log.isEnabledFor(logging.DEBUG)
user: fred password: saltybacon private_key: /root/mykey.pem private_key: mykey
from __future__ import absolute_import import os import json import logging import base64 import pprint import inspect import yaml import datetime from Crypto.Hash import SHA256 from Crypto.PublicKey import RSA from Crypto.Signature import PKCS1_v1_5
import salt.ext.six as six
log = logging.getLogger(__name__)
POLL_ALL_LOCATIONS = True
raise SaltCloudSystemExit( 'The query_instance action must be called with -a or --action.' )
return False
return False
pass #destroy(vm_['name'])
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'joyent', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
if 'id' not in item: item['id'] = item['name'] ret[item['name']] = item
for key in desired_keys: if key not in item: item[key] = None
to_del = [] if not full:
if not data: data = json.dumps({})
from __future__ import absolute_import import os import re import pprint import logging import msgpack from ast import literal_eval
from salt.utils import namespaced_function import salt.ext.six as six import salt.utils.cloud import salt.config as config from salt.utils import http from salt import syspaths
log = logging.getLogger(__name__)
_UA_PRODUCT = 'salt-cloud' _UA_VERSION = '0.2.0'
all_images.extend(conn.list_images())
try: tags = literal_eval(t)
try: metadata = literal_eval(md)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'gce', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
node_dict = show_instance(node_data.name, 'action')
user: myuser password: mypassword url: https://api.cloud.xmission.com:4465/paci/v1.0/ driver: parallels
from __future__ import absolute_import import time import pprint import logging
import salt.utils.cloud import salt.config as config from salt.exceptions import ( SaltCloudNotFound, SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
log = logging.getLogger(__name__)
content = ET.Element('ve')
name = ET.SubElement(content, 'name') name.text = vm_['name']
desc = ET.SubElement(content, 'description') desc.text = config.get_cloud_config_value( 'desc', vm_, __opts__, default=vm_['name'], search_global=False )
ram = ET.SubElement(content, 'ram-size') ram.text = config.get_cloud_config_value( 'ram', vm_, __opts__, default='256', search_global=False )
bandwidth = ET.SubElement(content, 'bandwidth') bandwidth.text = config.get_cloud_config_value( 'bandwidth', vm_, __opts__, default='100', search_global=False )
ip_num = ET.SubElement(content, 'no-of-public-ip') ip_num.text = config.get_cloud_config_value( 'ip_num', vm_, __opts__, default='1', search_global=False )
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'parallels', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
return
destroy(vm_['name'])
auth_minion: myminion config_profile: my_openstack_profile
ignore_cidr: 192.168.50.0/24
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 block_device: - source: image id: <image_id> dest: volume size: 100 shutdown: <preserve/remove> bootindex: 0
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 boot_volume: <volume id>
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 snapshot: <cinder snapshot id>
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 ephemeral: - size: 100 format: <swap/ext4>
centos7-2-iad-rackspace: provider: rackspace-iad size: general1-2 swap: <size>
from __future__ import absolute_import import os import logging import socket import pprint import yaml
import salt.ext.six as six import salt.utils import salt.client from salt.utils.openstack import nova try: import novaclient.exceptions except ImportError as exc: pass
log = logging.getLogger(__name__) request_log = logging.getLogger('requests')
script = namespaced_function(script, globals()) reboot = namespaced_function(reboot, globals())
raise SaltCloudSystemExit( 'The request_instance action must be called with -a or --action.' )
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'nova', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
data, vm_ = request_instance(vm_)
vm_['instance_id'] = data.id
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
if server_tmp is None: continue
create_volume = volume_create
attach_volume = volume_attach
create_attach_volumes = volume_create_attach
from __future__ import absolute_import import time import json import pprint import logging import hmac import base64 from hashlib import sha256
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
signature = _compute_signature(real_parameters, access_key_secret, 'GET', '/iaas/') real_parameters['signature'] = signature
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'qingcloud', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
destroy(vm_['name'])
salt.utils.cloud.bootstrap(vm_, __opts__)
from __future__ import absolute_import import logging import socket import pprint
import salt.utils import salt.config as config from salt.utils import namespaced_function from salt.exceptions import ( SaltCloudSystemExit, SaltCloudExecutionFailure, SaltCloudExecutionTimeout )
try: from libcloud.compute.base import NodeState import libcloud.security libcloud.security.CA_CERTS_PATH.append('/etc/ssl/certs/YaST-CA.pem') HAS_LIBCLOUD = True except ImportError: HAS_LIBCLOUD = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'rackspace', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
return False
return
destroy(vm_['name'])
api_key: <supersecretapi_key> driver: vultr
from __future__ import absolute_import import pprint import logging import time import urllib
import salt.config as config import salt.utils.cloud from salt.exceptions import ( SaltCloudConfigError, SaltCloudSystemExit )
log = logging.getLogger(__name__)
if name not in nodes: return {} salt.utils.cloud.cache_node(nodes[name], __active_provider_name__, __opts__) return nodes[name]
exc_info_on_loglevel=logging.DEBUG
ret = salt.utils.cloud.bootstrap(vm_, __opts__)
import logging
return True
if vm_info['profile'] and config.is_profile_configured( __opts__, __active_provider_name__ or 'virtualbox', vm_info['profile'] ) is False: return False
request_kwargs = { 'name': vm_info['name'], 'clone_from': vm_info['clonefrom'] }
if power: vb_start_vm(vm_name, timeout=boot_timeout) ips = vb_wait_for_network_address(wait_for_ip_timeout, machine_name=vm_name)
if deploy: vm_info['key_filename'] = key_filename vm_info['ssh_host'] = ip
return vm_result
from __future__ import absolute_import import os import time import json import pprint import logging import decimal
try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
log = logging.getLogger(__name__)
if vm_['profile'] and config.is_profile_configured(__opts__, __active_provider_name__ or 'digital_ocean', vm_['profile'], vm_=vm_) is False: return False
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
ssh_key_name = config.get_cloud_config_value( 'ssh_key_name', vm_, __opts__, search_global=False )
exc_info_on_loglevel=logging.DEBUG
return False
destroy(vm_['name'])
request.text
if request.status_code == 204: return True
time.sleep(0.5)
from __future__ import absolute_import, print_function, generators import os import copy import glob import time import signal import logging import traceback import multiprocessing import sys from itertools import groupby
from salt.exceptions import ( SaltCloudNotFound, SaltCloudException, SaltCloudSystemExit, SaltCloudConfigError )
try: import Crypto.Random except ImportError:
log = logging.getLogger(__name__)
if 'provider' in provider: driver = provider.pop('provider') else: driver = provider['driver']
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
raise SaltCloudConfigError( 'Either an instance (or list of names) or a provider must be ' 'specified, but not both.' )
exc_info_on_loglevel=logging.DEBUG
pmap[alias][driver] = []
continue
log.debug( 'The \'{0}\' cloud driver defined under \'{1}\' provider ' 'alias is unable to get the locations information'.format( driver, alias ) ) continue
exc_info_on_loglevel=logging.DEBUG
log.debug( 'The \'{0}\' cloud driver defined under \'{1}\' provider ' 'alias is unable to get the images information'.format( driver, alias ) ) continue
exc_info_on_loglevel=logging.DEBUG
log.debug( 'The \'{0}\' cloud driver defined under \'{1}\' provider ' 'alias is unable to get the sizes information'.format( driver, alias ) ) continue
exc_info_on_loglevel=logging.DEBUG
output_multip = enter_mainloop( _destroy_multiprocessing, parallel_data, pool_size=pool_size)
ret_multip = {} for obj in output_multip: ret_multip.update(obj)
for alias, driver, name in vms_to_destroy: ret = processed[alias][driver][name] if not ret: continue
if isinstance(ret, dict) and 'newname' in ret: salt.utils.cloud.remove_key( self.opts['pki_dir'], ret['newname'] ) continue
salt.utils.cloud.remove_key(self.opts['pki_dir'], os.path.basename(key_file)) continue
raise SaltCloudSystemExit( 'The following VM\'s were not found: {0}'.format( ', '.join(names) ) )
if 'provider' in vm_: vm_['driver'] = vm_.pop('provider')
vm_['pub_key'] = None vm_['priv_key'] = None
salt.utils.cloud.accept_key( self.opts['pki_dir'], vm_['pub_key'], key_id )
time.sleep(3)
if valid_function is False: if invalid_functions.get(fun) is None: invalid_functions.update({fun: []}) invalid_functions[fun].append(vm_name) continue
missing_vms = names.difference(invalid_func_vms) if missing_vms: ret['Not Found'] = list(missing_vms) ret['Not Actioned/Not Running'] = list(names)
if missing_vms: return ret
ret['Not Actioned/Not Running'] = list(names) ret['Not Found'] = list(names) return ret
pass
entries = {} for name, overrides in six.iteritems(mapped): overrides.setdefault('name', name) entries[name] = overrides map_[profile] = entries continue
mapped = [mapped]
alias, driver = profile_data.get('provider').split(':') provider_details = self.opts['providers'][alias][driver].copy() del provider_details['profiles']
provider_details.update(profile_data) profile_data = provider_details
ret['create'][nodename] = nodedata alias, driver = nodedata['provider'].split(':') defined.add((alias, driver, nodename))
matching = get_matching_by_name(name) if not matching: continue
for item in matching: if name not in ret['create']: break
ret['destroy'] = exist.difference(defined)
for name, profile in create_list: make_minion = salt.config.get_cloud_config_value( 'make_minion', profile, self.opts, default=True ) if make_minion is False: continue
master_profile.setdefault('preseed_minion_keys', {}) master_profile['preseed_minion_keys'].update({name: pub})
local_master = True
out.get('deploy_kwargs', {}) or out.pop('deploy_kwargs', {})
log.info( 'Since parallel deployment is in use, ssh console output ' 'is disabled. All ssh output will be logged though' ) opts['display_ssh_output'] = False
continue
local_master = True
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
exc_info_on_loglevel=logging.DEBUG
return data['alias'], data['driver'], ()
def _run_parallel_map_providers_query(*args, **kw): return communicator(run_parallel_map_providers_query)(*args[0], **kw)
import salt.defaults.exitcodes from salt.exceptions import SaltException
from __future__ import print_function from __future__ import absolute_import import os import sys import logging from salt.ext.six.moves import input
import salt.config import salt.defaults.exitcodes import salt.output import salt.utils from salt.utils import parsers from salt.utils.verify import check_user, verify_env, verify_files, verify_log
import salt.cloud import salt.utils.cloud from salt.exceptions import SaltCloudException, SaltCloudSystemExit import salt.ext.six as six import salt.syspaths as syspaths log = logging.getLogger(__name__)
self.parse_args()
verify_files([logfile], salt_master_user)
self.setup_logfile_logger() verify_log(self.config)
key, value = name.split('=', 1) kwargs[key] = value
print(msg) self.exit(1)
run_map = False
exc_info_on_loglevel=logging.DEBUG
import os import logging from salt.ext.six import string_types import salt.ext.six as six from salt.ext.six.moves import zip
import salt.utils.event import salt.client
import salt.utils import salt.utils.cloud import salt.config as config from salt.exceptions import SaltCloudNotFound, SaltCloudSystemExit
log = logging.getLogger(__name__)
import logging import re
try: import pymongo HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
log = logging.getLogger(__name__)
minion_id = kwargs['opts']['id'] if re_pattern: minion_id = re.sub(re_pattern, re_replace, minion_id)
log.debug( 'ext_tops.mongo: no document found in collection {0}'.format( collection ) ) return {}
__virtualname__ = 'reclass'
from reclass.adapters.salt import top as reclass_top from reclass.errors import ReclassException
reclass_opts = __opts__['master_tops']['reclass']
filter_out_source_path_option(reclass_opts)
set_inventory_base_uri_default(__opts__, kwargs)
minion_id = kwargs['opts']['id']
return reclass_top(minion_id, **reclass_opts)
import logging
log = logging.getLogger(__name__)
__virtualname__ = 'varstack'
import logging import subprocess
import yaml
import logging
log = logging.getLogger(__name__)
from __future__ import absolute_import import os
import salt.utils
import salt.ext.six as six
from salt import syspaths import salt.config import salt.loader from salt.client import mixins from salt.utils.error import raise_error
import logging import os
import yaml
import salt.config
import salt.utils.error
from __future__ import absolute_import import os
import salt.utils
import salt.ext.six as six
import os import hashlib
import salt.key import salt.crypt
import fnmatch import logging import os import re import time import stat import tempfile
import salt.ext.six as six
HAS_PWD = False
pillar = salt.utils.gitfs.GitPillar(opts) pillar.init_remotes( opts_dict['git'], git_pillar.PER_REMOTE_OVERRIDES ) ret.append(pillar)
keyfile = os.path.join( opts['cachedir'], '.{0}_key'.format(user.replace('\\', '_')) )
os.chmod(keyfile, stat.S_IRUSR | stat.S_IWUSR)
return False
if self.opts.get('permissive_pki_access', False) and stat.S_IWGRP & fmode.st_mode: return True elif stat.S_IWGRP & fmode.st_mode: return False
if not (stat.S_IWGRP & fmode.st_mode or stat.S_IWOTH & fmode.st_mode): return True
log.error( 'Top function {0} failed with error {1} for minion ' '{2}'.format( fun, exc, load['id'] ) )
return False
saveload_fstr = '{0}.save_load'.format(self.opts['master_job_cache']) self.mminion.returners[saveload_fstr](load['jid'], load)
if isinstance(self.opts['peer_run'][match], list): perms.update(self.opts['peer_run'][match])
log.warning( 'Minion id {0} is not who it says it is!'.format( load['id'] ) ) return {}
log.warning('Authentication failure of type "eauth" occurred.') return ''
del good
import sys from collections import namedtuple, Iterable, Sequence, Mapping import logging
from salt.utils.odict import OrderedDict import salt.ext.six as six
masters = [] for hostage in hostages: external = hostage['external'] internal = hostage['internal'] if external: external = parse_hostname(external, master_port) if not external:
import ioflo.base.deeding from ioflo.aid.odicting import odict
masterStack.keep.auto = raeting.AutoMode.always.value minionStack.keep.auto = raeting.AutoMode.always.value
import sys if sys.version_info < (2, 7): import unittest2 as unittest else: import unittest
self.join(other2, main)
self.join(other2, main)
self.join(other2, main)
from salt.daemons.flo import core from salt.daemons.test.plan import actors
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 5)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.addEnterDeed("TestOptsSetupMaster") self.addEnterDeed("SaltRaetManorLaneSetup") self.addEnterDeed("PresenterTestSetup") act = self.addRecurDeed("SaltRaetPresenter")
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
from __future__ import absolute_import, print_function import os
import ioflo.app.run from ioflo.base.consoling import getConsole
import sys from salt.ext.six.moves import map if sys.version_info < (2, 7): import unittest2 as unittest else: import unittest
import sys from salt.ext.six.moves import map if sys.version_info < (2, 7): import unittest2 as unittest else: import unittest
from salt.daemons.flo import core from salt.daemons.test.plan import actors
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 1)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
self.assertEqual(len(testStack.rxMsgs), 0) testStack.serviceAll() self.assertEqual(len(testStack.rxMsgs), 0)
import os import stat
self.clients.setsockopt(zmq.IPV4ONLY, 0)
from __future__ import absolute_import import os import sys import types import logging import traceback import multiprocessing import subprocess import json
import ioflo.base.deeding
stack.addRemote(RemoteYard(stack=stack, name='manor', lanename=lanename, dirpath=sockdirpath)) console.concise("Created Jobber Stack {0}\n".format(stack.name)) return stack
try: self.proc_run(msg) except Exception as exc: log.error( 'Exception caught by jobber: {0}'.format(exc), exc_info=True)
src_estate, src_yard, src_share = msg['route']['src'] salt.transport.jobber_estate_name = src_estate salt.transport.jobber_yard_name = src_yard
import multiprocessing import os
import ioflo.base.deeding
import salt.fileserver import salt.loader import salt.utils.minions import salt.daemons.masterapi
import salt.utils.reactor import salt.utils.event import ioflo.base.deeding
from __future__ import absolute_import import os
from . import core from . import worker from . import maint from . import reactor from . import zero from . import jobber from . import dummy
import salt.daemons.masterapi
import ioflo.app.run import salt.ext.six as six
from __future__ import absolute_import import logging
import ioflo.base.deeding
import time import os import multiprocessing import logging from salt.ext.six.moves import range
import salt.daemons.flo import salt.daemons.masterapi from raet import raeting from raet.lane.stacking import LaneStack from raet.lane.yarding import RemoteYard
import ioflo.base.deeding
from __future__ import absolute_import import os import time import random import logging import itertools from collections import deque from _socket import gaierror
HAS_PSUTIL = False try: import salt.utils.psutil_compat as psutil HAS_PSUTIL = True except ImportError: pass
import salt.ext.six as six from salt.ext.six.moves import range
if modules_max_memory is True: resource.setrlimit(resource.RLIMIT_AS, old_mem_limit) self.module_refresh.value = False
Ioinits = { 'road_stack': '.salt.road.manor.stack', }
Ioinits = { 'lane_stack': '.salt.lane.manor.stack', }
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return
log.error('Received message without share: {0}'.format(msg)) return
log.error('Received local command remotely! Ignoring: {0}'.format(msg)) return
if 'load' in msg: role = self.road_stack.value.nameRemotes[sender].role
if d_estate in self.road_stack.value.nameRemotes: self.road_stack.value.message(msg, self.road_stack.value.nameRemotes[d_estate].uid) return
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return
log.error('Lane Router Received message without share: {0}'.format(msg)) return
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return return
log.error('Received message without share: {0}'.format(msg)) return
if d_estate in self.road_stack.value.nameRemotes: self.road_stack.value.message(msg, self.road_stack.value.nameRemotes[d_estate].uid) return
if d_yard in self.lane_stack.value.nameRemotes: self.lane_stack.value.transmit(msg, self.lane_stack.value.nameRemotes[d_yard].uid) return return
log.error('Lane Router Received message without share: {0}'.format(msg)) return
import os
from ioflo.aid.odicting import odict
self.saltRaetKey.status(remote.role, remote.pubber.keyhex, remote.verfer.keyhex)
import copy import logging import time
import salt.defaults.exitcodes import salt.ext.six as six
from salt.output import nested nested.__opts__ = {} ret = nested.output(obj).rstrip() return ret
exc_str = exc_str_prefix + _nested_output(self.info)
from __future__ import absolute_import, print_function import os import copy import json import stat import shutil import fnmatch import hashlib import logging
import salt.crypt import salt.utils import salt.client import salt.exceptions import salt.utils.event import salt.daemons.masterapi from salt.utils import kinds from salt.utils.event import tagify
import salt.ext.six as six from salt.ext.six.moves import input try: import msgpack except ImportError: pass
else: mpub = self.opts['pki_dir'] + '/' + 'master.pub' if os.path.isfile(mpub): self.pubkey = mpub
else: mpriv = self.opts['pki_dir'] + '/' + 'master_sign.pem' if os.path.isfile(mpriv): self.privkey = mpriv
if self.opts['transport'] in ('zeromq', 'tcp'): key_dirs = self._check_minions_directories() else: key_dirs = self._check_minions_directories()
continue
keydata = { 'minion_id': minion_id, 'pub': pub, 'verify': verify}
import salt.utils.sdb
from __future__ import absolute_import, print_function import fnmatch import logging import os
import salt.client import salt.payload import salt.utils import salt.utils.jid import salt.minion import salt.returners
import salt.ext.six as six from salt.exceptions import SaltClientError
import logging
from salt.exceptions import CommandExecutionError
try: import pycontrol.pycontrol as f5 HAS_PYCONTROL = True except ImportError: HAS_PYCONTROL = False
import salt.utils import salt.fileserver
import salt.pillar import salt.utils.minions
import logging import os
import salt.cloud
log = logging.getLogger(__name__)
import os
netapi = salt.netapi.NetapiClient(__opts__) if not netapi._is_master_running(): raise salt.exceptions.SaltDaemonNotRunning( 'Salt Master must be running.')
from __future__ import absolute_import
import salt.client.ssh.client
from __future__ import absolute_import, print_function import time import os import copy import logging
import salt.client import salt.utils import salt.utils.virt import salt.utils.cloud import salt.key from salt.utils.odict import OrderedDict as _OrderedDict
import salt.ext.six as six
__func_alias__ = { 'list_': 'list' }
ret['ping_status'] = bool(len(done))
if not done: ret['result'] = False if not quiet: __jid_event__.fire_event({'message': ret}, 'progress') return ret
import os import sys
from __future__ import absolute_import, print_function import logging
import salt.utils.reactor import salt.syspaths import salt.utils.event import salt.utils.process from salt.ext.six import string_types
from __future__ import print_function from __future__ import absolute_import
import salt.utils.thin
from __future__ import absolute_import
import salt.output import salt.minion
import salt.ext.six as six
import logging
HAS_LIBS = False HAS_SIX = False try: import requests
try: import six except ImportError: pass
import yaml import json
import salt.utils import salt.utils.pagerduty from salt.ext.six import string_types
list_maintenance_windows = salt.utils.alias_function(list_windows, 'list_maintenance_windows')
list_escalation_policies = salt.utils.alias_function(list_policies, 'list_escalation_policies')
import os import logging import json
HAS_LIBS = False try: import dns.query import dns.update import dns.tsigkeyring HAS_LIBS = True except ImportError: HAS_LIBS = False
import salt.utils
while i > 1: p = parts.pop(0) i -= 1 popped.append(p)
from __future__ import absolute_import, print_function import itertools
import salt.client import salt.runner import salt.wheel
import salt.ext.six as six from salt.exceptions import SaltClientError
from __future__ import print_function from __future__ import absolute_import import socket
import salt.utils
from __future__ import print_function from __future__ import absolute_import
import salt.loader import salt.utils.event from salt.utils.event import tagify from salt.exceptions import SaltInvocationError
import salt.utils.error
import time import salt.ext.six as six from salt.ext.six.moves import range
from __future__ import absolute_import, print_function import logging
try: import paramiko HAS_PARAMIKO = True except ImportError: HAS_PARAMIKO = False
import salt.search
import json import logging
import salt.output import salt.utils.http
from __future__ import absolute_import, print_function import os
import salt.ext.six as six try: import msgpack except ImportError:
from salt.exceptions import CommandExecutionError, SaltRenderError import salt.utils import salt.utils.gitfs import logging import salt.minion import salt.loader import salt.template
PER_REMOTE_OVERRIDES = ('ssl_verify',)
import atexit import logging
HAS_LIBS = False try: import salt.ext.six as six HAS_LIBS = True except ImportError: try: import six HAS_LIBS = True except ImportError: pass
import logging
import salt.utils import salt.utils.minions
from __future__ import absolute_import, print_function import logging
import salt.loader import salt.utils import salt.utils.event from salt.exceptions import SaltInvocationError
orch = salt.utils.alias_function(orchestrate, 'orch') sls = salt.utils.alias_function(orchestrate, 'sls')
salt 'jerry' system.reboot && \\ salt-run state.event 'salt/minion/jerry/start' count=1 quiet=True && \\ salt 'jerry' state.highstate
salt -L 'kevin,stewart,dave' system.reboot && \\ salt-run state.event 'salt/minion/*/start' count=3 quiet=True && \\ salt -L 'kevin,stewart,dave' state.highstate
salt-run state.event | while read -r tag data; do echo $tag echo $data | jq -colour-output . done
import logging
import salt.utils.http
import logging
import salt.utils.extmods
from __future__ import absolute_import, print_function import os.path import logging
import salt.client import salt.utils.virt import salt.utils.cloud import salt.key from salt.exceptions import SaltClientError
import salt.ext.six as six
print(client_error)
from __future__ import absolute_import, print_function import os import operator import re import subprocess import tempfile import time import logging import uuid
import salt.ext.six as six
version_status[2] = master_version.string
import logging
import salt.pillar.git_pillar import salt.utils.gitfs from salt.exceptions import SaltRunnerError from salt.ext import six
log = logging.getLogger(__name__)
out_file = os.path.join(conn['formula_path'], new_name)
new_name = '{0}.sls.orig'.format(package) out_file = os.path.join(conn['pillar_path'], new_name)
out_file = os.path.join(salt.syspaths.CONFIG_DIR, new_name)
out_file = os.path.join(conn['reactor_path'], member.name)
member.name = member.name.replace('{0}/'.format(package), '')
member.name = '{0}.sls.orig'.format(package) out_path = conn['pillar_path']
member.name = member.name.replace('{0}/'.format(package), '') out_path = salt.syspaths.CONFIG_DIR
out_path = __opts__['reactor_path']
comps = member.path.split('/') if len(comps) > 1 and comps[0] == comps[1]: member.path = '/'.join(comps[1:])
from __future__ import absolute_import, print_function import os import yaml import tarfile import shutil import msgpack import datetime import hashlib import logging import pwd import grp import sys
log = logging.getLogger(__name__)
self._install_indv_pkg(package, out_file)
existing_files = self._pkgfiles_fun('check_existing', pkg_name, pkg_files, formula_def)
self._pkgdb_fun('register_pkg', pkg_name, formula_def, self.db_conn)
for member in pkg_files: member.uid = uid member.gid = gid member.uname = uname member.gname = gname
if dep in inspected: continue inspected.append(dep)
pkg_info = self._pkgdb_fun('info', package, self.db_conn) if pkg_info is None: raise SPMInvocationError('Package {0} not installed'.format(package))
log = logging.getLogger(__name__)
from __future__ import absolute_import import re
if isinstance(cmd, str): funs_to_check = [cmd] else: funs_to_check = cmd for fun in funs_to_check: if re.match(blacklisted_module, fun): return True
from __future__ import absolute_import import os import sys import copy import site import fnmatch import logging import datetime import traceback import re
import salt.ext.six as six from salt.ext.six.moves import map, range, reload_module
log.info(str(ret))
comps[1] = '.'.join(comps[1:len(comps)])
ex_sls.add(exc)
decrypt = salt.loader.render( self.opts, {}).get(self._pillar_enc)
try:
return ret
self.module_refresh() return
continue
chunk[key] = name
ex_sls.add(exc)
inject_globals['__env__'] = str(cdata['kwargs']['env'])
inject_globals['__env__'] = str(low['__env__'])
inject_globals['__env__'] = 'base'
if 'warnings' in cdata: ret.setdefault('warnings', []).extend(cdata['warnings'])
if fnmatch.fnmatch(chunk['__sls__'], req_val): if requisite == 'prereq': chunk['__prereq__'] = True reqs.append(chunk) found = True continue
if '.' in high[name]: comps = high[name].split('.') high[name] = { comps[0]: [comps[1]] } continue
return errors
if not slsmod: errors.append( 'Environment {0} contains an empty sls ' 'index'.format(saltenv) )
exc_info_on_loglevel=logging.DEBUG
if env_key in matches or fnmatch.filter(self.avail[env_key], inc_sls): resolved_envs = [env_key] else: resolved_envs = []
resolved_envs = [ aenv for aenv in matches if fnmatch.filter(self.avail[aenv], inc_sls) ]
continue
continue
continue
continue
if '.' in state[name]: comps = state[name].split('.') state[name] = {'__sls__': sls, '__env__': saltenv, comps[0]: [comps[1]]} continue
if not statefiles: statefiles = [sls_match]
high, ext_errors = self.state.reconcile_extend(high) errors += ext_errors
errors += self.state.verify_high(high) high, req_in_errors = self.state.requisite_in(high) errors += req_in_errors high = self.state.apply_exclude(high)
chunks = self.state.compile_high_data(high)
stack = []
self._pydsl_all_decls = {}
self._pydsl_render_stack = []
cls.stack = []
self.channel = salt.transport.Channel.factory(self.opts['master_uri'])
from __future__ import print_function from __future__ import absolute_import import glob import logging import os import re import sqlite3 as lite from salt.exceptions import SaltInvocationError
__virtualname__ = 'sqlite'
return __virtualname__
cur.executemany(cmd, newitems)
from __future__ import absolute_import from contextlib import contextmanager import json import sys
__virtualname__ = 'pgjsonb'
cur.executemany(cmd, newitems)
EX_GENERIC = 1
EX_THIN_PYTHON_INVALID = 10 EX_THIN_DEPLOY = 11 EX_THIN_CHECKSUM = 12 EX_MOD_DEPLOY = 13 EX_SCP_NOT_FOUND = 14
EX_AGGREGATE = 20
SALT_KEEPALIVE = 99
SALT_BUILD_FAIL = 101
DEFAULT_TARGET_DELIM = ':'
from salt.modules.tomcat import _extract_war_version
ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''}
if __opts__['test']: ret['result'] = None return ret
deploy_res = __salt__['tomcat.deploy_war'](war, name, 'yes', url, __env__, timeout, temp_war_location=temp_war_location)
ret = {'name': name, 'result': True, 'changes': {}, 'comment': ''}
if __opts__['test']: ret['result'] = None return ret
ret = { 'name': name, 'changes': {}, 'result': False, 'comment': '', }
existing_config = None if __salt__['marathon.has_app'](name): existing_config = __salt__['marathon.app'](name)['app']
from netaddr import IPAddress from netaddr.core import AddrFormatError
import dns.resolver
dns_reply = list() try: dns_reply = dns.resolver.query(peer) except dns.resolver.NoAnswer: continue for dns_ip in dns_reply: ip_only_peers.append(str(dns_ip))
import os
import salt.utils import salt.ext.six as six
import salt.ext.six as six
import salt.utils
settings = _normalize_server_settings(**settings)
if addresses: if addresses[0] == 'None': addresses[0] = None elif addresses is None: addresses = [None]
import logging import os
from __future__ import absolute_import import logging import os import os.path
import salt.utils
adds[k] = Tags[k]
myqueue: boto_sqs.present: - region: us-east-1 - profile: mysqsprofile
if __opts__['test']: ret['comment'] = 'SELinux mode is set to be changed to {0}'.format( tmode) ret['result'] = None return ret
from salt.exceptions import CommandExecutionError, CommandNotFoundError
import salt.ext.six as six
if pkg_ver: if installed_pkg_ver != pkg_ver: pkgs_to_install.append(pkg) else: pkgs_satisfied.append(installed_name_ver)
from __future__ import absolute_import import logging
import salt.utils
from __future__ import absolute_import
import salt.exceptions
ret = {'name': '', 'changes': {}, 'result': False, 'comment': ''}
current_state = __salt__['tuned.active']()
if profile not in valid_profiles: raise salt.exceptions.SaltInvocationError('Invalid Profile Name')
if profile in current_state: ret['result'] = True ret['comment'] = 'System already in the correct state' return ret
ret['result'] = None return ret
new_state = __salt__['tuned.profile'](profile)
ret['comment'] = 'The state of "{0}" was changed!'.format(profile)
ret['changes'] = { 'old': current_state, 'new': new_state, }
return ret
ret = {'name': 'off', 'changes': {}, 'result': False, 'comment': 'off'}
current_state = __salt__['tuned.active']()
if current_state == 'off': ret['result'] = True ret['comment'] = 'System already in the correct state' return ret
ret['result'] = None return ret
from salt.ext.six import string_types import salt.utils import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'docker'
return ':'.join((image, tag))
- volumes: /usr/local/etc/ssl/certs/example.crt: bind: /etc/ssl/certs/com.example.internal.crt ro: True /var/run: bind: /var/run/host/ ro: False
- volumes: - /usr/local/etc/ssl/certs/example.crt: bind: /etc/ssl/certs/com.example.internal.crt ro: True
bindvolumes = volumes
kw['force'] = True build_status = built(name, **kw) result = build_status['result'] status = _ret_status(build_status, name, result=result, changes={name: result}) return status
if already_exists: return _valid(comment='Container {0!r} already exists'.format(name)) dports, denvironment = {}, {}
already_exists and cinfos['out']['Image'] == iinfos['out']['Id']
import logging
from salt.modules import postgres
mode = 'create' mtdata = __salt__['postgres.create_metadata']( name, schema=schema, ext_version=ext_version, **db_args)
if __salt__['mysql.grant_exists']( grant, database, user, host, grant_option, escape, **connection_args):
import sys
import salt.utils
if __salt__['mysql.user_exists'](name, host, unix_socket=unix_socket, **connection_args):
ret['comment'] = ( 'User {0}@{1} is not present, so it cannot be removed' ).format(name, host) return ret
from __future__ import absolute_import import pprint
import salt.ext.six as six
import salt.utils.cloud as suc
test = __opts__.get('test', False) instance = __salt__['cloud.action'](fun='show_instance', names=names) __opts__['test'] = test return instance
import re
from __future__ import absolute_import import logging
import logging
import salt.utils
ret['changes'] = {'old': '', 'new': '{0}@{1}'.format(user, host)}
join = salt.utils.alias_function(joined, 'join')
from __future__ import absolute_import import logging
import salt.utils
ret['changes'] = {'old': '', 'new': name}
ret['changes'] = {'new': '', 'old': name}
from __future__ import absolute_import import logging
from __future__ import absolute_import import logging
import salt.utils
if not isinstance(servers, list): ret['result'] = False ret['comment'] = 'servers entry is not a list !' return ret
import copy import logging import sys
import salt.utils from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.modules.freebsdports import _normalize, _options_file_exists
if current_options: current_options = current_options[next(iter(current_options))] if default_options: default_options = default_options[next(iter(default_options))]
if options: ret['comment'] += ' ' + _build_option_string(options) return ret
loaded_mods = list(set(loaded_mods) & set(persist_mods))
not_loaded = list(set(mods) - set(already_loaded))
loaded_mods = list(set(loaded_mods) | set(persist_mods))
from salt.ext.six import string_types
if not isinstance(version, string_types) and version is not None: version = str(version)
name = '{0}-{1}'.format(name, version)
import salt.utils from salt.exceptions import CommandExecutionError, SaltInvocationError
return ret
state['new'] = __salt__['lxc.state'](name, path=path)
restart = False
import os.path
import salt.utils
ret['comment'] = 'Database {0} is already present, so cannot be created'\ .format(name) return ret
ret['comment'] = 'Database {0} is not present, so it cannot be removed'\ .format(name) return ret
import os.path import re
from salt.ext.six import string_types
if __grains__['os'] in ['MacOS', 'Darwin'] and opts == 'defaults': opts = 'noowners'
if isinstance(opts, string_types): opts = opts.split(',')
if not name == '/': name = name.rstrip('/')
mount_invisible_keys = [ 'actimeo', 'comment', 'direct-io-mode', 'password', 'retry', 'port', ]
mount_ignore_fs_keys = { 'ramfs': ['size'] }
mount_translate_options = { 'tcp': 'proto=tcp', 'udp': 'proto=udp', }
ret['comment'] = out ret['result'] = False return ret
ret['comment'] = 'Target was successfully mounted' ret['changes']['mount'] = True
if __grains__['os'] in ['MacOS', 'Darwin'] and config == '/etc/fstab': config = "/etc/auto_salt"
ret['comment'] = 'Cluster {0}/{1} is not present, so it cannot ' \ 'be removed'.format(version, name) return ret
host = name
import logging
schema_attr = __salt__['postgres.schema_get'](dbname, name, **db_args)
if schema_attr is None: cret = __salt__['postgres.schema_create'](dbname, name, owner=owner, **db_args) else: msg = 'Schema {0} already exists in database {1}' cret = None
from __future__ import absolute_import import logging from salt.ext.six import string_types
request_params.pop('IdentityPoolName', None) r = __salt__['boto_cognitoidentity.update_identity_pool'](**request_params)
_role_present(ret, IdentityPoolId, AuthenticatedRole, UnauthenticatedRole, conn_params)
from __future__ import absolute_import
__func_alias__ = { 'set_': 'set' }
import logging import os
import salt.version import salt.utils
__virtualname__ = 'virtualenv'
if requirements or pip_pkgs: before = set(__salt__['pip.freeze'](bin_env=name, user=user, use_vt=use_vt))
import logging import json
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
ensure my cloudwatch service exists: pagerduty_service.present: - name: my cloudwatch service - service: escalation_policy_id: "my escalation policy" type: aws_cloudwatch description: "my cloudwatch service controlled by salt"
resource_value = resource_object['service_key'] if '@' in resource_value: resource_value = resource_value[0:resource_value.find('@')]
import os
import salt.utils from salt.modules.cron import ( _needs_change, _cron_matched )
mode = __salt__['config.manage_mode']('0600') owner, group, crontab_dir = _get_cron_info()
source = name
source, source_hash = __salt__['file.source_list'](source, source_hash, __env__)
try: sfn, source_sum, comment = __salt__['file.get_managed']( cron_path, template, source, source_hash, owner, group, mode, __env__, context, defaults,
import salt.utils
from __future__ import absolute_import import time
from salt.exceptions import CommandExecutionError import salt.utils
try: if not _available(name, ret): return ret except CommandExecutionError as exc: ret['result'] = False ret['comment'] = exc.strerror return ret
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} set to be enabled'.format(name) return ret
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} set to be disabled'.format(name) return ret
if 'enabled' in kwargs: return _enabled_used_error(ret)
try: if not _available(name, ret): return ret except CommandExecutionError as exc: ret['result'] = False ret['comment'] = exc.strerror return ret
if __opts__['test']: ret['result'] = None ret['comment'] = 'Service {0} is set to start'.format(name) return ret
if 'enabled' in kwargs: return _enabled_used_error(ret)
date > /tmp/salt-run: cmd.run
echo "Working hard..."
from __future__ import absolute_import
import salt.utils from salt.exceptions import CommandExecutionError, SaltRenderError from salt.ext.six import string_types
data['stdout'] = '' if is_json else data.get('stdout', '')[:idx] state['changes'] = data
return state
cmd_kwargs = copy.deepcopy(cmd_kwargs) cmd_kwargs['use_vt'] = False
return True
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
watch = salt.utils.alias_function(wait, 'watch')
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
try: cmd_all = __salt__['cmd.run_all']( name, timeout=timeout, python_shell=True, **cmd_kwargs ) except CommandExecutionError as err: ret['comment'] = str(err) return ret
if source is None: source = name
if len(name.split()) > 1: cmd_kwargs.update({'args': name.split(' ', 1)[1]})
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
ret['comment'] = 'Tablespace {0} is not present, so it cannot ' \ 'be removed'.format(name) return ret
from __future__ import absolute_import
services = sorted(set(services))
for current_vname in current_communities: if current_vname not in communities: ret_communities['changes'][current_vname] = {'old': current_communities[current_vname], 'new': None}
from __future__ import absolute_import import re import logging
import salt.utils from salt.version import SaltStackVersion as _SaltStackVersion from salt.exceptions import CommandExecutionError, CommandNotFoundError
import salt.ext.six as six try: import pip HAS_PIP = True except ImportError: HAS_PIP = False
import sys del pip if 'pip' in sys.modules: del sys.modules['pip']
__virtualname__ = 'pip'
ret['result'] = True ret['prefix'] = '' ret['version_spec'] = []
ret = {'result': False, 'comment': None}
else: for prefix, state_pkg_name, version_spec in pkgs_details:
if out['result'] is None: ret['result'] = False ret['comment'] = out['comment'] return ret
if result is False: target_pkgs.append((prefix, state_pkg_name.replace(',', ';')))
elif result is True: already_installed_comments.append(out['comment'])
elif result is None: ret['result'] = None ret['comment'] = out['comment'] return ret
pkgs_str = ','.join([state_name for _, state_name in target_pkgs])
pkg_404_comms = []
if prefix: pipsearch = __salt__['pip.list'](prefix, bin_env, user=user, cwd=cwd)
from __future__ import absolute_import from distutils.version import LooseVersion import logging
import salt.utils
import logging
import salt.utils
from salt.exceptions import CommandExecutionError
from __future__ import absolute_import
import salt.utils
ret['changes'] = {name: __salt__['chocolatey.install'](name, version, source, force, install_args, override_args, force_x86, package_args)}
ret['changes'] = {name: __salt__['chocolatey.uninstall'](name, version, uninstall_args, override_args)}
import logging import sys
from salt.ext.six import string_types
__virtualname__ = 'buildout' log = logging.getLogger(__name__)
from __future__ import absolute_import import logging import os import re
import salt.ext.six as six
_repack_pkgs = _namespaced_function(_repack_pkgs, globals())
try: import msgpack except ImportError: import msgpack_pure as msgpack
if oper in ('=', ''): oper = '==' return oper, verstr
return {'name': name, 'changes': {}, 'result': False, 'comment': 'Invalidly formatted pkgs parameter. See ' 'minion log.'}
targets = [] problems = [] for pkgname, pkgver in six.iteritems(to_remove): origin = bool(re.search('/', pkgname))
origin = bool(re.search('/', name))
origin = bool(re.search('/', pkgname))
myminion: 2:7.4.160-1.el7
myminion: base: |_ httpd: 2.2.15-29.el6.centos updates: |_ httpd: 2.2.15-30.el6.centos
vim-enhanced: pkg.installed: - version: 7.4.160-1.el7 - ignore_epoch: True
if not version: version = __salt__['pkg.version'](name)
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
if not changes.get('purge_desired'): changes = changes['installed']
if os.path.isfile(rtag) and refresh: os.remove(rtag)
if isinstance(cur, six.string_types): cur = {desired_pkgs[0]: cur} if isinstance(avail, six.string_types): avail = {desired_pkgs[0]: avail}
if not pkgs: up_to_date = [] else: up_to_date = [x for x in pkgs if x not in targets]
targeted_pkgs = list(targets.keys()) if pkgs else None
changes = __salt__['pkg.install'](name, refresh=False, fromrepo=fromrepo, skip_verify=skip_verify, pkgs=targeted_pkgs, **kwargs)
vim-enhanced: pkg.removed: - version: 7.4.160-1.el7 - ignore_epoch: True
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
vim-enhanced: pkg.purged: - version: 7.4.160-1.el7 - ignore_epoch: True
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
ret['changes'] = exc.info.get('changes', {}) ret['comment'] = exc.strerror_without_changes
continue
import fnmatch import logging import time
import salt.syspaths import salt.utils import salt.utils.event import salt.ext.six as six from salt.ext.six import string_types
__virtualname__ = 'salt'
state_ret['result'] = None
__func_alias__ = { 'set_': 'set' }
from __future__ import absolute_import
import salt.ext.six as six
from __future__ import absolute_import import logging
import salt.ext.six as six
from __future__ import absolute_import import logging import os import re
import salt.utils from salt.exceptions import CommandExecutionError
run_check_cmd_kwargs = {'runas': user, 'python_shell': True} if 'shell' in __grains__: run_check_cmd_kwargs['shell'] = __grains__['shell']
if app: if dmg: cmd = 'ls -d *.app' out = __salt__['cmd.run'](cmd, cwd=mount_point, python_shell=True)
__salt__['macpackage.unmount'](mount_point)
return True
import salt.utils
import logging import os import shutil
import salt.utils from salt.states.git import _fail, _neutral_test
from __future__ import absolute_import import logging
import salt.utils
self.skipUI = skipUI self.skipDownloaded = skipDownloaded self.skipInstalled = skipInstalled self.skipReboot = skipReboot self.skipPresent = skipPresent self.skipHidden = skipHidden
self.download_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.install_collection = win32com.client.Dispatch('Microsoft.Update.UpdateColl')
self.win_downloader = self.update_session.CreateUpdateDownloader() self.win_downloader.Updates = self.download_collection
self.win_installer = self.update_session.CreateUpdateInstaller() self.win_installer.Updates = self.install_collection
self.download_results = None
self.install_results = None
log.debug('generated search string: {0}'.format(search_string)) return self.Search(search_string)
comment, passed, retries = _search(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _download(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _install(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _search(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
comment, passed, retries = _download(win_updater, retries) ret['comment'] += comment if not passed: ret['result'] = False return ret
from __future__ import absolute_import import logging
from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS
from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.modules.dockerng import ( CLIENT_TIMEOUT, STOP_TIMEOUT, VALID_CREATE_OPTS, _validate_input, _get_repo_tag ) import salt.utils import salt.ext.six as six
__virtualname__ = 'dockerng'
desired_volumes = sorted(list(data) + [ k for k in _image_get(config['image_path']) or [] if k not in data])
if bool(actual_data) != bool(data): ret.update({item: {'old': actual_data, 'new': data}})
if bool(actual_data) != bool(data): ret.update({item: {'old': actual_data, 'new': data}})
if actual_data != data: ret.update({item: {'old': actual_data, 'new': data}})
image = ':'.join(_get_repo_tag(name)) all_tags = __salt__['dockerng.list_tags']()
pass
ret['changes'] = image_update
ret['comment'] = 'Image \'{0}\' could not be {1}'.format(name, action)
targets.append(':'.join(_get_repo_tag(str(target))))
pull_result = __salt__['dockerng.pull']( image, client_timeout=client_timeout, )
create_kwargs = salt.utils.clean_kwargs(**copy.deepcopy(kwargs)) send_signal = create_kwargs.pop('send_signal', False)
new_container = True
ret['changes']['removed'] = removed_ids
pull_result = __salt__['dockerng.pull']( image, client_timeout=client_timeout, )
create_result = __salt__['dockerng.create']( image, name=name, validate_ip_addrs=False, validate_input=False, client_timeout=client_timeout, **create_kwargs )
ret['changes']['added'] = create_result
__salt__['dockerng.start']( name, )
diff[key] = changes_needed[key]
ret['changes']['diff'] = changes_needed comments.append('Container \'{0}\' was replaced'.format(name))
comments.append( 'Container \'{0}\' is already configured as specified' .format(name) )
containers = [__salt__['dockerng.inspect_container'](c)['Id'] for c in containers] networks = __salt__['dockerng.networks'](names=[name]) if networks:
kwargs['force'] = True return image_present(name, **kwargs)
ret = __salt__['k8s.label_present'](name, value, node, apiserver)
ret = __salt__['k8s.label_absent'](name, node, apiserver)
ret = __salt__['k8s.folder_absent'](name, node, apiserver)
import re import copy
base_dashboards_from_pillar = ([_DEFAULT_DASHBOARD_PILLAR] + base_dashboards_from_pillar) base_panels_from_pillar = ([_DEFAULT_PANEL_PILLAR] + base_panels_from_pillar) base_rows_from_pillar = [_DEFAULT_ROW_PILLAR] + base_rows_from_pillar
__virtualname__ = 'debconf'
if 'debconf.show' not in __salt__: return False
ret['comment'] = ('Database {0} is not present, so it cannot be removed' ).format(name) return ret
from salt.exceptions import SaltException
if __opts__['test'] is True:
from __future__ import absolute_import
import salt.ext.six as six
ret['result'] = None
if existing['code'] == 200:
elif existing['code'] == 404: response = __salt__['bigip.create_node'](hostname, username, password, name, address)
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
if new['code'] == 200:
else:
else: ret = _load_result(new, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A node with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(new, ret)
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A pool with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
else: ret = _load_result(deleted, ret)
if existing['code'] == 200:
else: ret = _load_result(new_member, ret)
if existing['code'] == 200:
if existing['code'] == 200:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
if existing['code'] == 200:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A Monitor with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404:
else: ret = _load_result(existing, ret)
if existing['code'] == 200:
elif existing['code'] == 404: ret['comment'] = 'A Profile with this name was not found.' else: ret = _load_result(existing, ret)
if existing['code'] == 200:
echo "manual" > /etc/init/salt-master.override
Ensure mylc exists: boto_lc.present: - name: mylc - image_id: ami-0b9c9f62 - profile: myprofile
from __future__ import absolute_import import logging import os import os.path from copy import deepcopy import json
'ID': val
}
if not bool(Versioning) and bool(_describe.get('Versioning')): Versioning = {'Status': 'Suspended'}
replication_item = ('Replication', 'put_replication', _describe.get('Replication', {}).get('ReplicationConfiguration'), _compare_replication, Replication, 'delete_replication')
if Replication is not None: config_items.append(versioning_item) config_items.append(replication_item) else: config_items.append(replication_item) config_items.append(versioning_item)
from __future__ import absolute_import import logging import os import os.path import json
import salt.utils from salt.ext.six import string_types
_describe = __salt__['boto_iot.describe_policy'](policyName=policyName, region=region, key=key, keyid=keyid, profile=profile)['policy']
_describe = __salt__['boto_iot.describe_topic_rule'](ruleName=ruleName, region=region, key=key, keyid=keyid, profile=profile)['rule']
ret['result'] = True return ret
import re
ret['comment'] = 'User {0} is already present'.format(name) return ret
ret['comment'] = 'User {0} is not present, so it cannot be removed'\ .format(name) return ret
from __future__ import absolute_import import json import logging
import salt.ext.six as six import json
agent_version = 1
if salt_params: for key, value in six.iteritems(params): params_from_salt[key] = value params_to_use = params_from_salt else: params_to_use = params
from __future__ import absolute_import import time import logging import re import traceback
from salt.utils import dictdiffer from salt.exceptions import CommandExecutionError
import salt.ext.six as six
else: ret['comment'] = 'Datasource updated.'
from __future__ import absolute_import
__virtualname__ = 'win_iis'
import logging
import salt.utils import salt.utils.validate.net from salt.ext.six.moves import range from salt.exceptions import CommandExecutionError
log = logging.getLogger(__name__)
__virtualname__ = 'network'
if gateway is not None: if not salt.utils.validate.net.ipv4_addr(gateway): errors.append('Gateway IP {0} is invalid.'.format(gateway))
if set(dns_servers or ['None']) != set(cur_dns_servers): changes['dns_servers'] = dns_servers
from __future__ import absolute_import import logging from time import time, sleep
import salt.utils.dictupdate as dictupdate from salt.utils import exactly_one from salt.exceptions import SaltInvocationError, CommandExecutionError
ret['result'] = False ret['comment'] = "Can't determine AllocationId for address {0}.".format(ip) return ret
if error: ret['changes'] = {} ret['result'] = False ret['comment'] = str(error)
import logging
from salt.ext.six import string_types import salt.utils
from __future__ import absolute_import import logging import os import os.path import hashlib import json
import salt.utils.dictupdate as dictupdate import salt.utils from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
update = True
Ensure mysecgroup exists: boto_secgroup.present: - name: mysecgroup - description: My security group - profile: myprofile
Ensure mysecgroup exists: boto_secgroup.present: - name: mysecgroup - description: My security group - profile: keyid: GKTADJGHEIQSXMKKRBJ08H key: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs region: us-east-1
import logging
import salt.utils.dictupdate as dictupdate from salt.exceptions import SaltInvocationError from salt.ext.six import string_types
if _rule.get('from_port') is None: _rule['from_port'] = -1 if _rule.get('to_port') is None: _rule['to_port'] = -1
from __future__ import absolute_import import logging import os import os.path import hashlib import re import json import yaml
import salt.utils from salt.ext.six import string_types
swagger = _Swagger(api_name, stage_name, lambda_funcname_format, swagger_file, common_args)
stage_vars = _get_stage_variables(stage_variables)
VENDOR_EXT_PATTERN = re.compile('^x-')
JSON_SCHEMA_DRAFT_4 = 'http://json-schema.org/draft-04/schema#'
AWS_API_DESCRIPTION = _dict_to_json_pretty({"provisioned_by": "Salt boto_apigateway.present State", "context": "See deployment or stage description"})
if 'schema' not in resobj: raise ValueError('missing schema field in path {0}, ' 'op {1}, response {2}'.format(path, opname, rescode))
for field in _Swagger.SWAGGER_OBJ_V2_FIELDS_REQUIRED: if field not in self._cfg: raise ValueError('Missing Swagger Object Field: {0}'.format(field))
ret['comment'] = 'stage {0} does not exist'.format(self._stage_name)
properties = obj_schema.get('properties') if properties: for _, prop_obj_schema in properties.iteritems(): dep_models_list.extend(self._build_dependent_model_list(prop_obj_schema))
models_dict.pop(next_model) for model, dep_list in models_dict.iteritems(): if next_model in dep_list: dep_list.remove(next_model)
_schema = self._update_schema_to_aws_notation(schema) _schema.update({'$schema': _Swagger.JSON_SCHEMA_DRAFT_4, 'title': '{0} Schema'.format(model)})
model_exists_response = __salt__['boto_apigateway.api_model_exists'](restApiId=self.restApiId, modelName=model, **self._common_aws_args)
lambda_desc = __salt__['boto_lambda.describe_function'](lambda_name, **self._common_aws_args)
from salt.ext.six import string_types
ret = {'name': name, 'result': False, 'comment': '', 'changes': {},
from __future__ import absolute_import import logging import time
try: from keystoneclient.apiclient.exceptions import \ Unauthorized as kstone_Unauthorized from glanceclient.exc import \ HTTPUnauthorized as glance_Unauthorized HAS_DEPENDENCIES = True except ImportError: HAS_DEPENDENCIES = False
while len(acceptable) > 1: if acceptable[0] == wait_for: break else: acceptable.pop(0)
if name in ret['changes']: ret['changes'][name]['new']['status'] = image['status']
image = __salt__['glance.image_show'](image['id'])
from __future__ import absolute_import, print_function import errno import logging import os
import salt.utils from salt.ext import six
if env is not None and not isinstance(env, dict): ret['comment'] = ('Invalidly-formatted \'env\' parameter. See ' 'documentation.') return ret
result = __salt__['splunk.update_user']( email, profile, **kwargs )
ret['result'] = None ret['comment'] = "No changes"
import salt.utils from salt.state import STATE_INTERNAL_KEYWORDS as _STATE_INTERNAL_KEYWORDS
continue
if chunk.get('fun') != low.get('fun'): continue
from __future__ import absolute_import import logging
import salt.utils from salt.exceptions import CommandExecutionError
log = logging.getLogger(__name__)
if not enabled: ret['result'] = True ret['comment'] = enabled_msg ret['changes'].update(enabled_changes) return ret
ret['result'] = True ret['comment'] = enabled_msg return ret
if clean_current_key[0] != clean_ssh_key[0] or clean_current_key[1] != clean_ssh_key[1]: ssh_key_changed = True
ssh_key_changed = True
try: lookup_key = _lookup_syslog_config(key) except KeyError: ret['comment'] = '\'{0}\' is not a valid config variable.'.format(key) return ret
ret['comment'] = 'Database {0} is not present, so it cannot ' \ 'be removed'.format(name) return ret
import logging import os
from salt import exceptions from salt.states.git import _fail, _neutral_test
import salt.loader import salt.utils import salt.utils.jid from salt.ext.six.moves import range
watch = salt.utils.alias_function(wait, 'watch')
{% for k, v in details['servers'].iteritems() %} {{ k }}: dellchassis.blade_idrac: - idrac_password: {{ v['idrac_password'] }} {% endfor %}
from __future__ import absolute_import import logging import os
from salt.exceptions import SaltInvocationError, CommandExecutionError
do_utc = False do_zone = False
if compzone is True: ret['result'] = True messages.append('Timezone {0} already set'.format(name)) else: do_zone = True
import difflib import salt.utils import salt.utils.network import salt.loader
import logging log = logging.getLogger(__name__)
apply_ranged_setting = False
if type == 'source': return ret
if apply_routes: try: __salt__['ip.apply_network_settings'](**kwargs) except AttributeError as error: ret['result'] = False ret['comment'] = str(error) return ret
if apply_net_settings: try: __salt__['ip.apply_network_settings'](**kwargs) except AttributeError as error: ret['result'] = False ret['comment'] = str(error) return ret
import re import os
from __future__ import absolute_import import logging
from __future__ import absolute_import from salt.exceptions import CommandExecutionError, CommandNotFoundError
import salt.ext.six as six
if pkg_ver: if installed_pkgs[pkg_name].get('version') != pkg_ver: pkgs_to_install.append(pkg) else: pkgs_satisfied.append(installed_name_ver)
cache_root_path = all_cached_pkgs[0] specific_pkg = '{0}/{1}/'.format(cache_root_path, name)
from __future__ import absolute_import import logging
import salt.utils.dictupdate as dictupdate
if subnet_names: for i in subnet_names: r = __salt__['boto_vpc.get_resource_id']('subnet', name=i, region=region, key=key, keyid=keyid, profile=profile)
if x['subnet_id'] not in subnet_ids and x['subnet_id'] is not None: to_delete.append(x['id'])
if user1['member_order'] == user2['member_order'] - 1: found = True break
import logging import salt.ext.six as six
is_stopped = False for proc in all_processes: if proc.startswith(name) \ and _is_stopped_state(all_processes[proc]['state']): is_stopped = True break
is_stopped = False for proc in all_processes: if proc.startswith(name) \ and _is_stopped_state(all_processes[proc]['state']): is_stopped = True break
ret['comment'] = "Service {0} doesn't exist".format(name)
return running( name, restart=restart, update=update, user=user, conf_file=conf_file, bin_env=bin_env )
from __future__ import absolute_import import logging import json import os
import salt.utils import salt.utils.odict as odict import salt.utils.dictupdate as dictupdate import salt.ext.six as six from salt.ext.six import string_types
try: from salt._compat import ElementTree as ET HAS_ELEMENT_TREE = True except ImportError: HAS_ELEMENT_TREE = False
import os import os.path import time import logging
import salt.utils from salt.ext.six.moves import range
log = logging.getLogger(__name__)
for i in range(10):
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError
from __future__ import absolute_import import os
import salt.utils as utils import salt.ext.six as six
if false_unsets is not True: ret['changes'].update({key: ''})
import logging
import salt.utils
__virtualname__ = 'system'
name = str(name)
name = str(name)
import logging import random from salt.state import _gen_tag from salt.exceptions import SaltInvocationError
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['changes'] = { 'testing': { 'old': 'Unchanged', 'new': 'Something pretended to change' } }
ret['result'] = random.choice([True, False])
present = _if_str_then_list(present) checks[None] = present boolean = _if_str_then_list(boolean) checks[bool] = boolean
integer = _if_str_then_list(integer) checks[int] = integer string = _if_str_then_list(string) checks[str] = string listing = _if_str_then_list(listing) checks[list] = listing dictionary = _if_str_then_list(dictionary) checks[dict] = dictionary
tenant = __salt__['keystone.tenant_get'](name=name, profile=profile, **connection_args)
role = __salt__['keystone.role_get'](name=name, profile=profile, **connection_args)
role = __salt__['keystone.service_get'](name=name, profile=profile, **connection_args)
from __future__ import absolute_import import logging
from salt.exceptions import CommandExecutionError import salt.utils
def __hash__(self): return (hash(self.srcport) ^ hash(self.destport) ^ hash(self.protocol) ^ hash(self.destaddr))
import salt.utils
apache: pkg: - installed - name: httpd service: - running - enable: True - name: httpd
return {'name': name, 'changes': {}, 'result': True, 'comment': ''}
- cn=foo,ou=users,dc=example,dc=com: - delete_others: True
- cn=admin,dc=example,dc=com: - delete_others: True - replace: cn: - admin description: - LDAP administrator objectClass: - simpleSecurityObject - organizationalRole userPassword: - {{pillar.ldap_admin_password}}
- 'olcDatabase={1}hdb,cn=config': - replace: olcRootDN: - cn=admin,dc=example,dc=com olcRootPW: []
pass
ldap3 = inspect.getmodule(connect)
dn_set = OrderedDict() dn_set.update(old) dn_set.update(new)
changed_old[dn] = o changed_new[dn] = n success_dn_set[dn] = True
try: return set((str(x) for x in thing)) except TypeError: return set((str(thing),))
from __future__ import absolute_import import difflib import itertools import logging import os import shutil import sys import traceback from collections import Iterable, Mapping, defaultdict
import salt.loader import salt.payload import salt.utils import salt.utils.templates import salt.utils.url from salt.utils.locales import sdecode from salt.exceptions import CommandExecutionError
import salt.ext.six as six from salt.ext.six.moves import zip_longest
return ret
pass
walk_l = list(_depth_limited_walk(name, max_depth)) walk_d = {} for i in walk_l: walk_d[i[0]] = (i[1], i[2])
fchange = _check_dir_meta(name, user, group, mode) if fchange: changes[name] = fchange if clean: keep = _gen_keep_files(name, require, walk_d)
return True, '', list(zip_longest(sources, source_hashes[:len(sources)]))
mode = __salt__['config.manage_mode'](mode)
if not __salt__['user.info'](user): user = __salt__['user.current']() if not user: user = 'SYSTEM'
mode = __salt__['config.manage_mode'](mode)
ret['comment'] = ('File {0} is not present and is not set for ' 'creation').format(name) return ret
return _error(ret, u_check)
source, source_hash = __salt__['file.source_list']( source, source_hash, __env__ )
if ret['changes']: ret = {'changes': {}, 'comment': '', 'name': name, 'result': True}
sfn = tmp_filename
if name[-1] == '/' and name != '/': name = name[:-1]
dir_mode = __salt__['config.manage_mode'](dir_mode) file_mode = __salt__['config.manage_mode'](file_mode)
return _error(ret, u_check)
if not children_only: ret, perms = __salt__['file.check_perms'](name, ret, user, group, dir_mode, follow_symlinks)
walk_l = list(_depth_limited_walk(name, max_depth)) walk_d = {} for i in walk_l: walk_d[i[0]] = (i[1], i[2])
dir_mode = __salt__['config.manage_mode'](dir_mode) file_mode = __salt__['config.manage_mode'](file_mode)
return _error(ret, u_check)
source_list = _validate_str_list(source)
if _ret['result'] is False or ret['result'] is True: ret['result'] = _ret['result']
if _ret['result'] is not True and _ret['comment']: add_comment(path, _ret['comment'])
pass_kwargs = {} faults = ['mode', 'makedirs'] for key in kwargs: if key not in faults: pass_kwargs[key] = kwargs[key]
srcpath = srcpath + '/'
if keep_symlinks: symlinks = __salt__['cp.list_master_symlinks'](__env__, srcpath) fns_ = process_symlinks(fns_, symlinks) for fn_ in fns_: if not fn_.strip(): continue
if maxdepth is not None: relpieces = relname.split('/') if not relpieces[-1]: relpieces = relpieces[:-1] if len(relpieces) > maxdepth + 1: continue
manage_directory(dirname) vdir.add(dirname)
all_files = __salt__['file.readdir'](name)
beginning_of_unix_time = datetime(1970, 1, 1)
return (None, None)
RETAIN_TO_DEPTH = { 'first_of_year': 1, 'first_of_month': 2, 'first_of_day': 3, 'first_of_hour': 4, 'most_recent': 5, }
retained_files |= get_first_n_at_depth(files_by_y_week_dow, first_of_week_depth, keep_count + 1)
- pattern: | CentOS \(2.6.32[^\n]+\n\s+root[^\n]+\n\)+
__salt__['file.comment_line'](name, regex, char, True, backup)
ret['result'] = __salt__['file.search'](name, unanchor_regex, multiline=True)
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
__salt__['file.comment_line'](name, regex, char, False, backup)
ret['result'] = __salt__['file.search']( name, '^[ \t]*{0}'.format(regex.lstrip('^')), multiline=True )
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
(ok_, err, sl_) = _unify_sources_and_hashes(source=source, source_hash=source_hash, sources=sources, source_hashes=source_hashes) if not ok_: return _error(ret, err)
touch(name, makedirs=makedirs) retry_res, retry_msg = _check_file(name) if not retry_res: return _error(ret, check_msg)
if sl_: tmpret = _get_template_texts(source_list=sl_, template=template, defaults=defaults, context=context) if not tmpret['result']: return tmpret text = tmpret['data']
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
(ok_, err, sl_) = _unify_sources_and_hashes(source=source, source_hash=source_hash, sources=sources, source_hashes=source_hashes) if not ok_: return _error(ret, err)
touch(name, makedirs=makedirs) retry_res, retry_msg = _check_file(name) if not retry_res: return _error(ret, check_msg)
if sl_: tmpret = _get_template_texts(source_list=sl_, template=template, defaults=defaults, context=context) if not tmpret['result']: return tmpret text = tmpret['data']
if not header: if __salt__['file.search']( name, salt.utils.build_whitespace_split_regex(chunk), multiline=True): continue
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
ret['changes']['diff'] = ( ''.join(difflib.unified_diff(slines, nlines)) )
return _error(ret, u_check)
name = os.path.join(name, os.path.basename(source))
ret['comment'] = ('File {0} is not present and is not set for ' 'creation').format(name) return ret
return True
ret = {'name': name, 'result': False, 'comment': '', 'changes': {},
ret = {'name': name, 'result': False, 'comment': '', 'changes': {},
ret['result'] = False ret['comment'] = 'Failed to create data pipeline {0}: {1}'.format( name, result_pipeline_definition['error']) return ret
if 'enabled' not in new_item: new_item['enabled'] = True
import re
from salt.exceptions import CommandExecutionError
if 'sysctl.default_config' in __salt__: config = __salt__['sysctl.default_config']() else: config = '/etc/sysctl.conf'
ret['result'] = None ret['comment'] = ( 'Sysctl option {0} would be changed to {1}'.format(name, value) ) return ret
from __future__ import absolute_import import logging
import salt.utils import salt.ext.six as six from salt.exceptions import CommandExecutionError
if existing_vhost == '' and perms == ['', '', '']: continue perm_need_change = True
from __future__ import absolute_import import logging import os import os.path import json
from salt.ext.six import string_types
import logging
import salt.utils
__virtualname__ = 'apt'
import logging import os
import salt.utils
certs = __salt__['keychain.list_certs'](keychain)
import logging
from salt.modules import postgres
mode = 'create' group_attr = __salt__['postgres.role_get']( name, return_password=not refresh_password, **db_args) if group_attr is not None: mode = 'update'
Ensure myservice dashboard is managed: grafana.dashboard_present: - name: myservice - dashboard_from_pillar: default - rows_from_pillar: - systemhealth - requests
if isinstance(pillar_rows, list): for row in pillar_rows: rows.append(row) else: rows.append(pillar_rows)
func = 'modjk.{0}'.format(cmd) args = [worker, lbn, profile] response = __salt__['publish.publish'](target, func, args, expr_form)
errors = [] minions = [] for minion in response: minions.append(minion) if not response[minion]: errors.append(minion)
if not status: ret['result'] = False return ret
for balancer in status: if not status[balancer]: ret['errors'].append(balancer) elif status[balancer]['activation'] != activation: ret['wrong_state'].append(balancer)
from __future__ import absolute_import import re import os import logging import tarfile from contextlib import closing
import salt.ext.six as six
from salt.exceptions import CommandExecutionError import salt.utils
try: file_result = file_result[next(six.iterkeys(file_result))] except AttributeError: pass
myrole: boto_iam_role.present: - profile: myiamprofile
myrole: boto_iam_role.present: - profile: key: GKTADJGHEIQSXMKKRBJ08H keyid: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs region: us-east-1
ret['comment'] = ('User {0} is not present, so it cannot be removed' ).format(name) return ret
import os import stat import itertools
import salt.runner import salt.utils import salt.config import salt.syspaths
winrepo_cachefile = os.path.join(winrepo_dir, winrepo_cachefile)
from __future__ import absolute_import import logging
result = __salt__['github.add_user']( name, profile=profile, **kwargs )
Ensure mykey key exists: boto_kms.key_present: - name: mykey - region: us-east-1 - profile: myprofile
Ensure myelb ELB exists: boto_elb.present: - name: myelb - region: us-east-1 - profile: myelbprofile
Ensure myelb ELB exists: boto_elb.present: - name: myelb - region: us-east-1 - profile: keyid: GKTADJGHEIQSXMKKRBJ08H key: askdjghsdfjkghWupUjasdflkdfklgjsdfjajkghs
- attributes: cross_zone_load_balancing: enabled: false - profile: myelbprofile
- alarms: UnHealthyHostCount: attributes: threshold: 2.0
from __future__ import absolute_import
import hashlib import re import salt.utils.dictupdate as dictupdate from salt.exceptions import SaltInvocationError import salt.ext.six as six
tmp = __salt__['config.option'](attributes_from_pillar, {}) if attributes: attributes = dictupdate.update(tmp, attributes) else: attributes = tmp
for p in listener_policies: if re.match(r'^ELBSecurityPolicy-\d{4}-\d{2}$', p): default_aws_policies.add(p)
from __future__ import absolute_import import logging
import salt.utils
import logging import re
value = str(value).lower()
import re import os.path import logging import difflib
import salt.utils
__virtualname__ = 'mongodb_user'
- user: admin - password: sekrit
user_exists = __salt__['mongodb.user_exists'](name, user, password, host, port, database, authdb) if user_exists is True: return ret
if not isinstance(user_exists, bool): ret['comment'] = user_exists ret['result'] = False return ret
if not isinstance(user_exists, bool): ret['comment'] = user_exists ret['result'] = False return ret
ret['comment'] = ('User {0} is not present, so it cannot be removed' ).format(name) return ret
Ensure myasg is deleted: boto_asg.absent: - name: myasg - force: True
from __future__ import absolute_import import hashlib import logging import copy
import salt.utils.dictupdate as dictupdate
import salt.ext.six as six
if scheduled_actions: tmp = dictupdate.update(tmp, scheduled_actions) return tmp
import logging
import logging import os
import salt.utils import salt.utils.files import salt.utils.atomicfile from salt.utils.odict import OrderedDict
__virtualname__ = 'smartos'
current_keys = set(current.keys()) state_keys = set(state.keys())
config = _load_config()
if isinstance(value, (bool)): value = 'true' if value else 'false' if not value: value = ""
ret['result'] = True ret['comment'] = 'property {0} already has value "{1}"'.format(name, value)
if not __opts__['test'] and len(ret['changes']) > 0: ret['result'] = _write_config(config)
config = _load_config()
ret['result'] = True ret['comment'] = 'property {0} deleted'.format(name) ret['changes'][name] = None del config[name]
ret['result'] = True ret['comment'] = 'property {0} is absent'.format(name)
if not __opts__['test'] and len(ret['changes']) > 0: ret['result'] = _write_config(config)
ret['result'] = True ret['comment'] = 'image {0} is present'.format(name)
ret['result'] = True ret['comment'] = 'image {0} is absent'.format(name)
images = []
for state in __salt__['state.show_lowstate'](): if 'state' not in state: continue
for image_uuid in __salt__['vmadm.list'](order='image_uuid'): if image_uuid in images: continue images.append(image_uuid)
vmconfig = _parse_vmconfig(vmconfig, vmconfig_type['instance']) log.debug('smartos.vm_present::{0}::vmconfig - {1}'.format(name, vmconfig))
if 'hostname' not in vmconfig: vmconfig['hostname'] = name
if vmconfig['hostname'] in __salt__['vmadm.list'](order='hostname'): ret['result'] = True
for prop in vmconfig['state']: if prop in vmconfig_type['instance'] or \ prop in vmconfig_type['collection'] or \ prop in vmconfig_type['create_only']: continue
vmconfig['changed'][prop] = vmconfig['state'][prop]
for collection in vmconfig_type['collection']: if collection in vmconfig_type['create_only']: continue
if 'set_{0}'.format(collection) not in vmconfig['changed']: vmconfig['changed']['set_{0}'.format(collection)] = {}
vmconfig['changed']['set_{0}'.format(collection)][prop] = vmconfig['state'][collection][prop]
if 'remove_{0}'.format(collection) not in vmconfig['changed']: vmconfig['changed']['remove_{0}'.format(collection)] = []
vmconfig['changed']['remove_{0}'.format(collection)].append(prop)
for instance in vmconfig_type['instance']: if instance in vmconfig_type['create_only']: continue
if instance in vmconfig['state'] and vmconfig['state'][instance] is not None: for state_cfg in vmconfig['state'][instance]: add_instance = True
for current_cfg in vmconfig['current'][instance]: if vmconfig_type['instance'][instance] not in state_cfg: continue
add_instance = False
if len(changed) > 0: for prop in changed: update_cfg[prop] = state_cfg[prop]
for prop in state_cfg: if isinstance(state_cfg[prop], (list)) and len(state_cfg[prop]) == 0: continue
if 'add_{0}'.format(instance) not in vmconfig['changed']: vmconfig['changed']['add_{0}'.format(instance)] = []
vmconfig['changed']['add_{0}'.format(instance)].append(state_cfg)
if instance in vmconfig['current'] and vmconfig['current'][instance] is not None: for current_cfg in vmconfig['current'][instance]: remove_instance = True
if instance in vmconfig['state'] and vmconfig['state'][instance] is not None: for state_cfg in vmconfig['state'][instance]: if vmconfig_type['instance'][instance] not in state_cfg: continue
remove_instance = False
if 'remove_{0}'.format(instance) not in vmconfig['changed']: vmconfig['changed']['remove_{0}'.format(instance)] = []
vmconfig['changed']['remove_{0}'.format(instance)].append( current_cfg[vmconfig_type['instance'][instance]] )
ret['result'] = True ret['comment'] = 'vm {0} is absent'.format(name)
if not __opts__['test']: if archive: __salt__['vmadm.update'](vm=name, key='hostname', archive_on_delete=True)
ret['result'] = True ret['comment'] = 'vm {0} already running'.format(name)
ret['result'] = True ret['comment'] = 'vm {0} already stopped'.format(name)
from __future__ import absolute_import import datetime import os import re import copy
import salt.exceptions import salt.utils
import salt.ext.six as six
import logging
from salt.modules import postgres import salt.ext.six as six
if encrypted is not False: encrypted = postgres._DEFAULT_PASSWORDS_ENCRYPTION password = postgres._maybe_encrypt_password(name, password, encrypted=encrypted)
mode = 'create' user_attr = __salt__['postgres.role_get']( name, return_password=not refresh_password, **db_args) if user_attr is not None: mode = 'update'
from __future__ import generators from __future__ import absolute_import import logging import socket
import salt.utils import salt.utils.cloud as suc from salt.exceptions import SaltCloudException
ret['comment'] = ret['comment'] + ' and will be started' ret['result'] = None return ret
from __future__ import absolute_import import sys
import salt.ext.six as six
if set(lgrp['members']) ^ set(members): change['members'] = members
if __opts__['test']: ret['result'] = None ret['comment'] = 'Group {0} set to be added'.format(name) return ret
if gid is not None: gid_group = None for lgrp in grps: if lgrp['gid'] == gid: gid_group = lgrp['name'] break
import logging
if __opts__['test']: ret['result'] = None ret['changes'] = {'reg': {'Will add': add_change}} return ret
ret['result'] = __salt__['reg.set_value'](hive=hive, key=key, vname=vname, vdata=vdata, vtype=vtype, use_32bit_registry=use_32bit_registry)
if __opts__['test']: ret['result'] = None ret['changes'] = {'reg': {'Will remove': remove_change}} return ret
if __opts__['test']: ret['result'] = None return ret
from __future__ import absolute_import import re import sys
import salt.ext.six as six
if source != '': source_path = __salt__['cp.get_url']( source, None, saltenv=__env__)
from __future__ import absolute_import import os import logging
import salt.utils from salt.utils.locales import sdecode, sdecode_if_string
from salt.ext.six import string_types, iteritems
ret['changes']['home'] = ''
upper_name = name.upper()
return ret
upper_name = name.upper()
return 'telemetry_alert' if 'telemetry.get_alert_config' in __salt__ else False
import logging from time import strftime, strptime, gmtime
__virtualname__ = 'zfs'
if ret['result']:
if '@' not in snapshot: ret['result'] = False ret['comment'] = 'invalid snapshot name: {0}'.format(snapshot)
if '@' not in snapshot: ret['result'] = False ret['comment'] = 'invalid snapshot name: {0}'.format(snapshot)
if not properties: properties = {}
if not properties: properties = {}
if not properties: properties = {}
prunable = [] snapshots = {} for key in schedule.keys(): snapshots[key] = []
needed_holds = [] current_timestamp = gmtime() for hold in snapshots.keys(): if schedule[hold] == 0: continue
needed_holds.append(hold)
from __future__ import absolute_import import re
ret = {'name': name, 'result': None, 'comment': '', 'changes': {},
from __future__ import absolute_import
import salt.utils
import salt.ext.six as six
if acl_name == '': _search_name = __current_perms[name].get('comment').get(_acl_type) else: _search_name = acl_name
if acl_name == '': _search_name = __current_perms[name].get('comment').get(_acl_type) else: _search_name = acl_name
from __future__ import absolute_import
from __future__ import absolute_import import datetime import math import sys import logging import copy
import salt.ext.six as six import salt.utils.dictupdate as dictupdate
from __future__ import absolute_import import time import datetime
if delta_remaining < delta_min: ret['comment'] = 'Certificate will expire in {0}, which is less than {1}'.format(delta_remaining, delta_min) return ret
from __future__ import absolute_import
from salt.exceptions import CommandExecutionError
import os
import salt.utils from salt.exceptions import CommandNotFoundError
ret['changes'] = {'feature': __salt__['win_servermanager.install'](name, recurse, restart)}
import logging log = logging.getLogger(__name__)
for key, value in kwargs.items(): if key in old: if value == 'max': value = old['{0}_max'.format(key)]
if error: ret['changes'] = {} ret['result'] = False ret['comment'] = str(error)
ret = { 'name': name, 'changes': {}, 'result': False, 'comment': '', }
existing_config = None if __salt__['chronos.has_job'](name): existing_config = __salt__['chronos.job'](name)['job']
from __future__ import absolute_import import sys import os.path
import salt.utils
import salt.ext.six as six
from __future__ import absolute_import import logging
import salt.utils
import salt.ext.six as six
log = logging.getLogger(__name__)
__virtualname__ = 'raid'
raids = __salt__['raid.list']() if raids.get(name): ret['comment'] = 'Raid {0} already present'.format(name) return ret
can_assemble = {} for dev in devices: cmd = 'mdadm -E {0}'.format(dev) can_assemble[dev] = __salt__['cmd.retcode'](cmd) == 0
if do_assemble: __salt__['raid.assemble'](name, devices, **kwargs) else: __salt__['raid.create'](name, level, devices, **kwargs)
__salt__['raid.save_config']()
import logging log = logging.getLogger(__name__)
from salt.ext import six
if not configured_probes: return { 'add': expected_probes }
if not expected_probes: return { 'remove': configured_probes }
for probe_name in new_probes_keys_set: new_probes[probe_name] = expected_probes.pop(probe_name)
for probe_name in remove_probes_keys_set: remove_probes[probe_name] = configured_probes.pop(probe_name)
configured_probes = rpm_probes_config.get('out', {}) if not isinstance(defaults, dict): defaults = {} expected_probes = _expand_probes(probes, defaults)
comment += ('\n' + config_comment)
from __future__ import absolute_import import sys
import salt.utils
kwargs['name'] = repo
if __grains__['os_family'] == 'RedHat': if not salt.utils.is_true(sanitizedkwargs[kwarg]): needs_update = True else: needs_update = True
if kwargs.get('clean_file', False): salt.utils.fopen(kwargs['file'], 'w').close()
ret['result'] = False ret['comment'] = \ 'Failed to configure repo \'{0}\': {1}'.format(name, exc) return ret
if ret['changes']: sys.modules[ __salt__['test.ping'].__module__ ].__context__.pop('pkg._avail', None)
from __future__ import absolute_import
from salt.utils import SaltInvocationError import logging log = logging.getLogger(__name__)
import os import fnmatch
import salt.utils from salt.exceptions import CommandExecutionError
import copy import logging import os import re import string from distutils.version import LooseVersion as _LooseVersion
import salt.utils import salt.utils.url from salt.exceptions import CommandExecutionError from salt.ext import six
return None
ret.append(local_branch if branch is None else branch) ret.append(desired_upstream)
ret['comment'] += '\n\nChanges made: ' + comments
git@github.com:user/repo.git: git.latest: - user: deployer - identity: /home/deployer/.ssh/id_rsa
git@github.com:user/repo.git: git.latest: - user: deployer - identity: - /home/deployer/.ssh/id_rsa - /home/deployer/.ssh/id_rsa_alternate
cret = mod_run_check( run_check_cmd_kwargs, onlyif, unless ) if isinstance(cret, dict): ret.update(cret) return ret
desired_upstream = None remote_rev_type = 'sha1'
remote_rev = None remote_rev_type = None
remote_rev = all_remote_refs['refs/tags/' + rev + '^{}'] remote_rev_type = 'tag'
remote_rev = all_remote_refs['refs/tags/' + rev] remote_rev_type = 'tag'
rev = rev.lower() remote_rev = rev remote_rev_type = 'sha1'
return _fail( ret, 'No revision matching \'{0}\' exists in the remote ' 'repository'.format(rev) )
pass
fast_forward = None
upstream = None
ret['comment'] = _format_comments(actions) return ret
if base_rev is None: fast_forward = True else: fast_forward = __salt__['git.merge_base']( target, refs=[base_rev, remote_rev], is_ancestor=True, user=user)
local_branch = local_rev = None
remote_ref_type = 'ref' if len(ref) <= 40 \ and all(x in string.hexdigits for x in ref): ref = ref.lower() remote_ref_type = 'hash'
hash_exists_locally = True
remotes = __salt__['git.remotes'](target, user=user, redact_auth=False)
current_fetch_url = None if remote in remotes: current_fetch_url = remotes[remote]['fetch']
mylocalrepo: git.config_unset: - name: foo.bar - value_regex: 'baz' - repo: /path/to/repo
mylocalrepo: git.config_unset: - name: foo.bar - all: True
mylocalrepo: git.config_unset: - name: 'foo\..+' - all: True
key = '^' + name.lstrip('^').rstrip('$') + '$'
pre_matches = __salt__['git.config_get_regexp']( cwd=repo, key=key, value_regex=value_regex, user=user, ignore_retcode=True, **{'global': global_} )
return ret
pre = __salt__['git.config_get_regexp']( cwd=repo, key=key, value_regex=None, user=user, ignore_retcode=True, **{'global': global_} )
mylocalrepo: git.config_set: - name: user.email - value: foo@bar.net - repo: /path/to/repo
mylocalrepo: git.config_set: - name: mysection.myattribute - multivar: - foo - bar - baz - repo: /path/to/repo
pre = __salt__['git.config_get']( cwd=repo, key=name, user=user, ignore_retcode=True, **{'all': True, 'global': global_} )
post = __salt__['git.config_set']( cwd=repo, key=name, value=value, multivar=multivar, user=user, **{'global': global_} )
return True
from __future__ import absolute_import
__virtualname__ = 'etcd'
__func_alias__ = { 'set_': 'set', 'rm_': 'rm' }
try:
if kwargs.get('sfun') in ['wait_rm_key', 'wait_rm']: return rm_( name, kwargs.get('profile'))
if error: ret['changes'] = {} ret['result'] = False ret['comment'] = str(error)
import logging
from __future__ import absolute_import import logging import os
from salt.exceptions import SaltInvocationError from salt.utils import exactly_one
mytopic: boto_sns.present: - region: us-east-1 - profile: mysnsprofile
import re
_subscriptions = __salt__['boto_sns.get_all_subscriptions_by_topic']( name, region=region, key=key, keyid=keyid, profile=profile )
_subscriptions = [ {'protocol': s['Protocol'], 'endpoint': s['Endpoint']} for s in _subscriptions ]
if matches is not None: subscription['endpoint'] = _endpoint.replace( matches.groupdict()['pass'], '****')
subscription['endpoint'] = _endpoint
import logging import os
import salt.utils import salt.ext.six as six
item_id_show = item_id if item in ['constraint'] or '=' in item_id: item_id_show = None
else: if is_existing['retcode'] in [0]: item_create_required = False
import re
ret['changes']['summary'] = _summary(result['stdout']) ret['result'] = True if not __opts__['test'] else None
commit = False current_rules = __salt__['firewall.get_rule'](name) if not current_rules: commit = True ret['changes'] = {'new rule': name}
from __future__ import absolute_import import pprint
if __opts__.get('requests_lib', False): from requests.exceptions import HTTPError else: from urllib2 import HTTPError
if __opts__.get('requests_lib', False): from requests.exceptions import HTTPError else: from urllib2 import HTTPError
import os import stat import logging
from salt.utils.odict import OrderedDict
__virtualname__ = 'zpool'
ret['result'] = False
properties_current = __salt__['zpool.get'](name)[name]
properties_update = [] for prop in properties: if prop not in properties_current: continue
for prop in properties_update: value = properties[prop] res = __salt__['zpool.set'](name, prop, value)
if isinstance(value, bool): value = 'on' if value else 'off' elif ' ' in value: value = "'{0}'".format(value)
params = [] params.append(name) for root_dev in layout:
from __future__ import absolute_import
import salt.ext.six as six
__virtualname__ = 'sysrc'
ret['result'] = None
ret['result'] = None
import logging import salt.utils
ret['changes'] = {'new': '', 'old': name}
import salt.utils from salt.ext.six.moves import map
BSD = ('OpenBSD', 'FreeBSD')
from __future__ import absolute_import import logging
import salt.utils
if __grains__['os'] in ['MacOS', 'Darwin']: ret['changes'] = {'new': []}
if __grains__['os'] in ['Windows']: changes_needed = False current_settings = __salt__['proxy.get_proxy_win']() current_domains = __salt__['proxy.get_proxy_bypass']()
if service not in current_settings: changes_needed = True break
changes_needed = True
if len(set(current_domains).intersection(bypass_domains)) != len(bypass_domains): changes_needed = True
import logging import copy
try: import confidant.client import confidant.formatter HAS_LIBS = True except ImportError: HAS_LIBS = False
log = logging.getLogger(__name__)
from __future__ import absolute_import import logging
from __future__ import absolute_import import logging import salt.utils.vault
import logging from uuid import uuid4 try: import couchdb HAS_COUCH = True except ImportError: HAS_COUCH = False
from salt.utils.decorators import memoize
__func_alias__ = {'set_': 'set'}
import logging
import salt.utils.memcached
import logging
from __future__ import absolute_import import logging
import logging import codecs try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
import msgpack
from os import environ
return environ.setdefault(key, value)
return environ.get(key)
port: 8000 ssl_crt: /etc/pki/api/certs/server.crt ssl_key: /etc/pki/api/certs/server.key debug: False disable_ssl: False websockets: True
ws = create_connection('wss://localhost:8000/all_events/d0ce6c1a37e99dcc0374392f272fe19c0090cca7')
ws.send('websocket client ready')
while listening_to_events:
ws.close()
ws = create_connection('wss://localhost:8000/formatted_events/d0ce6c1a37e99dcc0374392f272fe19c0090cca7')
ws.send('websocket client ready')
while listening_to_events:
ws.close()
logger.debug('Websocket already connected, returning') return
pass
logger.debug('Websocket already connected, returning') return
except Exception as err: logger.debug('Error! Ending server side websocket connection. Reason = {0}'.format(str(err))) break
pass
if mod_opts.get('websockets', False): from . import saltnado_websockets
(all_events_pattern, saltnado_websockets.AllEventsHandler), (formatted_events_pattern, saltnado_websockets.FormattedEventsHandler),
from __future__ import absolute_import import json import logging import threading import salt.ext.six as six
self.jobs = {}
self.minions = {}
curr_minion = {} curr_minion.update(minion_info) curr_minion.update({'id': minion}) minions[minion] = curr_minion
from __future__ import absolute_import, print_function
port: 8000 address: 0.0.0.0 backlog: 128 ssl_crt: /etc/pki/api/certs/server.crt ssl_key: /etc/pki/api/certs/server.key debug: False disable_ssl: False webhook_disable_auth: False cors_origin: null
import time import math import fnmatch import logging from copy import copy from collections import defaultdict
import cgi import yaml import tornado.httpserver import tornado.ioloop import tornado.web import tornado.gen from tornado.concurrent import Future from zmq.eventloop import ioloop import salt.ext.six as six
ioloop.install()
import salt.netapi import salt.utils import salt.utils.event from salt.utils.event import tagify import salt.client import salt.runner import salt.auth from salt.exceptions import EauthAuthenticationError
if not self.done(): self.set_result(future)
self.tag_map = defaultdict(list)
self.request_map = defaultdict(list)
self.timeout_map = {}
self._timeout_future(tag, future) if future in self.timeout_map: tornado.ioloop.IOLoop.current().remove_timeout(self.timeout_map[future]) del self.timeout_map[future]
if request._finished: future = Future() future.set_exception(TimeoutException()) return future
self.tag_map[tag].append(future) self.request_map[request].append((tag, future))
if AUTH_TOKEN_HEADER in self.request.headers: return self.request.headers[AUTH_TOKEN_HEADER] else: return self.get_cookie(AUTH_COOKIE_NAME)
accept_header = self.request.headers.get('Accept', '*/*') parsed_accept_header = [cgi.parse_header(h)[0] for h in accept_header.split(',')]
if not content_type: self.send_error(406)
self.start = time.time() self.connected = True
self.timeout_futures()
'text/plain': json.loads
header = cgi.parse_header(self.request.headers['Content-Type']) value, parameters = header return ct_in_map[value](data)
request_headers = self.request.headers.get('Access-Control-Request-Headers') allowed_headers = request_headers.split(',')
self.set_header('Access-Control-Allow-Headers', ','.join(allowed_headers))
self.set_header('Access-Control-Expose-Headers', 'X-Auth-Token')
self.set_header('Access-Control-Allow-Methods', 'OPTIONS, GET, POST')
except KeyError: self.send_error(400) return
try: perms = self.application.opts['external_auth'][token['eauth']][token['name']]
except KeyError: self.send_error(401) return
if not self._verify_auth(): self.redirect('/login') return
for low in self.lowstate: if not self._verify_client(low): return
if self.token is not None and 'token' not in low: low['token'] = self.token
if len(inflight_futures) == 0: continue
finished_future = yield Any(inflight_futures) try: b_ret = finished_future.result() except TimeoutException: break chunk_ret.update(b_ret) inflight_futures.remove(finished_future)
minions_remaining = pub_data['minions']
if syndic_min_wait is not None: yield syndic_min_wait chunk_ret = yield self.all_returns(pub_data['jid'], finish_futures=[job_not_running], minions_remaining=minions_remaining, )
try: minions_remaining.remove(event['data']['id']) except ValueError: pass if len(minions_remaining) == 0: raise tornado.gen.Return(chunk_ret)
pub_data = self.saltclients['local_async'](*f_call.get('args', ()), **f_call.get('kwargs', {}))
raise tornado.gen.Return(event['data']['return'])
if not self._verify_auth(): self.redirect('/login') return
if not self._verify_auth(): self.redirect('/login') return
if not self._verify_auth(): self.redirect('/login') return
tag = 'salt/netapi/hook' if tag_suffix: tag += tag_suffix
'headers': dict(self.request.headers),
return allowed_origins
import inspect import os
if not self._is_master_running(): raise salt.exceptions.SaltDaemonNotRunning( 'Salt Master is not available.')
from __future__ import absolute_import import cherrypy
self.pipe = None
self.token = None
self.opts = None
import logging import os
try: import cherrypy
if not cpy_error and 'port' in mod_opts: return __virtualname__
if cpy_error:
if 'port' not in mod_opts: logger.error("Not loading '%s'. 'port' not specified in config", __name__)
from __future__ import absolute_import import json import logging
import salt.ext.six as six
import salt.netapi
self.jobs = {}
self.minions = {}
dropped_minions = set(curr_minions) - set(minions_detected)
new_minions = set(minions_detected) - set(curr_minions)
curl -sSk https://localhost:8000/login \\ -c ~/cookies.txt \\ -H 'Accept: application/x-yaml' \\ -d username=saltdev \\ -d password=saltdev \\ -d eauth=auto
curl -sSk https://localhost:8000 \\ -b ~/cookies.txt \\ -H 'Accept: application/x-yaml' \\ -d client=local \\ -d tgt='*' \\ -d fun=test.ping
from __future__ import absolute_import import collections import itertools import functools import logging import json import StringIO import tarfile import time from multiprocessing import Process, Pipe
import cherrypy from cherrypy.lib import cpstats import yaml import salt.ext.six as six
import salt import salt.auth import salt.utils.event
import salt.netapi
try: from .tools import websockets from . import event_processor
if x_auth: cherrypy.request.cookie['session_id'] = x_auth
cherrypy.response.headers['Cache-Control'] = 'private'
resp_head['Access-Control-Allow-Origin'] = req_head.get('Origin', '*') resp_head['Access-Control-Expose-Headers'] = 'GET, POST' resp_head['Access-Control-Allow-Credentials'] = 'true'
if cherrypy.request.method == 'OPTIONS': cherrypy.serving.request.handler = cors_handler
ct_out_map = ( ('application/json', json.dumps), ('application/x-yaml', functools.partial( yaml.safe_dump, default_flow_style=False)), )
best = cherrypy.lib.cptools.accept([i for (i, _) in ct_out_map])
cherrypy.response.headers['Content-Type'] = best out = cherrypy.response.processors[best] return out(ret)
cherrypy._cpreqbody.process_urlencoded(entity) cherrypy.serving.request.unserialized_data = entity.params cherrypy.serving.request.raw_body = ''
ct_in_map = { 'application/x-www-form-urlencoded': urlencoded_processor, 'application/json': json_processor, 'application/x-yaml': yaml_processor, 'text/yaml': yaml_processor, 'text/plain': text_processor, }
cherrypy.request.lowstate = [data]
if cherrypy.request.config.get('tools.sessions.on', False): cherrypy.session.release_lock()
if not isinstance(lowstate, list): raise cherrypy.HTTPError(400, 'Lowstates must be a list')
if 'arg' in chunk and not isinstance(chunk['arg'], list): chunk['arg'] = [chunk['arg']]
if isinstance(ret, collections.Iterator): for i in ret: yield i else: yield ret
curl -sSik https://localhost:8000 \\ -d client=local \\ -d tgt='*' \\ -d fun='cmd.run' \\ -d arg='du -sh .' \\ -d arg='/path/to/dir'
curl -sSik https://localhost:8000 \\ -d client=runner \\ -d fun='jobs.lookup_jid' \\ -d jid='20150129182456704682' \\ -d outputter=highstate
if isinstance(cherrypy.serving.request.lowstate, list): creds = cherrypy.serving.request.lowstate[0] else: creds = cherrypy.serving.request.lowstate
if not salt_api_acl_tool(username, cherrypy.request): raise cherrypy.HTTPError(401)
token = self.auth.mk_token(creds) if 'token' not in token: raise cherrypy.HTTPError(401, 'Could not authenticate using provided credentials')
try: eauth = self.opts.get('external_auth', {}).get(token['eauth'], {})
perms = eauth.get(token['name'], []) perms.extend(eauth.get('*', []))
'tools.salt_token.on': True, 'tools.salt_auth.on': False,
if salt_token and self.resolver.get_token(salt_token): return True
cherrypy.session.release_lock()
'tools.salt_token.on': True, 'tools.salt_auth.on': False,
while listening_to_events: print ws.recv()
if not salt_token or not self.auth.get_tok(salt_token): raise cherrypy.HTTPError(401)
cherrypy.session.release_lock()
handler = cherrypy.request.ws_handler
pipe.recv()
proc = Process(target=event_stream, args=(handler, child_pipe)) proc.start()
'tools.lowdata_fmt.on': True,
'tools.salt_token.on': True, 'tools.salt_auth.on': True,
self.url_map.update({ self.apiopts.get('webhook_url', 'hook').lstrip('/'): Webhook, })
cherrypy.config.update(conf['global'])
cherrypy.config['saltopts'] = opts cherrypy.config['apiopts'] = apiopts
import salt import salt.netapi
saltenviron(environ)
try: ret = json.dumps({'return': resp}) except TypeError as exc: code = 500 ret = str(exc)
start_response(H[code], get_headers(ret, { 'Content-Type': 'application/json', })) return (ret,)
if '__opts__' not in globals(): globals()['__opts__'] = get_opts()
httpd = make_server('localhost', mod_opts['port'], application)
from __future__ import absolute_import import logging
import salt.client from salt.ext import six from salt.ext.six.moves import zip
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
from __future__ import absolute_import
import salt.utils.reactor
from __future__ import absolute_import import multiprocessing import logging
import salt import salt.loader import salt.utils from salt.utils.process import SignalHandlingMultiprocessingProcess
if salt.utils.is_windows(): runners = None utils = None funcs = None
from __future__ import absolute_import import datetime import json import logging import pprint import time try: import slackclient HAS_SLACKCLIENT = True except ImportError: HAS_SLACKCLIENT = False
import salt.client import salt.loader import salt.runner import salt.utils import salt.utils.event import salt.utils.http import salt.utils.slack
channel = sc.server.channels.find(_m['channel'])
cmdline = salt.utils.shlex_split(_text[len(trigger):]) cmd = cmdline[0] args = [] kwargs = {}
if valid_commands: if cmd not in valid_commands: channel.send_message('Using {0} is not allowed.'.format(cmd)) return
else: local = salt.client.LocalClient() ret = local.cmd('{0}'.format(target), cmd, args, kwargs)
fire('{0}/{1}'.format(tag, _m['type']), _m)
fire('{0}/{1}'.format(tag, _m['type']), _m)
from __future__ import absolute_import import logging import time import json
import salt.utils.event
try: import boto.sqs HAS_BOTO = True except ImportError: HAS_BOTO = False
import salt.utils.event from salt.ext import six
try: import certifi HAS_CERTIFI = True except ImportError: HAS_CERTIFI = False
try: import ssl HAS_SSL = True
import socket import random import time import codecs import uuid import logging import json
self.INVALID_TOKEN = ("\n\nIt appears the LOGENTRIES_TOKEN " "parameter you entered is incorrect!\n\n") self.LINE_SEP = _to_unicode(r'\u2028')
time.sleep(30) raise UserWarning("Unable to connect to room {0}".format(room))
if valid_users: if partner not in valid_users: target_room.message('{0} not authorized to run Salt commands'.format(partner)) return
if 'target' not in kwargs: target = '*' else: target = kwargs['target'] del kwargs['target']
if valid_commands: if cmd not in valid_commands: target_room.message('Using {0} is not allowed.'.format(cmd)) return
else: local = salt.client.LocalClient() ret = local.cmd('{0}'.format(target), cmd, args, kwargs)
from __future__ import absolute_import import json import logging
import salt.utils.event
import salt.thorium
from __future__ import absolute_import
try: import docker import docker.utils HAS_DOCKER_PY = True except ImportError: HAS_DOCKER_PY = False
CLIENT_TIMEOUT = 60
__virtualname__ = 'docker_events'
from __future__ import absolute_import import logging import json
import salt.utils.event
try: import logstash HAS_LOGSTASH = True except ImportError: HAS_LOGSTASH = False
from __future__ import absolute_import import logging
import salt.utils.jid
import salt.ext.six as six try:
__virtualname__ = 'cassandra'
from __future__ import absolute_import import logging import time import json
import salt.utils.jid import salt.returners
__virtualname__ = 'couchdb'
retc = ret.copy()
retc["_id"] = ret["jid"]
retc["timestamp"] = time.time()
_response = _request("GET", options['url'] + "_all_dbs") if options['db'] not in _response:
_response = _request("PUT", options['url'] + options['db'])
doc = _generate_doc(ret)
if 'total_rows' not in _response: log.error('Didn\'t get valid response from requesting all docs: {0}' .format(_response)) return {}
ret = {} for row in _response['rows']: jid = row['id'] if not salt.utils.jid.is_jid(jid): continue
options = _get_options(ret=None)
_ret = {}
for minion in get_minions():
if len(_response['rows']) < 1: continue
_ret[minion] = _response['rows'][0]['value']
if not ensure_views(): return []
_response = _request("GET", options['url'] + options['db'] + "/_design/salt/_view/minions?group=true")
if 'rows' not in _response: log.error('Unable to get available minions: {0}'.format(_response)) return []
_ret = [] for row in _response['rows']: _ret.append(row['key']) return _ret
options = _get_options(ret=None)
_response = _request("GET", options['url'] + options['db'] + "/_design/salt")
if 'error' in _response: return set_salt_view()
for view in get_valid_salt_views(): if view not in _response['views']: return set_salt_view()
return True
new_doc = {} new_doc['views'] = get_valid_salt_views() new_doc['language'] = "javascript"
import cgi import logging
import salt.ext.six.moves.http_client
_options['checktype'] = '1'
_options['checktype'] = str(_options['checktype'])
import json
import salt.utils import salt.utils.jid import salt.returners
try: import redis HAS_REDIS = True except ImportError: HAS_REDIS = False
__virtualname__ = 'redis'
import json import logging
try: import salt.utils.etcd_util HAS_LIBS = True except ImportError: HAS_LIBS = False
__virtualname__ = 'etcd'
client.set( '/'.join((path, 'minions', ret['id'])), ret['jid'], ttl=ttl, )
from contextlib import contextmanager import sys import time import logging
import salt.returners import salt.utils.jid import salt.exceptions
try: import psycopg2 import psycopg2.extras HAS_PG = True except ImportError: HAS_PG = False
__virtualname__ = 'pgjsonb'
if 'port' in _options: _options['port'] = int(_options['port']) return _options
pass
import yaml import pprint import logging import urllib
import salt.ext.six.moves.http_client
import salt.returners import salt.utils.slack
result = salt.utils.slack.query(function='message', api_key=api_key, method='POST', header_dict={'Content-Type': 'application/x-www-form-urlencoded'}, data=urllib.urlencode(parameters))
cfg = __salt__.get('config.option', __opts__)
_options = dict( _options_browser( cfg, ret_config, defaults, virtualname, attrs, ) )
_options.update( _fetch_profile_opts( cfg, virtualname, __salt__, _options, profile_attr, profile_attrs ) )
if ret and 'ret_kwargs' in ret: _options.update(ret['ret_kwargs'])
if isinstance(cfg, dict): c_cfg = cfg else: c_cfg = cfg('{0}'.format(virtualname), {})
if isinstance(cfg, dict): return c_cfg.get(attr_name, cfg.get(default_cfg_key)) else: return c_cfg.get(attr_name, cfg(default_cfg_key))
ret_cfg = cfg('{0}.{1}'.format(ret_config, virtualname), {})
ret_override_cfg = ret_cfg.get( attr_name, override_cfg_default ) if ret_override_cfg: return ret_override_cfg
return c_cfg.get(attr_name, cfg(default_cfg_key))
value = _fetch_option(cfg, ret_config, virtualname, options[option])
if defaults: if option in defaults: log.info('Using default for %s %s', virtualname, option) yield option, defaults[option] continue
continue
from __future__ import absolute_import import logging
import salt.minion
MMINION = None
import json import logging import uuid import time
import salt.returners import salt.utils.jid import salt.exceptions from salt.exceptions import CommandExecutionError
import logging
import salt.utils.jid
__virtualname__ = 'sentry'
from __future__ import absolute_import from datetime import tzinfo, datetime, timedelta import uuid import logging import json
import salt.utils.jid
from contextlib import contextmanager import sys import json import logging
import salt.returners import salt.utils.jid import salt.exceptions
try: import MySQLdb HAS_MYSQL = True except ImportError: HAS_MYSQL = False
__virtualname__ = 'mysql'
if 'port' in _options: _options['port'] = int(_options['port']) return _options
pass
from __future__ import absolute_import import collections import logging import socket import struct import time from contextlib import contextmanager
import salt.utils.jid import salt.returners
import salt.ext.six as six
__virtualname__ = 'carbon'
log.debug('Destroying carbon socket')
if not metric_base.startswith('virt.'): metric_base += '.' + ret['id'].replace('.', '_')
import logging import json import datetime
import salt.utils.jid import salt.returners
try: import sqlite3 HAS_SQLITE3 = True except ImportError: HAS_SQLITE3 = False
__virtualname__ = 'sqlite3'
data.pop() for minion, ret in data: ret[minion] = json.loads(ret)
from __future__ import absolute_import import logging
import salt.returners import salt.utils.jid
__virtualname__ = 'django'
from __future__ import absolute_import, print_function
import json import logging
try: import memcache HAS_MEMCACHE = True except ImportError: HAS_MEMCACHE = False
__virtualname__ = 'memcache'
memcacheoptions = (host, port)
_append_list(serv, 'minions', minion) _append_list(serv, 'jids', jid)
ret = {} for minion, data in six.iteritems(returns): ret[minion] = json.loads(data) return ret
ret = {} for minion, data in six.iteritems(returns): ret[minion] = json.loads(data) return ret
import os import logging import smtplib import StringIO from email.utils import formatdate
import salt.utils.jid import salt.returners import salt.loader from salt.template import compile_template
import json
import salt.utils.jid import salt.returners
try: import pyodbc #import psycopg2.extras HAS_ODBC = True except ImportError: HAS_ODBC = False
__virtualname__ = 'odbc'
import logging
import salt.utils.jid import salt.returners import salt.ext.six as six
try: import pymongo version = pymongo.version version = '.'.join(version.split('.')[:2]) HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
__virtualname__ = 'mongo'
ret[minion] = data['full_ret']
if host: self.host = host else: self.host = socket.gethostname()
if not eventtime: eventtime = str(int(time.time()))
if 'host' not in payload: payload.update({"host": self.host})
data = {"time": eventtime} data.update(payload)
r = requests.post(self.server_uri, data=json.dumps(data), headers=headers, verify=http_event_collector_SSL_verify)
if http_event_collector_debug: log.debug(r.text) log.debug(data)
if 'host' not in payload: payload.update({"host": self.host})
if http_event_collector_debug: log.debug('auto flushing')
if not eventtime: eventtime = str(int(time.time()))
data = {"time": eventtime} data.update(payload)
import salt.utils import salt.utils.jid
__virtualname__ = 'couchbase'
COUCHBASE_CONN = None DESIGN_NAME = 'couchbase_returner' VERIFIED_VIEWS = False
json = salt.utils.import_json() couchbase.set_json_converters(json.dumps, json.loads)
import json import logging import requests
import salt.utils.jid import salt.returners from salt.utils.decorators import memoize
try: import influxdb import influxdb.influxdb08 HAS_INFLUXDB = True except ImportError: HAS_INFLUXDB = False
influxDBVersionHeader = "X-Influxdb-Version"
__virtualname__ = 'influxdb'
json_return = json.dumps(ret['return']) del ret['return'] json_full_ret = json.dumps(ret)
import errno import glob import logging import os import shutil import time import hashlib import bisect
import salt.payload import salt.utils import salt.utils.files import salt.utils.jid import salt.exceptions
import salt.ext.six as six
if load['jid'] == 'req': load['jid'] = prep_jid(nocache=load.get('nocache', False))
salt.utils.atomicfile.atomic_open( os.path.join(hn_dir, RETURN_P), 'w+b' )
salt.utils.atomicfile.atomic_open( os.path.join(hn_dir, OUT_P), 'w+b' )
pass
dirs_to_remove = set()
t_path_dirs = os.listdir(t_path) if not t_path_dirs and t_path not in dirs_to_remove: dirs_to_remove.add(t_path) continue
shutil.rmtree(t_path)
shutil.rmtree(t_path)
import json
import salt.utils.jid import salt.returners
try: import psycopg2 HAS_POSTGRES = True except ImportError: HAS_POSTGRES = False
import pprint import logging
from salt.ext.six.moves.urllib.parse import urlencode as _urlencode
import salt.returners import salt.utils.pushover from salt.exceptions import SaltInvocationError
import json import pprint import logging
import salt.returners
import logging
import salt.utils.jid import salt.returners import salt.ext.six as six
try: import pymongo version = pymongo.version version = '.'.join(version.split('.')[:2]) HAS_PYMONGO = True except ImportError: HAS_PYMONGO = False
__virtualname__ = 'mongo'
ret[minion] = data['full_ret']
import json try: import syslog HAS_SYSLOG = True except ImportError: HAS_SYSLOG = False
import salt.utils.jid import salt.returners
__virtualname__ = 'syslog'
level = getattr(syslog, _options['level']) facility = getattr(syslog, _options['facility'])
logoption = 0 for opt in _options['options']: logoption = logoption | getattr(syslog, opt)
if 'tag' in _options: syslog.openlog(ident=_options['tag'], logoption=logoption) else: syslog.openlog(logoption=logoption)
syslog.syslog(facility | level, '{0}'.format(json.dumps(ret)))
syslog.closelog()
from __future__ import absolute_import import json import logging import re import sys
import salt.utils import salt.utils.jid import salt.ext.six as six
try: import psycopg2 HAS_POSTGRES = True except ImportError: HAS_POSTGRES = False
LOAD_P = '.load.p' MINIONS_P = '.minions.p' RETURN_P = 'return.p' OUT_P = 'out.p'
sleekxmpp_version = distutils.version.LooseVersion(sleekxmpp.__version__) valid_version = distutils.version.LooseVersion('1.3.1') if sleekxmpp_version >= valid_version: return __virtualname__
super(SendMsgBot, self).__init__(jid, password)
from __future__ import absolute_import from ctypes import CDLL, POINTER, Structure, CFUNCTYPE, cast, pointer, sizeof from ctypes import c_void_p, c_uint, c_char_p, c_char, c_int from ctypes.util import find_library
from salt.utils import get_group_list
PAM_PROMPT_ECHO_OFF = 1 PAM_PROMPT_ECHO_ON = 2 PAM_ERROR_MSG = 3 PAM_TEXT_INFO = 4
from __future__ import absolute_import import logging
try: from Crypto.Util import asn1 import OpenSSL HAS_DEPS = True except ImportError: HAS_DEPS = False
import salt.utils
algo = cert.get_signature_algorithm()
cert_asn1 = c.dump_certificate(c.FILETYPE_ASN1, cert)
der = asn1.DerSequence() der.decode(cert_asn1)
der_cert = der[0] #der_algo = der[1] der_sig = der[2]
der_sig_in = asn1.DerObject() der_sig_in.decode(der_sig)
sig0 = der_sig_in.payload
if sig0[0] != '\x00': raise Exception('Number of unused bits is strange') sig = sig0[1:]
from __future__ import print_function import os import collections import hashlib import time import logging import random import getpass from salt.ext.six.moves import input
import salt.config import salt.loader import salt.transport.client import salt.utils import salt.utils.minions import salt.payload
rm_tok = True
return auth_data
if load.get('fun', '') != 'saltutil.find_job': return good
if 'username' in ret and not ret['username']: ret['username'] = salt.utils.get_user()
from __future__ import absolute_import import logging
from __future__ import absolute_import import logging
import salt.utils.http
from __future__ import absolute_import import logging import salt.ext.six as six
from salt.exceptions import CommandExecutionError, SaltInvocationError
from jinja2 import Environment try: import ldap import ldap.modlist import ldap.filter HAS_LDAP = True except ImportError: HAS_LDAP = False
if paramvalues['binddn']: connargs['binddn'] = paramvalues['binddn'] if paramvalues['bindpw']: params['mandatory'].append('bindpw')
return _LDAPConnection(**connargs).ldap
paramvalues['binddn'] = _render_template(paramvalues['binddn'], username) paramvalues['binddn'] = ldap.filter.escape_filter_chars(paramvalues['binddn'])
if paramvalues['binddn']: connargs['binddn'] = paramvalues['binddn'] if paramvalues['bindpw']: params['mandatory'].append('bindpw')
connargs['bindpw'] = password
pass
from __future__ import absolute_import import logging
application: 6789012345 directory: 3456789012
from __future__ import absolute_import from __future__ import print_function import logging
BaseLoader = getattr(yaml, 'CSafeLoader', yaml.SafeLoader) BaseDumper = getattr(yaml, 'CSafeDumper', yaml.SafeDumper)
from __future__ import absolute_import import logging import datetime from copy import copy
from salt.serializers import DeserializationError, SerializationError from salt.utils.aggregation import aggregate, Map, Sequence from salt.utils.odict import OrderedDict
import yaml from yaml.nodes import MappingNode from yaml.constructor import ConstructorError from yaml.scanner import ScannerError import salt.ext.six as six
BaseLoader = getattr(yaml, 'CSafeLoader', yaml.SafeLoader) BaseDumper = yaml.SafeDumper if six.PY3 else getattr(yaml, 'CSafeDumper', yaml.SafeDumper)
reset = key_node.tag == u'!reset'
obj = self.construct_scalar(node) if six.PY2: obj = obj.encode('utf-8') return SLSString(obj)
if node.value == '': node.value = '0'
tag = self.resolve(yaml.nodes.ScalarNode, node.value, [True, True]) deep = False
cp.readfp(StringIO.StringIO(stream_or_string))
from __future__ import absolute_import import logging from copy import copy
from salt.log import setup_console_logger from salt.serializers import DeserializationError, SerializationError
import salt.ext.six as six
import msgpack if msgpack.loads(msgpack.dumps([1, 2, 3]), use_list=True) is None: raise ImportError available = True
try:
from __future__ import absolute_import import logging import gc import datetime
import salt.log import salt.crypt import salt.transport.frame from salt.exceptions import SaltReqTimeoutError
import salt.ext.six as six try: import zmq except ImportError: pass
import msgpack if msgpack.loads(msgpack.dumps([1, 2, 3]), use_list=True) is None: raise ImportError HAS_MSGPACK = True
try:
ret = msgpack.loads(msg, use_list=True, encoding=encoding)
return msgpack.dumps(msg, use_bin_type=use_bin_type)
def default(obj): return msgpack.ExtType(78, obj)
raise
fn_.write(self.dumps(msg, use_bin_type=True))
self._socket = self.context.socket(zmq.REQ) if hasattr(zmq, 'RECONNECT_IVL_MAX'): self._socket.setsockopt( zmq.RECONNECT_IVL_MAX, 5000 )
from __future__ import print_function, with_statement
try: SETUP_DIRNAME = os.path.dirname(__file__) except NameError: SETUP_DIRNAME = os.path.dirname(sys.argv[0])
'BOOTSTRAP_SCRIPT_VERSION', 'v2014.06.21'
IS_PY3 = sys.version_info > (3,)
from esky import bdist_esky import bbfreeze HAS_ESKY = True
PACKAGED_FOR_SALT_SSH_FILE = os.path.join(os.path.abspath(SETUP_DIRNAME), '.salt-ssh-package') PACKAGED_FOR_SALT_SSH = os.path.isfile(PACKAGED_FOR_SALT_SSH_FILE)
exec(compile(open(SALT_VERSION).read(), SALT_VERSION, 'exec'))
continue
continue
class WriteSaltVersion(Command):
if getattr(self.distribution, 'salt_version_hardcoded_path', None) is None: print('This command is not meant to be called on it\'s own') exit(1)
open(self.distribution.salt_version_hardcoded_path, 'w').write( INSTALL_VERSION_TEMPLATE.format( date=DATE, full_version_info=__saltstack_version__.full_info ) )
if getattr(self.distribution, 'salt_syspaths_hardcoded_path', None) is None: print('This command is not meant to be called on it\'s own') exit(1)
if getattr(self.distribution, 'salt_ssh_packaging_file', None) is None: print('This command is not meant to be called on it\'s own') exit(1)
open(self.distribution.salt_ssh_packaging_file, 'w').write('Packaged for Salt-SSH\n')
self.distribution.salt_installing_m2crypto_windows = True self.run_command('install-m2crypto-windows') self.distribution.salt_installing_m2crypto_windows = None
self.distribution.salt_installing_pycrypto_windows = True self.run_command('install-pycrypto-windows') self.distribution.salt_installing_pycrypto_windows = None
self.distribution.salt_download_windows_dlls = True self.run_command('download-windows-dlls') self.distribution.salt_download_windows_dlls = None
develop.run(self)
self.distribution.running_salt_sdist = True self.distribution.salt_version_hardcoded_path = os.path.join( base_dir, 'salt', '_version.py' ) self.run_command('write_salt_version')
Sdist.run(self)
{date:%A, %d %B %Y @ %H:%m:%S UTC}.
{date:%A, %d %B %Y @ %H:%m:%S UTC}.
build.run(self) if getattr(self.distribution, 'running_salt_install', False):
self.run_command('write_salt_version')
self.distribution.salt_syspaths_hardcoded_path = os.path.join( self.build_lib, 'salt', '_syspaths.py' ) self.run_command('generate_salt_syspaths')
self.distribution.running_salt_install = True self.distribution.salt_version_hardcoded_path = os.path.join( self.build_lib, 'salt', '_version.py' ) if IS_WINDOWS_PLATFORM:
self.distribution.salt_installing_m2crypto_windows = True self.run_command('install-m2crypto-windows') self.distribution.salt_installing_m2crypto_windows = None
self.distribution.salt_download_windows_dlls = True self.run_command('download-windows-dlls') self.distribution.salt_download_windows_dlls = None
inp = self.get_inputs() out = self.get_outputs() chmod = []
freezer_includes.extend([ 'cherrypy', 'dateutils', 'pyghmi', 'croniter', 'mako', 'gnupg', ])
def parse_command_line(self): args = distutils.dist.Distribution.parse_command_line(self)
from __future__ import print_function
if getattr(sys, 'frozen', False): application_path = os.path.dirname(sys.executable) elif __file__: application_path = os.path.dirname(__file__)
from sphinx.ext.autodoc import FunctionDocumenter as FunctionDocumenter
return self.module.__func_alias__.get(self.objpath[0], self.objpath[0])
list_item = nodes.list_item() list_item['classes'] = ['lit-item']
if len(list_item.children) == 2: enum.append(list_item) list_item = nodes.list_item() list_item['classes'] = ['lit-item']
bg = nodes.container() bg['classes'] = ['lit-background'] node.append(bg)
python_domain.PythonDomain.indices = []
indices = []
return ret
'user',
'cherrypy', 'cherrypy.lib', 'cherrypy.process', 'cherrypy.wsgiserver', 'cherrypy.wsgiserver.ssl_builtin',
sys.modules['cherrypy'].config = mock_decorator_with_params
import salt.version
intersphinx_mapping = { 'python2': ('http://docs.python.org/2', None), 'python3': ('http://docs.python.org/3', None) }
locale_dirs = ['locale/'] gettext_compact = False
on_saltstack = 'SALT_ON_SALTSTACK' in os.environ
if on_saltstack: html_search_template = 'googlesearch.html' else: html_search_template = 'searchbox.html'
authors = [ 'Thomas S. Hatch <thatch45@gmail.com> and many others, please see the Authors file', ]
from __future__ import absolute_import, print_function
from salttesting.case import TestCase
import cherrypy import salt.ext.six as six from salt.ext.six.moves import StringIO
cherrypy.config.update({'environment': "test_suite"})
cherrypy.server.unsubscribe()
h = {'Host': '127.0.0.1'}
fd = None if body is not None: h['content-length'] = '{0}'.format(len(body)) fd = StringIO(body)
app = cherrypy.tree.apps.get(app_path) if not app: raise AssertionError("No application mounted at '{0}'".format(app_path))
app.release_serving()
response.collapse_body() return request, response
bytes = bytearray
assertBadSplit("10:9:8:7:6:5:4:3:42.42.42.42")
for lhs in self.objects: for rhs in self.objects: if lhs is rhs: continue self.assertNotEqual(lhs, rhs)
collapsed = ipaddress.collapse_addresses([ip1, ip2]) self.assertEqual(list(collapsed), [ipaddress.IPv4Network('1.1.0.0/23')])
ip_same1 = ip_same2 = ipaddress.IPv4Network('1.1.1.1/32') self.assertEqual(list(ipaddress.collapse_addresses( [ip_same1, ip_same2])), [ip_same1])
self.assertEqual(ip1.compare_networks(ip2), -1) self.assertEqual(ip2.compare_networks(ip1), 1)
self.assertEqual(True, ipaddress.ip_address('100::').is_reserved) self.assertEqual(True, ipaddress.ip_network('4000::1/128').is_reserved)
self.assertEqual(self.ipv6_interface.with_hostmask, '2001:658:22a:cafe:200::1/::ffff:ffff:ffff:ffff')
self.assertIn('broadcast_address', self.ipv4_network._cache) self.assertIn('hostmask', self.ipv4_network._cache)
self.assertNotIn('broadcast_address', self.ipv6_network._cache) self.assertNotIn('hostmask', self.ipv6_network._cache)
from __future__ import absolute_import
self.app = app
from __future__ import absolute_import import unittest import logging
from __future__ import absolute_import import optparse import pprint
import salt.config import salt.wheel import salt.auth
from __future__ import absolute_import
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../..')
import integration from salt import fileclient
from __future__ import absolute_import import os import logging import pwd import shutil
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
import integration from salt.fileserver import gitfs
pass
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../..')
import integration from salt.fileserver import roots from salt import fileclient
self.master_opts['file_roots']['base'] = [os.path.join(integration.FILES, 'file', 'base')]
self.skipTest('This test fails when using tests/runtests.py. salt-runtests will be available soon.')
self.skipTest('This test fails when using tests/runtests.py. salt-runtests will be available soon.')
from __future__ import absolute_import
import integration
from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
import integration
from __future__ import absolute_import
import integration
from salttesting.helpers import ensure_in_syspath
time.sleep(10)
atexit.register(self.cleanup)
_ = args _ = kwargs
os.killpg(os.getpgid(process.pid), signal.SIGINT) term_sent = True continue
os.killpg(os.getpgid(process.pid), signal.SIGKILL) process.wait()
pass
kwargs['program'] = self.script
kwargs['program'] = self.script
_base.update(copy.deepcopy(_overrides)) return _base
from __future__ import absolute_import import os from time import sleep import textwrap
from salttesting.helpers import destructiveTest, ensure_in_syspath
import integration import salt.utils
from __future__ import absolute_import
import integration from salttesting import skipIf
import salt.runner
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
import errno import logging import os import shutil
import integration import salt.utils from salt import fileclient from salt.ext import six from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('..')
shutil.rmtree(path) os.makedirs(path)
for saltenv in SALTENVS: saltenv_root = os.path.join(FS_ROOT, saltenv) _new_dir(saltenv_root)
_new_dir(CACHE_ROOT)
from __future__ import absolute_import import os import traceback
from salttesting.helpers import ensure_in_syspath from salttesting.mixins import RUNTIME_VARS ensure_in_syspath('../../')
import integration from salt.output import display_output import salt.config
display_output(data, opts=self.minion_opts) self.assertTrue(True)
trace = traceback.format_exc() self.assertEqual(trace, '')
from __future__ import absolute_import import socket import logging import threading from multiprocessing import Queue
import msgpack
import salt.log.setup
break
break
from __future__ import absolute_import import time
import salt.utils.decorators
from __future__ import absolute_import import os import tempfile
import salt.utils
os.environ.get('TMPDIR', tempfile.gettempdir()) if salt.utils.is_darwin() else '/tmp'
TMP = os.path.join(SYS_TMP_DIR, 'salt-tests-tmpdir')
from __future__ import absolute_import
from __future__ import absolute_import import logging
log = logging.getLogger(__name__)
MY_NAME = 'test_ext_pillar_opts'
from __future__ import absolute_import import logging
from __future__ import absolute_import import socket import logging
import salt.utils.event
from tornado import gen from tornado import ioloop from tornado import netutil
io_loop = ioloop.IOLoop() io_loop.make_current()
self.sock.bind(('localhost', port)) self.sock.listen(5) netutil.add_accept_handler( self.sock, self.handle_connection, io_loop=self.io_loop, )
from salttesting import TestCase from salttesting.case import ShellTestCase from salttesting.mixins import CheckShellBinaryNameAndVersionMixIn from salttesting.parser import PNUM, print_header, SaltTestcaseParser from salttesting.helpers import requires_sshd_server from salttesting.helpers import ensure_in_syspath, RedirectStdStreams
ensure_in_syspath(CODE_DIR)
pass
import yaml import msgpack import salt.ext.six as six if salt.utils.is_windows(): import win32api
os.environ.get('TMPDIR', tempfile.gettempdir()) if salt.utils.is_darwin() else '/tmp'
for key in list(to_cleanup.keys()): instance = to_cleanup.pop(key) del instance
port = get_unused_localhost_port() usock.close() return port
transport = None if needs_daemon: transport = self.options.transport TestDaemon.transplant_configs(transport=transport)
break
pass
import salt.utils
proc_args.insert(0, sys.executable)
if terminal.stdout is not None: terminal.recv() if terminal.stderr is not None: terminal.recv_err() time.sleep(0.125)
for key in self.colors: self.colors[key] = ''
salt_log_setup.setup_multiprocessing_logging_listener( self.master_opts )
self._enter_mockbin()
time.sleep(5)
job_finished = True
syncing.remove(name) continue
print( ' {LIGHT_RED}*{ENDC} {0} Failed to sync {2}: ' '{1}'.format( name, output['ret'], modules_kind, **self.colors) ) return False
try: syncing.remove(name) except KeyError: print( ' {LIGHT_RED}*{ENDC} {0} already synced??? ' '{1}'.format(name, output, **self.colors) )
pass
orig[minion_tgt] = self._check_state_return( orig[minion_tgt] )
return ret
keys = list(keys)
keys = [keys]
raise RuntimeError('The passed keys need to be a list')
from __future__ import absolute_import import os import textwrap
from salttesting.helpers import ensure_in_syspath
import integration import salt.utils
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertEqual(self.run_function('xattr.list', [TEST_FILE]), {})
self.assertEqual(self.run_function('xattr.list', [NO_FILE]), 'ERROR: File not found: {0}'.format(NO_FILE))
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue( self.run_function('xattr.write', [TEST_FILE, 'spongebob', 'squarepants']))
self.assertEqual( self.run_function('xattr.read', [TEST_FILE, 'spongebob']), 'squarepants')
self.assertEqual( self.run_function('xattr.read', [NO_FILE, 'spongebob']), 'ERROR: File not found: {0}'.format(NO_FILE))
self.assertEqual( self.run_function('xattr.read', [TEST_FILE, 'patrick']), 'ERROR: Attribute not found: patrick')
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue( self.run_function('xattr.delete', [TEST_FILE, 'squidward']))
self.assertEqual( self.run_function('xattr.list', [TEST_FILE]), {'spongebob': 'squarepants', 'crabby': 'patty'})
self.assertEqual( self.run_function('xattr.delete', [NO_FILE, 'spongebob']), 'ERROR: File not found: {0}'.format(NO_FILE))
self.assertEqual( self.run_function('xattr.delete', [TEST_FILE, 'patrick']), 'ERROR: Attribute not found: patrick')
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertTrue(self.run_function('xattr.clear', [TEST_FILE]))
self.assertEqual(self.run_function('xattr.clear', [NO_FILE]), 'ERROR: File not found: {0}'.format(NO_FILE))
from __future__ import absolute_import import os import sys import textwrap import tempfile
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, skip_if_binaries_missing ) from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, patch ensure_in_syspath('../../')
import integration import salt.utils
self.skipTest('Unable to get the SHELL environment variable')
import pwd runas = pwd.getpwuid(os.getuid())[0]
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration
if os_grain['kernel'] not in 'Darwin': self.skipTest( 'Test not applicable to \'{kernel}\' kernel'.format( **os_grain ) )
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, requires_network ) ensure_in_syspath('../../')
import integration
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import random
import integration import salt.utils from salt.exceptions import CommandExecutionError
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
ASSIGN_CMD = 'net.inet.icmp.icmplim' CONFIG = '/etc/sysctl.conf'
self.has_conf = False self.val = self.run_function('sysctl.get', [ASSIGN_CMD])
if os.path.isfile(CONFIG): os.remove(CONFIG)
os.remove(self.conf)
self.__restore_sysctl()
os.remove(CONFIG)
from __future__ import absolute_import import os import time
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
time.sleep(20) ret = self.run_function('grains.item', ['setgrain'])
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ( ensure_in_syspath, skip_if_not_root, skip_if_binaries_missing ) from salttesting import skipIf ensure_in_syspath('../../')
import integration
import salt.ext.six as six
self.run_function('cmd.run', ['truncate -s 0 {0}'.format(f)])
from __future__ import absolute_import import os import string import random
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import salt.utils import integration
@skipIf(not salt.utils.is_linux(), 'These tests can only be run on linux') class UseraddModuleTest(integration.ModuleCase):
uid = uinfo['uid']
from __future__ import absolute_import import os
from salt.modules import beacons from salt.exceptions import CommandExecutionError import integration
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath
_save = self.run_function('beacons.save') self.assertTrue(_save['result'])
_delete = self.run_function('beacons.delete', ['ps']) self.assertTrue(_delete['result'])
self.run_function('beacons.save')
self.run_function('beacons.add', ['ps', {'apache2': 'stopped'}]) self.run_function('beacons.save')
self.run_function('beacons.delete', ['ps']) self.run_function('beacons.save')
_list = self.run_function('beacons.list', return_yaml=False) self.assertIn('ps', _list)
_list = self.run_function('beacons.list', return_yaml=False) self.assertFalse(_list['enabled'])
ret = self.run_function('beacons.disable_beacon', ['ps']) self.assertTrue(ret['result'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertFalse(_list['ps']['enabled'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertIn('ps', _list)
ret = self.run_function('beacons.enable') self.assertTrue(ret['result'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertTrue(_list['enabled'])
ret = self.run_function('beacons.enable_beacon', ['ps']) self.assertTrue(ret['result'])
_list = self.run_function('beacons.list', return_yaml=False) self.assertTrue(_list['ps']['enabled'])
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration
self.run_function('assistive.install', [OSA_SCRIPT, True])
osa_script = self.run_function('assistive.installed', [OSA_SCRIPT]) if osa_script: self.run_function('assistive.remove', [OSA_SCRIPT])
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
self.__clear_hosts() f = salt.utils.fopen(HFN, 'w') f.close()
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, requires_salt_modules, requires_system_grains, destructiveTest, ) ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration from salt.exceptions import CommandExecutionError
ADD_GROUP = __random_string() DEL_GROUP = __random_string() CHANGE_GROUP = __random_string() ADD_USER = __random_string() REP_USER_GROUP = __random_string()
ret = self.run_function('group.delete', [DEL_GROUP]) self.assertTrue(ret)
add_info = self.run_function('group.info', [ADD_GROUP]) if add_info: self.run_function('group.delete', [ADD_GROUP])
del_info = self.run_function('group.info', [DEL_GROUP]) if del_info: self.run_function('group.delete', [DEL_GROUP])
change_info = self.run_function('group.info', [CHANGE_GROUP]) if change_info: self.run_function('group.delete', [CHANGE_GROUP])
from __future__ import absolute_import import datetime import random import string
from salttesting.helpers import ensure_in_syspath, destructiveTest from salt.ext.six.moves import range ensure_in_syspath('../../')
import integration import salt.utils
ret = self.run_function('shadow.info', [TEST_USER]) self.assertEqual(ret['name'], TEST_USER)
ret = self.run_function('shadow.info', [NO_USER]) self.assertEqual(ret['name'], '')
self.assertEqual( self.run_function('shadow.get_account_created', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertEqual( self.run_function('shadow.get_last_change', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertEqual( self.run_function('shadow.get_login_failed_last', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertEqual( self.run_function('shadow.get_login_failed_count', [TEST_USER]), '0')
self.assertEqual( self.run_function('shadow.get_login_failed_count', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertTrue( self.run_function('shadow.set_maxdays', [TEST_USER, 20])) self.assertEqual( self.run_function('shadow.get_maxdays', [TEST_USER]), 20)
self.assertEqual( self.run_function('shadow.del_password', [NO_USER]), 'ERROR: User not found: {0}'.format(NO_USER))
self.assertTrue( self.run_function('shadow.set_password', [TEST_USER, 'Pa$$W0rd']))
self.assertEqual( self.run_function('shadow.set_password', [NO_USER, 'P@SSw0rd']), 'ERROR: User not found: {0}'.format(NO_USER))
from salttesting.helpers import ( destructiveTest, requires_network, requires_salt_modules, ensure_in_syspath ) ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import logging
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath ) ensure_in_syspath('../../')
import integration from salt.modules import mysql as mysqlmod
import salt.ext.six as six
db_name = 'foo 1' self._db_creation_loop(db_name=db_name, returning_name=db_name, test_conn=True, connection_user=self.user, connection_pass=self.password )
db_name = "foo'3" self._db_creation_loop(db_name=db_name, returning_name=db_name, test_conn=True, character_set='utf8', connection_user=self.user, connection_pass=self.password )
ret = self.run_function( 'mysql.db_remove', name=dbname, connection_user=self.user, connection_pass=self.password ) self.assertEqual(True, ret)
password_hash='*EEF6F854748ACF841226BB1C2422BEC70AE7F1FF', new_password_hash=user2_pwd_hash, connection_user=self.user, connection_pass=self.password, connection_charset='utf8', saltenv={"LC_ALL": "en_US.utf8"}
from __future__ import absolute_import import os import string import logging
from salttesting.helpers import ensure_in_syspath, requires_salt_modules from salttesting import skipIf ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import re
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
import salt.ext.six as six
funcs = self.run_function('sys.list_functions') self.assertIn('hosts.list_hosts', funcs) self.assertIn('pkg.install', funcs)
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration from salt.exceptions import CommandExecutionError
ADD_USER = __random_string() DEL_USER = __random_string() PRIMARY_GROUP_USER = __random_string() CHANGE_USER = __random_string()
ret = self.run_function('user.delete', [DEL_USER]) self.assertTrue(ret)
add_info = self.run_function('user.info', [ADD_USER]) if add_info: self.run_function('user.delete', [ADD_USER])
del_info = self.run_function('user.info', [DEL_USER]) if del_info: self.run_function('user.delete', [DEL_USER])
change_info = self.run_function('user.info', [CHANGE_USER]) if change_info: self.run_function('user.delete', [CHANGE_USER])
from __future__ import absolute_import import os import shutil
from salttesting import skipIf from salttesting.helpers import (ensure_in_syspath, destructiveTest) ensure_in_syspath('../../')
import integration import salt.utils
import salt.ext.six as six
if os.path.isfile('/etc/mtab'): shutil.move('/etc/mtab', '/tmp/mtab')
from __future__ import absolute_import, print_function import random import string
from salttesting.helpers import ensure_in_syspath, destructiveTest from salt.ext.six.moves import range ensure_in_syspath('../../')
import integration import salt.utils
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_remote_login', ['spongebob']))
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_remote_events', ['spongebob']))
ret = self.run_function('system.list_startup_disks') self.assertIsInstance(ret, list) self.assertIn(self.run_function('system.get_startup_disk'), ret)
self.assertIn( 'Invalid value passed for path.', self.run_function('system.set_startup_disk', ['spongebob']))
self.assertTrue(self.run_function('system.set_restart_delay', [90])) self.assertEqual( self.run_function('system.get_restart_delay'), '90 seconds')
self.assertIn( 'Invalid value passed for seconds.', self.run_funcdtion('system.set_restart_delay', [70]))
self.assertTrue( self.run_function('system.set_disable_keyboard_on_lock', [True])) self.assertTrue( self.run_function('system.get_disable_keyboard_on_lock'))
self.assertIn( 'Invalid String Value for Enabled', self.run_function('system.set_disable_keyboard_on_lock', ['spongebob']))
self.assertIn( 'Invalid value passed for arch', self.run_function('system.set_boot_arch', ['spongebob']))
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertIn( 'Service not found', self.run_function('service.show', ['spongebob']))
self.assertIn( ' Failed to error service', self.run_function('service.launchctl', ['error']))
self.assertIn( 'Service not found', self.run_function('service.list', ['spongebob']))
self.assertEqual('', self.run_function('service.status', ['spongebob']))
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, requires_salt_modules
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import time import subprocess
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
self.assertIn('sleep_service: started', ret) self.assertIn('sleep_service2: started', ret)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
self.assertRegexpMatches(ret, r'^\d{12}$')
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.version from salt import config
from __future__ import absolute_import import time import threading
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration from salt.utils import event
from __future__ import absolute_import import getpass import grp import pwd import os import shutil import sys
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules import file as filemod
os.makedirs(self.mydir)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import integration from salt.modules import djangomod as django
from __future__ import absolute_import import os import string import random
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration
uid = uinfo['uid']
from __future__ import absolute_import import sys
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration from salt.exceptions import CommandExecutionError
if os_grain['kernel'] not in 'Darwin': self.skipTest( 'Test not applicable to \'{kernel}\' kernel'.format( **os_grain ) )
certs_list = self.run_function('keychain.list_certs') if CERT_ALIAS in certs_list: self.run_function('keychain.uninstall', [CERT_ALIAS])
certs_list = self.run_function('keychain.list_certs') self.assertIn(CERT_ALIAS, certs_list)
self.run_function('keychain.uninstall', [CERT_ALIAS]) certs_list = self.run_function('keychain.list_certs')
try: self.assertNotIn(CERT_ALIAS, str(certs_list)) except CommandExecutionError: self.run_function('keychain.uninstall', [CERT_ALIAS])
from __future__ import absolute_import import os import tempfile
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertIsInstance( self.run_function('softwareupdate.list_available'), dict)
self.assertTrue(self.run_function('softwareupdate.reset_ignored')) self.assertEqual(self.run_function('softwareupdate.list_ignored'), [])
self.assertIn( 'spongebob', self.run_function('softwareupdate.list_ignored')) self.assertIn( 'squidward', self.run_function('softwareupdate.list_ignored'))
self.assertTrue( self.run_function('softwareupdate.schedule_enable', [True])) self.assertTrue(self.run_function('softwareupdate.schedule_enabled'))
self.assertTrue( self.run_function('softwareupdate.schedule_enable', [False])) self.assertFalse(self.run_function('softwareupdate.schedule_enabled'))
self.assertIsInstance( self.run_function('softwareupdate.update_all'), dict)
self.assertFalse( self.run_function('softwareupdate.update_available', ['spongebob']))
self.assertIn( 'Update not available', self.run_function('softwareupdate.update', ['spongebob']))
self.assertIn( 'Update not available', self.run_function('softwareupdate.download', ['spongebob']))
self.assertTrue(self.run_function('softwareupdate.reset_catalog')) self.assertEqual(self.run_function('softwareupdate.get_catalog'), 'Default')
self.assertTrue(self.run_function('softwareupdate.reset_catalog')) self.assertEqual(self.run_function('softwareupdate.get_catalog'), 'Default')
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil import textwrap
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
import salt.ext.six as six
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
ret = self.run_function('state.sls', mods='testappend.step-2') self.assertSaltTrueReturn(ret)
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if os.path.isfile(testfile): os.unlink(testfile)
ret = self.run_function('state.sls', mods='issue-1879', timeout=120) self.assertSaltTrueReturn(ret)
ret = self.run_function( 'state.sls', mods='issue-1879.step-1', timeout=120 ) self.assertSaltTrueReturn(ret)
ret = self.run_function( 'state.sls', mods='issue-1879.step-2', timeout=120 ) self.assertSaltTrueReturn(ret)
#)
#])
#])
state_run = self.run_function('state.sls', mods='requisites.onchanges_simple')
test_data = state_run['cmd_|-test_changing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onchanges_multiple')
test_data = state_run['cmd_|-test_two_changing_states_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
test_data = state_run['cmd_|-test_one_changing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onchanges_in_simple')
test_data = state_run['cmd_|-test_changes_expected_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onfail_simple')
test_data = state_run['cmd_|-test_failing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
test_data = state_run['cmd_|-test_non_failing_state_|-echo "Should not run"_|-run']['comment'] expected_result = 'State was not run because onfail req did not change' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.onfail_in_simple')
test_data = state_run['cmd_|-test_failing_state_|-echo "Success!"_|-run']['comment'] expected_result = 'Command "echo "Success!"" run' self.assertIn(expected_result, test_data)
test_data = state_run['cmd_|-test_non_failing_state_|-echo "Should not run"_|-run']['comment'] expected_result = 'State was not run because onfail req did not change' self.assertIn(expected_result, test_data)
state_run = self.run_function('state.sls', mods='requisites.listen_simple')
listener_state = 'cmd_|-listener_test_listening_change_state_|-echo "Listening State"_|-mod_watch' self.assertIn(listener_state, state_run)
absent_state = 'cmd_|-listener_test_listening_non_changing_state_|-echo "Only run once"_|-mod_watch' self.assertNotIn(absent_state, state_run)
state_run = self.run_function('state.sls', mods='requisites.listen_in_simple')
listener_state = 'cmd_|-listener_test_listening_change_state_|-echo "Listening State"_|-mod_watch' self.assertIn(listener_state, state_run)
absent_state = 'cmd_|-listener_test_listening_non_changing_state_|-echo "Only run once"_|-mod_watch' self.assertNotIn(absent_state, state_run)
state_run = self.run_function('state.sls', mods='requisites.listen_in_simple')
listener_state = 'cmd_|-listener_test_listen_in_resolution_|-echo "Successful listen_in resolution"_|-mod_watch' self.assertIn(listener_state, state_run)
state_run = self.run_function('state.sls', mods='requisites.listen_simple')
listener_state = 'cmd_|-listener_test_listening_resolution_one_|-echo "Successful listen resolution"_|-mod_watch' self.assertIn(listener_state, state_run)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
self.assertTrue( self.run_function( 'runtests_decorators.booldependsTrue' ) )
self.assertIn( 'is not available', self.run_function('runtests_decorators.booldependsFalse' ) )
from __future__ import absolute_import import os import pwd import shutil import re import tempfile
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
self.run_function('virtualenv.create', [self.venv_dir])
req_basepath = (self.venv_dir)
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing import salt.utils ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import, print_function
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import hashlib
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath, requires_salt_modules ensure_in_syspath('../../')
import integration
from __future__ import absolute_import from contextlib import closing import errno import logging import os import re import shutil import subprocess import tarfile import tempfile
from distutils.version import LooseVersion from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, skip_if_binaries_missing ) ensure_in_syspath('../..')
import integration
if exc.errno != errno.EEXIST: raise
os.chdir(self.repo) subprocess.check_call(['git', 'init', '--quiet', self.repo])
shutil.rmtree(clone_parent_dir)
self.assertTrue( self.run_function( 'git.clone', [clone_parent_dir, self.repo], name=clone_name ) ) shutil.rmtree(clone_parent_dir)
ret = self.run_function( 'git.merge', [self.repo], rev=self.branches[1] ) self.assertTrue('Fast-forward' in ret.splitlines())
self.assertEqual( self.run_function( 'git.rev_parse', [self.repo, 'HEAD'], opts='--abbrev-ref' ), 'master' )
self.assertTrue( 'ERROR' not in self.run_function( 'git.add', [self.repo, filename] ) )
from __future__ import absolute_import import datetime
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertTrue(self.run_function('timezone.set_time', ['3:14']))
self.assertEqual( self.run_function('timezone.set_zone', ['spongebob']), 'ERROR executing \'timezone.set_zone\': ' 'Invalid Timezone: spongebob')
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration import salt.utils from salt.exceptions import CommandExecutionError
ADD_PKG = 'algol68g' DEL_PKG = 'acme'
self.run_function('pkg.remove', [DEL_PKG]) del_list = self.run_function('pkg.list_pkgs') try: self.assertNotIn(DEL_PKG, del_list) except AssertionError: raise
if ADD_PKG in pkg_list: self.run_function('pkg.remove', [ADD_PKG]) if DEL_PKG in pkg_list: self.run_function('pkg.remove', [DEL_PKG])
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath, destructiveTest ensure_in_syspath('../../')
import integration import salt.utils
self.assertTrue( self.run_function('pkgutil.is_installed', ['com.apple.pkg.BaseSystemResources']))
self.assertFalse( self.run_function('pkgutil.is_installed', ['spongebob']))
self.assertFalse( self.run_function('pkgutil.is_installed', [TEST_PKG_NAME]))
self.run_function('cp.get_url', [TEST_PKG_URL, TEST_PKG])
self.assertTrue(self.run_function('pkgutil.forget', [TEST_PKG_NAME]))
from __future__ import absolute_import import re
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains )
import integration
self.run_function('desktop.set_output_volume', [current_vol])
from __future__ import absolute_import import random import string from salt.ext.six.moves import range
from __future__ import absolute_import from unittest2 import skipIf from integration.cloud.helpers import random_name from salt.utils import virtualbox import json import logging import os import unittest import integration
log.debug("running salt-cloud with %s", arg_str) output = self.run_script('salt-cloud', arg_str, catch_stderr, timeout=timeout)
if isinstance(output, tuple) and len(output) == 2: output = output[0]
if kw_function_args: args = [ "{0}='{1}'".format(key, value) for key, value in kw_function_args.iteritems() ]
from __future__ import absolute_import import os import random import string
from salttesting.helpers import ensure_in_syspath, expensiveTest from salttesting import skipIf
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'gogrid'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
import integration from salt.config import cloud_providers_config
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'ec2'
profile_str = 'ec2-config' providers = self.run_cloud('--list-providers')
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
instance = self.run_cloud('-p ec2-test {0}'.format(INSTANCE_NAME)) ret_str = '{0}:'.format(INSTANCE_NAME)
try: self.assertIn(ret_str, instance) except AssertionError: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME)) raise
delete = self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME)) ret_str = ' shutting-down'
try: self.assertIn(ret_str, delete) except AssertionError: raise
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string import time
from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'vultr'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
from __future__ import absolute_import import os import random import string from distutils.version import LooseVersion
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
from salt.ext.six.moves import range
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'azure' PROFILE_NAME = 'azure-test' REQUIRED_AZURE = '0.11.1'
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'linode'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
try:
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'profitbricks' DRIVER_NAME = 'profitbricks'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'joyent'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
import integration from salt.config import cloud_providers_config
from salttesting.helpers import ensure_in_syspath, expensiveTest
profile_str = 'gce-config:' provider = 'gce' providers = self.run_cloud('--list-providers') self.INSTANCE_NAME = _random_name()
path = os.path.join(integration.FILES, 'conf', 'cloud.providers.d', provider + '.conf') config = cloud_providers_config(path)
instance = self.run_cloud('-p gce-test {0}'.format(self.INSTANCE_NAME)) ret_str = '{0}:'.format(self.INSTANCE_NAME)
try: self.assertIn(ret_str, instance) except AssertionError: self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) raise
delete = self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) delete_str = ''.join(delete)
try: self.assertIn(self.INSTANCE_NAME, delete_str) self.assertIn('True', delete_str) except AssertionError: raise
instance = self.run_cloud('-p gce-test-extra {0}'.format(self.INSTANCE_NAME)) ret_str = '{0}:'.format(self.INSTANCE_NAME)
try: self.assertIn(ret_str, instance) except AssertionError: self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) raise
delete = self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME)) delete_str = ''.join(delete)
try: self.assertIn(self.INSTANCE_NAME, delete_str) self.assertIn('True', delete_str) except AssertionError: raise
query = self.run_cloud('--query') ret_str = ' {0}:'.format(self.INSTANCE_NAME)
if ret_str in query: self.run_cloud('-d {0} --assume-yes'.format(self.INSTANCE_NAME))
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config from salt.ext.six.moves import range
try:
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'rackspace' DRIVER_NAME = 'openstack'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
if ret in query: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import from salt.ext.six.moves import range
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath
import integration from salt.config import cloud_providers_config, vm_profiles_config from utils.virtualbox import vb_xpcom_to_attribute_dict, vb_clone_vm, vb_destroy_machine, vb_create_machine, \ vb_get_box, vb_machine_exists, XPCOM_ATTRIBUTES, vb_start_vm, vb_stop_vm, \ vb_get_network_addresses, vb_wait_for_network_address, machine_get_machinestate_str
log = logging.getLogger() log = logging.getLogger(__name__) info = log.info
MINIMAL_MACHINE_ATTRIBUTES = [ "id", "image", "size", "state", "private_ips", "public_ips", ]
profile_str = 'virtualbox-config' providers = self.run_cloud('--list-providers') log.debug("providers: %s", providers)
self.test_cloud_create() ret = self.run_cloud_destroy(INSTANCE_NAME)
self.assertIn(INSTANCE_NAME, ret.keys())
provider_str = CONFIG_NAME providers = self.run_cloud('--list-providers') log.debug("providers: %s", providers)
ip_addresses = vb_get_network_addresses(machine_name=BOOTABLE_BASE_BOX_NAME)
vb_start_vm(BOOTABLE_BASE_BOX_NAME) ip_addresses = vb_wait_for_network_address(20, machine_name=BOOTABLE_BASE_BOX_NAME) network_count = len(ip_addresses) self.assertGreater(network_count, 0)
from __future__ import absolute_import import os import random import string
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath, expensiveTest
import integration from salt.config import cloud_providers_config
INSTANCE_NAME = __random_name() PROVIDER_NAME = 'digital_ocean'
config = cloud_providers_config( os.path.join( integration.FILES, 'conf', 'cloud.providers.d', PROVIDER_NAME + '.conf' ) )
self.assertIn( finger_print, [i.strip() for i in _key] )
list_keypairs = self.run_cloud('-f list_keypairs {0}'.format(PROVIDER_NAME))
show_keypair = self.run_cloud('-f show_keypair {0} keyname={1}'.format(PROVIDER_NAME, 'MyPubKey'))
self.run_cloud('-f remove_key {0} id={1}'.format(PROVIDER_NAME, finger_print)) raise
self.assertTrue(self.run_cloud('-f remove_key {0} id={1}'.format(PROVIDER_NAME, finger_print)))
try: self.assertIn( 'True', [i.strip() for i in self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))] ) except AssertionError: raise
if INSTANCE_NAME in [i.strip() for i in self.run_cloud('--query')]: self.run_cloud('-d {0} --assume-yes'.format(INSTANCE_NAME))
from __future__ import absolute_import import integration
import salt.wheel
from __future__ import absolute_import import integration
import salt.auth import salt.wheel
from __future__ import absolute_import
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath
import salt.utils.event
e = salt.utils.event.get_event('minion', sock_dir=self.minion_opts['sock_dir'], opts=self.minion_opts)
from __future__ import absolute_import import os import sys import re import shutil import yaml from datetime import datetime import logging
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salttesting.helpers import ( destructiveTest )
if os.path.isfile(logfile): os.unlink(logfile)
ellapsed = datetime.now() - start timeout = ellapsed.seconds + 3
if os.path.isfile(this_minion_key): os.unlink(this_minion_key)
self.assertIn( 'Failed to setup the Syslog logging handler', '\n'.join(ret[1]) ) self.assertEqual(ret[2], 2)
ret = self.run_script( 'salt-call', '-c {0} --output-file={1} test.versions'.format( self.get_config_dir(), output_file_append ), catch_stderr=True, with_retcode=True )
self.run_script( 'salt-call', '-c {0} --output-file={1} -g'.format( self.get_config_dir(), output_file ), catch_stderr=True, with_retcode=True ) stat1 = os.stat(output_file)
os.umask(0o777)
self.assertTrue(stat1.st_size < stat2.st_size)
os.unlink(output_file)
os.umask(current_umask)
from __future__ import absolute_import import os import sys import getpass import platform import yaml import signal import shutil import tempfile import logging
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration from integration.utils import testprogram import salt.utils import salt.defaults.exitcodes
self._test_dir = tempfile.mkdtemp(prefix='salt-testdaemon-')
if self._test_dir and os.path.sep == self._test_dir[0]: shutil.rmtree(self._test_dir) self._test_dir = None
minion.setup() _minions.append(minion)
defaults.write( 'TIMEOUT=60\n' 'TICK=1\n' )
ret = self._run_initscript(init_script, minions, False, 'bogusaction', 2)
for minion in minions: minion.shutdown()
from __future__ import absolute_import import os import yaml import shutil import time
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
subnet = yaml_data['minion'][0]
self.assertIn( 'Failed to setup the Syslog logging handler', '\n'.join(ret[1]) ) self.assertEqual(ret[2], 2)
from __future__ import absolute_import import os import yaml import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
raise
from __future__ import absolute_import, print_function
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../')
test_options.pop(0) if len(test_options) <= 1: break
test_options.pop(0) if len(test_options) <= 1: break
from __future__ import absolute_import import os import pwd import grp import random
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, destructiveTest) ensure_in_syspath('../../')
import salt.utils from salt.utils.pycrypto import gen_hash import integration
for _ in range(20): next_index = random.randrange(len(alphabet)) password += alphabet[next_index]
hashed_pwd = gen_hash('salt', password, 'sha512')
set_pw_cmd = "shadow.set_password {0} '{1}'".format( self.userA, password if salt.utils.is_darwin() else hashed_pwd ) self.run_call(set_pw_cmd)
set_pw_cmd = "shadow.set_password {0} '{1}'".format( self.userB, password if salt.utils.is_darwin() else hashed_pwd ) self.run_call(set_pw_cmd)
from __future__ import absolute_import import os import textwrap
from salttesting.helpers import ensure_in_syspath
import integration import salt.utils
disabled_ret = ('first second third | wc -l ; export SALTY_VARIABLE=saltines ' '&& echo $SALTY_VARIABLE ; echo duh &> /dev/null') ret_key = 'test_|-shell_enabled_|-{0}_|-configurable_test_state'.format(disabled_ret)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import yaml import signal import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import yaml import shutil import tempfile
from salttesting.helpers import ensure_in_syspath from salttesting import skipIf ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import yaml import pipes import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
import salt.ext.six as six
old_cwd = None
raise
from __future__ import absolute_import import os import yaml import signal import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath, requires_salt_modules
import integration
from __future__ import absolute_import import inspect import tempfile import shutil import os import collections
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
import salt.ext.six as six from salt.ext.six.moves import range from salt.config import minion_config
self.loader = LazyLoader([self.module_dir], self.opts, tag='module')
self.assertTrue( inspect.isfunction( self.loader[self.module_name + '.loaded'] ) ) self.assertTrue(self.module_name + '.not_loaded' not in self.loader)
self.assertEqual(self.loader._dict, {}) self.assertTrue(inspect.isfunction(self.loader['test.ping']))
for key, val in six.iteritems(self.loader._dict): self.assertEqual(key.split('.', 1)[0], 'test')
self.assertFalse('test.missing_func' in self.loader._dict)
for key, func in six.iteritems(self.loader): break self.assertNotEqual(self.loader._dict, {})
for key, val in six.iteritems(func_globals['__opts__']): self.assertEqual(self.opts[key], val)
self.assertNotIn(self.module_key, self.loader)
self.assertTrue(inspect.isfunction(self.loader[self.module_key]))
for k, v in six.iteritems(self.loader._dict): self.assertTrue(k.startswith(self.module_name))
self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key + '2', self.loader)
self.assertNotIn(self.module_key + '3', self.loader) self.assertNotIn(self.module_key + '4', self.loader)
self.assertNotIn(self.module_key, self.loader)
for x in range(1, 3): self.update_module() self.loader.clear() self.assertEqual(self.loader[self.module_key](), self.count)
self.assertEqual(self.loader[self.module_key](), self.count) self.loader.clear() self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key, self.loader)
self.assertNotIn(self.module_key, self.loader)
self.update_module() self.update_lib() self.loader.clear() self.assertEqual(self.loader[self.module_key](), (self.count, self.lib_count))
self.rm_lib() self.loader.clear() self.assertNotIn(self.module_key, self.loader)
with open(os.path.join(self.module_dir, '__init__.py'), 'w') as fh: fh.write(deep_init_base) fh.flush()
for lib in self.libs: for x in xrange(5): self.update_lib(lib) self.loader.clear() self._verify_libs()
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath from salttesting.unit import skipIf ensure_in_syspath('../')
import integration from salt.config import minion_config
self.assertEqual({'k2': 'v2'}, grains['a_custom'])
self.assertIn('a_custom', __grain__) self.assertEqual({'k1': 'v1', 'k2': 'v2'}, __grain__['a_custom'])
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../')
import integration
self.assertIn('test.ping', funcs)
self.assertNotIn('brain.left_hemisphere', funcs)
self.assertIn('test.recho', funcs)
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.config import minion_config
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath
import integration import salt.loader import inspect import yaml
import salt.ext.six as six
self.assertNotEqual(global_vars, [], msg='No modules were loaded.')
func_name = inspect.stack()[1][3] names = next(six.itervalues(yaml.load(getattr(self, func_name).__doc__)))
for item in global_vars: for name in names: self.assertIn(name, list(item.keys()))
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
ret = self.run_run_plus(fun='fileserver.dir_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.dir_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.empty_dir_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.empty_dir_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.envs', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.envs', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.file_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.file_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], list)
ret = self.run_run_plus(fun='fileserver.symlink_list', args=['backend="roots"']) self.assertIsInstance(ret['fun'], dict)
ret = self.run_run_plus(fun='fileserver.symlink_list', args=['backend="[roots]"']) self.assertIsInstance(ret['fun'], dict)
ret = self.run_run_plus(fun='fileserver.update', args=['backend="roots"']) self.assertTrue(ret['fun'])
ret = self.run_run_plus(fun='fileserver.update', args=['backend="[roots]"']) self.assertTrue(ret['fun'])
from __future__ import absolute_import import os import shutil import tempfile
from salt.runners import winrepo from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
import integration
from __future__ import absolute_import
from salttesting.helpers import ( ensure_in_syspath, ) ensure_in_syspath('../../')
import integration
self.assertIsNot(bad_out, ret_output)
for item in good_out: self.assertIn(item, ret_output)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import json
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, with_system_user, skip_if_binaries_missing ) ensure_in_syspath('../../')
import integration import salt.utils
ret = self.run_state('ssh_known_hosts.present', test=True, **kwargs) self.assertSaltNoneReturn(ret)
self.run_state('ssh_known_hosts.present', **kwargs)
ret = self.run_state('ssh_known_hosts.present', test=True, **kwargs) self.assertSaltTrueReturn(ret)
ret = self.run_state('ssh_known_hosts.present', **dict(kwargs, name=GITHUB_IP)) self.assertSaltStateChangesEqual( ret, GITHUB_FINGERPRINT, keys=('new', 'fingerprint') )
ret = self.run_state('ssh_known_hosts.absent', test=True, **kwargs) self.assertSaltNoneReturn(ret)
ret = self.run_state('ssh_known_hosts.absent', **kwargs) self.assertSaltStateChangesEqual( ret, GITHUB_FINGERPRINT, keys=('old', 'fingerprint') )
ret = self.run_state('ssh_known_hosts.absent', **kwargs) self.assertSaltStateChangesEqual(ret, {})
ret = self.run_state('ssh_known_hosts.absent', test=True, **kwargs) self.assertSaltTrueReturn(ret)
contents='ssh-rsa AAAAB3NzaC1kc3MAAACBAL0sQ9fJ5bYTEyY== root'
from __future__ import absolute_import import os import shutil
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import textwrap import tempfile
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import time
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains, requires_salt_modules ) ensure_in_syspath('../../')
import integration import salt.utils
_PKG_TARGETS_DOT = { 'RedHat': {'5': 'python-migrate0.5', '6': 'tomcat6-el-2.1-api', '7': 'tomcat-el-2.2-api'} }
_PKG_TARGETS_EPOCH = { 'RedHat': {'7': 'comps-extras'}, }
self.assertTrue(pkg_targets)
self.assertFalse(version)
if os_family == 'FreeBSD': return
self.assertTrue(pkg_targets)
self.assertTrue(version)
self.assertTrue(pkg_targets) version = self.run_function('pkg.version', pkg_targets)
if os_family == 'FreeBSD': return
self.assertTrue(bool(pkg_targets))
self.assertTrue(bool(version))
self.assertFalse(bool(version))
if os_name == 'CentOS' \ and grains['osrelease'].startswith('5.'): target = target.replace('.i686', '.i386')
self.assertTrue(bool(version))
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import logging
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath ) ensure_in_syspath('../../')
import integration import salt.ext.six as six from salt.modules import mysql as mysqlmod
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil import socket
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
socket.setdefaulttimeout(10)
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils
from __future__ import absolute_import import os import time import subprocess
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import from distutils.version import LooseVersion import glob import grp import os import pwd import sys import shutil import stat import tempfile import textwrap import filecmp import textwrap
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, with_system_user_and_group )
import integration import salt.utils
import salt.ext.six as six
os.makedirs(name)
- file: {good_file}
with salt.utils.fopen(path_test, 'r') as fp_test_: self.assertTrue((sum(1 for _ in fp_test_) == 1))
with salt.utils.fopen(path_test, 'r') as fp_test_: self.assertTrue(fp_test_.read().startswith('en_US.UTF-8'))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
self.assertTrue(filecmp.cmp(path_test, path_out))
self.assertTrue(filecmp.cmp(path_test + '.bak', path_in))
for item in ret: self.assertSaltTrueReturn(item)
shutil.copyfile(path_in, path_test)
fstats_orig = os.stat(path_test)
age = 5*24*60*60
os.utime(path_test, (fstats_orig.st_mtime-age, fstats_orig.st_atime-age))
fstats_post = os.stat(path_test)
self.assertTrue(filecmp.cmp(path_in, path_test))
self.assertFalse(os.path.exists(path_test + '.bak'))
self.assertTrue(fstats_post.st_mtime, fstats_orig.st_mtime-age)
self.assertSaltTrueReturn(ret)
shutil.copyfile(path_in, path_test)
fstats_orig = os.stat(path_test)
age = 5*24*60*60
os.utime(path_test, (fstats_orig.st_mtime-age, fstats_orig.st_atime-age))
fstats_post = os.stat(path_test)
self.assertTrue(filecmp.cmp(path_in, path_test))
self.assertFalse(os.path.exists(path_test + '.bak'))
self.assertTrue(fstats_post.st_mtime, fstats_orig.st_mtime-age)
self.assertSaltTrueReturn(ret)
ret = self.run_state( 'file.append', name=name, text='cheese' ) self.assertSaltTrueReturn(ret) self.assertTrue(os.path.isfile(name))
ret = self.run_state( 'file.prepend', name=name, text='cheese' ) self.assertSaltTrueReturn(ret) self.assertTrue(os.path.isfile(name))
os.makedirs(name)
ret = self.run_function( 'state.sls', mods='testappend.issue-2227' ) self.assertSaltTrueReturn(ret)
ret = self.run_function( 'state.template_str', [template], timeout=120 )
tmp_file = os.path.join(integration.TMP, 'issue-2379-file-append.txt') salt.utils.fopen(tmp_file, 'w').write(
if os.path.isdir(tmp_dir): shutil.rmtree(tmp_dir) elif os.path.isfile(tmp_dir): os.remove(tmp_dir)
ret = self.run_state( 'file.directory', name=tmp_dir, follow_symlinks=True, user=user, group=group, recurse=['user', 'group'] ) self.assertSaltTrueReturn(ret)
if os.path.isdir(tmp_dir): shutil.rmtree(tmp_dir) elif os.path.isfile(tmp_dir): os.remove(tmp_dir)
ret = self.run_state( 'file.directory', name=tmp_dir, follow_symlinks=False, user=user, group=group, recurse=['user', 'group'] ) self.assertSaltTrueReturn(ret)
from __future__ import absolute_import import os
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import shutil
from salttesting import skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES
with salt.utils.fopen(requirements_file_path, 'a') as fhw: fhw.write('pep8==1.3.3\n')
try: ret = self.run_function( 'state.template_str', ['\n'.join(template)] )
if os.path.exists(venv_path): shutil.rmtree(venv_path) if os.path.exists(requirements_file_path): os.unlink(requirements_file_path) raise
with salt.utils.fopen(requirements_file_path, 'w') as fhw: fhw.write('zope.interface==4.0.1\n')
try: ret = self.run_function( 'state.template_str', ['\n'.join(template)] )
if os.path.exists(venv_path): shutil.rmtree(venv_path) if os.path.exists(requirements_file_path): os.unlink(requirements_file_path) raise
if os.path.exists(venv_path): shutil.rmtree(venv_path) if os.path.exists(requirements_file_path): os.unlink(requirements_file_path)
from __future__ import absolute_import import os import sys from random import randint import grp
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import salt.utils import integration
gid_from_name = False if grains['os_family'] == 'MacOS' else True
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
self.assertIsInstance(ret, list)
from __future__ import absolute_import
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
from __future__ import absolute_import import os import pwd import glob import shutil
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains, with_system_user ) ensure_in_syspath('../../')
import integration import salt.utils from salt.modules.virtualenv_mod import KNOWN_BINARY_NAMES from salt.exceptions import CommandExecutionError
import salt.ext.six as six
ret = self.run_function('virtualenv.create', [venv_dir]) self.assertEqual(ret['retcode'], 0)
ret = self.run_function('state.sls', mods='pip-installed-errors') self.assertSaltTrueReturn(ret)
self.skipTest( 'You don\'t have the required permissions to run this test' )
ret = self.run_function( 'state.sls', mods='pip-installed-weird-install' ) self.assertSaltTrueReturn(ret)
ret = self.run_function('virtualenv.create', [venv_dir]) self.assertEqual(ret['retcode'], 0)
self.assertEqual( self.run_function('pip.list', ['pip'], bin_env=venv_dir), {'pip': '6.0'} )
venv_dir = os.path.join( integration.TMP, 'pip-installed-specific-env' )
from __future__ import absolute_import
from salttesting import skipIf from salttesting.helpers import ( destructiveTest, ensure_in_syspath, requires_system_grains ) ensure_in_syspath('../../')
import integration import salt.utils
import salt.ext.six as six
self.assertReturnNonEmptySaltType(ret) for state_id, state_result in six.iteritems(ret): self.assertSaltTrueReturn(dict([(state_id, state_result)]))
self.assertReturnNonEmptySaltType(ret) for state_id, state_result in six.iteritems(ret): self.assertSaltTrueReturn(dict([(state_id, state_result)]))
from __future__ import absolute_import import os import shutil import socket import subprocess import tempfile
from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing ensure_in_syspath('../../')
import integration import salt.utils
socket.setdefaulttimeout(10)
from __future__ import absolute_import import re
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration
NO_BOTO_MODULE = True BOTO_NOT_CONFIGURED = True try: import boto NO_BOTO_MODULE = False try: boto.connect_iam() BOTO_NOT_CONFIGURED = False except boto.exception.NoAuthHandlerFound: pass except ImportError: pass
from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from __future__ import absolute_import from __future__ import print_function import json import time
from salt.netapi.rest_tornado import saltnado from unit.netapi.rest_tornado.test_handlers import SaltnadoTestCase
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath
self.assertEqual(response_obj['return'][0]['minion']['id'], 'minion')
self.application = application self.events_to_fire = 0 return application
else: ZMQIOLoop.current().add_timeout(time.time() + 0.5, self._stop)
from __future__ import absolute_import import json
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../../')
('arg', [1234]), ('kwarg', {'ext_source': 'redis'}),
from __future__ import absolute_import import os
from salttesting.unit import skipIf from salttesting.helpers import ( ensure_in_syspath, destructiveTest) ensure_in_syspath('../../../')
import salt.utils from tests import integration
if USERA in user_list: self.run_function('user.delete', [USERA], remove=True) #need to exit cherypy engine cherrypy.engine.exit()
from __future__ import absolute_import import os
from integration import TMP_CONF_DIR from salttesting import TestCase
import salt.config import salt.netapi
self.assertIn('tag', ret) ret.pop('tag')
from __future__ import absolute_import, print_function import subprocess import hashlib import pprint import optparse
from salt.utils import get_colors
import yaml import salt.ext.six as six
from __future__ import absolute_import
from salt.cli.batch import Batch
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath
from salt.utils import schema
try: import jsonschema import jsonschema.exceptions HAS_JSONSCHEMA = True except ImportError: HAS_JSONSCHEMA = False
try: import rfc3987 HAS_RFC3987 = True except ImportError: HAS_RFC3987 = False
from __future__ import absolute_import import os import time import signal import multiprocessing
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import salt.utils import salt.utils.process
import salt.ext.six as six
if process_manager._process_map.keys(): process_manager.send_signal_to_processes(signal.SIGILL) process_manager.stop_restarting() process_manager.kill_children()
if process_manager._process_map.keys(): process_manager.send_signal_to_processes(signal.SIGILL) process_manager.stop_restarting() process_manager.kill_children()
self.assertEqual(counter.value, 0) self.assertEqual(pool._job_queue.qsize(), 1)
from __future__ import absolute_import import copy
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils import configcomparer
from __future__ import absolute_import import os import copy
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
import salt.config from salt.utils.schedule import Schedule
from __future__ import absolute_import
from salt.utils import args
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import salt.utils.gitfs from salt.exceptions import FileserverConfigError
OPTS = {'cachedir': '/tmp/gitfs-test-cache'}
role_class(*args) role_class(*args)
role_class(*args)
self.assertRaises( FileserverConfigError, role_class, *args )
self.assertRaises( FileserverConfigError, role_class, *args )
from __future__ import absolute_import import os from os.path import join from shutil import rmtree from tempfile import mkdtemp
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils import salt.utils.find
from __future__ import absolute_import import copy
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils import dictupdate
from __future__ import absolute_import import getpass import os import sys import stat import shutil import resource import tempfile import socket
from salttesting import skipIf, TestCase from salttesting.helpers import ( ensure_in_syspath, requires_network, TestsLoggingHandler ) ensure_in_syspath('../../')
import salt.utils import integration from salt.utils.verify import ( check_user, verify_env, verify_socket, zmq_version, check_max_open_files, valid_id )
class FakeWriter(object): def __init__(self): self.output = ""
self.assertEqual( [logmsg_dbg.format(newmax)], handler.messages )
self.skipTest('We\'ve hit the max open files setting')
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salt.ext.six.moves import range
from salt.utils import mac_utils from salt.exceptions import SaltInvocationError, CommandExecutionError
from __future__ import absolute_import import os import sys import shutil import tempfile import stat
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration import salt.utils import salt.utils.find
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.exceptions import SaltInvocationError import salt.utils.boto import salt.utils.boto3
try: import boto import boto.exception from boto.exception import BotoServerError
self.assertNotEqual(id(boto_ec2_conn), id(boto3_ec2_conn))
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils.args import KWARG_REGEX
from __future__ import absolute_import import time
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import cache
self.assertRaises(KeyError, cd.__getitem__, 'foo')
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../../')
import integration import salt.utils
@patch('salt.utils.which', lambda exe: None) def test_missing_binary_in_linux(self): self.assertTrue( salt.utils.which('this-binary-does-not-exist') is None )
@patch('salt.utils.which', lambda exe: exe) def test_existing_binary_in_linux(self): self.assertTrue(salt.utils.which('this-binary-exists-under-linux'))
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import immutabletypes
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.aggregation import aggregate, Map, Scalar
from __future__ import absolute_import import os import shutil
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salt.utils.cache import context_cache
import salt.payload import salt.utils
with salt.utils.fopen(target_cache_file, 'rb') as fp_: target_cache_data = salt.payload.Serial(__opts__).load(fp_) self.assertDictEqual(__context__, target_cache_data)
cc = salt.utils.cache.ContextCache(__opts__, __name__) retrieved_cache = cc.get_cache_context() self.assertDictEqual(retrieved_cache, __context__)
@context_cache def _test_set_cache(): pass _test_set_cache()
@context_cache def _test_refill_cache(comparison_context): self.assertEqual(__context__, comparison_context)
from __future__ import absolute_import import os import sys import random import subprocess import time
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import fopen, is_darwin, vt
self.assertEqual( terminal.getwinsize(), (24, cols) ) terminal.wait() terminal.close()
try: if os.path.exists('/proc/sys/kernel/pty/nr'): with fopen('/proc/sys/kernel/pty/nr') as fh_: return int(fh_.read().strip())
self.skipTest( 'Unable to find out how many PTY\'s are open on Darwin - ' 'Skipping for now' )
if stdout is None and stderr is None: self.assertFalse(term.isalive())
self.assertEqual(buffer_o, expected_data) self.assertFalse(term.isalive())
if stdout is None and stderr is None: self.assertFalse(term.isalive())
self.assertEqual(buffer_e, expected_data) self.assertFalse(term.isalive())
if stdout is None and stderr is None: self.assertFalse(term.isalive())
time.sleep(0.1)
self.assertEqual(buffer_o, expected_data) self.assertFalse(term.isalive())
from __future__ import absolute_import
from salttesting import skipIf from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../../')
from salt.utils import network
from __future__ import absolute_import
from yaml.constructor import ConstructorError from salt.utils.yamlloader import SaltYamlSafeLoader import salt.utils
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON, mock_open
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.filebuffer import BufferedReader, InvalidFileMode
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.utils import etcd_util try: from urllib3.exceptions import ReadTimeoutError, MaxRetryError HAS_URLLIB3 = True except ImportError: HAS_URLLIB3 = False
mock.side_effect = ValueError self.assertEqual(client.get('not-found'), None)
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.utils.rsax931 import RSAX931Signer, RSAX931Verifier
from __future__ import absolute_import
import tornado.testing import tornado.gen from tornado.testing import AsyncTestCase
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.utils import http
from __future__ import absolute_import
from salt.utils.validate import net
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import import sys import warnings
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import warn_until, kwargs_warn_until from salt.version import SaltStackVersion
warnings.filterwarnings('always', '', DeprecationWarning, __name__)
with warnings.catch_warnings(record=True) as recorded_warnings: raise_warning() self.assertEqual( 'Deprecation Message!', str(recorded_warnings[0].message) )
with warnings.catch_warnings(record=True) as recorded_warnings: raise_named_version_warning() self.assertEqual( 'Deprecation Message!', str(recorded_warnings[0].message) )
warnings.filterwarnings('always', '', DeprecationWarning, __name__)
with warnings.catch_warnings(record=True) as recorded_warnings:
with warnings.catch_warnings(record=True) as recorded_warnings: kwargs_warn_until(
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON
ensure_in_syspath('../../') import salt.ext.six as six from salt.ext.six.moves import reload_module from salt.utils import locales
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( patch, DEFAULT, create_autospec, NO_MOCK, NO_MOCK_REASON ) ensure_in_syspath('../../')
from salt.utils import args from salt.utils.odict import OrderedDict from salt.exceptions import (SaltInvocationError, SaltSystemExit, CommandNotFoundError) from salt import utils
import os import datetime import yaml import zmq from collections import namedtuple
import salt.ext.six as six from salt.ext.six.moves import range try:
incorrect_jid_length = 2012 self.assertEqual(utils.jid.jid_to_time(incorrect_jid_length), '')
self.assertRaises(SaltInvocationError, utils.format_call, dummy_func, {'1': 2})
self.skipTest('\'timelib\' is not installed')
ret = utils.find_json(test_sample_json) self.assertDictEqual(ret, expected_ret)
garbage_prepend_json = '{0}{1}'.format(LORUM_IPSUM, test_sample_json) ret = utils.find_json(garbage_prepend_json) self.assertDictEqual(ret, expected_ret)
self.assertRaises(ValueError, utils.find_json, LORUM_IPSUM)
self.assertFalse(utils.is_bin_str(''))
yaml_key_val_pair = '- key1: val1' ret = utils.repack_dictlist(yaml_key_val_pair) self.assertDictEqual(ret, {'key1': 'val1'})
ret = utils.repack_dictlist(LORUM_IPSUM) self.assertDictEqual(ret, {})
self.assertEqual(str(ret['LIGHT_YELLOW']), str(ret['LIGHT_GRAY']))
with patch('sys.argv', ['salt-call']): ret = utils.daemonize_if({}) self.assertEqual(None, ret)
self.assertRaises(RuntimeError, utils.kwargs_warn_until, {}, [])
from __future__ import absolute_import import os import sys import posixpath import ntpath import platform import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import path_join
import salt.ext.six as six
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import format_call from salt.exceptions import SaltInvocationError
from __future__ import absolute_import import re
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import build_whitespace_split_regex
if [ -z '$debian_chroot' ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z '$debian_chroot' ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
if [ -z '$debian_chroot' ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi
from salttesting import (expectedFailure, skipIf) from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import integration from salt.utils.process import clean_proc from salt.utils import event
time.sleep(10)
if os.environ.get('TRAVIS_PYTHON_VERSION', None) is not None: time.sleep(10) else: time.sleep(2)
evt1 = me.get_event(wait=0, tag='evt1', no_block=False) self.assertGotEvent(evt1, {'data': 'foo1'})
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils import cloud from integration import TMP, CODE_DIR
if not os.path.isdir(GPG_KEYDIR): os.makedirs(GPG_KEYDIR)
try: import keyring import keyring.backend
keyring.set_keyring(TestKeyring()) HAS_KEYRING = True
from __future__ import absolute_import
import salt.utils.url
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salt.utils import decorators from salt.version import SaltStackVersion from salt.exceptions import CommandExecutionError
from __future__ import absolute_import import os import shutil import tempfile import uuid
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt://map.sls import Samba
from __future__ import absolute_import
from salt.beacons import adb
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch, Mock
from __future__ import absolute_import
from salt.beacons import glxinfo
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch, Mock
from __future__ import absolute_import import os import shutil import tempfile
from salt.beacons import inotify
from salttesting import skipIf, TestCase from salttesting.helpers import destructiveTest, ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON
try:
from __future__ import absolute_import import os import os.path import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
import salt.loader import salt.config import integration from salt.exceptions import SaltRenderError from salt.ext.six.moves import StringIO
import salt.ext.six as six
from __future__ import absolute_import import os import copy import tempfile import json import datetime import pprint
from salttesting.unit import skipIf, TestCase from salttesting.case import ModuleCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import yaml from jinja2 import Environment, DictLoader, exceptions try:
with self.assertRaises(exceptions.TemplateRuntimeError): env.from_string("{{ document|load_json }}").render(document="{'foo': 'it works'}")
with self.assertRaises(exceptions.TemplateRuntimeError): env.from_string('{{ document|load_json }}').render(document={"foo": "it works"})
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../')
import integration from salt import client from salt.exceptions import EauthAuthenticationError, SaltInvocationError, SaltClientError
with patch('os.path.exists', return_value=False): self.assertRaises(SaltClientError, lambda: self.client.pub('*', 'test.ping'))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, call, mock_open, NO_MOCK, NO_MOCK_REASON, MagicMock
import salt.utils from salt import crypt
try:
from __future__ import absolute_import import re
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt.version import SaltStackVersion
from __future__ import absolute_import import os import os.path import tempfile
from salttesting import TestCase from salttesting.mock import patch, MagicMock from salttesting.helpers import ensure_in_syspath
import integration import salt.config from salt.state import HighState from salt.utils.odict import OrderedDict, DefaultOrderedDict
from __future__ import absolute_import import os import copy import shutil import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
import salt.utils from salt.utils import files as util_files
import salt.ext.six as six
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
from salt.pillar import consul_pillar
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON
from salt.pillar import Pillar, git_pillar
raise RuntimeError("Infinite loop detected")
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, call, patch from salttesting.helpers import ensure_in_syspath
from salt.pillar import nodegroups
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.pillar import sqlite3
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.pillar import sqlcipher
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.pillar import mysql
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON
from salt.pillar import hg_pillar HGLIB = hg_pillar.hglib
from __future__ import absolute_import import tempfile
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../')
import salt.pillar
pillar.client.get_state = MagicMock( return_value={ 'dest': '/path/to/pillar/files/foo.sls', 'source': 'salt://foo.sls' } )
matcher = Matcher.return_value matcher.confirm_top.return_value = True
client = get_file_client.return_value client.cache_file.return_value = self.top_file.name
matcher = Matcher.return_value matcher.confirm_top.return_value = True
client = get_file_client.return_value client.cache_file.return_value = self.top_file.name
loaderCls = MockLoader
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath
from salt import template
from __future__ import absolute_import
import salt.transport.client
del self.pub_channel
from __future__ import absolute_import import os import threading
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../') import integration
from unit.transport.req_test import ReqChannelMixin from unit.transport.pub_test import PubChannelMixin
cls.req_server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_opts) cls.req_server_channel.pre_fork(cls.process_manager)
from __future__ import absolute_import import os import logging
import integration
from __future__ import absolute_import import os import threading import platform import time
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../')
from unit.transport.req_test import ReqChannelMixin from unit.transport.pub_test import PubChannelMixin
cls.req_server_channel = salt.transport.server.ReqServerChannel.factory(cls.master_opts) cls.req_server_channel.pre_fork(cls.process_manager)
from __future__ import absolute_import import logging from salt.ext.six.moves import StringIO
from salttesting.case import TestCase from salttesting.helpers import ensure_in_syspath, TestsLoggingHandler
from salt.log import setup as saltlog from salt.log.handlers import StreamHandler
log = saltlog.SaltLoggingClass(__name__)
log_format = '[%(name)-15s] %(message)s' handler = TestsLoggingHandler(format=log_format) log.addHandler(handler)
log.removeHandler(handler)
log_format = '[%(name)s] %(message)s' handler = TestsLoggingHandler(format=log_format) log.addHandler(handler)
log.removeHandler(handler)
stream1 = StringIO() stream2 = StringIO() handler1 = StreamHandler(stream1) handler2 = StreamHandler(stream2)
stream1 = StringIO() stream2 = StringIO() handler1 = StreamHandler(stream1) handler2 = StreamHandler(stream2)
stream1 = StringIO() stream2 = StringIO() handler1 = StreamHandler(stream1) handler2 = StreamHandler(stream2)
from __future__ import absolute_import, print_function
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath
from salt.config.schemas import ssh as ssh_schemas from salt.config.schemas.minion import MinionConfiguration
try: import jsonschema import jsonschema.exceptions HAS_JSONSCHEMA = True except ImportError: HAS_JSONSCHEMA = False
'minion_opts': ssh_schemas.DictItem(title='Minion Options', description='Dictionary of minion options', properties=MinionConfiguration()).serialize(),
from __future__ import absolute_import import logging import os import shutil import tempfile from contextlib import contextmanager
from salttesting import TestCase from salttesting.mock import MagicMock, patch from salttesting.helpers import ensure_in_syspath, TestsLoggingHandler from salt.exceptions import CommandExecutionError
import salt.minion import salt.utils import salt.utils.network import integration from salt import config as sconfig from salt.exceptions import SaltCloudConfigError
import yaml
MOCK_HOSTNAME = 'very.long.complex.fqdn.that.is.crazy.extra.long.example.com'
self.assertEqual(config['key_logfile'], os.path.join('/', 'key')) self.assertNotEqual(config['key_logfile'], '//key')
config = sconfig.master_config('/etc/salt/master') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
config = sconfig.minion_config('/etc/salt/minion') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
salt.utils.fopen(minion_config, 'w').write( 'blah: false\n' 'root_dir: {0}\n' 'log_file: {1}\n'.format(tempdir, minion_config) )
config = sconfig.minion_config(minion_config)
self.assertTrue(config['blah'])
salt.utils.fopen(master_config, 'w').write( 'blah: false\n' 'root_dir: {0}\n' 'log_file: {1}\n'.format(tempdir, master_config) )
config = sconfig.master_config(master_config)
self.assertTrue(config['blah'])
self.assertTrue(search_paths[0].endswith(etc_deploy_path))
self.assertTrue(search_paths[1].endswith(deploy_path))
config = sconfig.cloud_config('/etc/salt/cloud') self.assertEqual(config['log_file'], env_fpath) os.environ.clear() os.environ.update(original_environ)
os.environ['SALT_CLOUD_CONFIG'] = env_fpath config = sconfig.cloud_config(fpath) self.assertEqual(config['log_file'], fpath)
os.environ.clear() os.environ.update(original_environ)
self.assertIn( deploy_dir_path, default_config['deploy_scripts_search_path'] )
self.assertEqual( deploy_dir_path, default_config['deploy_scripts_search_path'][0] )
from __future__ import absolute_import import os import sys import shutil import tempfile import textwrap import copy
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath
import integration import salt.loader import salt.config import salt.utils from salt.state import HighState from salt.utils.pydsl import PyDslError
import salt.ext.six as six from salt.ext.six.moves import StringIO
finally: HIGHSTATE.pop_active()
state('A').cmd.run('echo this is state A', cwd='/')
extend(state('.start').stateconf.require(stateconf='xxx::goal'))
extend(state('.goal').stateconf.require_in(stateconf='yyy::start'))
from __future__ import absolute_import import time import errno import threading
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath, MockWraps from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch ensure_in_syspath('../')
import salt.payload from salt.utils.odict import OrderedDict import salt.exceptions
import msgpack import zmq import salt.ext.six as six
SREQTestCase.thread_running.clear() SREQTestCase.echo_server.join()
assert sreq.send_auto({}) == {'enc': 'clear', 'load': {}}
assert sreq.send_auto({'load': 'foo'}) == {'load': 'foo', 'enc': 'clear'}
sreq.destroy()
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.renderers import gpg from salt.exceptions import SaltRenderError
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath
import salt.state from salt.config import minion_config from salt.template import compile_template_str from salt.serializers import yamlex
from __future__ import absolute_import import os import shutil import tempfile
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath, destructiveTest
)
from __future__ import absolute_import import tornado.stack_context import tornado.gen from tornado.testing import AsyncTestCase, gen_test import threading import time
from salttesting import TestCase from salt.ext.six.moves import range from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.context import ContextDict, NamespacedDictWrapper
num_concurrent_tasks = 5
self.cd['foo'] = 'global'
self.assertEqual( dict(self.cd), {'foo': 'global'}, )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import smf
smf.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import grub_legacy from salt.exceptions import CommandExecutionError
grub_legacy.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import cassandra
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import useradd from salt.exceptions import CommandExecutionError import pwd
useradd.__grains__ = {} useradd.__salt__ = {} useradd.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import rdp
rdp.__salt__ = {}
IS_RDP = rdp.__virtual__()
from __future__ import absolute_import import os
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import pip from salt.exceptions import CommandExecutionError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import haproxyconn
haproxyconn.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath, TestsLoggingHandler from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from salt.modules import alternatives
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import keyboard
keyboard.__salt__ = {} keyboard.__grains__ = {'os_family': ''}
from __future__ import absolute_import
from salt.modules import cmdmod from salt.exceptions import CommandExecutionError from salt.log import LOG_LEVELS
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, NO_MOCK, NO_MOCK_REASON, patch ) from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import solr import os
solr.__salt__ = {} solr.__opts__ = {}
from __future__ import absolute_import import salt.utils import sys import types
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, Mock, NO_MOCK, NO_MOCK_REASON )
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
from salt.modules import win_network
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import launchctl
launchctl.__salt__ = {}
from __future__ import absolute_import import os
from salt.exceptions import CommandExecutionError from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import systemd
systemd.__salt__ = {} systemd.__context__ = {}
from __future__ import absolute_import import sys
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pam
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pw_group
pw_group.__grains__ = {} pw_group.__salt__ = {} pw_group.__context__ = {} pw_group.grinfo = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import service import os
service.__grains__ = {} service.__salt__ = {}
from __future__ import absolute_import import grp
from salttesting import TestCase from salttesting.mock import MagicMock, patch
from salt.modules import mac_group from salt.exceptions import SaltInvocationError, CommandExecutionError
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_apigateway
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
diff = self._diff_list_dicts(api_keys, items_dt, 'id')
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from salt.exceptions import CommandExecutionError from salt.modules import mac_assistive as assistive
from __future__ import absolute_import
from salt.modules import vsphere from salt.exceptions import CommandExecutionError
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
HOST = '1.2.3.4' USER = 'root' PASSWORD = 'SuperSecret!' ERROR = 'Some Testing Error Message'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import saltcloudmod
saltcloudmod.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import defaults
defaults.__grains__ = {} defaults.__salt__ = {} defaults.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_autoruns
win_autoruns.__salt__ = {} win_autoruns.__grains__ = {}
IS_WIN = win_autoruns.__virtual__()
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ilo
ilo.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import schedule from salt.utils.event import SaltEvent
schedule.__salt__ = {} schedule.__opts__ = {} schedule.__pillar__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import logadm
logadm.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pw_user from salt.exceptions import CommandExecutionError try: import pwd HAS_PWD = True except ImportError: HAS_PWD = False
pw_user.__grains__ = {} pw_user.__salt__ = {} pw_user.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import firewalld
firewalld.__grains__ = {} firewalld.__salt__ = {} firewalld.__context__ = {} firewalld.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import znc
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import bower from salt.exceptions import CommandExecutionError
bower.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import htpasswd
htpasswd.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import mac_desktop from salt.exceptions import CommandExecutionError
mac_desktop.__salt__ = {}
from __future__ import absolute_import from datetime import datetime
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_system
try:
import re self.assertTrue(re.search(r'^\d{2}:\d{2} \w{2}$', win_tm))
from __future__ import absolute_import
from mock import call
from salt.modules import gentoo_service
gentoo_service.__grains__ = {} gentoo_service.__salt__ = {} gentoo_service.__context__ = {} gentoo_service.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils import os.path from salt.modules import key
key.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import gnomedesktop
gnomedesktop.__grains__ = {} gnomedesktop.__salt__ = {} gnomedesktop.__context__ = {} gnomedesktop.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.modules.gem as gem
from __future__ import absolute_import, print_function from mock import call import re
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, patch ensure_in_syspath('../../')
from salt.modules import postgres from salt.exceptions import SaltInvocationError
from __future__ import absolute_import
from salttesting.case import ModuleCase
import salt.loader
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_service
try: WINAPI = True import win32serviceutil except ImportError: WINAPI = False
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import debian_service
debian_service.__grains__ = {} debian_service.__salt__ = {} debian_service.__context__ = {} debian_service.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, Mock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import localemod from salt.exceptions import CommandExecutionError
localemod.__context__ = {} localemod.__grains__ = {} localemod.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ipset
ipset.__salt__ = {}
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salt.exceptions import SaltInvocationError from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import state
state.__salt__ = {} state.__context__ = {} state.__opts__ = {} state.__pillar__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import xapi
from __future__ import absolute_import
from salt.modules import win_certutil as certutil
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath
from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON, )
from salt.modules import zpool
from salt.utils.odict import OrderedDict
zpool.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import drbd
drbd.__grains__ = {} drbd.__salt__ = {} drbd.__context__ = {}
from __future__ import absolute_import import textwrap
from salt.modules import parallels from salt.exceptions import SaltInvocationError
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
import salt.ext.six as six
with patch('salt.utils.which', mock_true): ret = parallels.__virtual__() self.assertTrue(ret) self.assertEqual(ret, 'parallels')
str_args = 'electrolytes --aqueous --anion hydroxide --cation=ammonium free radicals -- hydrogen' _validate_ret(parallels._normalize_args(str_args))
list_args = ' '.join(str_args) _validate_ret(parallels._normalize_args(list_args))
tuple_args = tuple(list_args) _validate_ret(parallels._normalize_args(tuple_args))
other_args = {'anion': 'hydroxide', 'cation': 'ammonium'} _validate_ret(parallels._normalize_args(other_args))
mock_plain = MagicMock() with patch.object(parallels, 'prlctl', mock_plain): parallels.list_vms(runas=runas) mock_plain.assert_called_once_with('list', [], runas=runas)
mock_stop = MagicMock() with patch.object(parallels, 'prlctl', mock_stop): parallels.stop(name, runas=runas) mock_stop.assert_called_once_with('stop', [name], runas=runas)
self.assertRaises(SaltInvocationError, parallels.snapshot_id_to_name, name, '{8-4-4-4-12}')
mock_no_data = MagicMock(return_value='') with patch.object(parallels, 'prlctl', mock_no_data): self.assertRaises(SaltInvocationError, parallels.snapshot_id_to_name, name, snap_id)
self.assertEqual(parallels._validate_snap_name(name, snap_id), snap_id)
mock_prlctl = MagicMock(return_value=guid_str) with patch.object(parallels, 'prlctl', mock_prlctl): parallels.list_snapshots(name) mock_prlctl.assert_called_once_with('snapshot-list', [name], runas=None)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import apache
apache.__grains__ = {} apache.__salt__ = {} apache.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.exceptions import CommandExecutionError from salt.modules import parted
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import twilio_notify
from __future__ import absolute_import import os
from salttesting.case import ModuleCase from salttesting.mixins import RUNTIME_VARS
import salt.config import salt.loader
from __future__ import absolute_import
from salt.modules import mac_keychain as keychain
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import import logging from copy import deepcopy
try: import boto import boto.ec2.elb HAS_BOTO = True except ImportError: HAS_BOTO = False
import salt.config import salt.loader from salt.modules import boto_elb
from salttesting import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salt.exceptions import CommandExecutionError
from salt.modules import zypper
zypper.__salt__ = dict() zypper.__context__ = dict() zypper.rpm = None
self.assertEqual(len(installed), 2)
for pkg_name, pkg_info in installed.items(): self.assertEqual(installed[pkg_name].get('source'), run_out[pkg_name]['source_rpm'])
for pn_key, pn_val in run_out['virgo-dummy'].items(): if pn_key == 'source_rpm': continue self.assertEqual(installed['virgo-dummy'][pn_key], pn_val)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.utils.odict import OrderedDict from salt.modules import pillar as pillarmod
if __name__ == '__main__': from integration import run_tests run_tests(PillarModuleTestCase, needs_daemon=False)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import random_org
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import locate
locate.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import qemu_img import os
qemu_img.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import data
data.__grains__ = {} data.__salt__ = {} data.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import debconfmod import os
debconfmod.__grains__ = {} debconfmod.__salt__ = {} debconfmod.__context__ = {} debconfmod.__opts__ = {}
from __future__ import absolute_import import json import hashlib import base64 import time from subprocess import Popen, PIPE
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath, skip_if_binaries_missing
import salt.modules.k8s as k8s
self.assertTrue(isinstance(kubectl_out, dict))
b = kubectl_out.get("data", {}) self.assertTrue(isinstance(kubectl_out, dict)) self.assertEqual(expected_data, b)
from __future__ import absolute_import
from salt.modules import aliases from salt.exceptions import SaltInvocationError
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import import os import shutil
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
import salt.utils.odict from salt.modules import seed from salttesting.helpers import ensure_in_syspath
seed.__salt__ = {} seed.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import incron
incron.__grains__ = {} incron.__salt__ = {} incron.__context__ = {} incron.__opts__ = {}
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch
from salt.modules import kmod
from __future__ import absolute_import import sys import re
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import virt from salt.modules import config from salt._compat import ElementTree as ET import salt.utils
import yaml import salt.ext.six as six
self.assertTrue(len(controllers) == 0)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import monit
monit.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.modules import dig
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import svn
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( mock_open, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import nfs3
nfs3.__grains__ = {} nfs3.__salt__ = {} nfs3.__context__ = {} nfs3.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( patch, MagicMock, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import pagerduty import json
pagerduty.__opts__ = {} pagerduty.__salt__ = { 'config.option': MagicMock(return_value=None) }
from __future__ import absolute_import import copy import logging import os import subprocess from distutils.version import LooseVersion
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
git_mod.__salt__ = {} git_mod.__context__ = {} log = logging.getLogger(__name__)
return {'stdout': _cmd_run_values[' '.join(key)], 'stderr': '', 'retcode': 0, 'pid': 12345}
return not WORKTREE_INFO[key].get('stale', False)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_ntp
win_ntp.__salt__ = {}
IS_WIN = win_ntp.__virtual__()
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import sysbench
sysbench.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import munin
munin.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import varnish
varnish.__salt__ = {}
from __future__ import absolute_import
from salt.modules import archive from salt.exceptions import CommandNotFoundError from salt.utils import which_bin
archive.__salt__ = {} archive.__pillar__ = {} archive.__grains__ = {"id": "0"} archive.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import influx
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import djangomod
djangomod.__grains__ = {} djangomod.__salt__ = {} djangomod.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import hadoop
hadoop.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import sensors
sensors.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import drac
drac.__grains__ = {} drac.__salt__ = {} drac.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import openstack_config from salt.exceptions import CommandExecutionError
openstack_config.__salt__ = {}
from __future__ import absolute_import
from salt.modules import win_powercfg as powercfg
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( MagicMock, NO_MOCK, NO_MOCK_REASON, patch )
import salt.utils.s3 from salt.modules import s3
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import postfix
postfix.__salt__ = {}
from __future__ import absolute_import
import shutil import tempfile import os from distutils.version import LooseVersion try:
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import destructiveTest
from salt.modules import tls import integration
tls.__grains__ = {} tls.__salt__ = {} tls.__context__ = {} tls.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_path
win_path.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import supervisord
from __future__ import absolute_import
from salt.modules import mac_pkgutil
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch
source = "/foo/bar/fubar.pkg" package_id = "com.foo.fubar.pkg"
_install_from_path.assert_called_with(source)
source = "/foo/bar/fubar.pkg" package_id = "com.foo.fubar.pkg"
self.assertEqual(_install_from_path.called, 0)
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_elasticsearch_domain
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salt.utils import is_linux from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
try: import salt.modules.shadow as shadow HAS_SHADOW = True except ImportError: HAS_SHADOW = False
import salt.ext.six as six
from __future__ import absolute_import
from salt.modules import cp from salt.utils import templates from salt.exceptions import CommandExecutionError
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
cp.__salt__ = {} cp.__opts__ = {} cp.__pillar__ = {} cp.__grains__ = {} cp.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pyenv
pyenv.__grains__ = {} pyenv.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON ensure_in_syspath('../../')
from salt.modules import portage_config
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_groupadd
try: import win32com import pythoncom import pywintypes HAS_WIN_LIBS = True except ImportError: HAS_WIN_LIBS = False
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_disk
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import sdb
sdb.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import match
match.__grains__ = {} match.__salt__ = {} match.__opts__ = {} match.__pillar__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import neutron
from __future__ import absolute_import
from salt.modules import mac_brew
from salttesting import skipIf, TestCase from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
mac_brew.__context__ = {} mac_brew.__salt__ = {} mac_brew.__opts__ = {'user': MagicMock(return_value='bar')}
from __future__ import absolute_import import re
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
__salt__
from salt.ext.six.moves import builtins as __builtin__ __builtin__.__salt__ = {}
assert False
self.fail('An exception should be thrown')
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_cloudtrail
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import modjk
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import bridge
bridge.__grains__ = {}
from __future__ import absolute_import from textwrap import dedent
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
import salt from salt.modules import syslog_ng
from __future__ import absolute_import
from salt.modules import linux_sysctl from salt.modules import systemd from salt.exceptions import CommandExecutionError
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
linux_sysctl.__salt__ = {} linux_sysctl.__context__ = {} systemd.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import introspect
introspect.__salt__ = {}
from __future__ import absolute_import import sys
from salttesting import skipIf, TestCase from salttesting.helpers import ( ensure_in_syspath, TestsLoggingHandler, ForceImportErrorOn ) from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import virtualenv_mod from salt.exceptions import CommandExecutionError
virtualenv_mod.__salt__ = {'cmd.which_bin': lambda _: 'pyvenv'}
from __future__ import absolute_import
from salt.modules import proxy as proxy
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) import os
from salt.modules import daemontools from salt.exceptions import CommandExecutionError
daemontools.__grains__ = {} daemontools.__salt__ = {} daemontools.__context__ = {} daemontools.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, patch)
from salt.modules import riak
riak.__salt__ = {}
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt import salt.utils.fsutils from salt.modules import btrfs from salt.exceptions import CommandExecutionError
btrfs.__grains__ = {} btrfs.__salt__ = {} btrfs.__context__ = {}
from __future__ import absolute_import import sys import types
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, Mock, NO_MOCK, NO_MOCK_REASON )
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
from salt.modules import win_dns_client
win_dns_client.__salt__ = {} win_dns_client.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath
from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON, )
from salt.modules import zfs from salt.utils.odict import OrderedDict
zfs.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pkgutil from salt.exceptions import CommandExecutionError, MinionError
pkgutil.__salt__ = {} pkgutil.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
from salt.modules import mdadm
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_s3_bucket
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import s6
s6.__salt__ = {} s6.SERVICE_DIR = '/etc/service'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import mine
mine.__salt__ = {} mine.__opts__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_vpc from salt.exceptions import SaltInvocationError, CommandExecutionError from salt.modules.boto_vpc import _maybe_set_name_tag, _maybe_set_tags from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
import salt.ext.six as six try: import boto import boto3 from boto.exception import BotoServerError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto_version = '2.8.0' required_moto_version = '0.3.7'
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch ensure_in_syspath('../../')
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call ensure_in_syspath('../../')
import salt.modules.rvm as rvm
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
from salt.modules import sqlite3 import salt
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON
from salt.modules import extfs
from __future__ import absolute_import
from salttesting import TestCase from salttesting.mock import ( MagicMock, mock_open, patch, ) from salt.modules import hosts from salt.ext.six.moves import StringIO
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import dpkg
dpkg.__grains__ = {} dpkg.__salt__ = {} dpkg.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, call, Mock
from salt.modules import ps import salt.ext.six as six
from __future__ import absolute_import
from salt.modules import mac_sysctl from salt.exceptions import CommandExecutionError
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, mock_open, patch, call, NO_MOCK, NO_MOCK_REASON )
mac_sysctl.__salt__ = {}
from __future__ import absolute_import from __future__ import unicode_literals import sys import time from salttesting import TestCase, skipIf from salttesting.helpers import destructiveTest from salt.modules import reg as win_mod_reg try:
TIMEINT = int(time.time())
UNICODETEST_WITH_SIGNS = 'Testing Unicode \N{COPYRIGHT SIGN},\N{TRADE MARK SIGN},\N{REGISTERED SIGN} '+TIMESTR UNICODETEST_WITHOUT_SIGNS = 'Testing Unicode'+TIMESTR UNICODE_TEST_KEY = 'UnicodeKey \N{TRADE MARK SIGN} '+TIME_INT_UNICODE UNICODE_TEST_KEY_DEL = 'Delete Me \N{TRADE MARK SIGN} '+TIME_INT_UNICODE
test = isinstance(test_list, tuple) and (not test_list[0]) self.assertTrue(test)
test = isinstance(test_list, tuple) and (not test_list[0]) self.assertTrue(test)
test_success = win_mod_reg.delete_value( 'HKEY_LOCAL_MACHINE', subkey, vname ) self.assertTrue(test_success)
test_success = win_mod_reg.delete_key_recursive('HKEY_CURRENT_USER', subkey) self.assertTrue(test_success)
test_success = win_mod_reg.delete_key_recursive('HKEY_LOCAL_MACHINE', subkey) self.assertTrue(test_success)
from __future__ import absolute_import import copy
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.exceptions import SaltException from salt.modules import grains as grainsmod from salt.utils import dictupdate
from salt.utils.odict import OrderedDict
res = grainsmod.filter_by(dict1, grain='xxx') self.assertIs(res, None)
res = grainsmod.filter_by(dict1) self.assertIs(res, None)
res = grainsmod.filter_by(dict1, grain='xxx', merge=mdict1, default='Z') self.assertEqual(res, mdict1)
res = grainsmod.filter_by(dict1, grain='xxx', default='Z') self.assertIs(res, None)
res = grainsmod.filter_by(dict2, grain='xxx', default='xxx', base='default') self.assertEqual(res, dict2['default'])
res = grainsmod.filter_by(dict2, grain='xxx', base='default') self.assertEqual(res, dict2['default'])
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import devmap import os.path
devmap.__grains__ = {} devmap.__salt__ = {} devmap.__context__ = {} devmap.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch
from salt.exceptions import CommandExecutionError from salt.modules import mac_xattr as xattr import salt.utils.mac_utils
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import glusterfs from salt.exceptions import SaltInvocationError
glusterfs.__salt__ = {}
self.assertTrue(glusterfs.create_volume('newvolume', 'host1:/brick')) self.assertFalse(mock_start_volume.called)
self.assertTrue(glusterfs.create_volume('newvolume', 'host1:/brick', start=True)) self.assertTrue(mock_start_volume.called)
self.assertFalse(glusterfs.create_volume('newvolume', 'host1:/brick', start=True))
self.assertFalse(glusterfs.delete_volume('Newvolume3'))
self.assertFalse(glusterfs.delete_volume('Newvolume1', False)) self.assertFalse(mock_run.called) self.assertFalse(mock_stop_volume.called)
self.assertTrue(glusterfs.delete_volume('Newvolume1')) self.assertTrue(mock_run.called) self.assertTrue(mock_stop_volume.called)
mock_run.return_value = xml_command_fail self.assertFalse(glusterfs.add_volume_bricks('Newvolume1', ['new:/path']))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import nagios import os
nagios.__salt__ = {}
from __future__ import absolute_import import os import tempfile
from salttesting.unit import TestCase from salttesting.helpers import ensure_in_syspath
import salt.utils from salt.modules import ini_manage as ini
option1=main1
option2=main2
test1=value 1
test3 = value 3B
option1 = main1
option2 = main2
test1 = value 1
test3 = new value 3B
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import hipchat
hipchat.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import swift
from __future__ import absolute_import import os.path
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import linux_lvm from salt.exceptions import CommandExecutionError
linux_lvm.__salt__ = {}
from __future__ import absolute_import, print_function
from salt.exceptions import CommandExecutionError from salt.modules import uptime
from __future__ import absolute_import import logging
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, mock_open, call, NO_MOCK, NO_MOCK_REASON )
from salt.modules import dnsutil
from __future__ import absolute_import import yaml
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import pkg_resource import salt.ext.six as six
pkg_resource.__grains__ = {} pkg_resource.__salt__ = {}
from __future__ import absolute_import
from salt.modules import status from salt.exceptions import CommandExecutionError
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, )
status.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import deb_apache deb_apache.__grains__ = {} deb_apache.__salt__ = {} deb_apache.__context__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import debian_ip
import jinja2.exceptions
debian_ip.__grains__ = {} debian_ip.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( create_autospec, MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import etcd_mod from salt.utils import etcd_util
etcd_mod.__opts__ = {} etcd_mod.__utils__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import keystone
keystone.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import chef
chef.__grains__ = {} chef.__salt__ = {} chef.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf from tests.unit import ModuleTestCase, hasDependency from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath from salt.modules import servicenow
from __future__ import absolute_import import time
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import ldapmod
ldapmod.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import http import salt.utils.http
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import logrotate
logrotate.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import at
import salt.utils
at.__grains__ = {} at.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import lvs
lvs.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import smtp
from __future__ import absolute_import import uuid
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import iptables
iptables.__grains__ = {} iptables.__salt__ = {} iptables.__context__ = {} iptables.__opts__ = {}
self.assertEqual(iptables.build_rule(**{'if': '!eth0'}), '! -i eth0')
self.assertEqual(iptables.build_rule(**{'if': 'not eth0'}), '! -i eth0')
self.assertEqual(iptables.build_rule(dports=['!80', 443], proto='tcp'), '-p tcp -m multiport ! --dports 80,443')
self.assertEqual(iptables.build_rule(jump='REDIRECT', **{'to-port': 8080}), '--jump REDIRECT --to-port 8080')
self.assertEqual(iptables.build_rule(jump='LOG', **{'log-prefix': 'long prefix'}), '--jump LOG --log-prefix "long prefix"')
self.assertEqual(iptables.build_rule(jump='LOG', **{'log-prefix': 'spam: '}), '--jump LOG --log-prefix "spam: "')
self.assertEqual(iptables.build_rule(jump='CLUSTERIP', **{'new': ''}), '--jump CLUSTERIP --new ')
self.assertEqual(iptables.build_rule(**{'match-set': 'src flag1,flag2'}), '-m set --match-set src flag1,flag2')
self.assertEqual(iptables.build_rule(**{'match-set': '!src flag'}), '-m set ! --match-set src flag')
#self.assertEqual(iptables.build_rule(jump='CONNSECMARK',
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import dnsmasq
import os
dnsmasq.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch
from salt.modules import config
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.modules import puppet from salt.exceptions import CommandExecutionError
puppet.__salt__ = {}
from __future__ import absolute_import import grp import pwd
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON
from salt.modules import mac_user from salt.exceptions import SaltInvocationError, CommandExecutionError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, mock_open, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import nftables import salt.utils from salt.exceptions import CommandExecutionError
nftables.__grains__ = {} nftables.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import genesis
genesis.__grains__ = {} genesis.__salt__ = {} genesis.__context__ = {} genesis.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON
from salt.modules import mac_power from salt.exceptions import SaltInvocationError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import rpm
rpm.__salt__ = {}
from __future__ import absolute_import
from salt.modules import win_license as license
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import nova
nova.__grains__ = {} nova.__salt__ = {} nova.__context__ = {} nova.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_shadow import salt.utils
win_shadow.__salt__ = {}
from __future__ import absolute_import import socket import os.path
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.ext.six as six import salt.utils from salt.modules import network from salt.exceptions import CommandExecutionError if six.PY2: import salt.ext.ipaddress
network.__grains__ = {} network.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import guestfs
guestfs.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import oracle import os
oracle.__salt__ = {} oracle.cx_Oracle = object()
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_iot
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_lambda from salt.exceptions import SaltInvocationError
from tempfile import NamedTemporaryFile import logging import os
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ret import salt.loader
ret.__opts__ = {} ret.__salt__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader from salt.modules import boto_cognitoidentity
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock ensure_in_syspath('../../')
from salt.modules import linux_acl from salt.exceptions import CommandExecutionError
def test_version(self): pass
from __future__ import absolute_import
from salt.modules import mac_package as macpackage
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, call )
from __future__ import absolute_import
from __future__ import absolute_import import os.path import glob
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import qemu_nbd
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import memcached from salt.exceptions import CommandExecutionError, SaltInvocationError from salt.ext.six import integer_types
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import sysmod
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salt.modules import cpan cpan.__grains__ = {} cpan.__salt__ = {} cpan.__context__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.modules import artifactory
import salt.ext.six as six
from __future__ import absolute_import import sys import types
import salt.ext.six as six
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
wmi = types.ModuleType('wmi') sys.modules['wmi'] = wmi
import salt.modules.win_status as status
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import bluez from salt.exceptions import CommandExecutionError import salt.utils.validate.net
bluez.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, call )
from salt.modules import win_firewall
win_firewall.__salt__ = {}
IS_WIN = win_firewall.__virtual__()
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, Mock, NO_MOCK, NO_MOCK_REASON, patch )
from salt.modules import dockerng as dockerng_mod from salt.exceptions import CommandExecutionError, SaltInvocationError
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import rabbitmq from salt.exceptions import CommandExecutionError
rabbitmq.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import mod_random import salt.utils.pycrypto from salt.exceptions import SaltInvocationError
mod_random.__grains__ = {} mod_random.__salt__ = {} mod_random.__context__ = {} mod_random.__opts__ = {}
from __future__ import absolute_import
from salt.modules import win_dism as dism
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call
from salt.modules import cron from salt.ext.six.moves import builtins, StringIO
set_crontab(
set_crontab(
set_crontab(
def test__get_cron_cmdstr(self): self.assertEqual('crontab /tmp', cron._get_cron_cmdstr(STUB_PATH))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, patch, NO_MOCK, NO_MOCK_REASON
from salt.modules import groupadd
import grp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import rh_ip import jinja2.exceptions import os
rh_ip.__grains__ = {} rh_ip.__salt__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.modules.blockdev as blockdev import salt.utils
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, Mock, patch
from __future__ import absolute_import
import contextlib import textwrap import json try: import dns.query import dns.tsigkeyring HAS_DNS = True except ImportError: HAS_DNS = False
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import ddns
ddns.__grains__ = {} ddns.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import redismod from datetime import datetime
redismod.__grains__ = {} redismod.__salt__ = {} redismod.__context__ = {} redismod.__opts__ = {}
from __future__ import absolute_import, print_function import os
from salt.modules import deb_postgres
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import rsync from salt.exceptions import CommandExecutionError, SaltInvocationError
rsync.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call
from salt.modules import mysql
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import system
system.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import powerpath
powerpath.__salt__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.utils.odict import OrderedDict from salt.modules import jboss7
__salt__
from salt.ext.six.moves import builtins as __builtin__ __builtin__.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import publish import salt.crypt import salt.transport from salt.exceptions import SaltReqTimeoutError
publish.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import win_ip from salt.exceptions import CommandExecutionError, SaltInvocationError
win_ip.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, mock_open, NO_MOCK, NO_MOCK_REASON )
from salt.modules import poudriere
poudriere.__salt__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.modules import scsi import os import salt.utils import copy
scsi.__salt__ = {} scsi.__context__ = {}
cmd_mock = MagicMock(return_value=lsscsi) with patch.dict(scsi.__salt__, {'cmd.run_all': cmd_mock}): self.assertDictEqual(scsi.ls_(get_size=False), result)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import rbenv import os
rbenv.__grains__ = {} rbenv.__salt__ = {}
from __future__ import absolute_import import json
from salt.modules import kapacitor
from salttesting import TestCase from salttesting.mock import Mock, patch
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) from salttesting.helpers import ensure_in_syspath
from salt.modules import moosefs
moosefs.__salt__ = {}
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
event.__grains__ = {} event.__salt__ = {} event.__context__ = {} event.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import pecl
from __future__ import absolute_import import random import string from copy import deepcopy
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
from salt.utils.odict import OrderedDict from salt.modules import boto_secgroup
group_vpc = conn.create_security_group(name=group_name, description=group_description, vpc_id=vpc_id) retrieved_group_id = boto_secgroup.get_group_id(group_name, **conn_parameters) self.assertEqual(group_classic.id, retrieved_group_id)
group_vpc = conn.create_security_group(name=group_name, description=group_description, vpc_id=vpc_id) retrieved_group_id = boto_secgroup.get_group_id(group_name, group_vpc, **conn_parameters) self.assertEqual(group_vpc.id, retrieved_group_id)
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import hg
hg.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch )
from salt.modules import win_timezone
win_timezone.__salt__ = {}
IS_WIN = win_timezone.__virtual__()
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import netscaler
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import npm from salt.exceptions import CommandExecutionError import json
npm.__salt__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import serverdensity_device from salt.exceptions import CommandExecutionError
from __future__ import absolute_import import textwrap
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import rh_service
rh_service.__salt__ = {}
from __future__ import absolute_import import os import tempfile import logging import shutil
import salt.ext.six as six from salt.ext.six.moves.urllib.error import URLError from salt.ext.six.moves.urllib.request import urlopen
from salttesting import TestCase, skipIf from salttesting.helpers import ( ensure_in_syspath, requires_network, skip_if_binaries_missing ) ensure_in_syspath('../..')
self.assertTrue( ('Got ' in comment and 'Generated script' in comment) or ('setuptools>=0.7' in comment) )
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.mock import ( mock_open, MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.exceptions import CommandExecutionError
mount.__grains__ = {} mount.__salt__ = {} mount.__context__ = {}
from __future__ import absolute_import
from salt.modules import mac_defaults as macdefaults
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON )
ensure_in_syspath('../../') from salt.modules import ssh from salt.exceptions import CommandExecutionError
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, patch ensure_in_syspath('../../')
from salt.modules import nginx
from __future__ import absolute_import import os import tempfile import textwrap
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import MagicMock, patch
import salt.utils from salt.modules import file as filemod from salt.modules import config as configmod from salt.modules import cmdmod from salt.exceptions import CommandExecutionError
filemod.replace(self.tfile.name, r'Etiam', 'Salticus', flags=['MULTILINE', 'ignorecase'])
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import environ import os
environ.__grains__ = {} environ.__salt__ = {} environ.__context__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.modules import raet_publish import salt.transport from salt.exceptions import SaltReqTimeoutError
raet_publish.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.modules import composer from salt.exceptions import CommandExecutionError, CommandNotFoundError, SaltInvocationError
composer.__grains__ = {} composer.__salt__ = {} composer.__context__ = {} composer.__opts__ = {}
mock = MagicMock(return_value=False) with patch.object(composer, '_valid_composer', mock): self.assertRaises(CommandNotFoundError, composer.install, 'd')
mock = MagicMock(return_value=True) with patch.object(composer, '_valid_composer', mock): self.assertRaises(SaltInvocationError, composer.install, None)
mock = MagicMock(return_value=False) with patch.object(composer, '_valid_composer', mock): self.assertRaises(CommandNotFoundError, composer.update, 'd')
from __future__ import absolute_import import os import platform
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
import salt.utils from salt.grains import core
core.__salt__ = {}
from __future__ import absolute_import from copy import deepcopy
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from salt.cloud.clouds import vmware from salt.exceptions import SaltCloudSystemExit
HAS_LIBS = True try:
from __future__ import absolute_import
from salttesting import TestCase
from salt.cloud.clouds import saltify
saltify.__opts__ = {} saltify.__opts__['providers'] = {}
from __future__ import absolute_import import libcloud.security import platform import os
from salt.cloud.clouds import dimensiondata from salt.exceptions import SaltCloudSystemExit
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import import libcloud.security import platform import os
from salt.cloud.clouds import gce from salt.exceptions import SaltCloudSystemExit
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import NO_MOCK, NO_MOCK_REASON from salttesting.helpers import ensure_in_syspath
from salt.cloud.clouds import linode
self.assertFalse(linode._validate_name('-foo'))
self.assertFalse(linode._validate_name('_foo'))
self.assertFalse(linode._validate_name('foo-'))
self.assertFalse(linode._validate_name('foo_'))
self.assertFalse(linode._validate_name(''))
self.assertFalse(linode._validate_name('ab'))
self.assertTrue(linode._validate_name('abc'))
self.assertEqual(len(long_name), 48) self.assertTrue(linode._validate_name(long_name))
long_name += '1' self.assertEqual(len(long_name), 49) self.assertFalse(linode._validate_name(long_name))
self.assertFalse(linode._validate_name('foo;bar'))
self.assertFalse(linode._validate_name('fooàààààbar'))
self.assertFalse(linode._validate_name('foo bar'))
self.assertTrue(linode._validate_name('foo123bar'))
self.assertTrue(linode._validate_name('foo-bar'))
self.assertTrue(linode._validate_name('foo_bar'))
self.assertTrue(linode._validate_name('1foo')) self.assertTrue(linode._validate_name('foo0'))
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import MagicMock, NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
from salt.cloud.clouds import opennebula from salt.exceptions import SaltCloudSystemExit, SaltCloudNotFound
opennebula.__active_provider_name__ = '' opennebula.__opts__ = {} VM_NAME = 'my-vm'
from __future__ import absolute_import
from salttesting import TestCase
import salt.cloud.libcloudfuncs as libcloud
from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, MagicMock, NO_MOCK, NO_MOCK_REASON
import integration from salt.cli import daemons
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import patch, call, NO_MOCK, NO_MOCK_REASON, MagicMock
import salt.master import integration from salt import auth
self.clear._send_pub = lambda payload: True
self.clear.mminion.returners = {'.prep_jid': lambda x: 1}
self.clear.publish(self.valid_clear_load) self.assertEqual(fire_event_mock.call_args[0][0]['fun'], 'test.ping')
sys_doc_load = self.valid_clear_load sys_doc_load['fun'] = 'sys.doc' self.clear.publish(sys_doc_load)
self.assertEqual(fire_event_mock.call_args[0][0]['fun'], 'test.echo')
self.valid_clear_load['fun'] = 'sys.doc' self.assertNotEqual(fire_event_mock.call_args[0][0]['fun'], 'sys.doc')
pass
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, patch )
from salt.runners import cache import salt.utils
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import rabbitmq_cluster
rabbitmq_cluster.__salt__ = {} rabbitmq_cluster.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rdp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import sysctl
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import integration from salt.states import pip_state
try: import pip HAS_PIP = True except ImportError: HAS_PIP = False
try: original_pip_version = pip.__version__ pip.__version__ = MagicMock( side_effect=AttributeError( 'Faked missing __version__ attribute' ) ) except AttributeError: pass
if hasattr(pip, '__version__'): pip.__version__ = original_pip_version
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) import os
from salt.states import virtualenv_mod
virtualenv_mod.__salt__ = {} virtualenv_mod.__opts__ = {} virtualenv_mod.__env__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import alternatives
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import keyboard
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_update
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import process
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_network
win_network.__salt__ = {} win_network.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rabbitmq_plugin
from __future__ import absolute_import import contextlib
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import service
service.__salt__ = {} service.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_elasticache
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_sqs
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_servermanager
win_servermanager.__salt__ = {} win_servermanager.__opts__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import yaml
from unit.modules.boto_apigateway_test import BotoApiGatewayTestCaseMixin
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
self.conn.create_stage.side_effect = ClientError(error_content, 'create_stage') self.conn.create_deployment.side_effect = ClientError(error_content, 'create_deployment')
self.conn.put_method.return_value = method_ret self.conn.put_integration.return_value = method_integration_ret self.conn.put_method_response.return_value = method_response_200_ret self.conn.put_intgration_response.return_value = method_integration_response_200_ret
self.conn.get_rest_apis.return_value = no_apis_ret self.conn.create_rest_api.side_effect = ClientError(error_content, 'create_rest_api')
self.conn.put_method.side_effect = ClientError(error_content, 'put_method')
self.conn.put_method.return_value = method_ret self.conn.put_integration.side_effect = ClientError(error_content, 'put_integration')
self.conn.put_method.return_value = method_ret self.conn.put_integration.return_value = method_integration_ret self.conn.put_method_response.side_effect = ClientError(error_content, 'put_method_response')
from __future__ import absolute_import
from salt.states import mac_assistive as assistive
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import from contextlib import contextmanager
from salttesting import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import schedule
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_sns
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import bower from salt.exceptions import CommandExecutionError
bower.__salt__ = {} bower.__opts__ = {'test': False}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import zk_concurrency
zk_concurrency.__salt__ = {} zk_concurrency.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( mock_open, NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import augeas
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import htpasswd
htpasswd.__salt__ = {} htpasswd.__opts__ = {'test': False}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import xmpp
xmpp.__salt__ = {} xmpp.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_system
win_system.__salt__ = {} win_system.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON)
from salt.states import gnomedesktop
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.states.gem as gem gem.__salt__ = {} gem.__opts__ = {'test': False}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, Mock, MagicMock, patch
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import group
group.__salt__ = {} group.__opts__ = {}
from __future__ import absolute_import from inspect import ArgSpec
from salt.states import module
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, call, patch)
from salt.states import ipset
expected_calls = expected_calls[:1]
from __future__ import absolute_import
from salt.states import win_certutil as certutil
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import apache_module
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, mock_open)
from salt.states import apache import salt.utils
from __future__ import absolute_import
from salt.states import host
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import splunk_search
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import apache_site
from __future__ import absolute_import
from salt.states import mac_keychain as keychain
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, call )
from __future__ import absolute_import import copy
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_elb
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import ssh_known_hosts
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( patch, MagicMock, NO_MOCK, NO_MOCK_REASON )
from salt.states import test
test.__salt__ = {} test.__opts__ = {} test.__low__ = {'__reqs__': {'watch': ''}}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import debconfmod
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import incron
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import kmod
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import vbox_guest
vbox_guest.__salt__ = {} vbox_guest.__opts__ = {}
from __future__ import absolute_import
from salt.exceptions import CommandExecutionError from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import timezone
timezone.__salt__ = {} timezone.__opts__ = {}
from __future__ import absolute_import import os
from salt.states import svn
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import pagerduty
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_user
from __future__ import absolute_import import os import tempfile
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import archive as archive
archive.__salt__ = {} archive.__opts__ = {"cachedir": "/tmp", "test": False} archive.__env__ = 'test'
db = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import pkgng
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mysql_user import salt
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import drac
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import modjk_worker
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import influxdb_user
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import openstack_config
from __future__ import absolute_import
from salt.states import win_powercfg as powercfg
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_database
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rabbitmq_vhost
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_path
win_path.__salt__ = {} win_path.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import supervisord
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_elasticsearch_domain_test import BotoElasticsearchDomainTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ipmi
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import pyenv
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import sysrc
sysrc.__salt__ = {} sysrc.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import selinux
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_cloudtrail_test import BotoCloudTrailTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
import salt.config from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON ) import os
from salt.states import winrepo
winrepo.__salt__ = {} winrepo.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import lxc import salt.utils
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON)
from salt.states import modjk import salt.ext.six as six
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_asg
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rabbitmq_policy
from __future__ import absolute_import import yaml import re import tempfile import os
class SyslogNGTestCase(TestCase): def test_generate_source_config(self): self._config_generator_template(SOURCE_1_CONFIG, SOURCE_1_EXPECTED)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ports import os
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import slack
from __future__ import absolute_import
from salt.states import proxy as proxy
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch, call )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_dns_client
win_dns_client.__salt__ = {} win_dns_client.__opts__ = {}
from __future__ import absolute_import
from salttesting.case import TestCase from salttesting.helpers import ensure_in_syspath
from salt.states import boto_secgroup from salt.utils.odict import OrderedDict
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import grafana
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import mdadm
mdadm.__salt__ = {} mdadm.__opts__ = {}
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_s3_bucket_test import BotoS3BucketTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mongodb_database
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader import salt.utils.boto
from unit.modules.boto_vpc_test import BotoVpcTestCaseMixin
try: import boto import boto3 from boto.exception import BotoServerError
conn_parameters['key'] = ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(50))
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import disk
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch ensure_in_syspath('../../')
import salt.modules.rvm import salt.states.rvm as rvm
import salt.ext.six as six
self.assertEqual(mock.call_count, 0)
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import pyrax_queues
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_iam_role
import os import yaml
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from __future__ import absolute_import
from salt.states import mac_xattr as xattr
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import import socket
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import glusterfs import salt.utils.cloud import salt.modules.glusterfs as mod_glusterfs
glusterfs.__salt__ = {'glusterfs.peer': mod_glusterfs.peer} glusterfs.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_lc
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_cloudwatch_alarm
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( MagicMock, NO_MOCK, NO_MOCK_REASON, patch)
from salt.states import ini_manage
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import hipchat
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import apache_conf
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import status
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import user
user.__salt__ = {} user.__opts__ = {} user.__grains__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import keystone
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_extension
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_route53
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import chef
from __future__ import absolute_import
from salt.states import http
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import at
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_group
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import smtp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import ssh_auth
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import iptables
iptables.__salt__ = {} iptables.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import lvs_server
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mongodb_user
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import nftables
nftables.__salt__ = {} nftables.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, Mock, MagicMock, patch )
from salt.states import grafana_datasource
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import lvm
from __future__ import absolute_import
from salt.states import win_license as license
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import makeconf
from __future__ import absolute_import import sys
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import network
network.__salt__ = {} network.__grains__ = {} network.__opts__ = {}
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import locale
locale.__salt__ = {} locale.__opts__ = {}
from __future__ import absolute_import import os.path
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import cmd
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_iot_test import BotoIoTTestCaseMixin
try: import boto import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_lambda_test import BotoLambdaTestCaseMixin, TempZipFile
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch from salttesting.helpers import ensure_in_syspath
import salt.config import salt.loader
import logging
from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from unit.modules.boto_cognitoidentity_test import BotoCognitoIdentityTestCaseMixin
try: import boto3 from botocore.exceptions import ClientError HAS_BOTO = True except ImportError: HAS_BOTO = False
required_boto3_version = '1.2.1'
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import linux_acl
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import influxdb_database
from __future__ import absolute_import
from salt.states import mac_package as macpackage
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import memcached
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import artifactory
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import saltmod
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import win_firewall import salt.utils
win_firewall.__salt__ = {} win_firewall.__opts__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ntp
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salt.exceptions import SaltInvocationError from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, Mock, NO_MOCK, NO_MOCK_REASON, patch )
from salt.exceptions import CommandExecutionError from salt.modules import dockerng as dockerng_mod from salt.states import dockerng as dockerng_state
self.assertNotEqual(v['Name'], name)
self.assertEqual(1, len(removed)) volumes.remove(removed[0]) return removed[0]
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_dynamodb
from __future__ import absolute_import
from salt.states import win_dism as dism
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
set_crontab(
set_crontab(
from __future__ import absolute_import import os
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import blockdev import salt.utils
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import ddns
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import boto_ec2
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import redismod
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import alias
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import aws_sqs
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import layman
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import powerpath
from __future__ import absolute_import import os
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, mock_open, patch)
from salt.states import virt import salt.utils
from __future__ import absolute_import
from salttesting.unit import skipIf, TestCase from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from salt.states import jboss7 from salt.exceptions import CommandExecutionError
import salt.ext.six as six
__salt__
__builtin__.__salt__ = {}
datasource_properties = {'connection-url': 'jdbc:/old-connection-url'} ds_status = {'created': False}
result = jboss7.datasource_exists(name='appDS', jboss_config={}, datasource_properties=datasource_properties, profile=None)
binding_status = {'created': False}
result = jboss7.bindings_exist(name='bindings', jboss_config={}, bindings={'env': 'DEV'}, profile=None)
binding_status = {'updated': False}
result = jboss7.bindings_exist(name='bindings', jboss_config={}, bindings={'env': 'DEV2'}, profile=None)
__salt__['jboss7.read_simple_binding'].return_value = {'success': True, 'result': {'value': 'DEV2'}}
result = jboss7.bindings_exist(name='bindings', jboss_config={}, bindings={'env': 'DEV2'})
try:
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_schema
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch, NO_MOCK, NO_MOCK_REASON )
from salt.states import tomcat
tomcat.__salt__ = {} tomcat.__opts__ = {} tomcat.__env__ = {}
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import rbenv
from __future__ import absolute_import
from salttesting import TestCase from salttesting.mock import Mock, patch, mock_open
from salt.states import kapacitor
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mysql_grants
from __future__ import absolute_import
from salt.states import event
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import quota
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import pecl
from __future__ import absolute_import import os
from salt.states import hg
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import npm
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import serverdensity_device
from __future__ import absolute_import import os
from salttesting import skipIf from salttesting.helpers import ( ensure_in_syspath, requires_network, )
import salt.utils from unit.modules.zcbuildout_test import Base, KNOWN_VIRTUALENV_BINARY_NAMES from salt.modules import zcbuildout as modbuildout from salt.states import zcbuildout as buildout from salt.modules import cmdmod as cmd
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mount import os
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import postgres_cluster
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import cloud import salt.utils.cloud
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import eselect
from __future__ import absolute_import
from salt.states import mac_defaults as macdefaults
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch )
from __future__ import absolute_import from datetime import datetime from dateutil.relativedelta import relativedelta import json import pprint import tempfile
from salttesting import skipIf, TestCase from salttesting.helpers import destructiveTest, ensure_in_syspath from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, call, mock_open, patch)
import yaml
pillar_value = 'i am the pillar value\n'
pillar_mock = MagicMock(return_value=pillar_value) filestate.__salt__['pillar.get'] = pillar_mock
self.assertEqual(None, ret)
ts = datetime(starting.year, starting.month, starting.day - starting.weekday())
fake_no_match_file_list = generate_fake_files(format='no_match_%Y%m%dT%H%M%S.tar.bz2', every=relativedelta(days=1))
if len(new_retains) < fake_retain[retainable]: new_retains.add(fake_file_list[0]) retained_files |= new_retains
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch )
from salt.states import reg
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import mysql_query import os
from __future__ import absolute_import import os
from salttesting import TestCase from salttesting.helpers import ensure_in_syspath from salttesting.mock import ( MagicMock, patch ) ensure_in_syspath('../../')
import salt.states.environ as envstate import salt.modules.environ as envmodule
ret = envstate.setenv('test', 'other') self.assertEqual(ret['changes'], {})
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import lvs_service
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import aptpkg
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import portage_config
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.mock import ( NO_MOCK, NO_MOCK_REASON, MagicMock, patch)
from salt.states import composer
from __future__ import absolute_import import os
from salttesting.unit import skipIf from salttesting.case import TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../..')
try: import tornado.testing import tornado.concurrent from tornado.testing import AsyncTestCase HAS_TORNADO = True except ImportError: HAS_TORNADO = False
class AsyncTestCase(object): pass
futures = [] for x in range(0, 3): future = tornado.concurrent.Future() future.add_done_callback(self.stop) futures.append(future)
any_ = saltnado.Any(futures) self.assertIs(any_.done(), False)
futures[0].set_result('foo') self.wait()
self.assertEqual(any_.result(), futures[0])
from __future__ import absolute_import import json import yaml import os
from salttesting.unit import skipIf from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../..')
try: from salt.netapi.rest_tornado import saltnado from salt.netapi.rest_tornado import saltnado_websockets HAS_TORNADO = True except ImportError: HAS_TORNADO = False import salt.auth
class AsyncHTTPTestCase(object): pass
response = self.fetch('/', headers={'Accept': self.content_type_map['xml']}) self.assertEqual(response.code, 406)
response = self.fetch('/', method='POST', body=json.dumps(valid_lowstate), headers={'Content-Type': self.content_type_map['json']})
request_lowstate = { "client": "local", "tgt": "*", "fun": "test.fib", "arg": ["10"] }
request_lowstate = [{ "client": "local", "tgt": "*", "fun": "test.fib", "arg": "10" }]
request_lowstate = { "client": "local", "tgt": "*", "fun": "test.fib", "arg": "10" }
response = self.fetch('/', method='POST', body=json.dumps(request_lowstate), headers={'Content-Type': self.content_type_map['json']})
response = self.fetch('/login', method='POST', body=urlencode(self.auth_creds), headers={'Content-Type': self.content_type_map['form']})
response = self.fetch('/login', method='POST', body=json.dumps(self.auth_creds_dict), headers={'Content-Type': self.content_type_map['json']})
response = self.fetch('/login', method='POST', body=yaml.dump(self.auth_creds_dict), headers={'Content-Type': self.content_type_map['yaml']})
from __future__ import absolute_import import json
import yaml
from __future__ import absolute_import import os
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, patch
from salt import minion from salt.utils import event from salt.exceptions import SaltSystemExit import salt.syspaths
from __future__ import absolute_import
from salttesting import TestCase, skipIf from salttesting.helpers import ensure_in_syspath from salttesting.mock import NO_MOCK, NO_MOCK_REASON, MagicMock, patch
from salt.returners import smtp_return as smtp
from __future__ import absolute_import import os import shutil import tempfile
from salttesting import TestCase, skipIf from salttesting.helpers import destructiveTest, ensure_in_syspath from salttesting.mock import ( MagicMock, NO_MOCK, NO_MOCK_REASON, patch )
import salt.utils from salt.returners import local_cache
jid_dir, jid_file = self._make_tmp_jid_dirs(create_files=False)
self.assertEqual(jid_file, None)
with patch.dict(local_cache.__opts__, {'keep_jobs': 0.00000001}): local_cache.clean_old_jobs()
self.assertEqual([], os.listdir(TMP_JID_DIR))
jid_dir, jid_file = self._make_tmp_jid_dirs(create_files=False)
self.assertEqual(jid_file, None)
local_cache.clean_old_jobs()
jid_dir_name = jid_dir.rpartition('/')[2]
self.assertEqual([jid_dir_name], os.listdir(TMP_JID_DIR))
jid_dir, jid_file = self._make_tmp_jid_dirs()
jid_dir_name = jid_file.rpartition('/')[2] self.assertEqual(jid_dir_name, 'jid')
with patch('os.path.isfile', MagicMock(return_value=False)) as mock: local_cache.clean_old_jobs()
self.assertEqual([], os.listdir(TMP_JID_DIR))
jid_dir, jid_file = self._make_tmp_jid_dirs()
jid_dir_name = jid_file.rpartition('/')[2] self.assertEqual(jid_dir_name, 'jid')
with patch.dict(local_cache.__opts__, {'keep_jobs': 0.00000001}): local_cache.clean_old_jobs()
self.assertEqual([], os.listdir(TMP_JID_DIR))
if not os.path.exists(TMP_JID_DIR): os.makedirs(TMP_JID_DIR)
temp_dir = tempfile.mkdtemp(dir=TMP_JID_DIR)
from __future__ import absolute_import
from salttesting import TestCase, expectedFailure from salttesting.helpers import ensure_in_syspath
from __future__ import absolute_import
from salttesting import skipIf, TestCase from salttesting.helpers import ensure_in_syspath ensure_in_syspath('../../')
from textwrap import dedent
import jinja2
from salt.serializers import json, yamlex, yaml, msgpack, python, configparser from salt.serializers import SerializationError from salt.utils.odict import OrderedDict
assert isinstance(sls_data, dict) assert isinstance(yml_data, dict) assert sls_data == yml_data
assert isinstance(sls_data, OrderedDict) assert not isinstance(yml_data, OrderedDict)
assert isinstance(sls_data, dict) assert isinstance(yml_data, dict) assert sls_data == yml_data
assert isinstance(sls_data, OrderedDict) assert not isinstance(yml_data, OrderedDict)
obj = OrderedDict([ ('foo', 1), ('bar', 2), ('baz', {'qux': True}) ])
yml_obj = obj.copy()
final_obj = OrderedDict(yaml.deserialize(yml_src)) assert obj != final_obj
assert sls_obj.__str__() == '{foo: bar, baz: qux}' assert sls_obj.__repr__() == '{foo: bar, baz: qux}'
assert sls_obj['foo'].__str__() == '"bar"' assert sls_obj['foo'].__repr__() == '"bar"'
serialized = configparser.serialize(data).strip() assert serialized == "[foo]\nbar = baz", serialized
from __future__ import absolute_import, print_function import os import tempfile import time
from salttesting.parser import PNUM, print_header from salttesting.parser.cover import SaltCoverageTestingParser
os.environ['EXPENSIVE_TESTS'] = 'True'
if not self.options.name and not \ self._check_enabled_suites(include_unit=True, include_cloud_provider=True): self._enable_suites(include_unit=True)
TestDaemon.transplant_configs(transport=self.options.transport)
prev_soft, prev_hard = resource.getrlimit(resource.RLIMIT_NOFILE)
min_soft = MAX_OPEN_FILES[limits]['soft_limit'] min_hard = MAX_OPEN_FILES[limits]['hard_limit']
set_limits = False if prev_soft < min_soft: soft = min_soft set_limits = True else: soft = prev_soft
return [True]
if not self._check_enabled_suites(include_cloud_provider=True) and not self.options.name: return status
return [True]
self.set_filehandle_limits('unit')
return status
continue
from __future__ import absolute_import, print_function import os import pwd import time import signal import optparse import subprocess import random import tempfile import shutil import sys
import salt
import yaml import salt.ext.six as six
if opts['root_dir']: tmpdir = os.path.join(opts['root_dir'], 'tmp') else: tmpdir = opts['root_dir']
sys.stdout.write('Generating master config...') self.mkconf() print('done')
if __name__ == '__main__': swarm = Swarm(parse()) try: swarm.start() finally: swarm.shutdown()
from __future__ import absolute_import, print_function import glob import os import re import sys import json import time import shutil import optparse import subprocess import random
import yaml try: import requests HAS_REQUESTS = True except ImportError: HAS_REQUESTS = False
if HAS_REQUESTS is False: parser.error( 'The python \'requests\' library needs to be installed' )
cloud_downtime = random.randint(0, opts.splay) print('Sleeping random period before calling salt-cloud: {0}'.format(cloud_downtime)) time.sleep(cloud_downtime)
#sys.exit(retcode)
print('Cloud configuration files provisioned via pillar.')
retcode = 1
retcode = 1
retcode = 1 if outstr: raise
download_packages(opts)
download_unittest_reports(opts) if opts.test_without_coverage is False: download_coverage_report(opts)
from __future__ import absolute_import from __future__ import print_function import sys import getopt import re import email.utils import datetime
from __future__ import absolute_import, print_function import os import sys import pprint
import msgpack
from __future__ import absolute_import, print_function import socket from struct import unpack import pcapy import sys
cap = pcapy.open_live(self.iface, 65536, 1, 0)
if protocol == 6:
ip_header = packet[eth_length:20+eth_length]
iph = unpack('!BBHHHBBH4s4s', ip_header)
if 'SYN' in flags and len(flags) == 1: return 10 elif 'FIN' in flags: return 12
if 'SYN' in flags and len(flags) == 1: return 100 elif 'FIN' in flags: return 120
args = vars(ArgParser().parse_args())
r_time = 0
ports = [4505, 4506]
stat['4505/est'], stat['4506/est'] = next(SaltNetstat().run())
for item in stat: stat[item] = 0 r_time = s_time
from __future__ import absolute_import, print_function import optparse import pprint import time import os
import salt.utils.event
import salt.ext.six as six
opts.log_file = os.path.join(opts.artifact_dir, 'salt-buildpackage.log')
_run_command(['yum', '-y', 'install'] + build_reqs)
try: sdist = _make_sdist(opts, python_bin=python_bin) except NameError: sdist = _make_sdist(opts)
cmd = ['rpmbuild', '-ba'] cmd.extend(define_opts) cmd.append(spec_path) stdout, stderr, rcode = _run_command(cmd)
from __future__ import absolute_import import atexit import os import readline import sys from code import InteractiveConsole
import salt.client import salt.config import salt.loader import salt.output import salt.pillar import salt.runner
import jinja2
__opts__ = salt.config.client_config( os.environ.get('SALT_MINION_CONFIG', '/etc/salt/minion'))
if 'grains' not in __opts__ or not __opts__['grains']: __opts__['grains'] = salt.loader.grains(__opts__)
if 'file_client' not in __opts__ or not __opts__['file_client']: __opts__['file_client'] = 'local'
if 'id' not in __opts__ or not __opts__['id']: __opts__['id'] = 'saltsh_mid'
__salt__ = salt.loader.minion_mods(__opts__) __grains__ = __opts__['grains']
readline.set_history_length(300)
import sys import time import datetime
import salt.config import salt.client.raet
runtime_reqs_sec = self.total_complete / elapsed_time.total_seconds() print('Recalibrating. Current reqs/sec: {0}'.format(runtime_reqs_sec)) return
indices = np.array(indices, dtype=int) indices.shape = (-1, 2) return indices
step = None if 0 in cost_matrix.shape else _step1
results = np.array(np.where(state.marked == 1)).T
if state.transposed: results = results[:, ::-1]
state.marked[state.marked == 2] = 0 return _step3
warnings.simplefilter('ignore', _NonBLASDotWarning)
raise TypeError('Expected sequence or array-like, got ' 'estimator %s' % x)
joined += ','
spmatrix = spmatrix.asformat(accept_sparse[0]) changed_format = True
spmatrix = spmatrix.astype(dtype)
spmatrix = spmatrix.copy()
dtype_numeric = dtype == "numeric"
dtype_orig = None
dtype = np.float64
dtype = None
dtype = dtype[0]
array = np.array(array, dtype=dtype, order=order, copy=copy)
raise _NotFittedError(msg % {'name': type(estimator).__name__})
update_wrapper(self, fn)
from ..preprocessing import LabelEncoder
weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
y_subsample = y[indices, k] classes_subsample = np.unique(y_subsample)
weight_k[in1d(y_full, list(classes_missing))] = 0.
version.append(x)
out *= .5 np.tanh(out, out) out += 1 out *= .5
if 'order' in signature(np.copy).parameters: def safe_copy(X): return np.copy(X, order='K') else: safe_copy = np.copy
def astype(array, dtype, copy=True): if not copy and array.dtype == dtype: return array return array.astype(dtype)
warnings.simplefilter('always') sp.csr_matrix([1.0, 2.0, 3.0]).max(axis=0)
value = np.zeros_like(X.data)
def argpartition(a, kth, axis=-1, kind='introselect', order=None): return np.argsort(a, axis=axis, order=order)
def frombuffer_empty(buf, dtype): if len(buf) == 0: return np.empty(0, dtype=dtype) else: return np.frombuffer(buf, dtype=dtype)
ar1 = np.asarray(ar1).ravel() ar2 = np.asarray(ar2).ravel()
if not assume_unique: ar1, rev_idx = np.unique(ar1, return_inverse=True) ar2 = np.unique(ar2)
from ._scipy_sparse_lsqr_backport import lsqr as sparse_lsqr
weight_cdf = sample_weight[sorted_idx].cumsum() percentile_idx = np.searchsorted( weight_cdf, (percentile / 100.) * weight_cdf[-1]) return array[sorted_idx[percentile_idx]]
import os import inspect import pkgutil import warnings import sys import re import platform import struct
from urllib2 import urlopen from urllib2 import HTTPError
from urllib.request import urlopen from urllib.error import HTTPError
try: WindowsError except NameError: WindowsError = None
from nose.tools import assert_equal from nose.tools import assert_not_equal from nose.tools import assert_true from nose.tools import assert_false from nose.tools import assert_raises from nose.tools import raises from nose import SkipTest from nose import with_setup
assert_raises_regexp = assert_raises_regex
if not len(w) > 0: raise AssertionError("No warning raised when calling %s" % func.__name__)
for index in [i for i, x in enumerate(found) if x]:
def assert_no_warnings(func, *args, **kw):
clean_warning_registry() with warnings.catch_warnings(record=True) as w: warnings.simplefilter('always')
w = [e for e in w if e.category is not np.VisibleDeprecationWarning]
clean_warning_registry() with warnings.catch_warnings(): warnings.simplefilter("ignore", self.category) return fn(*args, **kwargs)
if isinstance(exceptions, tuple): names = " or ".join(e.__name__ for e in exceptions) else: names = exceptions.__name__
for name in datasets: datasets[name] = datasets[name].T
from sklearn import datasets datasets.mldata.urlopen = mock_mldata_urlopen(mock_datasets)
from sklearn import datasets datasets.mldata.urlopen = urlopen
estimators = [c for c in estimators if not is_abstract(c[1])]
if not include_meta_estimators: estimators = [c for c in estimators if not c[0] in META_ESTIMATORS] if type_filter is not None: if not isinstance(type_filter, list): type_filter = [type_filter] else:
return sorted(set(estimators), key=itemgetter(0))
import matplotlib.pyplot as plt plt.figure()
shutil.rmtree(folder_path)
try: return X.iloc[indices] except ValueError: warnings.warn("Copying input dataframe for slicing.", DataConversionWarning) return X.copy().iloc[indices]
return X.take(indices, axis=0)
arnorm = alfa * beta if arnorm == 0: print(msg[0]) return x, istop, itn, r1norm, r2norm, anorm, acond, arnorm, xnorm, var
rhobar1 = sqrt(rhobar**2 + damp**2) cs1 = rhobar / rhobar1 sn1 = damp / rhobar1 psi = sn1 * phibar phibar = cs1 * phibar
cs, sn, rho = _sym_ortho(rhobar1, beta)
t1 = phi / rho t2 = -theta / rho dk = (1 / rho) * w
acond = anorm * sqrt(ddnorm) res1 = phibar**2 res2 = res2 + psi**2 rnorm = sqrt(res1 + res2) arnorm = alfa * abs(tau)
r1sq = rnorm**2 - dampsq * xxnorm r1norm = sqrt(abs(r1sq)) if r1sq < 0: r1norm = -r1norm r2norm = rnorm
if test3 <= ctol: istop = 3 if test2 <= atol: istop = 2 if test1 <= rtol: istop = 1
yield check_estimators_empty_data_messages
yield check_pipeline_consistency
yield check_estimators_nan_inf
yield check_estimators_overwrite_params
yield check_estimators_pickle
yield check_estimators_unfitted if 'class_weight' in Classifier().get_params().keys(): yield check_class_weight_classifiers
yield check_clustering yield check_estimators_partial_fit_n_features
estimator.set_params(alpha=.5)
estimator.set_params(n_components=1)
estimator.set_params(k=1)
X -= X.min() - .1 this_X = NotAnArray(X) this_y = NotAnArray(np.asarray(y)) _check_transformer(name, Transformer, this_X, this_y)
with warnings.catch_warnings(record=True): transformer = Transformer() set_random_state(transformer) set_testing_parameters(transformer)
transformer_clone = clone(transformer) X_pred = transformer_clone.fit_transform(X, y=y_)
assert_equal(X_pred.shape[0], n_samples)
if hasattr(X, 'T'): assert_raises(ValueError, transformer.transform, X.T)
msg = name + ' is non deterministic on 32bit Python' raise SkipTest(msg)
assert_raises(ValueError, e.fit, X_zero_samples, [])
X -= X.min()
y = multioutput_estimator_convert_y_2d(name, y)
with warnings.catch_warnings(record=True): estimator = Estimator()
pickled_estimator = pickle.dumps(estimator) unpickled_estimator = pickle.loads(pickled_estimator)
alg.fit(X) alg.fit(X.tolist())
if name is 'SpectralClustering': return set_random_state(alg) with warnings.catch_warnings(record=True): pred2 = alg.fit_predict(X) assert_array_equal(pred, pred2)
if hasattr(clusterer, "random_state"): clusterer.set_params(random_state=0)
assert_raises(ValueError, classifier.decision_function, X.T) assert_raises(ValueError, classifier.decision_function, X.T)
X -= X.min()
X, y = _boston_subset()
return
with warnings.catch_warnings(record=True): estimator = Estimator() set_testing_parameters(estimator) set_random_state(estimator) estimator.fit(X, y) y_pred = estimator.predict(X)
X -= X.min() - .1 y_names = np.array(["one", "two", "three"])[y]
with warnings.catch_warnings(record=True): regressor_1 = Regressor() regressor_2 = Regressor() set_testing_parameters(regressor_1) set_testing_parameters(regressor_2) set_random_state(regressor_1) set_random_state(regressor_2)
rng = np.random.RandomState(0) X = rng.normal(size=(10, 4)) y = multioutput_estimator_convert_y_2d(name, X[:, 0]) regressor = Regressor()
regressor.n_components = 1
continue
raise SkipTest
raise SkipTest
classifier.set_params(n_iter=1000)
classifier.set_params(class_weight='balanced') coef_balanced = classifier.fit(X, y).coef_.copy()
n_samples = len(y) n_classes = float(len(np.unique(y)))
X -= X.min() with warnings.catch_warnings(record=True): estimator = Estimator()
params = estimator.get_params() original_params = deepcopy(params)
estimator.fit(X, y)
new_params = estimator.get_params() for param_name, original_value in original_params.items(): new_value = new_params[param_name]
est.sparsify() assert_true(sparse.issparse(est.coef_)) pred = est.predict(X) assert_array_equal(pred, pred_orig)
est = pickle.loads(pickle.dumps(est)) assert_true(sparse.issparse(est.coef_)) pred = est.predict(X) assert_array_equal(pred, pred_orig)
with warnings.catch_warnings(record=True): estimator_1 = Estimator() estimator_2 = Estimator() set_testing_parameters(estimator_1) set_testing_parameters(estimator_2) set_random_state(estimator_1) set_random_state(estimator_2)
init = getattr(estimator.__init__, 'deprecated_original', estimator.__init__)
return
init_params = init_params[1:]
assert_true(init_param.default is None) continue
if "MultiTask" in name: return np.reshape(y, (-1, 1)) return y
if not (name == 'HuberRegressor' and estimator.n_iter_ is None): assert_greater_equal(estimator.n_iter_, 1)
if name in CROSS_DECOMPOSITION: for iter_ in estimator.n_iter_: assert_greater_equal(iter_, 1) else: assert_greater_equal(estimator.n_iter_, 1)
_unique_labels = _FN_UNIQUE_LABELS.get(label_type, None) if not _unique_labels: raise ValueError("Unknown label type: %s" % repr(ys))
if (len(set(isinstance(label, string_types) for label in ys_labels)) > 1): raise ValueError("Mix of label input types (string and number)")
return 'unknown'
if y.ndim > 2 or (y.dtype == object and len(y) and not isinstance(y.flat[0], string_types)):
if y.dtype.kind == 'f' and np.any(y != y.astype(int)): return 'continuous' + suffix
clf.classes_ = unique_labels(classes) return True
if 0 in classes_k: class_prior_k[classes_k == 0] += zeros_samp_weight_sum
if 0 not in classes_k and y_nnz[k] < y.shape[0]: classes_k = np.insert(classes_k, 0, 0) class_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum)
def __init__(self, array): self.array = array self.shape = array.shape self.ndim = array.ndim self.iloc = ArraySlicingWrapper(array)
return self.array
if np_version < (1, 7, 1): _ravel = np.ravel else: _ravel = partial(np.ravel, order='K')
if X.flags.c_contiguous: return check_array(X.T, copy=False, order='F'), True else: return check_array(X, copy=False, order='F'), False
return np.dot(A, B)
Q = random_state.normal(size=(A.shape[1], size))
if power_iteration_normalizer == 'auto': if n_iter <= 2: power_iteration_normalizer = 'none' else: power_iteration_normalizer = 'LU'
Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic') return Q
n_iter = 4 n_iter_specified = False
M = M.T
B = safe_sparse_dot(Q.T, M)
Uhat, s, V = linalg.svd(B, full_matrices=False) del B U = np.dot(Q, Uhat)
U, V = svd_flip(U, V, u_based_decision=False)
return V[:n_components, :].T, s[:n_components], U[:, :n_components].T
vmax = arr.max(axis=0) out = np.log(np.sum(np.exp(arr - vmax), axis=0)) out += vmax return out
above_cutoff = (abs(s) > cond * np.max(abs(s))) psigma_diag = np.zeros_like(s) psigma_diag[above_cutoff] = 1.0 / s[above_cutoff]
last_sum = last_mean * last_sample_count new_sum = X.sum(axis=0)
ret = line_search_wolfe2(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)
while k < maxiter: fgrad, fhess_p = grad_hess(xk, *args)
xsupi = _cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
if (not directed) and isspmatrix_csc(csgraph): csgraph = csgraph.T
csgraph_from_dense = None
from __future__ import division import numpy as np import scipy.sparse as sp import operator import array
idx = idx.item(0)
if a.ndim == 0: return idx
if class_probability is None: class_prob_j = np.empty(shape=classes[j].shape[0]) class_prob_j.fill(1 / classes[j].shape[0]) else: class_prob_j = np.asarray(class_probability[j])
if 0 not in classes[j]: classes[j] = np.insert(classes[j], 0, 0) class_prob_j = np.insert(class_prob_j, 0, 0.0)
from sklearn.utils.linear_assignment_ import _hungarian
([[400, 150, 400], [400, 450, 600], [300, 225, 300]],
([[400, 150, 400, 1], [400, 450, 600, 2], [300, 225, 300, 3]],
([[10, 10, 8], [9, 8, 1], [9, 7, 4]], 18 ),
([[10, 10, 8, 11], [9, 8, 1, 1], [9, 7, 4, 10]], 15 ),
([[], []], 0 ),
b_float32 = astype(a_int32, dtype=np.float32, copy=False) assert_equal(b_float32.dtype, np.float32)
assert_false(np.may_share_memory(b_float32, a_int32))
c_int32 = astype(a_int32, dtype=np.int32, copy=False) assert_true(c_int32 is a_int32)
d_int32 = astype(a_int32, dtype=np.int32, copy=True) assert_false(np.may_share_memory(d_int32, a_int32))
def test_invalid_sample_without_replacement_algorithm(): assert_raises(ValueError, sample_without_replacement, 5, 4, "unknown")
assert_raises(ValueError, sample_without_replacement, 0, 1) assert_raises(ValueError, sample_without_replacement, 1, 2)
assert_equal(sample_without_replacement(0, 0).shape, (0, ))
assert_equal(sample_without_replacement(5, 0).shape, (0, )) assert_equal(sample_without_replacement(5, 1).shape, (1, ))
assert_raises(ValueError, sample_without_replacement, -1, 5) assert_raises(ValueError, sample_without_replacement, 5, -1)
n_population = 100
assert_equal(np.size(sample_without_replacement(0, 0)), 0)
n_population = 10
n_trials = 10000
n_expected = combinations(n_population, n_samples, exact=True)
check_estimator(AdaBoostClassifier) check_estimator(MultiTaskElasticNet)
msg = "AttributeError or ValueError not raised by predict" assert_raises_regex(AssertionError, msg, check_estimators_unfitted, "estimator", NoSparseClassifier)
check_estimators_unfitted("estimator", CorrectNotFittedErrorClassifier)
X = np.arange(12).reshape(3, 4)
X_inf = np.arange(4).reshape(2, 2).astype(np.float) X_inf[0, 0] = np.inf assert_raises(ValueError, check_array, X_inf)
X_nan = np.arange(4).reshape(2, 2).astype(np.float) X_nan[0, 0] = np.nan assert_raises(ValueError, check_array, X_nan)
assert_equal(X.format, X_checked.format)
assert_equal(X_checked.format, accept_sparse[0])
if (X.dtype == X_checked.dtype and X.format == X_checked.format): assert_true(X is X_checked)
X_dense = check_array([[1, 2], [3, 4]]) assert_true(isinstance(X_dense, np.ndarray)) assert_raises(ValueError, check_array, X_ndim.tolist())
X_no_array = NotAnArray(X_dense) result = check_array(X_no_array) assert_true(isinstance(result, np.ndarray))
msg = "0 feature(s) (shape=(1, 0)) while a minimum of 1 is required." assert_raise_message(ValueError, msg, check_array, [[]])
msg = "0 sample(s) (shape=(0,)) while a minimum of 1 is required." assert_raise_message(ValueError, msg, check_array, [], ensure_2d=False)
msg = "Singleton array array(42) cannot be considered a valid collection." assert_raise_message(TypeError, msg, check_array, 42, ensure_2d=False)
X_checked = assert_warns(DeprecationWarning, check_array, [42], ensure_2d=True) assert_array_equal(np.array([[42]]), X_checked)
assert_raise_message(ValueError, msg, check_X_y, X, y, ensure_min_samples=2, ensure_2d=False)
assert_raise_message(ValueError, msg, check_X_y, X, y, ensure_min_features=3, allow_nd=True)
assert_raises(ValueError, check_symmetric, arr_bad)
for arr_format, arr in test_arrays.items(): assert_warns(UserWarning, check_symmetric, arr) assert_raises(ValueError, check_symmetric, arr, raise_exception=True)
assert_raises(ValueError, check_is_fitted, ARDRegression, "coef_") assert_raises(TypeError, check_is_fitted, "SVR", "support_")
assert_raises_regexp(TypeError, 'estimator', check_consistent_length, [1, 2], RandomForestRegressor())
dist_matrix[dist_matrix != 0] = 1
dist_dict = defaultdict(int) dist_dict.update(single_source_shortest_path_length(dist_matrix, i))
assert_true(check_random_state(None) is np.random.mtrand._rand) assert_true(check_random_state(np.random) is np.random.mtrand._rand)
with warnings.catch_warnings(record=True) as w: warnings.simplefilter("always")
with warnings.catch_warnings(record=True) as w: warnings.simplefilter("always")
assert_true(resample() is None)
random_state = check_random_state(42)
v0 = random_state.uniform(-1,1, A.shape[0]) w, _ = eigsh(A, k=k, sigma=0.0, v0=v0)
assert_greater_equal(w[0], 0)
X.setflags(write=False) X_df_readonly = pd.DataFrame(X) with warnings.catch_warnings(record=True): X_df_ro_indexed = safe_indexing(X_df_readonly, inds)
some_range = range(10) joined_range = list(chain(*[some_range[slice] for slice in gen_even_slices(10, 3)])) assert_array_equal(some_range, joined_range)
slices = gen_even_slices(10, -1) assert_raises_regex(ValueError, "gen_even_slices got n_packs=-1, must be" " >=1", next, slices)
X[0, 0] = 0 X[2, 1] = 0 X[4, 3] = 0 X_lil = sp.lil_matrix(X) X_lil[1, 0] = 0 X[1, 0] = 0
X[0, 0] = 0 X[2, 1] = 0 X[4, 3] = 0 X_lil = sp.lil_matrix(X) X_lil[1, 0] = 0 X[1, 0] = 0
last_mean = np.zeros(n_features) last_var = np.zeros_like(last_mean) last_n = 0
X = np.vstack(data_chunks) X_lil = sp.lil_matrix(X) X_csr = sp.csr_matrix(X_lil) X_csc = sp.csc_matrix(X_lil)
assert_raises(TypeError, csc_median_axis_0, sp.csr_matrix(X))
class_counts = np.bincount(y)[2:] assert_almost_equal(np.dot(cw, class_counts), y.shape[0]) assert_true(cw[0] < cw[1] < cw[2])
assert_array_almost_equal(np.asarray([1.0, 2.0, 3.0]), cw)
sample_weight = compute_sample_weight({1: 2, 2: 1}, y) assert_array_almost_equal(sample_weight, [2., 2., 2., 1., 1., 1.])
sample_weight = compute_sample_weight(None, y) assert_array_almost_equal(sample_weight, [1., 1., 1., 1., 1., 1., 1.])
assert_raises(ValueError, compute_sample_weight, {1: 2, 2: 1}, y, range(4))
assert_raises(ValueError, compute_sample_weight, {1: 2, 2: 1}, y_)
assert_raises(ValueError, compute_sample_weight, [{1: 2, 2: 1}], y_)
xi_, yi, swi, idx = dataset._next_py() xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))
xi_, yi, swi, idx = dataset._random_py() xi = sp.csr_matrix((xi_), shape=(1, X.shape[1]))
[{0: 'a', 1: 'b'}, {0: 'a'}],
np.array([[], []]),
np.array([[[0, 1], [2, 3]], [[4, 5], [6, 7]]]),
assert_raises(ValueError, unique_labels)
assert_array_equal(unique_labels(np.array([[0, 0, 1], [1, 0, 1], [0, 0, 0]])), np.arange(3))
for format in ["binary", "multiclass", "multilabel-indicator"]: for y in EXAMPLES[format]: unique_labels(y)
for example in NON_ARRAY_LIKE_EXAMPLES: assert_raises(ValueError, unique_labels, example)
mix_clf_format = product(EXAMPLES["multilabel-indicator"], EXAMPLES["multiclass"] + EXAMPLES["binary"])
if group == 'multilabel-indicator' and issparse(example): sparse_assert_, sparse_exp = assert_true, 'True' else: sparse_assert_, sparse_exp = assert_false, 'False'
if issparse(example): example = example.toarray()
rng = np.random.RandomState(0) x = rng.randint(10, size=(10, 5)) weights = np.ones(x.shape)
mode_result = 6
x = np.array([1e-40] * 1000000) logx = np.log(x) assert_almost_equal(np.exp(logsumexp(logx)), x.sum())
n_samples = 100 n_features = 500 rank = 5 k = 10
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=rank, tail_strength=0.0, random_state=0) assert_equal(X.shape, (n_samples, n_features))
U, s, V = linalg.svd(X, full_matrices=False)
assert_almost_equal(s[:k], sa)
assert_almost_equal(np.dot(U[:, :k], V[:k, :]), np.dot(Ua, Va))
X = sparse.csr_matrix(X)
Ua, sa, Va = \ randomized_svd(X, k, power_iteration_normalizer=normalizer, random_state=0) assert_almost_equal(s[:rank], sa[:rank])
n_samples = 100 n_features = 500 rank = 5 k = 10
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=rank, tail_strength=0.1, random_state=0) assert_equal(X.shape, (n_samples, n_features))
_, s, _ = linalg.svd(X, full_matrices=False)
_, sa, _ = randomized_svd(X, k, n_iter=0, power_iteration_normalizer=normalizer, random_state=0)
assert_greater(np.abs(s[:k] - sa).max(), 0.01)
_, sap, _ = randomized_svd(X, k, power_iteration_normalizer=normalizer, random_state=0)
assert_almost_equal(s[:k], sap, decimal=3)
n_samples = 100 n_features = 500 rank = 5 k = 10
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=rank, tail_strength=1.0, random_state=0) assert_equal(X.shape, (n_samples, n_features))
assert_greater(np.abs(s[:k] - sa).max(), 0.1)
_, sap, _ = randomized_svd(X, k, n_iter=5, power_iteration_normalizer=normalizer)
assert_almost_equal(s[:k], sap, decimal=3)
n_samples = 100 n_features = 500 rank = 4 k = 10
assert_almost_equal(s2, s3)
rs = np.random.RandomState(1999) n_samples = 20 n_features = 10 X = rs.randn(n_samples, n_features)
u_flipped, _, v_flipped = randomized_svd(mat, 3, flip_sign=True) u_based, v_based = max_loading_is_positive(u_flipped, v_flipped) assert_true(u_based) assert_false(v_based)
u_flipped_with_transpose, _, v_flipped_with_transpose = randomized_svd( mat, 3, flip_sign=True, transpose=True) u_based, v_based = max_loading_is_positive( u_flipped_with_transpose, v_flipped_with_transpose) assert_true(u_based) assert_false(v_based)
x = np.arange(3) assert_array_equal(x[:, np.newaxis], cartesian((x,)))
def naive_log_logistic(x): return np.log(1 / (1 + np.exp(-x)))
if fast_dot is np.dot: return
E = np.empty(0) assert_raises(ValueError, _fast_dot, E, E)
assert_raises(ValueError, _fast_dot, A, A[0])
assert_raises(ValueError, _fast_dot, A.T, np.array([A, A]))
assert_raises(ValueError, _fast_dot, A, A[0, :][None, :])
assert_raises(ValueError, _fast_dot, A, A)
for dtype in ['f8', 'f4']: A = A.astype(dtype) B = B.astype(dtype)
C = np.dot(A.T, A) C_ = fast_dot(A.T, A) assert_almost_equal(C, C_, decimal=5)
A = rng.random_sample([2, 2]) for dtype in ['f8', 'f4']: A = A.astype(dtype) B = B.astype(dtype)
def two_pass_var(X): mean = X.mean(axis=0) Y = X.copy() return np.mean((Y - mean)**2, axis=0)
if np.abs(np_var(A) - two_pass_var(A)).max() < 1e-6: stable_var = np_var else: stable_var = two_pass_var
assert_greater(np.abs(stable_var(A) - one_pass_var(A)).max(), tol)
incremental_count = batch.shape[0] sample_count = batch.shape[0]
assert_less(0, 1) _assert_less(0, 1) assert_raises(AssertionError, assert_less, 1, 0) assert_raises(AssertionError, _assert_less, 1, 0)
assert_greater(1, 0) _assert_greater(1, 0) assert_raises(AssertionError, assert_greater, 0, 1) assert_raises(AssertionError, _assert_greater, 0, 1)
set_random_state(lda, 3) set_random_state(tree, 3) assert_equal(tree.random_state, 3)
assert_raises(AssertionError, assert_raise_message, (ValueError, AttributeError), "test", _no_raise)
def _warning_function(): warnings.warn("deprecation warning", DeprecationWarning)
@ignore_warnings def decorator_no_warning(): _warning_function() _multiple_warning_function()
def context_manager_no_warning(): with ignore_warnings(): _warning_function()
#`clean_warning_registry()` is called internally by assert_warns class TestWarns(unittest.TestCase): def test_warn(self): def f(): warnings.warn("yo") return 3
warnings.simplefilter("ignore", UserWarning) assert_equal(assert_warns(UserWarning, f), 3)
assert_equal(sys.modules['warnings'].filters, [])
assert_warns(UserWarning, f) failed = True
init = cls.__init__
_SEUPD_WHICH = ['LM', 'SM', 'LA', 'SA', 'BE']
_NEUPD_WHICH = ['LM', 'SM', 'LR', 'SR', 'LI', 'SI']
buf = buf[offset:offset+size+1][:-1] data = np.ndarray(shape, dtype, buf, order=order) data.fill(0) return data
self.resid = np.array(v0, copy=True) info = 1
self.resid = np.zeros(n, tp) info = 0
self.sigma = 0
ishfts = 1 self.mode = mode self.iparam[0] = ishfts self.iparam[2] = maxiter self.iparam[3] = 1 self.iparam[6] = mode
self.workd = _aligned_zeros(3 * n, self.tp) self.workl = _aligned_zeros(self.ncv * (self.ncv + 8), self.tp)
self.workd[yslice] = self.OP(self.workd[xslice])
self.workd = _aligned_zeros(3 * n, self.tp) self.workl = _aligned_zeros(3 * self.ncv * (self.ncv + 2), self.tp)
self.rwork = _aligned_zeros(self.ncv, self.tp.lower())
self.workd[yslice] = self.OP(self.workd[xslice])
d = dr + 1.0j * di
z = zr.astype(self.tp.upper())
d = d[:nreturned] z = z[:, :nreturned]
tol = 2 * np.finfo(M.dtype).eps
tol = 2 * np.finfo(A.dtype).eps
mode = 1 M_matvec = None Minv_matvec = None if Minv is not None: raise ValueError("Minv should not be " "specified with M = None.")
mode = 2 if Minv is None: Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol) else: Minv = _aslinearoperator_with_dtype(Minv) Minv_matvec = Minv.matvec M_matvec = _aslinearoperator_with_dtype(M).matvec
mode = 1 M_matvec = None Minv_matvec = None if Minv is not None: raise ValueError("Minv should not be " "specified with M = None.")
mode = 2 if Minv is None: Minv_matvec = get_inv_matvec(M, symmetric=True, tol=tol) else: Minv = _aslinearoperator_with_dtype(Minv) Minv_matvec = Minv.matvec M_matvec = _aslinearoperator_with_dtype(M).matvec
if Minv is not None: raise ValueError("Minv should not be specified when sigma is")
else: raise ValueError("unrecognized mode '%s'" % mode)
eigvals, eigvec = eigsh(XH_X, k=k, tol=tol ** 2, maxiter=maxiter, ncv=ncv, which=which, v0=v0)
if which == 'LM':
eigvals = np.maximum(eigvals.real, 0)
import scipy.sparse as sp import numpy as np
if m > n: m, n = n, m
X.indptr[m + 2:n] += nz_n - nz_m X.indptr[m + 1] = m_start + nz_n X.indptr[n] = n_stop - nz_m
data = np.copy(X.data[start: end]) nz = n_samples - data.size median[f_ind] = _get_median(data, nz)
log_prob_x = logsumexp(jll, axis=1) return jll - np.atleast_2d(log_prob_x).T
total_mu = (n_new * new_mu + n_past * mu) / n_total
epsilon = 1e-9 * np.var(X, axis=0).max()
self.sigma_[:, :] -= epsilon
if self.priors is None: self.class_prior_ = self.class_count_ / self.class_count_.sum()
self.class_log_prior_ = (np.log(self.class_count_) - np.log(self.class_count_.sum()))
Y = Y.astype(np.float64) if sample_weight is not None: sample_weight = np.atleast_2d(sample_weight) Y *= check_array(sample_weight).T
self._count(X, Y)
self._update_feature_log_prob() self._update_class_log_prior(class_prior=class_prior) return self
Y = Y.astype(np.float64) if sample_weight is not None: sample_weight = np.atleast_2d(sample_weight) Y *= check_array(sample_weight).T
def _get_coef(self): return (self.feature_log_prob_[1:] if len(self.classes_) == 2 else self.feature_log_prob_)
jll = safe_sparse_dot(X, (self.feature_log_prob_ - neg_prob).T) jll += self.class_log_prior_ + neg_prob.sum(axis=1)
components = rng.binomial(1, 0.5, (n_components, n_features)) * 2 - 1 return 1 / np.sqrt(n_components) * components
data = rng.binomial(1, 0.5, size=np.size(indices)) * 2 - 1
components = sp.csr_matrix((data, indices, indptr), shape=(n_components, n_features))
self.components_ = self._make_random_matrix(self.n_components_, n_features)
assert_equal( self.components_.shape, (self.n_components_, n_features), err_msg=('An error has occurred the self.components_ matrix has ' ' not the proper shape.'))
X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True, y_numeric=True)
config.add_subpackage('__check_build') config.add_subpackage('_build_utils')
config.add_extension( '_isotonic', sources=['_isotonic.c'], include_dirs=[numpy.get_include()], libraries=libraries, )
config.add_subpackage('linear_model') config.add_subpackage('utils')
config.add_subpackage('tests')
warnings.filterwarnings('always', category=DeprecationWarning, module='^{0}\.'.format(re.escape(__name__)))
__SKLEARN_SETUP__
else: from . import __check_build from .base import clone
'clone']
from _dummy_thread import get_ident as _get_ident
if key not in self: root = self.__root last = root[0] last[1] = root[0] = self.__map[key] = [last, root, key] dict_setitem(self, key, value)
dict_delitem(self, key) link_prev, link_next, key = self.__map.pop(key) link_prev[1] = link_next link_next[0] = link_prev
PY2 = sys.version_info[0] == 2 PY3 = sys.version_info[0] == 3
MAXSIZE = int((1 << 31) - 1)
delattr(tp, self.name) return result
int2byte = operator.methodcaller("to_bytes", 1, "big")
logging.debug("[%s]: %s" % (self, msg))
time_lapse = time.time() - self.start_time full_msg = "%s: %.2fs, %.1f min" % (msg, time_lapse, time_lapse / 60)
self.last_time = time.time()
protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER else pickle.HIGHEST_PROTOCOL) Pickler.__init__(self, self.stream, protocol=protocol) self._hash = hashlib.new(hash_name)
dispatch[type(len)] = save_global dispatch[type(object)] = save_global dispatch[type(Pickler)] = save_global dispatch[type(pickle.dump)] = save_global
Pickler.save(self, _ConsistentSet(set_items))
import numpy as np self.np = np if hasattr(np, 'getbuffer'): self._getbuffer = np.getbuffer else: self._getbuffer = memoryview
obj = (klass, ('HASHED', obj.dtype, obj.shape, obj.strides))
from cPickle import loads from cPickle import dumps
from pickle import Pickler
from multiprocessing.pool import Pool
SYSTEM_SHARED_MEM_FS = '/dev/shm'
FOLDER_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR FILE_PERMISSIONS = stat.S_IRUSR | stat.S_IWUSR
return a
return _get_backing_memmap(b)
mode = 'r+'
return np.memmap(filename, dtype=dtype, shape=shape, mode=mode, offset=offset, order=order)
base = np.memmap(filename, dtype=dtype, shape=total_buffer_len, mode=mode, offset=offset, order=order) return as_strided(base, shape=shape, strides=strides)
a_start, a_end = np.byte_bounds(a) m_start = np.byte_bounds(m)[0] offset = a_start - m_start
offset += m.offset
order = 'C'
strides = None total_buffer_len = None
strides = a.strides total_buffer_len = (a_end - a_start) // a.itemsize
return _reduce_memmap_backed(a, m)
return (loads, (dumps(np.asarray(a), protocol=HIGHEST_PROTOCOL),))
return _reduce_memmap_backed(a, m)
try: os.makedirs(self._temp_folder) os.chmod(self._temp_folder, FOLDER_PERMISSIONS) except OSError as e: if e.errno != errno.EEXIST: raise e
load(filename, mmap_mode=self._mmap_mode).max()
return (load, (filename, self._mmap_mode))
self.dispatch = Pickler.dispatch.copy()
self.dispatch_table = copyreg.dispatch_table.copy()
def dispatcher(self, obj): reduced = reduce_func(obj) self.save_reduce(obj=obj, *reduced) self.dispatch[type] = dispatcher
self.put = send
if prewarm == "auto": prewarm = not use_shared_mem forward_reduce_ndarray = ArrayMemmapReducer( max_nbytes, pool_folder, mmap_mode, verbose, prewarm=prewarm) forward_reducers[np.ndarray] = forward_reduce_ndarray forward_reducers[np.memmap] = reduce_memmap
backward_reduce_ndarray = ArrayMemmapReducer( None, pool_folder, mmap_mode, verbose) backward_reducers[np.ndarray] = backward_reduce_ndarray backward_reducers[np.memmap] = reduce_memmap
_FUNCTION_HASHES = weakref.WeakKeyDictionary()
def __getstate__(self): return {"valid": self.valid, "value": self.value}
def __init__(self, func): self.func = func
pass
#
doc = func.__doc__
#
func_code, source_file, first_line = get_func_code(self.func) func_dir = self._get_func_dir() func_code_file = os.path.join(func_dir, 'func_code.py')
return _load_output(output_dir, _get_func_fullname(self.func), timestamp=self.timestamp, mmap_mode=self.mmap_mode, verbose=self._verbose)
#
#
return functools.partial(self.cache, ignore=ignore, verbose=verbose, mmap_mode=mmap_mode)
#
cachedir = self.cachedir[:-7] if self.cachedir is not None else None return (self.__class__, (cachedir, self.mmap_mode, self.compress, self._verbose))
_ZFILE_PREFIX = asbytes('ZF') _MAX_LEN = len(hex_str(2 ** 64))
file_handle.seek(0) return magic
file_handle.write(asbytes(length.ljust(_MAX_LEN))) file_handle.write(zlib.compress(asbytes(data), compress))
np_ver = [int(x) for x in unpickler.np.__version__.split('.', 2)[:2]]
self._npy_counter = 1 if protocol is None: protocol = (pickle.DEFAULT_PROTOCOL if PY3_OR_LATER else pickle.HIGHEST_PROTOCOL)
try: import numpy as np except ImportError: np = None self.np = np
if type(obj) is self.np.memmap: obj = self.np.asarray(obj) return Pickler.save(self, obj)
obj, filename = self._write_array(obj, filename) self._filenames.append(filename) self._npy_counter += 1
print('Failed to save %s to .npy file:\n%s' % ( type(obj), traceback.format_exc()))
if PY3_OR_LATER: dispatch[pickle.BUILD[0]] = load_build else: dispatch[pickle.BUILD] = load_build
compress = 3
raise ValueError( 'Second argument should be a filename, %s (type %s) was given' % (filename, type(filename)) )
JOBLIB_SPAWNED_PROCESS = "__JOBLIB_SPAWNED_PARALLEL__"
MIN_IDEAL_BATCH_DURATION = .2
MAX_IDEAL_BATCH_DURATION = 2
raise WorkerInterrupt()
if check_pickle: pickle.dumps(function)
self.results = batch()
old_duration = self.parallel._smoothed_batch_duration if old_duration == 0: new_duration = this_batch_duration else: new_duration = 0.8 * old_duration + 0.2 * this_batch_duration self.parallel._smoothed_batch_duration = new_duration
backend = "multiprocessing"
self._mp_context = backend backend = "multiprocessing"
self._pool = None self._output = None self._jobs = list() self._managed_pool = False
self._lock = threading.Lock()
return 1
self.exceptions = [TransportableException]
self._pool = None
self._pool = None warnings.warn( 'Multiprocessing-backed parallel loops cannot be nested,' ' setting n_jobs=1', stacklevel=3) return 1
self._pool = None warnings.warn( 'Multiprocessing backed parallel loops cannot be nested' ' below threads, setting n_jobs=1', stacklevel=3) return 1
os.environ[JOBLIB_SPAWNED_PROCESS] = '1'
self.exceptions.extend([KeyboardInterrupt, WorkerInterrupt])
if self._aborting: return
batch_size = 1
batch_size = old_batch_size
self._smoothed_batch_duration = 0
batch_size = self.batch_size
return False
time.sleep(0.01) continue
self._aborting = True
exception_type = _mk_exception(exception.etype)[0] exception = exception_type(report)
self._aborting = False if not self._managed_pool: n_jobs = self._initialize_pool() else: n_jobs = self._effective_n_jobs()
self._original_iterator = None self._pre_dispatch_amount = 0
iterator = itertools.islice(iterator, pre_dispatch)
while self.dispatch_one_batch(iterator): self._iterating = True else: self._iterating = False
self._iterating = False
size += (stat.st_size // 512 + 1) * 512
RM_SUBDIRS_RETRY_TIME = 0.1
err_count = 0 while True: try: shutil.rmtree(fullname, False, None) break except os.error: if err_count > 0: raise err_count += 1 time.sleep(RM_SUBDIRS_RETRY_TIME)
from tokenize import open as open_py_source
from codecs import lookup, BOM_UTF8 import re from io import TextIOWrapper, open cookie_re = re.compile("coding[:=]\s*([-\w.]+)")
raise SyntaxError("unknown encoding: " + encoding)
raise SyntaxError('encoding problem: utf-8')
rec_check = records[tb_offset:] try: rname = rec_check[0][1] if rname == '<ipython console>' or rname.endswith('<string>'): return rec_check except IndexError: pass
names = []
unique_names = uniq_stable(names)
try: etype = etype.__name__ except AttributeError: pass
try: records = _fixed_getframes(etb, context, tb_offset) except: raise print('\nUnfortunately, your original traceback can not be ' 'constructed.\n') return ''
mp = int(os.environ.get('JOBLIB_MULTIPROCESSING', 1)) or None if mp: try: import multiprocessing as mp import multiprocessing.pool except ImportError: mp = None
if mp is not None: try: _sem = mp.Semaphore()
if mp is not None: try: from multiprocessing.context import assert_spawning except ImportError: from multiprocessing.forking import assert_spawning else: assert_spawning = None
JoblibException.__init__(self, message, etype) self.message = message self.etype = etype
locals().update(_mk_common_exceptions())
source_lines = list(islice(source_file_obj, first_line - 1, None))
import urllib.parse quote = urllib.parse.quote
module = ''
arg_spec_for_format = arg_spec[:7 if PY3_OR_LATER else 4]
raise ValueError( 'ignore_lst must be a list of parameters to ignore ' '%s (type %s) was given' % (ignore_lst, type(ignore_lst)))
args = [func.__self__, ] + args
raise ValueError( 'Wrong number of arguments for %s:\n' ' %s was called.' % (_signature_str(name, arg_spec), _function_called_str(name, args, kwargs)) )
#self.debug(msg)
return meth
return sig.replace(parameters=tuple(sig.parameters.values())[1:])
wrapped = obj.__wrapped__
call = _get_user_defined_method(type(obj), '__call__', 'im_func') if call is not None: sig = signature(call)
return sig.replace(parameters=tuple(sig.parameters.values())[1:])
msg = 'no signature found for builtin function {0!r}'.format(obj) raise ValueError(msg)
if self._annotation is not _empty: formatted = '{0}:{1}'.format(formatted, formatannotation(self._annotation))
break
break
args.extend(arg)
args.append(arg)
kwargs.update(arg)
kwargs[param_name] = arg
non_default_count = pos_count - pos_default_count for name in positional[:non_default_count]: annotation = annotations.get(name, _empty) parameters.append(Parameter(name, annotation=annotation, kind=_POSITIONAL_OR_KEYWORD))
for offset, name in enumerate(positional[non_default_count:]): annotation = annotations.get(name, _empty) parameters.append(Parameter(name, annotation=annotation, kind=_POSITIONAL_OR_KEYWORD, default=defaults[offset]))
if func_code.co_flags & 0x04: name = arg_names[pos_count + keyword_only_count] annotation = annotations.get(name, _empty) parameters.append(Parameter(name, annotation=annotation, kind=_VAR_POSITIONAL))
for name in keyword_only: default = _empty if kwdefaults is not None: default = kwdefaults.get(name, _empty)
if func_code.co_flags & 0x08: index = pos_count + keyword_only_count if func_code.co_flags & 0x04: index += 1
for param_name, param in self.parameters.items(): if (param._partial_kwarg and param_name not in kwargs): kwargs[param_name] = param.default
values = [arg_val] values.extend(arg_vals) arguments[param.name] = tuple(values) break
kwargs_param = param continue
if (not partial and param.kind != _VAR_POSITIONAL and param.default is _empty): raise TypeError('{arg!r} parameter lacking default value'. \ format(arg=param_name))
arguments[kwargs_param.name] = kwargs
render_kw_only_separator = False
is_x_old_in_X = int(mask.sum() < X.shape[0])
y_subpopulation = np.zeros((max(n_subsamples, n_features))) lstsq, = get_lapack_funcs(('gelss',), (X_subpopulation, y_subpopulation))
alpha_ = 1. / np.var(y) lambda_ = 1.
for iter_ in range(self.n_iter):
keep_lambda = np.ones(n_features, dtype=bool)
alpha_ = 1. / np.var(y) lambda_ = np.ones(n_features)
keep_lambda = lambda_ < self.threshold_lambda coef_[~keep_lambda] = 0
config.add_subpackage('tests')
return 4.0 / (max_squared_sum + int(fit_intercept) + 4.0 * alpha_scaled)
return 1.0 / (max_squared_sum + int(fit_intercept) + alpha_scaled)
if max_iter is None: max_iter = 1000
alpha_scaled = float(alpha) / n_samples
n_classes = int(y.max()) + 1 if loss == 'multinomial' else 1
if sample_weight is None: sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
coef_init = np.zeros((n_features, n_classes), dtype=np.float64, order='C')
solve_triangular_args = {'check_finite': False}
L = np.empty((max_features, max_features), dtype=X.dtype)
L = np.zeros((max_features, max_features), dtype=X.dtype)
warnings.warn(premature, RuntimeWarning, stacklevel=2) break
L = np.empty((max_features, max_features), dtype=Gram.dtype)
L = np.zeros((max_features, max_features), dtype=Gram.dtype)
warnings.warn(premature, RuntimeWarning, stacklevel=3) break
n_nonzero_coefs = max(int(0.1 * X.shape[1]), 1)
copy_Gram = True
self.n_nonzero_coefs_ = max(int(0.1 * n_features), 1)
import itertools from abc import ABCMeta, abstractmethod import warnings
n_samples, n_features = X.shape
X -= X.mean(axis=0) y -= y.mean()
alphas /= alphas[0] alphas = alphas[::-1] coefs = coefs[:, ::-1] mask = alphas >= eps mask[0] = True alphas = alphas[mask] coefs = coefs[:, mask] return alphas, coefs
linear_loss = y - safe_sparse_dot(X, w) if fit_intercept: linear_loss -= intercept abs_linear_loss = np.abs(linear_loss) outliers_mask = abs_linear_loss > epsilon * sigma
outliers = abs_linear_loss[outliers_mask] num_outliers = np.count_nonzero(outliers_mask) n_non_outliers = X.shape[0] - num_outliers
outliers_sw = sample_weight[outliers_mask] n_sw_outliers = np.sum(outliers_sw) outlier_loss = (2. * epsilon * np.sum(outliers_sw * outliers) - sigma * n_sw_outliers * epsilon ** 2)
non_outliers = linear_loss[~outliers_mask] weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers weighted_loss = np.dot(weighted_non_outliers.T, non_outliers) squared_loss = weighted_loss / sigma
X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers) grad[:n_features] = ( 2. / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers))
grad[:n_features] += alpha * 2. * w
grad[-1] = n_samples grad[-1] -= n_sw_outliers * epsilon ** 2 grad[-1] -= squared_loss / sigma
if fit_intercept: grad[-2] = -2. * np.sum(weighted_non_outliers) / sigma grad[-2] -= 2. * epsilon * np.sum(sw_outliers)
bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1)) bounds[-1][0] = 1e-12
out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)
if grad.shape[0] > n_features: grad[-1] = z0.sum() return out, grad
out = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w) return out
if fit_intercept: grad[-1] = z0.sum()
dd_intercept = np.squeeze(np.array(dX.sum(axis=0)))
if fit_intercept: ret[:n_features] += s[-1] * dd_intercept ret[-1] = dd_intercept.dot(s[:n_features]) ret[-1] += d.sum() * s[-1] return ret
loss, grad, p = _multinomial_loss_grad(w, X, Y, alpha, sample_weight) sample_weight = sample_weight[:, np.newaxis]
pos_class = classes[1]
le = LabelEncoder() if isinstance(class_weight, dict) or multi_class == 'multinomial': class_weight_ = compute_class_weight(class_weight, classes, y) sample_weight *= class_weight_[le.fit_transform(y)]
if class_weight in ("auto", "balanced"): class_weight_ = compute_class_weight(class_weight, mask_classes, y_bin) sample_weight *= class_weight_[le.fit_transform(y_bin)]
le = LabelEncoder() Y_multi = le.fit_transform(y)
y_test = check_array(y_test, dtype=np.float64, ensure_2d=False)
if self.multi_class == 'multinomial': classes_ = [None] warm_start_coef = [warm_start_coef]
cv = check_cv(self.cv, y, classifier=True) folds = list(cv.split(X, y))
n_classes = 1 labels = labels[1:]
iter_labels = labels if self.multi_class == 'multinomial': iter_labels = [None]
raise ValueError("class_weight provided should be a " "dict or 'balanced'")
if self.multi_class == 'multinomial': scores = multi_scores coefs_paths = multi_coefs_paths
X, y, _, _, _ = _preprocess_data(X, y, fit_intercept, normalize, copy=False)
_, _, X_offset, _, X_scale = _preprocess_data(X, y, fit_intercept, normalize, return_mean=True) mean_dot = X_offset * np.sum(y)
return self
precompute = False
X_train = check_array(X_train, 'csc', dtype=dtype, order=X_order) alphas, coefs, _ = path(X_train, y_train, **path_params) del X_train, y_train
coefs = coefs[np.newaxis, :, :] y_offset = np.atleast_1d(y_offset) y_test = y_test[:, np.newaxis]
copy_X = self.copy_X and self.fit_intercept
alphas = np.tile(np.sort(alphas)[::-1], (n_l1_ratio, 1))
if not (self.n_jobs == 1 or self.n_jobs is None): path_params['copy_X'] = False
cv = check_cv(self.cv)
folds = list(cv.split(X)) best_mse = np.inf
else: self.alphas_ = np.asarray(alphas[0])
X = check_array(X, dtype=np.float64, order='F', copy=self.copy_X and self.fit_intercept) y = check_array(y, dtype=np.float64, ensure_2d=False)
return self
self.t_ = None
self._get_penalty_type(self.penalty) self._get_learning_rate_type(self.learning_rate)
sample_weight = np.ones(n_samples, dtype=np.float64, order='C')
sample_weight = np.asarray(sample_weight, dtype=np.float64, order="C")
random_state = check_random_state(est.random_state) seed = random_state.randint(0, np.iinfo(np.int32).max)
self._expanded_class_weight = compute_class_weight(self.class_weight, self.classes_, y) sample_weight = self._validate_sample_weight(sample_weight, n_samples)
classes = np.unique(y)
self.t_ = None
prob_sum = prob.sum(axis=1) all_zero = (prob_sum == 0) if np.any(all_zero): prob[all_zero, :] = 1 prob_sum[all_zero] = len(self.classes_)
prob /= prob_sum.reshape((prob.shape[0], -1))
sample_weight = self._validate_sample_weight(sample_weight, n_samples)
self.t_ = None
seed = random_state.randint(0, np.iinfo(np.int32).max)
sqrt_alpha = np.sqrt(alpha)
n_samples, n_features = X.shape n_targets = y.shape[1]
n_samples = K.shape[0] n_targets = y.shape[1]
sw = np.sqrt(np.atleast_1d(sample_weight)) y = y * sw[:, np.newaxis] K *= np.outer(sw, sw)
K.flat[::n_samples + 1] += alpha[0]
dual_coef = linalg.solve(K, y, sym_pos=True, overwrite_a=False)
K.flat[::n_samples + 1] -= alpha[0]
dual_coefs = np.empty([n_targets, n_samples])
if not sparse.issparse(X) or has_sw: solver = 'cholesky' else: solver = 'sparse_cg'
X, y = _rescale_data(X, y, sample_weight)
solver = 'svd'
solver = 'svd'
max_squared_sum = row_norms(X, squared=True).max()
coef = coef.ravel()
raise ValueError( "%s doesn't support multi-label classification" % ( self.__class__.__name__))
sample_weight = (sample_weight * compute_sample_weight(self.class_weight, y))
return (v_prime * Q ** 2).sum(axis=-1)
if len(y.shape) != 1: G_diag = G_diag[:, np.newaxis] return G_diag, c
G_diag = G_diag[:, np.newaxis]
warnings.warn("non-uniform sample weights unsupported for svd, " "forcing usage of eigen") gcv_mode = 'eigen'
_pre_compute = self._pre_compute_svd _errors = self._errors_svd _values = self._values_svd
def identity_estimator(): pass identity_estimator.decision_function = lambda y_predict: y_predict identity_estimator.predict = lambda y_predict: y_predict
sample_weight = (sample_weight * compute_sample_weight(self.class_weight, y))
min_samples = X.shape[1] + 1
residual_threshold = np.median(np.abs(y - np.median(y)))
n_samples = X.shape[0] sample_idxs = np.arange(n_samples)
subset_idxs = sample_without_replacement(n_samples, min_samples, random_state=random_state) X_subset = X[subset_idxs] y_subset = y[subset_idxs]
if (self.is_data_valid is not None and not self.is_data_valid(X_subset, y_subset)): continue
if sample_weight is None: base_estimator.fit(X_subset, y_subset) else: base_estimator.fit(X_subset, y_subset, sample_weight=sample_weight[subset_idxs])
if (self.is_model_valid is not None and not self.is_model_valid(base_estimator, X_subset, y_subset)): continue
y_pred = base_estimator.predict(X)
inlier_mask_subset = residuals_subset < residual_threshold n_inliers_subset = np.sum(inlier_mask_subset)
inlier_idxs_subset = sample_idxs[inlier_mask_subset] X_inlier_subset = X[inlier_idxs_subset] y_inlier_subset = y[inlier_idxs_subset]
score_subset = base_estimator.score(X_inlier_subset, y_inlier_subset)
if (n_inliers_subset == n_inliers_best and score_subset < score_best): continue
n_inliers_best = n_inliers_subset score_best = score_subset inlier_mask_best = inlier_mask_subset X_inlier_best = X_inlier_subset y_inlier_best = y_inlier_subset
if (n_inliers_best >= self.stop_n_inliers or score_best >= self.stop_score or self.n_trials_ >= _dynamic_max_trials(n_inliers_best, n_samples, min_samples, self.stop_probability)): break
base_estimator.fit(X_inlier_best, y_inlier_best)
seed = rng.randint(1, np.iinfo(np.int32).max)
X_var *= X.shape[0] X_std = np.sqrt(X_var, X_var) del X_var X_std[X_std == 0] = 1 inplace_column_scale(X, 1. / X_std)
X_var *= X.shape[0] X_scale = np.sqrt(X_var, X_var) del X_var X_scale[X_scale == 0] = 1 inplace_column_scale(X, 1. / X_scale)
prob /= prob.sum(axis=1).reshape((prob.shape[0], -1)) return prob
X, y = _rescale_data(X, y, sample_weight)
X, y, X_offset, y_offset, X_scale = _preprocess_data( X, y, fit_intercept=fit_intercept, normalize=normalize, copy=copy)
precompute = 'auto' Xy = None
if isinstance(precompute, six.string_types) and precompute == 'auto': precompute = (n_samples > n_features)
precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype, order='C') np.dot(X.T, X, out=precompute)
Xy = np.empty(shape=n_features, dtype=common_dtype, order='C') np.dot(X.T, y, out=Xy)
n_targets = y.shape[1] Xy = np.empty(shape=(n_features, n_targets), dtype=common_dtype, order='F') np.dot(y.T, X, out=Xy.T)
from sklearn.externals.six.moves import cStringIO as StringIO import sys old_stdout = sys.stdout try: sys.stdout = StringIO()
assert_true(ocur == X.shape[1])
assert_true(ocur == X.shape[1])
X, y = 3 * diabetes.data, diabetes.target G = np.dot(X.T, X) Xy = np.dot(X.T, y)
y = [5, 0, 5] for X in ([[5, 0], [0, 5], [10, 10]],
X = 3 * diabetes.data
assert_equal(len(lars.alphas_), 7)
X = diabetes.data Y = np.vstack([diabetes.target, diabetes.target ** 2]).T n_targets = Y.shape[1]
lars_broken = linear_model.LassoLarsIC('<unknown>') assert_raises(ValueError, lars_broken.fit, X, y)
splitted_data = train_test_split(X, y, random_state=42) with TempMemmap(splitted_data) as (X_train, X_test, y_train, y_test): _lars_path_residues(X_train, y_train, X_test, y_test, copy=False)
X = 3 * diabetes.data
X = [[1], [2]] Y = [1, 2]
X = [[1]] Y = [0]
for n_samples, n_features in ((6, 5), ):
reg = LinearRegression(fit_intercept=intercept) reg.fit(X, y, sample_weight=sample_weight) coefs1 = reg.coef_ inter1 = reg.intercept_
reg.fit(X, y, sample_weights_OK) reg.fit(X, y, sample_weights_OK_1) reg.fit(X, y, sample_weights_OK_2)
X, y = make_regression(random_state=random_state)
expected_X_norm = (np.sqrt(X.shape[0]) * np.mean((X - expected_X_mean) ** 2, axis=0) ** .5)
X = sparse.csr_matrix(X)
assert_greater(clf.score(X_test, y_test), 0.99)
clf_unconstrained = LassoCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, n_jobs=1) clf_unconstrained.fit(X, y) assert_true(min(clf_unconstrained.coef_) < 0)
clf_constrained = LassoCV(n_alphas=3, eps=1e-1, max_iter=max_iter, positive=True, cv=2, n_jobs=1) clf_constrained.fit(X, y) assert_true(min(clf_constrained.coef_) >= 0)
X, y, X_test, y_test = build_dataset(n_samples=200, n_features=100, n_informative_features=100) max_iter = 150
assert_almost_equal(clf.alpha_, min(clf.alphas_)) assert_equal(clf.l1_ratio_, min(clf.l1_ratio))
assert_greater(clf.score(X_test, y_test), 0.99)
enetcv_unconstrained = ElasticNetCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, n_jobs=1) enetcv_unconstrained.fit(X, y) assert_true(min(enetcv_unconstrained.coef_) < 0)
enetcv_constrained = ElasticNetCV(n_alphas=3, eps=1e-1, max_iter=max_iter, cv=2, positive=True, n_jobs=1) enetcv_constrained.fit(X, y) assert_true(min(enetcv_constrained.coef_) >= 0)
clf = MultiTaskLasso(alpha=1, tol=1e-8).fit(X, Y) assert_true(0 < clf.dual_gap_ < 1e-5) assert_array_almost_equal(clf.coef_[0], clf.coef_[1])
assert_greater(n_iter_reference, 2)
model.fit(X, y) n_iter_cold_start = model.n_iter_ assert_equal(n_iter_cold_start, n_iter_reference)
model.set_params(warm_start=True) model.fit(X, y) n_iter_warm_start = model.n_iter_ assert_equal(n_iter_warm_start, 1)
final_alpha = 1e-5 low_reg_model = ElasticNet(alpha=final_alpha).fit(X, y)
high_reg_model = ElasticNet(alpha=final_alpha * 10).fit(X, y) assert_greater(low_reg_model.n_iter_, high_reg_model.n_iter_)
warm_low_reg_model = deepcopy(high_reg_model) warm_low_reg_model.set_params(warm_start=True, alpha=final_alpha) warm_low_reg_model.fit(X, y) assert_greater(low_reg_model.n_iter_, warm_low_reg_model.n_iter_)
clf_random = ElasticNet(selection='invalid') assert_raises(ValueError, clf_random.fit, X, y)
X = check_array(X, order='C', dtype='float64') assert_raises(ValueError, clf.fit, X, y, check_input=False)
F, _ = f_regression(X, y)
scaling = 0.3 coef_grid, scores_path = lasso_stability_path(X, y, scaling=scaling, random_state=42, n_resampling=30)
scaling = 0.3 selection_threshold = 0.5
iris = load_iris() X = iris.data[:, [0, 2]] y = iris.target X = X[y != 2] y = y[y != 2]
iris = load_iris() X = iris.data[:, [0, 2]] y = iris.target X = X[y != 2] y = y[y != 2]
X, _, _, _, _ = _preprocess_data(X, y, True, True)
if (isinstance(self, SparseSGDClassifierTestCase) or isinstance(self, SparseSGDRegressorTestCase)): decay = .01
clf = self.factory(alpha=0.01, eta0=0.01, n_iter=5, shuffle=False, learning_rate=lr) clf.fit(X, Y)
clf3 = self.factory(alpha=0.01, eta0=0.01, n_iter=5, shuffle=False, warm_start=True, learning_rate=lr) clf3.fit(X, Y)
clf = self.factory(alpha=0.01, n_iter=5, shuffle=False) clf.fit(X, Y) Y_ = np.array(Y)[:, np.newaxis]
self.factory(alpha=0, learning_rate="optimal")
assert_array_equal(clf.predict(T), true_result)
self.factory(l1_ratio=1.1)
self.factory(learning_rate="<unknown>")
self.factory(eta0=0, learning_rate="constant")
self.factory(alpha=-.1)
self.factory(penalty='foobar', l1_ratio=0.85)
self.factory(loss="foobar")
self.factory(n_iter=-10000)
self.factory(shuffle="false")
self.factory(coef_init=np.zeros((3,))).fit(X, Y)
self.factory().fit(X, Y, coef_init=np.zeros((3,)))
self.factory().fit(X, Y, intercept_init=np.zeros((3,)))
self.factory().fit(X5, Y5, intercept_init=0)
y = np.dot(X, w) y = np.sign(y)
self.factory(alpha=0.01, n_iter=20).fit(X2, np.ones(9))
clf = self.factory(loss='squared_loss', learning_rate='constant', eta0=eta, alpha=alpha, fit_intercept=True, n_iter=1, average=True, shuffle=False)
clf = self.factory() assert_raises(ValueError, clf.fit, X2, Y2, coef_init=np.zeros((2, 2)))
clf = self.factory().fit(X2, Y2, coef_init=np.zeros((3, 2)))
clf = self.factory() assert_raises(ValueError, clf.fit, X2, Y2, intercept_init=np.zeros((1,)))
clf = self.factory().fit(X2, Y2, intercept_init=np.zeros((3,)))
clf = self.factory(loss="log", alpha=0.01, n_iter=10).fit(X2, Y2)
x = X.mean(axis=0) d = clf.decision_function([x])
n = len(X4) rng = np.random.RandomState(13) idx = np.arange(n) rng.shuffle(idx)
clf.sparsify() assert_true(sp.issparse(clf.coef_)) pred = clf.predict(X) assert_array_equal(pred, Y)
clf = pickle.loads(pickle.dumps(clf)) assert_true(sp.issparse(clf.coef_)) pred = clf.predict(X) assert_array_equal(pred, Y)
clf = self.factory(alpha=0.1, n_iter=1000, fit_intercept=False, class_weight={1: 0.001}) clf.fit(X, y)
assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
assert_almost_equal(clf.coef_, clf_weighted.coef_, decimal=2)
clf = self.factory(alpha=0.1, n_iter=1000, class_weight={0: 0.5}) clf.fit(X, Y)
clf = self.factory(alpha=0.1, n_iter=1000, class_weight=[0.5]) clf.fit(X, Y)
assert_array_almost_equal(clf.coef_, clf_balanced.coef_, 6)
X_0 = X[y == 0, :] y_0 = y[y == 0]
clf.fit(X, y, sample_weight=[0.001] * 3 + [1] * 2)
assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
clf = self.factory(alpha=0.1, n_iter=1000, fit_intercept=False) clf.fit(X, Y, sample_weight=np.arange(7))
clf.partial_fit(X3, Y3)
assert_true(id1, id2)
assert_true(id1, id2)
clf = self.factory() clf.fit(X2, Y2)
clf = self.factory(alpha=0.01, n_iter=5, shuffle=False) clf.fit(X, Y) assert_true(hasattr(clf, "coef_"))
y = [["ham", "spam"][i] for i in LabelEncoder().fit_transform(Y)] clf.fit(X[:, :-1], y)
self.factory(penalty='foobar', l1_ratio=0.85)
self.factory(loss="foobar")
y = np.dot(X, w)
y = np.dot(X, w)
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() \ + np.random.randn(n_samples, 1).ravel()
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()
ground_truth_coef = rng.randn(n_features) y = np.dot(X, ground_truth_coef)
assert_true(id1, id2)
X, y = datasets.make_classification(n_samples=1000, n_features=100, n_informative=20, random_state=1234)
rng = np.random.RandomState(0) n_samples = 100 n_features = 10
X_scaled = MinMaxScaler().fit_transform(X) assert_true(np.isfinite(X_scaled).all())
model.fit(X_scaled, y) assert_true(np.isfinite(model.coef_).all())
for p, y, expected in cases: assert_almost_equal(loss_function.dloss(p, y), expected)
raise SkipTest("XFailed Test") diabetes = datasets.load_diabetes() X, y = diabetes.data, diabetes.target
clf.fit(X, y) assert_array_equal(np.diff(clf.scores_) > 0, True)
X = X[:5, :] y = y[:5] clf.fit(X, y) assert_array_equal(np.diff(clf.scores_) > 0, True)
test = [[1], [3], [4]] assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)
X = np.array([[1], [2], [3]]) Y = np.array([1, 2, 3]) clf = ARDRegression(compute_score=True) clf.fit(X, Y)
test = [[1], [3], [4]] assert_array_almost_equal(clf.predict(test), [1, 3, 4], 2)
X, y = make_regression( n_samples=n_samples, n_features=n_features, random_state=0, noise=0.05)
assert_array_almost_equal(huber_warm.coef_, huber_warm_coef, 1)
if huber_warm.n_iter_ is not None: assert_equal(1, huber_warm.n_iter_)
assert_greater(ridge_outlier_score, huber_outlier_score)
n_samples, n_features = 6, 5 y = rng.randn(n_samples) X = rng.randn(n_samples, n_features)
ridge.fit(X, y, sample_weight=np.ones(n_samples)) assert_greater(ridge.score(X, y), 0.47)
ridge.fit(X, y, sample_weight=np.ones(n_samples)) assert_greater(ridge.score(X, y), 0.9)
coefs2 = ridge_regression( X * np.sqrt(sample_weight)[:, np.newaxis], y * np.sqrt(sample_weight), alpha=alpha, solver=solver) assert_array_almost_equal(coefs, coefs2)
est = Ridge(alpha=alpha, fit_intercept=intercept, solver=solver) est.fit(X, y, sample_weight=sample_weight) coefs = est.coef_ inter = est.intercept_
n_samples, n_features = 5, 4 y = rng.randn(n_samples) X = rng.randn(n_samples, n_features)
ridge = Ridge(alpha=penalties[:-1]) assert_raises(ValueError, ridge.fit, X, y)
n_samples = X_diabetes.shape[0]
assert_almost_equal(errors, errors2) assert_almost_equal(values, values2)
assert_almost_equal(errors, errors3) assert_almost_equal(values, values3)
ridge_gcv.fit(filter_(X_diabetes), y_diabetes) alpha_ = ridge_gcv.alpha_ ret.append(alpha_)
scorer = get_scorer('mean_squared_error') ridge_gcv4 = RidgeCV(fit_intercept=False, scoring=scorer) ridge_gcv4.fit(filter_(X_diabetes), y_diabetes) assert_equal(ridge_gcv4.alpha_, alpha_)
ridge_gcv.fit(filter_(X_diabetes), y_diabetes, sample_weight=np.ones(n_samples)) assert_equal(ridge_gcv.alpha_, alpha_)
Y = np.vstack((y_diabetes, y_diabetes)).T
Y = np.vstack((y_diabetes, y_diabetes)).T n_features = X_diabetes.shape[1]
ret_dense = test_func(DENSE_FILTER) ret_sparse = test_func(SPARSE_FILTER) if ret_dense is not None and ret_sparse is not None: assert_array_almost_equal(ret_dense, ret_sparse, decimal=3)
reg = RidgeClassifier(class_weight={1: 0.001}) reg.fit(X, y)
assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([-1]))
reg = RidgeClassifier(class_weight='balanced') reg.fit(X, y) assert_array_equal(reg.predict([[0.2, -1.0]]), np.array([1]))
reg = RidgeClassifierCV(class_weight={1: 0.001}, alphas=[.01, .1, 1, 10]) reg.fit(X, y)
rng = rng = np.random.RandomState(42)
y = rng.randn(n_samples) r.fit(x, y) assert_equal(r.cv_values_.shape, (n_samples, n_alphas))
n_responses = 3 y = rng.randn(n_samples, n_responses) r.fit(x, y) assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))
parameters = {'alpha': alphas} fit_params = {'sample_weight': sample_weight} gs = GridSearchCV(Ridge(), parameters, fit_params=fit_params, cv=cv) gs.fit(X, y)
ridge.fit(X, y, sample_weights_OK) ridge.fit(X, y, sample_weights_OK_1) ridge.fit(X, y, sample_weights_OK_2)
n_targets = 2 X, y = X_diabetes, y_diabetes y_n = np.tile(y, (n_targets, 1)).T
check_predictions(LogisticRegression(random_state=0), X, Y1) check_predictions(LogisticRegression(random_state=0), X_sp, Y1)
n_samples, n_features = iris.data.shape
target = (iris.target > 0).astype(np.intp) target = np.array(["setosa", "not-setosa"])[target]
n_samples, n_features = iris.data.shape target = iris.target_names[iris.target] clf = LogisticRegression(random_state=0).fit(iris.data, target)
rng = np.random.RandomState(0) X_ = rng.random_sample((5, 10)) y_ = np.ones(X_.shape[0]) y_[0] = 0
y_wrong = y_[:-1] assert_raises(ValueError, clf.fit, X, y_wrong)
assert_raises(ValueError, clf.fit(X_, y_).predict, rng.random_sample((3, 12)))
clf = LogisticRegression(random_state=0) clf.fit(X, Y1) clf.coef_[:] = 0 clf.intercept_[:] = 0 assert_array_almost_equal(clf.decision_function(X), 0)
Xnan = np.array(X, dtype=np.float64) Xnan[0, 1] = np.nan LogisticRegression(random_state=0).fit(Xnan, Y1)
assert_array_almost_equal(lr1.coef_, lr2.coef_) msg = "Arrays are not almost equal to 6 decimals" assert_raise_message(AssertionError, msg, assert_array_almost_equal, lr1.coef_, lr3.coef_)
w = np.zeros(n_features + 1) loss_interp, grad_interp = _logistic_loss_and_grad( w, X, y, alpha=1. ) assert_array_almost_equal(loss, loss_interp)
loss, grad = _logistic_loss_and_grad(w, X, y, alpha=1.) grad_2, hess = _logistic_grad_hess(w, X, y, alpha=1.) assert_array_almost_equal(grad, grad_2)
vector = np.zeros_like(grad) vector[1] = 1 hess_col = hess(vector)
assert_almost_equal(loss_interp + 0.5 * (w[-1] ** 2), loss)
assert_array_almost_equal(grad_interp[:n_features], grad[:n_features]) assert_almost_equal(grad_interp[-1] + alpha * w[-1], grad[-1])
train, target = iris.data, iris.target n_samples, n_features = train.shape
n_cv = 2 cv = StratifiedKFold(n_cv) precomputed_folds = list(cv.split(train, target))
clf = LogisticRegressionCV(cv=precomputed_folds) clf.fit(train, target)
clf1 = LogisticRegressionCV(cv=precomputed_folds) target_copy = target.copy() target_copy[target_copy == 0] = 1 clf1.fit(train, target_copy)
assert_array_almost_equal(clf.scores_[2], clf1.scores_[2]) assert_array_almost_equal(clf.intercept_[2:], clf1.intercept_) assert_array_almost_equal(clf.coef_[2][np.newaxis, :], clf1.coef_)
classes = np.unique(y) class_weight = compute_class_weight("balanced", classes, y) class_weight_dict = dict(zip(classes, class_weight)) return class_weight_dict
X = iris.data[45:, :] y = iris.target[45:] solvers = ("lbfgs", "newton-cg") class_weight_dict = _compute_class_weight_dictionary(y)
X = iris.data[45:100, :] y = iris.target[45:100] solvers = ("lbfgs", "newton-cg", "liblinear") class_weight_dict = _compute_class_weight_dictionary(y)
assert_warns_message(DeprecationWarning, "class_weight='auto' heuristic is deprecated", model.fit, X, y)
n_samples, n_features, n_classes = 50, 20, 3 X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=10, n_classes=n_classes, random_state=0)
assert_almost_equal(ref_i.coef_, clf_i.coef_, decimal=3) assert_almost_equal(ref_w.coef_, clf_w.coef_, decimal=3) assert_almost_equal(ref_i.intercept_, clf_i.intercept_, decimal=3)
vec = np.zeros(n_features * n_classes) vec[0] = 1 hess_col = hessp(vec)
X = np.zeros((5, 5)) assert_array_equal(clf.predict(X), np.zeros(5))
clf_multi_loss = log_loss(y, clf_multi.predict_proba(X)) clf_wrong_loss = log_loss(y, clf_multi._predict_proba_lr(X)) assert_greater(clf_wrong_loss, clf_multi_loss)
X, y_bin = iris.data, iris.target.copy() y_bin[y_bin == 2] = 0
if sp_version >= (0, 12): solvers.append('lbfgs')
X, y = iris.data, iris.target y_bin = y.copy() y_bin[y_bin == 2] = 0
n_classes = 1 if solver in ('liblinear', 'sag'): break
X, y = iris.data, iris.target
if sp_version >= (0, 12): solvers.append('lbfgs')
fermat_weber = fmin_bfgs(cost_func, median, disp=False) assert_array_almost_equal(median, fermat_weber) assert_warns(ConvergenceWarning, _spatial_median, X, max_iter=30, tol=0.)
assert_array_almost_equal(theil_sen.coef_, lstq.coef_, 9)
with no_stdout_stderr(): TheilSenRegressor(verbose=True, random_state=0).fit(X, y) TheilSenRegressor(verbose=True, max_subpopulation=10, random_state=0).fit(X, y)
clf = PassiveAggressiveClassifier().fit(X, y) assert_array_equal(clf.classes_, np.unique(y))
clf = PassiveAggressiveClassifier(C=0.1, n_iter=100, class_weight={1: 0.001}, random_state=100) clf.fit(X2, y2)
assert_array_equal(clf.predict([[0.2, -1.0]]), np.array([-1]))
clf = PassiveAggressiveClassifier(class_weight="balanced") assert_raises(ValueError, clf.partial_fit, X, y, classes=np.unique(y))
clf_balanced = PassiveAggressiveClassifier(C=0.1, n_iter=1000, class_weight="balanced") clf_balanced.fit(X2, y2)
assert_almost_equal(clf.coef_, clf_weighted.coef_, decimal=2) assert_almost_equal(clf.coef_, clf_balanced.coef_, decimal=2)
clf = ElasticNet() clf.coef_ = [1, 2, 3]
f = ignore_warnings X = sp.lil_matrix((3, 1)) X[0, 0] = -1 X[2, 0] = 1
T = sp.lil_matrix((3, 1)) T[0, 0] = 2 T[1, 0] = 3 T[2, 0] = 4
w = random_state.randn(n_features, n_targets)
y = np.dot(X, w) X = sp.csc_matrix(X) if n_targets == 1: y = np.ravel(y) return X, y
d_clf = ElasticNet(alpha=alpha, l1_ratio=0.8, fit_intercept=fit_intercept, max_iter=max_iter, tol=1e-7, positive=positive, warm_start=True) d_clf.fit(X_train.toarray(), y_train)
assert_less(np.sum(s_clf.coef_ != 0.0), 2 * n_informative)
assert_equal(np.sum(s_clf.coef_ != 0.0), n_informative)
estimator.fit(X, y) coef, intercept, dual_gap = (estimator.coef_, estimator.intercept_, estimator.dual_gap_)
def squared_dloss(p, y): return p - y
if sparse: decay = .01
if sparse: decay = .01
idx = int(rng.rand(1) * n_samples) entry = X[idx] seen.add(idx)
max_squared_sum = 4 + 9 + 16 max_squared_sum_ = row_norms(X, squared=True).max() assert_almost_equal(max_squared_sum, max_squared_sum_, decimal=4)
y = 0.5 * X.ravel()
y = 0.5 * X.ravel() + rng.randn(n_samples, 1).ravel()
X, y = iris.data, iris.target.astype(np.float64) n_samples, n_features = X.shape n_classes = len(np.unique(y))
assert_array_almost_equal(grad_1, grad_2) assert_almost_equal(loss_1, loss_2)
X = np.arange(-200, 200) y = 0.2 * X + 20 data = np.column_stack([X, y])
ransac_estimator.fit(X, y)
ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_ ).astype(np.bool_) ref_inlier_mask[outliers] = False
base_estimator = LinearRegression() ransac_estimator = RANSACRegressor(base_estimator, min_samples=2, residual_threshold=0.0, random_state=0)
yyy = np.column_stack([y, y, y])
ransac_estimator.fit(X, yyy)
ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_ ).astype(np.bool_) ref_inlier_mask[outliers] = False
def test_ransac_residual_metric(): residual_metric1 = lambda dy: np.sum(np.abs(dy), axis=1) residual_metric2 = lambda dy: np.sum(dy ** 2, axis=1)
ransac_estimator0.fit(X, y) assert_warns(DeprecationWarning, ransac_estimator2.fit, X, y) assert_array_almost_equal(ransac_estimator0.predict(X), ransac_estimator2.predict(X))
ransac_estimator.fit(X, y)
ref_inlier_mask = np.ones_like(ransac_estimator.inlier_mask_ ).astype(np.bool_) ref_inlier_mask[outliers] = False
assert_equal(_dynamic_max_trials(100, 100, 2, 0.99), 1)
assert_equal(_dynamic_max_trials(1, 100, 10, 0), 0) assert_equal(_dynamic_max_trials(1, 100, 10, 1), float('inf'))
assert_equal(ransac_estimator.inlier_mask_.shape[0], n_samples)
assert_array_equal(ransac_estimator.inlier_mask_, ref_inlier_mask)
base_estimator = Lasso() ransac_estimator = RANSACRegressor(base_estimator) assert_raises(ValueError, ransac_estimator.fit, X, y, weights)
sign_active = np.empty(max_features, dtype=np.int8) drop = False
X = X.copy('F')
if n_iter > 0: ss = ((prev_alpha[0] - alpha_min) / (prev_alpha[0] - alpha[0])) coef[:] = prev_coef + ss * (coef - prev_coef) alpha[0] = alpha_min
if n_active: linalg.solve_triangular(L[:n_active, :n_active], L[n_active, :n_active], trans=0, lower=1, overwrite_b=True, **solve_triangular_args)
least_squares, info = solve_cholesky(L[:n_active, :n_active], sign_active[:n_active], lower=True)
least_squares[...] = 1 AA = 1.
AA = 1. / np.sqrt(np.sum(least_squares * sign_active[:n_active]))
eq_dir = np.dot(X.T[:n_active].T, least_squares) corr_eq_dir = np.dot(X.T[n_active:], eq_dir)
corr_eq_dir = np.dot(Gram[:n_active, n_active:].T, least_squares)
sign_active[idx] = -sign_active[idx]
prev_coef = coef prev_alpha[0] = alpha[0] coef = np.zeros_like(coef)
Cov -= gamma_ * corr_eq_dir
if drop and method == 'lasso':
[arrayfuncs.cholesky_delete(L[:n_active, :n_active], ii) for ii in idx]
drop_idx = [active.pop(ii) for ii in idx]
alphas = alphas[:n_iter + 1] coefs = coefs[:n_iter + 1]
precompute = self.precompute if hasattr(precompute, '__array__'): Gram = precompute elif precompute == 'auto': Gram = 'auto' else: Gram = None return Gram
cv = check_cv(self.cv, classifier=False)
all_alphas = np.unique(all_alphas) stride = int(max(1, int(len(all_alphas) / float(self.max_n_alphas)))) all_alphas = all_alphas[::stride]
i_best_alpha = np.argmin(mse_path.mean(axis=-1)) best_alpha = all_alphas[i_best_alpha]
self.alpha_ = best_alpha self.cv_alphas_ = all_alphas self.cv_mse_path_ = mse_path
Lars.fit(self, X, y) return self
return self.alpha_
df[k] = np.sum(mask)
score = estimator.predict_proba(X)[:, 1]
Y = np.array([e.predict_proba(X)[:, 1] for e in self.estimators_]).T
Y = np.concatenate(((1 - Y), Y), axis=1)
Y /= np.sum(Y, axis=1)[:, np.newaxis]
self.code_book_ = random_state.random_sample((n_classes, code_size_)) self.code_book_[self.code_book_ > 0.5] = 1
if n_local_trials is None: n_local_trials = 2 + int(np.log(n_clusters))
center_id = random_state.randint(n_samples) if sp.issparse(X): centers[0] = X[center_id].toarray() else: centers[0] = X[center_id]
closest_dist_sq = euclidean_distances( centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True) current_pot = closest_dist_sq.sum()
for c in range(1, n_clusters): rand_vals = random_state.random_sample(n_local_trials) * current_pot candidate_ids = np.searchsorted(closest_dist_sq.cumsum(), rand_vals)
distance_to_candidates = euclidean_distances( X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)
best_candidate = None best_pot = None best_dist_sq = None for trial in range(n_local_trials): new_dist_sq = np.minimum(closest_dist_sq, distance_to_candidates[trial]) new_pot = new_dist_sq.sum()
if (best_candidate is None) or (new_pot < best_pot): best_candidate = candidate_ids[trial] best_pot = new_pot best_dist_sq = new_dist_sq
if sp.issparse(X): centers[c] = X[best_candidate].toarray() else: centers[c] = X[best_candidate] current_pot = best_pot closest_dist_sq = best_dist_sq
if not sp.issparse(X) or hasattr(init, '__array__'): X_mean = X.mean(axis=0) if not sp.issparse(X): X -= X_mean
x_squared_norms = row_norms(X, squared=True)
algorithm = "full"
centers = _init_centroids(X, n_clusters, init, random_state=random_state, x_squared_norms=x_squared_norms) if verbose: print("Initialization complete")
distances = np.zeros(shape=(X.shape[0],), dtype=np.float64)
for i in range(max_iter): centers_old = centers.copy() labels, inertia = \ _labels_inertia(X, x_squared_norms, centers, precompute_distances=precompute_distances, distances=distances)
if sp.issparse(X): centers = _k_means._centers_sparse(X, labels, n_clusters, distances) else: centers = _k_means._centers_dense(X, labels, n_clusters, distances)
best_labels, best_inertia = \ _labels_inertia(X, x_squared_norms, best_centers, precompute_distances=precompute_distances, distances=distances)
distances[:] = mindist
X = self._check_fit_data(X) return self.fit(X)._transform(X)
nearest_center, inertia = _labels_inertia(X, x_squared_norms, centers, distances=distances)
counts[to_reassign] = np.min(counts[~to_reassign])
if sp.issparse(X): return inertia, _k_means._mini_batch_update_csr( X, x_squared_norms, centers, counts, nearest_center, old_center_buffer, compute_squared_diff)
k = centers.shape[0] squared_diff = 0.0 for center_idx in range(k): center_mask = nearest_center == center_idx count = center_mask.sum()
centers[center_idx] *= counts[center_idx]
centers[center_idx] += np.sum(X[center_mask], axis=0)
counts[center_idx] += count
centers[center_idx] /= counts[center_idx]
if compute_squared_diff: diff = centers[center_idx].ravel() - old_center_buffer.ravel() squared_diff += np.dot(diff, diff)
batch_inertia /= model.batch_size centers_squared_diff /= model.batch_size
if tol > 0.0 and ewa_diff <= tol: if verbose: print('Converged (small centers change) at iteration %d/%d' % (iteration_idx + 1, n_iter)) return True
context['ewa_diff'] = ewa_diff context['ewa_inertia'] = ewa_inertia context['ewa_inertia_min'] = ewa_inertia_min context['no_improvement'] = no_improvement return False
old_center_buffer = np.zeros(n_features, np.double)
old_center_buffer = np.zeros(0, np.double)
cluster_centers = _init_centroids( X, self.n_clusters, self.init, random_state=random_state, x_squared_norms=x_squared_norms, init_size=init_size)
batch_inertia, centers_squared_diff = _mini_batch_step( X_valid, x_squared_norms[validation_indices], cluster_centers, counts, old_center_buffer, False, distances=None, verbose=self.verbose)
convergence_context = {}
for iteration_idx in range(n_iter): minibatch_indices = random_state.randint( 0, n_samples, self.batch_size)
if _mini_batch_convergence( self, iteration_idx, n_iter, tol, n_samples, centers_squared_diff, batch_inertia, convergence_context, verbose=self.verbose): break
self.cluster_centers_ = _init_centroids( X, self.n_clusters, self.init, random_state=self.random_state_, x_squared_norms=x_squared_norms, init_size=self.init_size)
def _mean_shift_single_seed(my_mean, X, nbrs, max_iter): bandwidth = nbrs.get_params()['radius']
i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth, return_distance=False)[0] points_within = X[i_nbrs] if len(points_within) == 0:
if (extmath.norm(my_mean - my_old_mean) < stop_thresh or completed_iterations == max_iter): return tuple(my_mean), len(points_within) completed_iterations += 1
raise ValueError("No point was within bandwidth=%f of any seed." " Try a different seeding strategy \ or increase the bandwidth." % bandwidth)
bin_sizes = defaultdict(int) for point in X: binned_point = np.round(point / bin_size) bin_sizes[tuple(binned_point)] += 1
import os from os.path import join
S.flat[::(n_samples + 1)] = preference
tmp = np.zeros((n_samples, n_samples))
e = np.zeros((n_samples, convergence_iter))
np.add(A, S, tmp) I = np.argmax(tmp, axis=1)
np.subtract(S, Y[:, None], tmp) tmp[ind, I] = S[ind, I] - Y2
tmp *= 1 - damping R *= damping R += tmp
np.maximum(R, 0, tmp) tmp.flat[::n_samples + 1] = R.flat[::n_samples + 1]
tmp *= 1 - damping A *= damping A -= tmp
E = (np.diag(A) + np.diag(R)) > 0 e[:, it % convergence_iter] = E K = np.sum(E, axis=0)
cluster_centers_indices = np.unique(labels) labels = np.searchsorted(cluster_centers_indices, labels)
import warnings
vectors = vectors / np.sqrt((vectors ** 2).sum(axis=1))[:, np.newaxis]
while (svd_restarts < max_svd_restarts) and not has_converged:
rotation = np.zeros((n_components, n_components)) rotation[:, 0] = vectors[random_state.randint(n_samples), :].T
last_objective_value = ncut_value rotation = np.dot(Vh.T, U.T)
from __future__ import division
self.centroids_ = self.init_centroids_[:n_samples + 1, :] self.squared_norm_ = self.init_sq_norm_[:n_samples + 1]
dist_matrix = np.dot(self.centroids_, subcluster.centroid_) dist_matrix *= -2. dist_matrix += self.squared_norm_ closest_index = np.argmin(dist_matrix) closest_subcluster = self.subclusters_[closest_index]
if closest_subcluster.child_ is not None: split_child = closest_subcluster.child_.insert_cf_subcluster( subcluster)
closest_subcluster.update(subcluster) self.init_centroids_[closest_index] = \ self.subclusters_[closest_index].centroid_ self.init_sq_norm_[closest_index] = \ self.subclusters_[closest_index].sq_norm_ return False
else: new_subcluster1, new_subcluster2 = _split_node( closest_subcluster.child_, threshold, branching_factor) self.update_split_subclusters( closest_subcluster, new_subcluster1, new_subcluster2)
else: merged = closest_subcluster.merge_subcluster( subcluster, self.threshold) if merged: self.init_centroids_[closest_index] = \ closest_subcluster.centroid_ self.init_sq_norm_[closest_index] = \ closest_subcluster.sq_norm_ return False
elif len(self.subclusters_) < self.branching_factor: self.append_subcluster(subcluster) return False
else: self.append_subcluster(subcluster) return True
self.dummy_leaf_ = _CFNode(threshold, branching_factor, is_leaf=True, n_features=n_features) self.dummy_leaf_.next_leaf_ = self.root_ self.root_.prev_leaf_ = self.dummy_leaf_
if not sparse.issparse(X): iter_func = iter else: iter_func = _iterate_sparse_X
self._global_clustering() return self
has_partial_fit = hasattr(self, 'partial_fit_')
if not (is_fitted or has_partial_fit): raise NotFittedError("Fit training data before predicting")
self._subcluster_norms = row_norms( self.subcluster_centers_, squared=True)
self.subcluster_labels_ = clusterer.fit_predict( self.subcluster_centers_)
S = -euclidean_distances(X, squared=True) preference = np.median(S) * 10 cluster_centers_indices, labels = affinity_propagation( S, preference=preference)
_, labels_no_copy = affinity_propagation(S, preference=preference, copy=False) assert_array_equal(labels, labels_no_copy)
af = AffinityPropagation(affinity="euclidean") labels = af.fit_predict(X) labels2 = af.predict(X) assert_array_equal(labels, labels2)
af = AffinityPropagation(affinity="euclidean") assert_raises(ValueError, af.predict, X)
S = np.dot(X, X.T) af = AffinityPropagation(affinity="precomputed") af.fit(S) assert_raises(ValueError, af.predict, X)
def __init__(self): pass
return (np.where([True, True, False, False, True])[0], np.where([False, False, True, True])[0])
S, rows, cols = make_checkerboard((30, 30), 3, noise=0.5, random_state=0)
assert_raises(ValueError, model.fit, mat) continue
generator = np.random.RandomState(0) mat = generator.rand(100, 100) scaled = _log_normalize(mat) + 1 _do_bistochastic_test(scaled)
x_squared_norms = (X ** 2).sum(axis=1) labels_array, inertia_array = _labels_inertia( X, x_squared_norms, noisy_centers) assert_array_almost_equal(inertia_array, inertia_gold) assert_array_equal(labels_array, labels_gold)
x_squared_norms_from_csr = row_norms(X_csr, squared=True) labels_csr, inertia_csr = _labels_inertia( X_csr, x_squared_norms_from_csr, noisy_centers) assert_array_almost_equal(inertia_csr, inertia_gold) assert_array_equal(labels_csr, labels_gold)
rng = np.random.RandomState(42) old_centers = centers + rng.normal(size=centers.shape)
X_mb = X[:10] X_mb_csr = X_csr[:10] x_mb_squared_norms = x_squared_norms[:10] x_mb_squared_norms_csr = x_squared_norms_csr[:10]
old_inertia, incremental_diff = _mini_batch_step( X_mb, x_mb_squared_norms, new_centers, counts, buffer, 1, None, random_reassign=False) assert_greater(old_inertia, 0.0)
labels, new_inertia = _labels_inertia( X_mb, x_mb_squared_norms, new_centers) assert_greater(new_inertia, 0.0) assert_less(new_inertia, old_inertia)
effective_diff = np.sum((new_centers - old_centers) ** 2) assert_almost_equal(incremental_diff, effective_diff)
old_inertia_csr, incremental_diff_csr = _mini_batch_step( X_mb_csr, x_mb_squared_norms_csr, new_centers_csr, counts_csr, buffer_csr, 1, None, random_reassign=False) assert_greater(old_inertia_csr, 0.0)
labels_csr, new_inertia_csr = _labels_inertia( X_mb_csr, x_mb_squared_norms_csr, new_centers_csr) assert_greater(new_inertia_csr, 0.0) assert_less(new_inertia_csr, old_inertia_csr)
effective_diff = np.sum((new_centers_csr - old_centers) ** 2) assert_almost_equal(incremental_diff_csr, effective_diff)
assert_array_equal(labels, labels_csr) assert_array_almost_equal(new_centers, new_centers_csr) assert_almost_equal(incremental_diff, incremental_diff_csr) assert_almost_equal(old_inertia, old_inertia_csr) assert_almost_equal(new_inertia, new_inertia_csr)
centers = km.cluster_centers_ assert_equal(centers.shape, (n_clusters, n_features))
assert_equal(v_measure_score(true_labels, labels), 1.0) assert_greater(km.inertia_, 0.0)
assert_raises(ValueError, km.fit, [[0., 1.]])
this_labels = np.unique(this_labels, return_index=True)[1][this_labels] np.testing.assert_array_equal(this_labels, labels)
km = KMeans(precompute_distances="wrong") assert_raises(ValueError, km.fit, X)
assert_raises_regex(ValueError, "n_init", KMeans(n_init=0).fit, X) assert_raises_regex(ValueError, "n_init", KMeans(n_init=-1).fit, X)
assert_warns(RuntimeWarning, mb_k_means.fit, X)
mb_k_means = MiniBatchKMeans(init="random", n_clusters=n_clusters, random_state=42, n_init=10).fit(X) _check_fitted_model(mb_k_means)
mb_k_means = MiniBatchKMeans(init="random", n_clusters=n_clusters, random_state=42, n_init=10).fit(X_csr) _check_fitted_model(mb_k_means)
mb_k_means = MiniBatchKMeans(n_clusters=20, batch_size=201, random_state=42, init="random") mb_k_means.fit(zeroed_X) assert_greater(mb_k_means.cluster_centers_.any(axis=1).sum(), 10)
assert_greater(mb_k_means.cluster_centers_.any(axis=1).sum(), 10)
for this_X in (X, X_csr): mb_k_means = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, random_state=42) mb_k_means.fit(this_X)
msg = "does not match the number of clusters" assert_raises_regex(ValueError, msg, MiniBatchKMeans(init=test_init, random_state=42).fit, X_csr)
mb_k_means = MiniBatchKMeans(n_clusters=3, init=test_init, random_state=42).fit(X_csr) _check_fitted_model(mb_k_means)
for X_minibatch in np.array_split(X, 10): km.partial_fit(X_minibatch)
labels = km.predict(X) assert_equal(v_measure_score(true_labels, labels), 1.0)
my_X = X.copy() km = KMeans(copy_x=False, n_clusters=n_clusters, random_state=42) km.fit(my_X) _check_fitted_model(km)
assert_array_almost_equal(my_X, X)
assert_equal(len(np.unique(km.labels_)), 3)
pred = km.predict(km.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
pred = km.predict(X) assert_array_equal(pred, km.labels_)
pred = km.fit_predict(X) assert_array_equal(pred, km.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
pred = mb_k_means.predict(X) assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)
assert_array_equal(mb_k_means.predict(X_csr), mb_k_means.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)
assert_array_equal(mb_k_means.predict(X_csr), mb_k_means.labels_)
pred = mb_k_means.predict(mb_k_means.cluster_centers_) assert_array_equal(pred, np.arange(n_clusters))
assert_array_equal(mb_k_means.predict(X), mb_k_means.labels_)
assert_equal(v_measure_score(true_labels, labels), 1.0) assert_greater(inertia, 0.0)
assert_warns(RuntimeWarning, k_means, X, n_clusters=n_clusters, init=centers)
assert_raises(ValueError, k_means, X, n_clusters=X.shape[0] + 1)
brc_partial.set_params(n_clusters=3) brc_partial.partial_fit(None) assert_array_equal(brc_partial.subcluster_labels_, brc.subcluster_labels_)
rng = np.random.RandomState(0) X = generate_clustered_data(n_clusters=3, n_features=3, n_samples_per_cluster=10)
gc = AgglomerativeClustering(n_clusters=10) brc2 = Birch(n_clusters=gc) brc2.fit(X) assert_array_equal(brc1.subcluster_labels_, brc2.subcluster_labels_) assert_array_equal(brc1.labels_, brc2.labels_)
clf = ElasticNet() brc3 = Birch(n_clusters=clf) assert_raises(ValueError, brc3.fit, X)
brc4 = Birch(threshold=10000.) assert_warns(UserWarning, brc4.fit, X)
X, y = make_blobs(n_samples=100, centers=10) brc = Birch(n_clusters=10) brc.fit(X)
X, y = make_blobs() branching_factor = 9
brc = Birch(n_clusters=None, branching_factor=1, threshold=0.01) assert_raises(ValueError, brc.fit, X)
X, y = make_blobs(n_samples=80, centers=4) brc = Birch(threshold=0.5, n_clusters=None) brc.fit(X) check_threshold(brc, 0.5)
bandwidth = estimate_bandwidth(X, n_samples=200) assert_true(0.9 <= bandwidth <= 1.5)
bandwidth = 1.2
ms = MeanShift(bandwidth=1.2) labels = ms.fit_predict(X) labels2 = ms.predict(X) assert_array_equal(labels, labels2)
ms = MeanShift() assert_false(hasattr(ms, "cluster_centers_")) assert_false(hasattr(ms, "labels_"))
with warnings.catch_warnings(record=True): test_bins = get_bin_seeds(X, 0.01, 1) assert_array_equal(test_bins, X)
from tempfile import mkdtemp import shutil from functools import partial
FeatureAgglomeration().fit(X)
dis = cosine_distances(X)
res = linkage_tree(X, affinity=manhattan_distances) assert_array_equal(res[0], linkage_tree(X, affinity="manhattan")[0])
clustering = AgglomerativeClustering( n_clusters=10, connectivity=connectivity.toarray(), affinity="manhattan", linkage="ward") assert_raises(ValueError, clustering.fit, X)
assert_raises(ValueError, agglo.fit, X[:0])
n, p, k = 10, 5, 3 rng = np.random.RandomState(0)
assert_raises(ValueError, _hc_cut, n_leaves + 1, children, n_leaves)
n, p = 10, 5 rng = np.random.RandomState(0)
n, p = 10, 5 rng = np.random.RandomState(0)
children_unstructured = out_unstructured[0] children_structured = out_structured[0]
assert_array_equal(children_unstructured, children_structured)
dist_unstructured = out_unstructured[-1] dist_structured = out_structured[-1]
assert_array_equal(linkage_X_ward[:, :2], out_X_unstructured[0]) assert_array_equal(linkage_X_ward[:, :2], out_X_structured[0])
assert_array_almost_equal(linkage_X_ward[:, 2], out_X_unstructured[4]) assert_array_almost_equal(linkage_X_ward[:, 2], out_X_structured[4])
assert_array_equal(X_truth[:, :2], out_X_unstructured[0]) assert_array_equal(X_truth[:, :2], out_X_structured[0])
assert_array_almost_equal(X_truth[:, 2], out_X_unstructured[4]) assert_array_almost_equal(X_truth[:, 2], out_X_structured[4])
rng = np.random.RandomState(0) X = rng.randn(10, 2) connectivity = kneighbors_graph(X, 5, include_self=False)
agc = AgglomerativeClustering(n_clusters=2, connectivity=connectivity) agc.fit(X) n_samples = X.shape[0] n_nodes = agc.children_.shape[0] assert_equal(n_nodes, n_samples - 1)
rng = np.random.RandomState(0) X = rng.rand(5, 5)
connectivity = np.eye(5)
assert_greater(np.mean(labels == true_labels), .3)
sp = SpectralClustering(n_clusters=2, affinity='<unknown>') assert_raises(ValueError, sp.fit, X)
eps = 0.8 min_samples = 10 metric = 'euclidean' core_samples, labels = dbscan(X, metric=metric, eps=eps, min_samples=min_samples)
n_clusters_1 = len(set(labels)) - int(-1 in labels) assert_equal(n_clusters_1, n_clusters)
eps = 0.8 min_samples = 10 metric = distance.euclidean core_samples, labels = dbscan(X, metric=metric, eps=eps, min_samples=min_samples, algorithm='ball_tree')
n_clusters_1 = len(set(labels)) - int(-1 in labels) assert_equal(n_clusters_1, n_clusters)
eps = 0.8 min_samples = 10
n_clusters_1 = len(set(labels)) - int(-1 in labels) assert_equal(n_clusters_1, n_clusters)
X = [[1., 2.], [3., 4.]]
assert_raises(ValueError, dbscan, [[0], [1]], sample_weight=[2]) assert_raises(ValueError, dbscan, [[0], [1]], sample_weight=[2, 3, 4])
D = pairwise_distances(X) core3, label3 = dbscan(D, sample_weight=sample_weight, metric='precomputed') assert_array_equal(core1, core3) assert_array_equal(label1, label3)
est = DBSCAN().fit(X, sample_weight=sample_weight) core4 = est.core_sample_indices_ label4 = est.labels_ assert_array_equal(core1, core4) assert_array_equal(label1, label4)
core_samples, labels = dbscan(X, algorithm=algorithm, eps=1, min_samples=3) assert_array_equal(core_samples, [2]) assert_array_equal(labels, [-1, 0, 0, 0, -1, -1, -1])
core_samples, labels = dbscan(X, algorithm=algorithm, eps=1, min_samples=4) assert_array_equal(core_samples, []) assert_array_equal(labels, -np.ones(n_samples))
if metric == 'precomputed' and sparse.issparse(X): neighborhoods = np.empty(X.shape[0], dtype=object)
neighborhoods = neighbors_model.radius_neighbors(X, eps, return_distance=False)
labels = -np.ones(X.shape[0], dtype=np.intp)
core_samples = np.asarray(n_neighbors >= min_samples, dtype=np.uint8) dbscan_inner(core_samples, neighborhoods, labels) return np.where(core_samples)[0], labels
self.components_ = X[self.core_sample_indices_].copy()
self.components_ = np.empty((0, X.shape[1]))
v0 = random_state.uniform(-1, 1, A.shape[0]) _, u = eigsh(A, ncv=self.n_svd_vecs, v0=v0)
connectivity = connectivity + connectivity.T
if not sparse.isspmatrix_lil(connectivity): if not sparse.isspmatrix(connectivity): connectivity = sparse.lil_matrix(connectivity) else: connectivity = connectivity.tolil()
n_components, labels = connected_components(connectivity)
parent = np.arange(n_nodes, dtype=np.intp) used_node = np.ones(n_nodes, dtype=bool) children = [] if return_distance: distances = np.empty(n_nodes - n_samples)
moments_1[k] = moments_1[i] + moments_1[j] moments_2[k] = moments_2[i] + moments_2[j]
[heappush(inertia, (ini[idx], k, coord_col[idx])) for idx in range(n_additions)]
n_leaves = n_samples children = [c[::-1] for c in children]
distances = np.sqrt(2. * distances) return children, n_components, n_leaves, parent, distances
i, j = np.triu_indices(X.shape[0], k=1) X = X[i, j]
affinity = 'euclidean'
diag_mask = (connectivity.row != connectivity.col) connectivity.row = connectivity.row[diag_mask] connectivity.col = connectivity.col[diag_mask] connectivity.data = connectivity.data[diag_mask] del diag_mask
distances = paired_distances(X[connectivity.row], X[connectivity.col], metric=affinity)
A = np.empty(n_nodes, dtype=object) inertia = list()
parent = np.arange(n_nodes, dtype=np.intp) used_node = np.ones(n_nodes, dtype=np.intp) children = []
for k in xrange(n_samples, n_nodes): while True: edge = heappop(inertia) if used_node[edge.a] and used_node[edge.b]: break i = edge.a j = edge.b
distances[k - n_samples] = edge.weight
n_i = used_node[i] n_j = used_node[j] used_node[k] = n_i + n_j used_node[i] = used_node[j] = False
n_leaves = n_samples
children = np.array(children)[:, ::-1]
def _complete_linkage(*args, **kwargs): kwargs['linkage'] = 'complete' return linkage_tree(*args, **kwargs)
compute_full_tree = self.n_clusters < max(100, .02 * n_samples)
import warnings import operator import sys import time
self.store_precision = True
X = check_array(X, ensure_min_features=2, ensure_min_samples=2, estimator=self)
self.store_precision = True
path = list() n_alphas = self.alphas inner_verbose = max(0, self.verbose - 1)
warnings.simplefilter('ignore', ConvergenceWarning)
import warnings import numbers import numpy as np from scipy import linalg from scipy.stats import chi2
if remaining_iterations == 0: if verbose: print('Maximum number of iterations reached') results = location, covariance, det, support, dist
if support_fraction is None: n_support = int(np.ceil(0.5 * (n_samples + n_features + 1))) else: n_support = int(support_fraction * n_samples)
from __future__ import division import warnings import numpy as np from scipy import linalg
self.covariance_ = covariance if self.store_precision: self.precision_ = pinvh(covariance) else: self.precision_ = None
test_cov = empirical_covariance( X_test - self.location_, assume_centered=True) res = log_likelihood(test_cov, self.get_precision())
centered_obs = observations - self.location_ mahalanobis_dist = np.sum( np.dot(centered_obs, precision) * centered_obs, 1)
if not alpha == 0: assert_array_less(np.diff(costs), 0)
indices = np.arange(10, 13)
GraphLassoCV(alphas=[0.8, 0.5], tol=1e-1, n_jobs=1).fit(X)
launch_mcd_on_dataset(1000, 5, 450, 0.1, 0.1, 540)
launch_mcd_on_dataset(1700, 5, 800, 0.1, 0.1, 870)
launch_mcd_on_dataset(500, 1, 100, 0.001, 0.001, 350)
rnd = np.random.RandomState(0) X = rnd.normal(size=(3, 1)) mcd = MinCovDet() mcd.fit(X)
cov = EmpiricalCovariance(assume_centered=True) cov.fit(X) assert_array_equal(cov.location_, np.zeros(X.shape[1]))
cov = ShrunkCovariance(shrinkage=0.5) cov.fit(X) assert_array_almost_equal( shrunk_covariance(empirical_covariance(X), shrinkage=0.5), cov.covariance_, 4)
cov = ShrunkCovariance() cov.fit(X) assert_array_almost_equal( shrunk_covariance(empirical_covariance(X)), cov.covariance_, 4)
cov = ShrunkCovariance(shrinkage=0.) cov.fit(X) assert_array_almost_equal(empirical_covariance(X), cov.covariance_, 4)
cov = ShrunkCovariance(shrinkage=0.5, store_precision=False) cov.fit(X) assert(cov.precision_ is None)
X_centered = X - X.mean(axis=0) lw = LedoitWolf(assume_centered=True) lw.fit(X_centered) shrinkage_ = lw.shrinkage_
lw = LedoitWolf(store_precision=False, assume_centered=True) lw.fit(X_centered) assert_almost_equal(lw.score(X_centered), score_, 4) assert(lw.precision_ is None)
lw = LedoitWolf(store_precision=False) lw.fit(X) assert_almost_equal(lw.score(X), score_, 4) assert(lw.precision_ is None)
X_small = X[:, :4] lw = LedoitWolf() lw.fit(X_small) shrinkage_ = lw.shrinkage_
lw = LedoitWolf(block_size=25).fit(X) assert_almost_equal(lw.covariance_, cov)
oa = OAS(store_precision=False, assume_centered=True) oa.fit(X_centered) assert_almost_equal(oa.score(X_centered), score_, 4) assert(oa.precision_ is None)
oa = OAS(store_precision=False) oa.fit(X) assert_almost_equal(oa.score(X), score_, 4) assert(oa.precision_ is None)
from __future__ import division import warnings import numpy as np
if not assume_centered: X = X - X.mean(0)
n_splits = int(n_features / block_size) X2 = X ** 2 emp_cov_trace = np.sum(X2, axis=0) / n_samples mu = np.sum(emp_cov_trace) / n_features
alpha = np.mean(emp_cov ** 2) num = alpha + mu ** 2 den = (n_samples + 1.) * (alpha - (mu ** 2) / n_features)
if self.assume_centered: self.location_ = np.zeros(X.shape[1]) else: self.location_ = X.mean(0)
rho, _ = spearmanr(x, y) increasing_bool = rho >= 0
rho_0 = math.tanh(F - 1.96 * F_se) rho_1 = math.tanh(F + 1.96 * F_se)
C = np.dot(sample_weight, y * y) * 10 if y_min is not None: y[0] = y_min sample_weight[0] = C if y_max is not None: y[-1] = y_max sample_weight[-1] = C
self.f_ = lambda x: y.repeat(x.shape)
if self.increasing == 'auto': self.increasing_ = check_increasing(X, y) else: self.increasing_ = self.increasing
self._X_ = X = unique_X self._y_ = y = isotonic_regression(unique_y, unique_sample_weight, self.y_min, self.y_max, increasing=self.increasing_)
self.X_min_, self.X_max_ = np.min(X), np.max(X)
return X, y
X, y = self._build_y(X, y, sample_weight)
self._necessary_X_, self._necessary_y_ = X, y
self._build_f(X, y) return self
state = dict(self.__dict__) state.pop('f_', None) return state
y = check_array(y, accept_sparse='csr', ensure_2d=False, dtype=None)
pos_switch = pos_label == 0 if pos_switch: pos_label = -neg_label
y_in_classes = in1d(y, classes) y_seen = y[y_in_classes] indices = np.searchsorted(sorted_class, y_seen) indptr = np.hstack((0, np.cumsum(y_in_classes)))
if np.any(classes != sorted_class): indices = np.searchsorted(sorted_class, classes) Y = Y[:, indices]
y_i_all_argmax = np.flatnonzero(y_data_repeated_max == y.data)
if row_max[-1] == 0: y_i_all_argmax = np.append(y_i_all_argmax, [len(y.data)])
class_mapping = defaultdict(int) class_mapping.default_factory = class_mapping.__len__ yt = self._transform(y, class_mapping)
tmp = sorted(class_mapping, key=class_mapping.get)
if array.size > 0: mode = stats.mode(array) most_frequent_value = mode[0][0] most_frequent_count = mode[1][0] else: most_frequent_value = 0 most_frequent_count = 0
if self.axis == 0: X = check_array(X, accept_sparse='csc', dtype=np.float64, force_all_finite=False)
if missing_values == 0: n_zeros_axis = np.zeros(X.shape[not axis], dtype=int) else: n_zeros_axis = X.shape[axis] - np.diff(X.indptr)
if strategy == "mean": if missing_values != 0: n_non_missing = n_zeros_axis
mask_missing_values = _get_mask(X.data, missing_values) mask_valids = np.logical_not(mask_missing_values)
with np.errstate(all="ignore"): return np.ravel(sums) / np.ravel(n_non_missing)
columns = [col[astype(mask, bool, copy=False)] for col, mask in zip(columns_all, mask_valids)]
if strategy == "median": median = np.empty(len(columns)) for i, column in enumerate(columns): median[i] = _get_median(column, n_zeros_axis[i])
elif strategy == "most_frequent": most_frequent = np.empty(len(columns))
elif strategy == "most_frequent":
if axis == 0: X = X.transpose() mask = mask.transpose()
if self.axis == 1: X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)
if hasattr(self, 'scale_'): del self.scale_ del self.min_ del self.n_samples_seen_ del self.data_min_ del self.data_max_ del self.data_range_
self._reset() return self.partial_fit(X, y)
X = check_array(X, copy=False, ensure_2d=False, warn_on_dtype=True, dtype=FLOAT_DTYPES) original_ndim = X.ndim
if hasattr(self, 'scale_'): del self.scale_ del self.n_samples_seen_ del self.mean_ del self.var_
self._reset() return self.partial_fit(X, y)
if hasattr(self, 'scale_'): del self.scale_ del self.n_samples_seen_ del self.max_abs_
self._reset() return self.partial_fit(X, y)
if not hasattr(self, 'n_samples_seen_'): self.n_samples_seen_ = X.shape[0] else: max_abs = np.maximum(self.max_abs_, max_abs) self.n_samples_seen_ += X.shape[0]
X = check_array(X, accept_sparse=('csr', 'csc'), copy=False, ensure_2d=False, dtype=FLOAT_DTYPES) original_ndim = X.ndim
XP = np.empty((n_samples, self.n_output_features_), dtype=X.dtype)
return X
return transform(X)
imputer = Imputer(missing_values, strategy=strategy, axis=0) imputer.fit(sparse.csc_matrix(X)) X_trans = imputer.transform(sparse.csc_matrix(X.copy()))
X = np.random.randn(10, 2) X[::2] = np.nan
length = arr.size if hasattr(arr, 'size') else len(arr) return np.nan if length == 0 else np.median(arr, *args, **kwargs)
length = arr.size if hasattr(arr, 'size') else len(arr) return np.nan if length == 0 else np.mean(arr, *args, **kwargs)
rng = np.random.RandomState(0)
X[:, j] = np.hstack((v, z, p))
np.random.RandomState(j).shuffle(X[:, j]) np.random.RandomState(j).shuffle(X_true[:, j])
if strategy == "median": cols_to_keep = ~np.isnan(X_true).any(axis=0) else: cols_to_keep = ~np.isnan(X_true).all(axis=0)
X = np.array([
pipeline = Pipeline([('imputer', Imputer(missing_values=0)), ('tree', tree.DecisionTreeRegressor(random_state=0))])
import pickle
X_orig = sparse_random_matrix(5, 5, density=0.75, random_state=0)
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
X_scaled_back = scaler.inverse_transform(X_scaled) assert_array_almost_equal(X_scaled_back, X)
X_list = [1., 3., 5., 0.] X_arr = np.array(X_list)
x_scaled = assert_no_warnings(scale, x) assert_array_almost_equal(scale(x), np.zeros(8))
rng = np.random.RandomState(0) n_features = 5 n_samples = 4 X = rng.randn(n_samples, n_features)
assert_true(X_scaled is not X)
X_scaled_back = scaler.inverse_transform(X_scaled) assert_true(X_scaled_back is not X) assert_true(X_scaled_back is not X_scaled) assert_array_almost_equal(X_scaled_back, X)
assert_true(X_scaled is not X)
assert_true(X_scaled is X)
assert_true(X_scaled is not X)
X = X_2d n = X.shape[0]
scaler_batch = MinMaxScaler().fit(X)
batch0 = slice(0, chunk_size) scaler_batch = MinMaxScaler().fit(X[batch0]) scaler_incr = MinMaxScaler().partial_fit(X[batch0])
scaler_batch = MinMaxScaler().fit(X)
X = X_2d n = X.shape[0]
scaler_batch = StandardScaler(with_std=False).fit(X)
scaler_batch = StandardScaler().fit(X)
scaler = StandardScaler(with_mean=False).fit(X) scaler_incr = StandardScaler(with_mean=False)
scaler_incr = scaler_incr.partial_fit(chunk)
X = np.array([[1.], [0.], [0.], [5.]]) X_csr = sparse.csr_matrix(X) X_csc = sparse.csc_matrix(X)
X = X_2d[:100, :]
assert_equal((i + 1), scaler_incr.n_samples_seen_)
scaler = MinMaxScaler(feature_range=(2, 1)) assert_raises(ValueError, scaler.fit, X)
X = [[0., 1., +0.5], [0., 1., -0.1], [0., 1., +1.1]]
X_trans = minmax_scale(X) assert_array_almost_equal(X_trans, X_expected_0_1) X_trans = minmax_scale(X, feature_range=(1, 2)) assert_array_almost_equal(X_trans, X_expected_1_2)
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
X_scaled_back = scaler.inverse_transform(X_scaled) assert_array_almost_equal(X_scaled_back, X)
assert_true(X_scaled is not X) assert_true(X_csr_scaled is not X_csr)
rng = np.random.RandomState(42) X = rng.randint(20, size=(4, 5))
assert_true(X_scaled is not X) assert_true(X_csr_scaled is not X_csr)
rng = np.random.RandomState(42) X = rng.randn(4, 5)
assert_raises(ValueError, scale, X_csr, with_mean=True) assert_raises(ValueError, StandardScaler(with_mean=True).fit, X_csr)
scaler = StandardScaler(with_mean=True).fit(X) assert_raises(ValueError, scaler.transform, X_csr) assert_raises(ValueError, scaler.transform, X_csc)
X = [np.nan, 5, 6, 7, 8] assert_raises_regex(ValueError, "Input contains NaN, infinity or a value too large", scale, X)
X_csc_scaled = scale(X_csr.tocsc(), with_mean=False) assert_array_almost_equal(X_scaled, X_csc_scaled.toarray())
assert_raises(ValueError, scale, X_csr, with_mean=False, axis=1)
assert_true(X_scaled is not X)
X_csr_scaled = scale(X_csr, with_mean=False, with_std=False, copy=True) assert_array_almost_equal(X_csr.toarray(), X_csr_scaled.toarray())
X_trans = maxabs_scale(X) assert_array_almost_equal(X_trans, X_expected)
X = np.array([[1, 2, 0], [0, 0, 0]], dtype=np.uint8)
for X in [X_1row, X_1col, X_list_1row, X_list_1row]:
X_scaled_back = scaler.inverse_transform(X_scaled) assert_array_almost_equal(X_scaled_back, X)
X_1d = X_1row.ravel() max_abs = np.abs(X_1d).max() assert_array_almost_equal(X_1d / max_abs, maxabs_scale(X_1d, copy=True))
X = X_2d[:100, :] n = X.shape[0]
scaler_batch = MaxAbsScaler().fit(X)
batch0 = slice(0, chunk_size) scaler_batch = MaxAbsScaler().fit(X[batch0]) scaler_incr = MaxAbsScaler().partial_fit(X[batch0])
scaler_batch = MaxAbsScaler().fit(X)
X_dense[3, :] = 0.0
indptr_3 = X_sparse_unpruned.indptr[3] indptr_4 = X_sparse_unpruned.indptr[4] X_sparse_unpruned.data[indptr_3:indptr_4] = 0.0
X_sparse_pruned = sparse.csr_matrix(X_dense)
for X in (X_dense, X_sparse_pruned, X_sparse_unpruned):
X_dense[3, :] = 0.0
indptr_3 = X_sparse_unpruned.indptr[3] indptr_4 = X_sparse_unpruned.indptr[4] X_sparse_unpruned.data[indptr_3:indptr_4] = 0.0
X_sparse_pruned = sparse.csr_matrix(X_dense)
for X in (X_dense, X_sparse_pruned, X_sparse_unpruned):
X_dense[3, :] = 0.0
indptr_3 = X_sparse_unpruned.indptr[3] indptr_4 = X_sparse_unpruned.indptr[4] X_sparse_unpruned.data[indptr_3:indptr_4] = 0.0
X_sparse_pruned = sparse.csr_matrix(X_dense)
for X in (X_dense, X_sparse_pruned, X_sparse_unpruned):
assert_raises(ValueError, binarizer.transform, sparse.csc_matrix(X))
centerer = KernelCenterer() K_fit_centered = np.dot(X_fit_centered, X_fit_centered.T) K_fit_centered2 = centerer.fit_transform(K_fit) assert_array_almost_equal(K_fit_centered, K_fit_centered2)
assert_array_equal(X_trans, [[0., 1., 0., 1., 1.], [1., 0., 1., 0., 1.]])
assert_raises(ValueError, enc.fit, [[0], [-1]])
enc.fit([[0], [1]]) assert_raises(ValueError, enc.transform, [[0], [-1]])
cat = [False, False, False] _check_one_hot(X, X2, cat, 3)
cat = [True, True, True] _check_one_hot(X, X2, cat, 5)
oh = OneHotEncoder(handle_unknown='error') oh.fit(X) assert_raises(ValueError, oh.transform, y)
oh = OneHotEncoder(handle_unknown='42') oh.fit(X) assert_raises(ValueError, oh.transform, y)
scalers = [StandardScaler(with_mean=False, with_std=False), MinMaxScaler(), MaxAbsScaler()]
scaler.fit_transform(X_2d)
testing.assert_array_equal( FunctionTransformer(np.log1p).transform(X), np.log1p(X), )
testing.assert_array_equal(F.transform(X), np.around(X, decimals=3))
testing.assert_array_equal(F.transform(X), np.around(X, decimals=1))
testing.assert_array_equal(F.transform(X), np.around(X, decimals=1))
one_class = np.array([0, 0, 0, 0]) lb = LabelBinarizer().fit(one_class)
assert_raises(ValueError, _inverse_binarize_thresholding, y=csr_matrix([[1, 2], [2, 1]]), output_type="foo", classes=[1, 2], threshold=0)
y_seq_of_seqs = [[], [1, 2], [3], [0, 1, 3], [2]] assert_raises(ValueError, LabelBinarizer().fit_transform, y_seq_of_seqs)
assert_raises(ValueError, _inverse_binarize_thresholding, y=csr_matrix([[1, 2], [2, 1]]), output_type="foo", classes=[1, 2, 3], threshold=0)
assert_raises(ValueError, _inverse_binarize_thresholding, y=np.array([[1, 2, 3], [2, 1, 3]]), output_type="binary", classes=[1, 2, 3], threshold=0)
le = LabelEncoder() ret = le.fit_transform([1, 1, 4, 5, -1, 0]) assert_array_equal(ret, [2, 2, 3, 4, 0, 1])
le = LabelEncoder() assert_raises(ValueError, le.transform, []) assert_raises(ValueError, le.inverse_transform, [])
le = LabelEncoder() le.fit([1, 2, 3, 1, -1]) assert_raises(ValueError, le.inverse_transform, [-1])
mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit_transform(inp), indicator_mat) assert_array_equal(mlb.classes_, [1, 3, 2])
mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat) assert_array_equal(mlb.classes_, [1, 3, 2])
inp = iter(inp) mlb = MultiLabelBinarizer(classes=[1, 3, 2]) assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat)
mlb = MultiLabelBinarizer() assert_array_equal(mlb.fit(inp).transform(inp), indicator_mat) assert_array_equal(mlb.inverse_transform(indicator_mat), inp)
mlb = MultiLabelBinarizer() assert_array_equal(mlb.fit_transform(inp), indicator_mat) assert_array_equal(mlb.classes_, classes) assert_array_equal(mlb.inverse_transform(indicator_mat), inp)
assert_raises(ValueError, mlb.inverse_transform, np.array([[1]])) assert_raises(ValueError, mlb.inverse_transform, np.array([[1, 1, 1]]))
binarized = label_binarize(y, classes, neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output) assert_array_equal(toarray(binarized), expected) assert_equal(issparse(binarized), sparse_output)
y_type = type_of_target(y) if y_type == "multiclass": inversed = _inverse_binarize_multiclass(binarized, classes=classes)
base_estimator = LinearSVC(random_state=0)
mean_proba = np.zeros((X.shape[0], len(self.classes_))) for calibrated_classifier in self.calibrated_classifiers_: proba = calibrated_classifier.predict_proba(X) mean_proba += proba
if n_classes == 2: proba[:, 0] = 1. - proba[:, 1] else: proba /= np.sum(proba, axis=1)[:, np.newaxis]
proba[np.isnan(proba)] = 1. / n_classes
proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0
** blas_info)
loss_l = self.loss.lower()
loss_l = self.loss.lower()
n_class = dual_coef.shape[0] + 1
alpha1 = dual_coef[class2 - 1, sv_locs[class1]:sv_locs[class1 + 1]] alpha2 = dual_coef[class1, sv_locs[class2]:sv_locs[class2 + 1]]
_sparse_kernels = ["linear", "poly", "rbf", "sigmoid", "precomputed"]
kernel = self.kernel return kernel == "precomputed" or callable(kernel)
self.class_weight_ = np.empty(0) return column_or_1d(y, warn=True).astype(np.float64)
kernel = self.kernel if callable(kernel): kernel = 'precomputed'
X = self._validate_for_predict(X) X = self._compute_kernel(X)
if self._impl in ['c_svc', 'nu_svc'] and len(self.classes_) == 2: return -dec_func.ravel()
if sp.issparse(coef): coef.data.flags.writeable = False else: coef.flags.writeable = False return coef
coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)
n_iter_ = max(n_iter_) if n_iter_ >= max_iter and verbose > 0: warnings.warn("Liblinear failed to converge, increase " "the number of iterations.", ConvergenceWarning)
X_blobs, y_blobs = make_blobs(n_samples=100, centers=10, random_state=0) X_blobs = sparse.csr_matrix(X_blobs)
digits = load_digits() X, y = digits.data[:50], digits.target[:50] X_test = sparse.csr_matrix(digits.data[50:100])
assert_array_almost_equal(coef_dense, coef_sorted.toarray())
assert_false(X_sparse_unsorted.has_sorted_indices) assert_false(X_test_unsorted.has_sorted_indices)
assert_array_almost_equal(coef_unsorted.toarray(), coef_sorted.toarray()) assert_array_almost_equal(sparse_svc.predict_proba(X_test_unsorted), sparse_svc.predict_proba(X_test))
svc = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo') clf = svc.fit(iris.data, iris.target)
assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)
clf = svm.NuSVC(nu=0.0) assert_raises(ValueError, clf.fit, X_sp, Y)
clf = svm.LinearSVC(random_state=0).fit(X, Y) sp_clf = svm.LinearSVC(random_state=0).fit(X_sp, Y)
pred = np.argmax(sp_clf.decision_function(iris.data), 1) assert_array_almost_equal(pred, clf.predict(iris.data.toarray()))
clf.sparsify() assert_array_equal(pred, clf.predict(iris.data)) sp_clf.sparsify() assert_array_equal(pred, sp_clf.predict(iris.data))
X_, y_ = make_classification(n_samples=200, n_features=100, weights=[0.833, 0.167], random_state=0)
clf = svm.SVC() clf.fit(X_sp, Y) assert_array_equal(clf.predict([X[2]]), [1.])
test_svm.test_dense_liblinear_intercept_handling(svm.LinearSVC)
X_blobs, _ = make_blobs(n_samples=100, centers=10, random_state=0) X_blobs = sparse.csr_matrix(X_blobs)
a = svm.SVC(C=1, kernel=lambda x, y: x * y.T, probability=True, random_state=0) b = base.clone(a)
iris = datasets.load_iris() rng = check_random_state(42) perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
pred2 = svm.libsvm.cross_validation(iris.data, iris.target.astype(np.float64), 5, kernel='linear', random_seed=0) assert_array_equal(pred, pred2)
KT = np.zeros_like(KT) for i in range(len(T)): for j in clf.support_: KT[i, j] = np.dot(T[i], X[j])
clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]])) clf.fit(X, y) assert_raises(ValueError, clf.predict, X)
clf = svm.OneClassSVM() clf.fit(X) pred = clf.predict(T)
clf = svm.OneClassSVM() rnd = check_random_state(2)
X = 0.3 * rnd.randn(100, 2) X_train = np.r_[X + 2, X - 2]
clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1) clf.fit(X_train)
clf = svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo').fit(iris.data, iris.target)
clf = svm.SVC(kernel='rbf', gamma=1, decision_function_shape='ovo') clf.fit(X, Y)
X, y = make_blobs(n_samples=80, centers=5, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
reg = svm.SVR(kernel='linear', C=0.1).fit(X, y)
reg = svm.SVR(kernel='rbf', gamma=1).fit(X, y)
clf = svm.SVC(class_weight={1: 0.1}) clf.fit(X, Y) assert_array_almost_equal(clf.predict(X), [2] * 6)
assert_raises(ValueError, svm.SVC(C=-1).fit, X, Y)
clf = svm.NuSVC(nu=0.0) assert_raises(ValueError, clf.fit, X, Y)
clf = svm.SVC(kernel='precomputed') assert_raises(ValueError, clf.fit, X, Y)
clf = svm.SVC() assert_raises(ValueError, clf.fit, X, Y, sample_weight=range(len(X) - 1))
clf = svm.SVC().fit(X, Y) assert_raises(ValueError, clf.predict, sparse.lil_matrix(X))
assert_raises_regexp(ValueError, ".*loss='l3' is not supported.*", svm.LinearSVC(loss="l3").fit, X, y)
def test_linearsvx_loss_penalty_deprecations(): X, y = [[0.0], [1.0]], [0, 1]
clf = svm.LinearSVC(random_state=0).fit(X, Y)
assert_true(clf.fit_intercept)
clf = svm.LinearSVC(penalty='l1', loss='squared_hinge', dual=False, random_state=0).fit(X, Y) assert_array_equal(clf.predict(T), true_result)
clf = svm.LinearSVC(penalty='l2', dual=True, random_state=0).fit(X, Y) assert_array_equal(clf.predict(T), true_result)
clf = svm.LinearSVC(penalty='l2', loss='hinge', dual=True, random_state=0) clf.fit(X, Y) assert_array_equal(clf.predict(T), true_result)
dec = clf.decision_function(T) res = (dec > 0).astype(np.int) + 1 assert_array_equal(res, true_result)
assert_true((ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > .9)
assert_true((ovr_clf.coef_ != cs_clf.coef_).all())
X, y = make_classification(n_classes=2, random_state=0)
clf.intercept_scaling = 1 clf.fit(X, y) assert_almost_equal(clf.intercept_, 0, decimal=5)
clf.intercept_scaling = 100 clf.fit(X, y) intercept1 = clf.intercept_ assert_less(intercept1, -1)
clf.intercept_scaling = 1000 clf.fit(X, y) intercept2 = clf.intercept_ assert_array_almost_equal(intercept1, intercept2, decimal=2)
X = [[2, 1], [3, 1], [1, 3], [2, 3]] y = [0, 0, 1, 1]
import os
clf = svm.LinearSVC(verbose=1) clf.fit(X, Y)
X = np.random.RandomState(21).randn(10, 3) y = np.random.RandomState(12).randn(10)
param_grid = [param_grid]
product = partial(reduce, operator.mul) return sum(product(len(v) for v in p.values()) if p else 1 for p in self.param_grid)
for sub_grid in self.param_grid: if not sub_grid: if ind == 0: return {} else: ind -= 1 continue
keys, values_lists = zip(*sorted(sub_grid.items())[::-1]) sizes = [len(v_list) for v_list in values_lists] total = np.product(sizes)
ind -= total
all_lists = np.all([not hasattr(v, "rvs") for v in self.param_distributions.values()]) rnd = check_random_state(self.random_state)
param_grid = ParameterGrid(self.param_distributions) grid_size = len(param_grid)
n_fits = len(out) n_folds = len(cv)
best = sorted(grid_scores, key=lambda x: x.mean_validation_score, reverse=True)[0] self.best_params_ = best.parameters self.best_score_ = best.mean_validation_score
from __future__ import division
raise ValueError("The constant target value must be " "present in training data")
n_samples = int(X.shape[0]) rs = check_random_state(self.random_state)
n_classes_ = [n_classes_] classes_ = [classes_] class_prior_ = [class_prior_] constant = [constant]
n_samples = int(X.shape[0]) rs = check_random_state(self.random_state)
n_classes_ = [n_classes_] classes_ = [classes_] class_prior_ = [class_prior_] constant = [constant]
self.converged_ = False
current_log_likelihood = None self.converged_ = False
log_likelihoods, responsibilities = self.score_samples(X) current_log_likelihood = log_likelihoods.mean()
responsibilities = np.zeros((X.shape[0], self.n_components))
ESTIMATE_PRECISION_ERROR_MESSAGE = ("The algorithm has diverged because of " "too few samples per components. Try to " "decrease the number of components, " "or increase reg_covar.")
_, n_features = self.means_.shape
self._check_parameters(X)
do_init = not(self.warm_start and hasattr(self, 'converged_')) n_init = self.n_init if do_init else 1
log_resp = weighted_log_prob - log_prob_norm[:, np.newaxis]
X, y = make_blobs(random_state=1) for Model in [DPGMM, VBGMM]: dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=1)
X, y = make_blobs(random_state=1) for Model in [DPGMM, VBGMM]: dpgmm = Model(n_components=10, random_state=1, alpha=20, n_iter=50, verbose=2)
cv = (rng.rand() + 1.0) ** 2 samples = mixture.sample_gaussian( mu, cv, covariance_type='spherical', n_samples=n_samples)
@ignore_warnings(category=DeprecationWarning) def test_eval(self): if not self.do_test_eval:
X = g.sample(n_samples=100) g = self.model(n_components=self.n_components, covariance_type=self.covariance_type, random_state=rng, min_covar=1e-1, n_iter=1, init_params=params) g.fit(X)
@ignore_warnings(category=DeprecationWarning) def score(self, g, X): return g.score(X).sum()
@ignore_warnings(category=DeprecationWarning) def test_aic(): n_samples, n_dim, n_components = 50, 3, 2 X = rng.randn(n_samples, n_dim)
X = rng.randn(100, 2)
gmm.fit(X)
for covariance_type in ["full", "tied", "diag", "spherical"]: yield check_positive_definite_covars, covariance_type
@ignore_warnings(category=DeprecationWarning) def test_verbose_first_level(): X = rng.randn(30, 5) X[:10] += 2 g = mixture.GMM(n_components=2, n_init=2, verbose=1)
@ignore_warnings(category=DeprecationWarning) def test_verbose_second_level(): X = rng.randn(30, 5) X[:10] += 2 g = mixture.GMM(n_components=2, n_init=2, verbose=2)
rng = np.random.RandomState(0) X = rng.rand(10, 2)
weights = rand_data.weights g = GaussianMixture(weights_init=weights, n_components=n_components) g.fit(X) assert_array_equal(weights, g.weights_init)
means = rand_data.means g.means_init = means g.fit(X) assert_array_equal(means, g.means_init)
precisions_not_pos = np.ones((n_components, n_features, n_features)) precisions_not_pos[0] = np.eye(n_features) precisions_not_pos[0, 0, 0] = -1.
g.precisions_init = precisions_bad_shape[covar_type] assert_raise_message(ValueError, "The parameter '%s precision' should have " "the shape of" % covar_type, g.fit, X)
g.precisions_init = precisions_not_positive[covar_type] assert_raise_message(ValueError, "'%s precision' should be %s" % (covar_type, not_positive_errors[covar_type]), g.fit, X)
g.precisions_init = rand_data.precisions[covar_type] g.fit(X) assert_array_equal(rand_data.precisions[covar_type], g.precisions_init)
rng = np.random.RandomState(0) n_samples, n_features = 500, 2
rng = np.random.RandomState(0) n_samples, n_features, n_components = 500, 2, 2
rng = np.random.RandomState(0) n_samples, n_features, n_components = 500, 2, 2
rng = np.random.RandomState(0) n_samples, n_features = 500, 2
rng = np.random.RandomState(0) rand_data = RandomData(rng) n_samples = 500 n_features = rand_data.n_features n_components = rand_data.n_components
precs_full = np.array([np.diag(1. / np.sqrt(x)) for x in covars_diag])
precs_chol_diag = 1. / np.sqrt(covars_diag) log_prob = _estimate_log_gaussian_prob_diag(X, means, precs_chol_diag) assert_array_almost_equal(log_prob, log_prob_naive)
covars_tied = np.array([x for x in covars_diag]).mean(axis=0) precs_tied = np.diag(np.sqrt(1. / covars_tied))
rng = np.random.RandomState(0) rand_data = RandomData(rng, scale=5) n_samples = rand_data.n_samples n_features = rand_data.n_features n_components = rand_data.n_components
assert_raise_message(NotFittedError, "This GaussianMixture instance is not fitted " "yet. Call 'fit' with appropriate arguments " "before using this method.", g.predict, X)
rng = np.random.RandomState(0) rand_data = RandomData(rng) n_features = rand_data.n_features n_components = rand_data.n_components
assert_allclose(np.sort(g.weights_), np.sort(rand_data.weights), rtol=0.1, atol=1e-2)
assert_allclose(ecov.error_norm(prec_pred[k]), 0, atol=0.1)
g = GaussianMixture(n_components=n_components, n_init=1, max_iter=2, reg_covar=0, random_state=random_state, warm_start=False) h = GaussianMixture(n_components=n_components, n_init=1, max_iter=1, reg_covar=0, random_state=random_state, warm_start=True)
g = GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-6) h = GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-6)
gmm2 = GaussianMixture(n_components=n_components, n_init=1, reg_covar=0, random_state=rng, covariance_type=covar_type).fit(X) assert_greater(gmm2.score(X), gmm1.score(X))
rng = np.random.RandomState(0) rand_data = RandomData(rng, scale=7) n_components = rand_data.n_components
for _ in range(300): prev_log_likelihood = current_log_likelihood try: current_log_likelihood = gmm.fit(X).score(X) except ConvergenceWarning: pass assert_greater_equal(current_log_likelihood, prev_log_likelihood)
rng = np.random.RandomState(0) n_samples, n_features = 10, 5
del dgamma1, dgamma2, sd
X = check_array(X) if X.ndim == 1: X = X[:, np.newaxis]
current_log_likelihood = None self.converged_ = False
curr_logprob, z = self.score_samples(X)
if prev_log_likelihood is not None: change = abs(current_log_likelihood - prev_log_likelihood) if change < self.tol: self.converged_ = True break
self._do_mstep(X, z, self.params)
z = np.zeros((X.shape[0], self.n_components))
n_samples_per_label = np.bincount(labels)
indices = np.argsort(n_samples_per_label)[::-1] n_samples_per_label = n_samples_per_label[indices]
n_samples_per_fold = np.zeros(n_folds)
label_to_fold = np.zeros(len(unique_labels))
for label_index, weight in enumerate(n_samples_per_label): lightest_fold = np.argmin(n_samples_per_fold) n_samples_per_fold[lightest_fold] += weight label_to_fold[indices[label_index]] = lightest_fold
if self.shuffle: rng = check_random_state(self.random_state) else: rng = self.random_state
self.labels = np.array(labels, copy=True) self.unique_labels = np.unique(labels) self.n_unique_labels = len(self.unique_labels)
permutation = rng.permutation(self.n) ind_test = permutation[:self.n_test] ind_train = permutation[self.n_test:self.n_test + self.n_train] yield ind_train, ind_test
return v
if sp.issparse(preds[0]): preds = sp.vstack(preds, format=preds[0].format) else: preds = np.concatenate(preds) return preds[inv_locs]
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
raise ValueError("Cannot use a custom kernel function. " "Precompute the kernel matrix instead.")
n_features = X.shape[1] if self.n_features_to_select is None: n_features_to_select = n_features // 2 else: n_features_to_select = self.n_features_to_select
while np.sum(support_) > n_features_to_select: features = np.arange(n_features)[support_]
estimator = clone(self.estimator) if self.verbose > 0: print("Fitting estimator with %d features." % np.sum(support_))
if coefs.ndim > 1: ranks = np.argsort(safe_sqr(coefs).sum(axis=0)) else: ranks = np.argsort(safe_sqr(coefs))
ranks = np.ravel(ranks)
threshold = min(step, np.sum(support_) - n_features_to_select)
if step_score: self.scores_.append(step_score(estimator, features)) support_[features[ranks][:threshold]] = False ranking_[np.logical_not(support_)] += 1
features = np.arange(n_features)[support_] self.estimator_ = clone(self.estimator) self.estimator_.fit(X[:, features], y)
if step_score: self.scores_.append(step_score(self.estimator_, features)) self.n_features_ = support_.sum() self.support_ = support_ self.ranking_ = ranking_
rfe = RFE(estimator=self.estimator, n_features_to_select=n_features_to_select, step=self.step)
self.grid_scores_ = scores[::-1] / cv.get_n_splits(X, y) return self
from __future__ import division
nn = NearestNeighbors(metric='chebyshev', n_neighbors=n_neighbors)
nn.set_params(algorithm='kd_tree')
mask = label_counts > 1 n_samples = np.sum(mask) label_counts = label_counts[mask] k_all = k_all[mask] c = c[mask] radius = radius[mask]
try: mask = importances >= threshold except TypeError: raise ValueError("Invalid threshold: all features are discarded.")
names_t_actual = sel.transform([feature_names]) assert_array_equal(feature_names_t, names_t_actual.ravel())
assert_raises(ValueError, sel.transform, np.array([[1], [2]]))
assert_raises(ValueError, sel.transform, np.array([[1], [2]]))
names_inv_actual = sel.inverse_transform([feature_names_t]) assert_array_equal(feature_names_inv, names_inv_actual.ravel())
assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))
assert_raises(ValueError, sel.inverse_transform, np.array([[1], [2]]))
X = [[2, 1, 2], [9, 1, 1], [6, 1, 2], [0, 1, 2]] y = [0, 1, 2, 2]
Xtrans = Xtrans.toarray() Xtrans2 = mkchi2(k=2).fit_transform(Xsp, y).toarray() assert_equal(Xtrans, Xtrans2)
Xcoo = coo_matrix(X) mkchi2(k=2).fit_transform(Xcoo, y)
assert_array_equal(rfe.get_support(), rfe_svc.get_support())
clf_sparse = SVC(kernel="linear") rfe_sparse = RFE(estimator=clf_sparse, n_features_to_select=4, step=0.1) rfe_sparse.fit(X_sparse, y) X_r_sparse = rfe_sparse.transform(X_sparse)
assert_array_equal(X_r, iris.data)
iris = load_iris() score = cross_val_score(rfe, iris.data, iris.target) assert_greater(score.min(), .7)
selector = RFE(estimator, step=0.01) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
selector = RFE(estimator, step=0.20) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
selector = RFE(estimator, step=5) sel = selector.fit(X, y) assert_equal(sel.support_.sum(), n_features // 2)
f, p = f_oneway(X.astype(np.float), y) assert_array_almost_equal(f, fint, decimal=4) assert_array_almost_equal(p, pint, decimal=4)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
rng = np.random.RandomState(0) X = rng.rand(10, 20) y = np.arange(10).astype(np.int)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
assert_equal(X_r2inv.getnnz(), X_r.getnnz())
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=20, n_features=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=20, n_features=10, shuffle=False, random_state=0)
X, y = make_classification(n_samples=200, n_features=20, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, flip_y=0.0, class_sep=10, shuffle=False, random_state=0)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
assert_array_equal(X_2.astype(bool), univariate_filter.inverse_transform(X_r.astype(bool)))
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0, noise=10)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0, noise=10)
def single_fdr(alpha, n_informative, random_state): X, y = make_regression(n_samples=150, n_features=20, n_informative=n_informative, shuffle=False, random_state=random_state, noise=10)
false_discovery_rate = np.mean([single_fdr(alpha, n_informative, random_state) for random_state in range(30)]) assert_greater_equal(alpha, false_discovery_rate)
if false_discovery_rate != 0: assert_greater(false_discovery_rate, alpha / 10)
X, y = make_regression(n_samples=200, n_features=20, n_informative=5, shuffle=False, random_state=0)
X0 = np.array([[10000, 9999, 9998], [1, 1, 1]]) y = [0, 1]
X_train = np.array([[0, 0, 0], [1, 1, 1]]) y_train = [0, 1]
X = [[0, 1, 0], [0, -1, -1], [0, .5, .5]] y = [1, 0, 1]
x = np.array([0, 1, 1, 0, 0]) y = np.array([1, 0, 0, 0, 1])
mean = np.zeros(2)
sigma_1 = 1 sigma_2 = 10 corr = 0.5 cov = np.array([ [sigma_1**2, corr * sigma_1 * sigma_2], [corr * sigma_1 * sigma_2, sigma_2**2] ])
I_theory = (np.log(sigma_1) + np.log(sigma_2) - 0.5 * np.log(np.linalg.det(cov)))
for n_neighbors in [3, 5, 7]: I_computed = _compute_mi(x, y, False, False, n_neighbors) assert_almost_equal(I_computed, I_theory, 1)
for n_neighbors in [3, 5, 7]: I_computed = _compute_mi(x, y, True, False, n_neighbors) assert_almost_equal(I_computed, I_theory, 1)
n_samples = 100 x = np.random.uniform(size=n_samples) > 0.5
mi = mutual_info_classif(X, y, discrete_features=True) assert_array_equal(np.argsort(-mi), np.array([0, 2, 1]))
for X in [data, csr_matrix(data)]: X = VarianceThreshold(threshold=.4).fit_transform(X) assert_equal((len(data), 1), X.shape)
sample_weight = np.ones(y.shape) sample_weight[y == 1] *= 100
model = SelectFromModel(clf, prefit=False) model.fit(data, y) assert_array_equal(model.transform(data), X_transform)
model = SelectFromModel(clf, prefit=True) assert_raises(ValueError, model.fit, data, y)
est.fit(data, y) threshold = 0.5 * np.mean(est.feature_importances_) mask = est.feature_importances_ > threshold assert_array_equal(X_transform, data[:, mask])
model.threshold = 1.0 assert_greater(X_transform.shape[1], model.transform(data).shape[1])
scores = as_float_array(scores, copy=True) scores[np.isnan(scores)] = np.finfo(scores.dtype).min return scores
f = np.asarray(f).ravel() prob = special.fdtrc(dfbn, dfwn, f) return f, prob
chisq = f_obs chisq -= f_exp chisq **= 2 chisq /= f_exp chisq = chisq.sum(axis=0) return chisq, special.chdtrc(k - 1, chisq)
corr = safe_sparse_dot(y, X) corr /= row_norms(X.T) corr /= norm(y)
mask[np.argsort(scores, kind="mergesort")[-self.k:]] = 1 return mask
possible_params = selector._get_param_names() possible_params.remove('score_func') selector.set_params(**{possible_params[0]: self.param})
if ((X.data if sparse else X) < 0).any(): raise ValueError("Entries of X must be non-negative.")
exp_doc_topic = np.exp(_dirichlet_expectation_2d(doc_topic_distr))
suff_stats = np.zeros(exp_topic_word_distr.shape) if cal_sstats else None
exp_doc_topic_d = exp_doc_topic[idx_d, :].copy() exp_topic_word_d = exp_topic_word_distr[:, ids]
for _ in xrange(0, max_iters): last_d = doc_topic_d
norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d) + EPS
if cal_sstats: norm_phi = np.dot(exp_doc_topic_d, exp_topic_word_d) + EPS suff_stats[:, ids] += np.outer(exp_doc_topic_d, cnts / norm_phi)
self.components_ = self.random_state_.gamma( init_gamma, init_var, (self.n_topics, n_features))
self.exp_dirichlet_component_ = np.exp( _dirichlet_expectation_2d(self.components_))
random_state = self.random_state_ if random_init else None
doc_topics, sstats_list = zip(*results) doc_topic_distr = np.vstack(doc_topics)
suff_stats = np.zeros(self.components_.shape) for sstats in sstats_list: suff_stats += sstats suff_stats *= self.exp_dirichlet_component_
_, suff_stats = self._e_step(X, cal_sstats=True, random_init=True, parallel=parallel)
self.exp_dirichlet_component_ = np.exp( _dirichlet_expectation_2d(self.components_)) self.n_batch_iter_ += 1 return
if not hasattr(self, 'components_'): self._init_latent_vars(n_features)
doc_topic_distr /= doc_topic_distr.sum(axis=1)[:, np.newaxis] return doc_topic_distr
score += _loglikelihood(doc_topic_prior, doc_topic_distr, dirichlet_doc_topic, self.n_topics)
if sub_sampling: doc_ratio = float(self.total_samples) / n_samples score *= doc_ratio
score += _loglikelihood(topic_word_prior, self.components_, dirichlet_component_, n_features)
nsqrt = sqrt(n_samples) llconst = n_features * log(2. * np.pi) + n_components var = np.var(X, axis=0)
if self.n_components == 0: return np.diag(1. / self.noise_variance_) if self.n_components == n_features: return linalg.inv(self.get_covariance())
if not hasattr(self, 'n_samples_seen_'): self.n_samples_seen_ = 0 self.mean_ = .0 self.var_ = .0
col_mean, col_var, n_total_samples = \ _incremental_mean_and_var(X, last_mean=self.mean_, last_variance=self.var_, last_sample_count=self.n_samples_seen_)
U, S, V = randomized_svd(X, n_components, random_state=random_state) W, H = np.zeros(U.shape), np.zeros(V.shape)
x_p_nrm, y_p_nrm = norm(x_p), norm(y_p) x_n_nrm, y_n_nrm = norm(x_n), norm(y_n)
if m_p > m_n: u = x_p / x_p_nrm v = y_p / y_p_nrm sigma = m_p else: u = x_n / x_n_nrm v = y_n / y_n_nrm sigma = m_n
if norm(grad * np.logical_or(grad < 0, H > 0)) < tol: break
tolW = max(0.001, tol) * np.sqrt(init_grad) tolH = tolW
proj_grad_W = squared_norm(gradW * np.logical_or(gradW < 0, W > 0)) proj_grad_H = squared_norm(gradH * np.logical_or(gradH < 0, H > 0))
if l2_reg != 0.: HHt.flat[::n_components + 1] += l2_reg if l1_reg != 0.: XHt -= l1_reg
permutation = np.asarray(permutation, dtype=np.intp) return _update_cdnmf_fast(W, HHt, XHt, permutation)
Ht = check_array(H.T, order='C') X = check_array(X, accept_sparse='csr')
U *= sqrt(X.shape[0])
U *= S[:self.n_components_]
if self.n_components is None: n_components = X.shape[1] else: n_components = self.n_components
self.mean_ = np.mean(X, axis=0) X -= self.mean_
U, V = svd_flip(U, V)
explained_variance_ = (S ** 2) / n_samples total_var = explained_variance_.sum() explained_variance_ratio_ = explained_variance_ / total_var
if n_components < min(n_features, n_samples): self.noise_variance_ = explained_variance_[n_components:].mean() else: self.noise_variance_ = 0.
self.mean_ = np.mean(X, axis=0) X -= self.mean_
U, S, V = randomized_svd(X, n_components=n_components, n_iter=self.iterated_power, flip_sign=True, random_state=random_state)
self.mean_ = np.mean(X, axis=0) X -= self.mean_ if self.n_components is None: n_components = X.shape[1] else: n_components = self.n_components
import warnings import numpy as np from scipy import linalg
return np.dot(np.dot(u * (1. / np.sqrt(s)), u.T), W)
for j in range(n_components): w = w_init[j, :].copy() w /= np.sqrt((w ** 2).sum())
lim = max(abs(abs(np.diag(fast_dot(W1, W.T))) - 1)) W = W1 if lim < tol: break
def _logcosh(x, fun_args=None):
X = check_array(X, copy=whiten, dtype=FLOAT_DTYPES).T
X_mean = X.mean(axis=-1) X -= X_mean[:, np.newaxis]
u, d, _ = linalg.svd(X, full_matrices=False)
X1 *= np.sqrt(p)
copy_cov = False cov = np.dot(dictionary, X.T)
lasso_lars = LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, fit_path=False) lasso_lars.fit(dictionary.T, X.T, Xy=cov) new_code = lasso_lars.coef_
lars = Lars(fit_intercept=False, verbose=verbose, normalize=False, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False) lars.fit(dictionary.T, X.T, Xy=cov) new_code = lars.coef_
if code.ndim == 1: code = code[np.newaxis, :] return code
code = np.empty((n_samples, n_components)) slices = list(gen_even_slices(n_samples, _get_n_jobs(n_jobs)))
alpha = float(alpha) random_state = check_random_state(random_state)
dictionary = np.array(dictionary, order='F')
ii = -1
current_cost = 0.5 * residuals + alpha * np.sum(np.abs(code)) errors.append(current_cost)
if dE < tol * errors[-1]: if verbose == 1: print("") elif verbose: print("--- Convergence reached after %d iterations" % ii) break
alpha = float(alpha) random_state = check_random_state(random_state)
ii = iter_offset - 1
dictionary = _update_dict(dictionary, B, A, verbose=verbose, random_state=random_state)
if callback is not None: callback(locals())
X = check_array(X) n_samples, n_features = X.shape
if sp.issparse(X) and X.getformat() not in ["csr", "csc"]: X = X.tocsr()
Sigma = Sigma[::-1] U, VT = svd_flip(U[:, ::-1], VT[::-1])
K = self._centerer.fit_transform(K)
if self.eigen_solver == 'auto': if K.shape[0] > 200 and n_components < 10: eigen_solver = 'arpack' else: eigen_solver = 'dense' else: eigen_solver = self.eigen_solver
indices = self.lambdas_.argsort()[::-1] self.lambdas_ = self.lambdas_[indices] self.alphas_ = self.alphas_[:, indices]
if self.remove_zero_eig or self.n_components is None: self.alphas_ = self.alphas_[:, self.lambdas_ > 0] self.lambdas_ = self.lambdas_[self.lambdas_ > 0]
if self.n_components_ == 0: return np.eye(n_features) / self.noise_variance_ if self.n_components_ == n_features: return linalg.inv(self.get_covariance())
assert_array_almost_equal(comp_a[:9], comp_r[:9]) assert_array_almost_equal(comp_a[9:], comp_r[9:], decimal=2)
tsvd = TruncatedSVD(n_components=52, random_state=42) Xt = tsvd.fit_transform(X) Xinv = tsvd.inverse_transform(Xt) assert_array_almost_equal(Xinv, Xdense, decimal=1)
for svd_10, svd_20 in svds_10_v_20: assert_array_almost_equal( svd_10.explained_variance_ratio_, svd_20.explained_variance_ratio_[:10], decimal=5, )
for svd_10, svd_20 in svds_10_v_20: assert_greater( svd_20.explained_variance_ratio_.sum(), svd_10.explained_variance_ratio_.sum(), )
for svd in svds: assert_array_less(0.0, svd.explained_variance_ratio_)
for svd in svds: assert_array_less(svd.explained_variance_ratio_.sum(), 1.0)
for svd_sparse, svd_dense in svds_sparse_v_dense: assert_array_almost_equal(svd_sparse.explained_variance_ratio_, svd_dense.explained_variance_ratio_)
inv = not callable(kernel)
assert_not_equal(X_fit_transformed.size, 0)
X_pred_transformed = kpca.transform(X_pred) assert_equal(X_pred_transformed.shape[1], X_fit_transformed.shape[1])
if inv: X_pred2 = kpca.inverse_transform(X_pred_transformed) assert_equal(X_pred2.shape, X_pred.shape)
state = np.random.RandomState(0) X = state.rand(10, 10) kpca = KernelPCA(random_state=state).fit(X) transformed1 = kpca.transform(X)
X_pred_transformed = kpca.transform(X_pred) assert_equal(X_pred_transformed.shape[1], X_fit_transformed.shape[1])
kpca = KernelPCA() Xt = kpca.fit_transform(X) assert_equal(Xt.shape, (3, 0))
X, y = make_circles(n_samples=400, factor=.3, noise=.05, random_state=0)
train_score = Perceptron().fit(X, y).score(X, y) assert_less(train_score, 0.8)
kpca = KernelPCA(kernel="rbf", n_components=2, fit_inverse_transform=True, gamma=2.) X_kpca = kpca.fit_transform(X)
train_score = Perceptron().fit(X_kpca, y).score(X_kpca, y) assert_equal(train_score, 1.0)
X = iris.data
cov = pca.get_covariance() precision = pca.get_precision() assert_array_almost_equal(np.dot(cov, precision), np.eye(X.shape[1]), 12)
pca = PCA(svd_solver='full') pca.fit(X) assert_almost_equal(pca.explained_variance_ratio_.sum(), 1.0, 3)
X = iris.data d = X.shape[1]
for n_comp in np.arange(1, d): pca = PCA(n_components=n_comp, svd_solver='arpack', random_state=0)
cov = pca.get_covariance() precision = pca.get_precision() assert_array_almost_equal(np.dot(cov, precision), np.eye(d), 12)
X = iris.data
for n_comp in np.arange(1, X.shape[1]): pca = PCA(n_components=n_comp, svd_solver='randomized', random_state=0)
cov = pca.get_covariance() precision = pca.get_precision() assert_array_almost_equal(np.dot(cov, precision), np.eye(X.shape[1]), 12)
n_components = 10
rng = np.random.RandomState(0) n_samples = 100 n_features = 80 n_components = 30 rank = 50
assert_greater(X.std(axis=0).std(), 43.8)
X_whitened = pca.fit_transform(X_.copy()) assert_equal(X_whitened.shape, (n_samples, n_components)) X_whitened2 = pca.transform(X_) assert_array_almost_equal(X_whitened, X_whitened2)
assert_almost_equal(X_unwhitened.std(axis=0).std(), 74.1, 1)
@ignore_warnings def test_explained_variance(): rng = np.random.RandomState(0) n_samples = 100 n_features = 80
X_pca = pca.transform(X) assert_array_almost_equal(pca.explained_variance_, np.var(X_pca, axis=0))
X = datasets.make_classification(n_samples, n_features, n_informative=n_features-2, random_state=rng)[0]
rng = np.random.RandomState(0) n, p = 50, 3
rng = np.random.RandomState(0) n, p = 50, 3
pca = PCA(n_components=2, whiten=True, svd_solver=solver) pca.fit(X) ll2 = pca.score(X) assert_true(ll1 > ll2)
pca = PCA(n_components=50) pca.fit(X) pca_test = PCA(n_components=50, svd_solver='full') pca_test.fit(X) assert_array_almost_equal(pca.components_, pca_test.components_)
A = np.abs(random_state.randn(30, 10)) NMF(n_components=15, random_state=0, tol=1e-2).fit(A)
from scipy.sparse import csc_matrix
A = np.abs(random_state.randn(10, 10)) A[:, 2 * np.arange(5)] = 0
Y = np.dot(U, V)
spca_lasso = SparsePCA(n_components=3, method='cd', random_state=0, alpha=alpha) spca_lasso.fit(Y) assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
rng = np.random.RandomState(0)
spca_lasso = MiniBatchSparsePCA(n_components=3, method='cd', alpha=alpha, random_state=0).fit(Y) assert_array_almost_equal(spca_lasso.components_, spca_lars.components_)
def g_test(x): return x ** 3, (3 * x ** 2).mean(axis=-1)
if whiten: assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))
ica = FastICA(n_components=1, whiten=False, random_state=0) assert_warns(UserWarning, ica.fit, m) assert_true(hasattr(ica, 'mixing_'))
rng = np.random.RandomState(0)
mixing = rng.randn(6, 2) m = np.dot(mixing, s)
assert_almost_equal(s_, np.dot(np.dot(mixing_, k_), m))
if not add_noise: assert_almost_equal(np.dot(s1_, s1) / n_samples, 1, decimal=3) assert_almost_equal(np.dot(s2_, s2) / n_samples, 1, decimal=3)
if n_components == X.shape[1]: assert_array_almost_equal(X, X2)
X = iris.data batch_size = X.shape[0] // 3 ipca = IncrementalPCA(n_components=2, batch_size=batch_size) pca = PCA(n_components=2) pca.fit_transform(X)
Yt = IncrementalPCA(n_components=2).fit(X).transform(Xt)
Yt /= np.sqrt((Yt ** 2).sum())
assert_almost_equal(np.abs(Yt[0][0]), 1., 1)
rng = np.random.RandomState(1999) n, p = 50, 3
ipca = IncrementalPCA(n_components=2, batch_size=10).fit(X) Y = ipca.transform(X) Y_inverse = ipca.inverse_transform(Y) assert_almost_equal(X, Y_inverse, decimal=3)
X = [[0, 1], [1, 0]] for n_components in [-1, 0, .99, 3]: assert_raises(ValueError, IncrementalPCA(n_components, batch_size=10).fit, X)
rng = np.random.RandomState(1999) n, p = 50, 3
X = iris.data
rng = np.random.RandomState(1999) n_samples = 100 n_features = 3 X = rng.randn(n_samples, n_features) + 5 * rng.rand(1, n_features)
from sklearn.externals.six.moves import cStringIO as StringIO import sys
@ignore_warnings def test_factor_analysis(): rng = np.random.RandomState(0) n_samples, n_features, n_components = 20, 5, 3
W = rng.randn(n_components, n_features) h = rng.randn(n_samples, n_components) noise = rng.gamma(1, size=n_features) * rng.randn(n_samples, n_features)
X = np.dot(h, W) + noise
scov = np.cov(X, rowvar=0., bias=1.)
n_topics, X = _build_sparse_mtx() prior = 1. / n_topics lda_1 = LatentDirichletAllocation(n_topics=n_topics, doc_topic_prior=prior, topic_word_prior=prior, random_state=0) lda_2 = LatentDirichletAllocation(n_topics=n_topics, random_state=0)
rng = np.random.RandomState(0) n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, evaluate_every=1, learning_method='batch', random_state=rng) lda.fit(X)
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
top_idx = set(component.argsort()[-3:][::-1]) assert_true(tuple(sorted(top_idx)) in correct_idx_grps)
X = np.ones((5, 10))
X = -np.ones((5, 10)) lda = LatentDirichletAllocation() regex = r"^Negative values in data passed" assert_raises_regexp(ValueError, regex, lda.fit, X)
n_topics, X = _build_sparse_mtx() lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=10, random_state=0) distr = lda.fit_transform(X) perplexity_1 = lda.perplexity(X, distr, sub_sampling=False)
normalizer = proba_k.sum(axis=1)[:, np.newaxis] normalizer[normalizer == 0.0] = 1.0 proba_k /= normalizer
self.centroids_ = np.empty((n_classes, n_features), dtype=np.float64) nk = np.zeros(n_classes)
raise ValueError( "kd_tree algorithm does not support callable metric '%s'" % metric)
return self.metric == 'precomputed'
n_neighbors += 1
neigh_ind = neigh_ind[ sample_range, np.argsort(dist[sample_range, neigh_ind])]
if return_distance: dist, neigh_ind = result else: neigh_ind = result
dup_gr_nbrs = np.all(sample_mask, axis=1) sample_mask[:, 0][dup_gr_nbrs] = False
if X is not None: X = check_array(X, accept_sparse='csr') n_samples1 = X.shape[0] else: n_samples1 = self._fit_X.shape[0]
if mode == 'connectivity': A_data = np.ones(n_samples1 * n_neighbors) A_ind = self.kneighbors(X, n_neighbors, return_distance=False)
neigh_ind = np.empty(n_samples, dtype='object') neigh_ind[:] = neigh_ind_list
if return_distance: dist, neigh_ind = results else: neigh_ind = results
self._choose_algorithm(self.algorithm, self.metric)
out = np.packbits((projected > 0).astype(int)).view(dtype=HASH_DTYPE) return out.reshape(projected.shape[0], -1)
return np.empty(0, dtype=np.int), np.empty(0, dtype=float)
left_mask = np.tril(np.ones((tri_size, tri_size), dtype=int))[:, 1:] right_mask = left_mask[::-1, ::-1]
n_candidates = 0 candidate_set = set() min_candidates = self.n_candidates * self.n_estimators while (max_depth > self.min_hash_match and (n_candidates < min_candidates or len(candidate_set) < n_neighbors)):
self.hash_functions_ = [] self.trees_ = [] self.original_indices_ = []
bin_queries = np.asarray([hasher.transform(X)[:, 0] for hasher in self.hash_functions_]) bin_queries = np.rollaxis(bin_queries, 1)
depths = [_find_longest_prefix_match(tree, tree_queries, MAX_HASH_SIZE, self._left_mask, self._right_mask) for tree, tree_queries in zip(self.trees_, np.rollaxis(bin_queries, 1))]
n_samples = 12 n_features = 2 n_iter = 10 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features)
assert_raises(ValueError, lshf.kneighbors, X[0])
assert_equal(neighbors.shape[1], n_neighbors)
assert_raises(ValueError, lshf.radius_neighbors, X[0])
query = X[rng.randint(0, n_samples)].reshape(1, -1)
mean_dist = np.mean(pairwise_distances(query, X, metric='cosine')) neighbors = lshf.radius_neighbors(query, radius=mean_dist, return_distance=False)
distances, neighbors = lshf.radius_neighbors(query, radius=mean_dist, return_distance=True) assert_array_less(distances[0], mean_dist)
n_queries = 5 queries = X[rng.randint(0, n_samples, n_queries)] distances, neighbors = lshf.radius_neighbors(queries, return_distance=True)
assert_equal(distances.shape, (n_queries,)) assert_equal(distances.dtype, object) assert_equal(neighbors.shape, (n_queries,)) assert_equal(neighbors.dtype, object)
sorted_dists_exact = np.sort(distances_exact[0]) sorted_dists_approx = np.sort(distances_approx[0])
assert_true(np.all(np.less_equal(sorted_dists_exact, sorted_dists_approx)))
nnbrs = NearestNeighbors(algorithm='brute', metric='cosine').fit(X)
lsfh = LSHForest(min_hash_match=0, n_candidates=n_points).fit(X)
query = [[1., 0.]]
dists = pairwise_distances(query, X, metric='cosine').ravel()
assert_almost_equal(dists[0], 0, decimal=5)
assert_almost_equal(dists[1], 1 - np.cos(np.pi / 4))
assert_almost_equal(dists[2], 1)
assert_almost_equal(dists[3], 2, decimal=5)
exact_dists, exact_idx = nnbrs.radius_neighbors(query, radius=1) approx_dists, approx_idx = lsfh.radius_neighbors(query, radius=1)
n_samples = 12 n_features = 2 n_iter = 10 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features)
assert_true(np.all(np.diff(distances[0]) >= 0))
n_samples = 12 n_features = 2 n_estimators = 5 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features)
n_samples = 12 n_samples_partial_fit = 3 n_features = 2 rng = np.random.RandomState(42) X = rng.rand(n_samples, n_features) X_partial_fit = rng.rand(n_samples_partial_fit, n_features)
ignore_warnings(lshf.partial_fit)(X) assert_array_equal(X, lshf._fit_X)
assert_raises(ValueError, lshf.partial_fit, np.random.randn(n_samples_partial_fit, n_features - 1))
assert_equal(lshf._fit_X.shape[0], n_samples + n_samples_partial_fit) assert_equal(len(lshf.original_indices_[0]), n_samples + n_samples_partial_fit) assert_equal(len(lshf.trees_[1]), n_samples + n_samples_partial_fit)
lshf = LSHForest(min_hash_match=32) ignore_warnings(lshf.fit)(X_train)
lshf = LSHForest(min_hash_match=31) ignore_warnings(lshf.fit)(X_train)
n_samples_sizes = [5, 10, 20] n_features = 3 rng = np.random.RandomState(42)
iris = datasets.load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
digits = datasets.load_digits() perm = rng.permutation(digits.target.size) digits.data = digits.data[perm] digits.target = digits.target[perm]
neighbors.kneighbors_graph = ignore_warnings(neighbors.kneighbors_graph) neighbors.radius_neighbors_graph = ignore_warnings( neighbors.radius_neighbors_graph)
with np.errstate(divide='ignore'): retval = 1. / dist return retval ** 2
X = rng.rand(n_samples, n_features)
X = rng.random_sample((10, 3))
nbrs_X = neighbors.NearestNeighbors(n_neighbors=3) nbrs_X.fit(X) dist_X, ind_X = getattr(nbrs_X, method)(Y)
dist_X, ind_X = getattr(nbrs_X, method)(None) dist_D, ind_D = getattr(nbrs_D, method)(None) assert_array_almost_equal(dist_X, dist_D) assert_array_almost_equal(ind_X, ind_D)
assert_raises(ValueError, getattr(nbrs_D, method), X)
rng = np.random.RandomState(random_state)
knn.fit(X, y_str) y_pred = knn.predict(X[:n_test_pts] + epsilon) assert_array_equal(y_pred, y_str[:n_test_pts])
rng = check_random_state(0) n_features = 2 n_samples = 40 n_output = 3
rnn_mo = neighbors.RadiusNeighborsClassifier(weights=weights, algorithm=algorithm) rnn_mo.fit(X_train, y_train) y_pred_mo = rnn_mo.predict(X_test)
rng = check_random_state(0) n_features = 5 n_samples = 50 n_output = 3
knn_mo = neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm) knn_mo.fit(X_train, y_train) y_pred_mo = knn_mo.predict(X_test)
y_pred_proba_mo = knn_mo.predict_proba(X_test) assert_equal(len(y_pred_proba_mo), n_output)
rng = check_random_state(0) n_features = 5 n_samples = 40 n_output = 4
X = np.array([[0, 1], [1.01, 1.], [2, 0]])
A = neighbors.kneighbors_graph(X, 1, mode='connectivity', include_self=True) assert_array_equal(A.toarray(), np.eye(A.shape[0]))
rng = np.random.RandomState(seed) X = rng.randn(10, 10) Xcsr = csr_matrix(X)
X = np.array([[0, 1], [1.01, 1.], [2, 0]])
rng = np.random.RandomState(seed) X = rng.randn(10, 10) Xcsr = csr_matrix(X)
assert_raises(ValueError, neighbors.NearestNeighbors, algorithm='blah')
V = rng.rand(n_features, n_features) VI = np.dot(V, V.T)
if (algorithm == 'kd_tree' and metric not in neighbors.KDTree.valid_metrics): assert_raises(ValueError, neighbors.NearestNeighbors, algorithm=algorithm, metric=metric, metric_params=metric_params) continue
dist_array = pairwise_distances(X).flatten() np.sort(dist_array) radius = dist_array[15]
for algorithm in ALGORITHMS:
assert_array_almost_equal(dist1, dist2)
bt1_pyfunc = BallTree(X, metric=dist_func, leaf_size=1, p=2)
simultaneous_sort(dist, ind)
i = np.argsort(dist2, axis=1) row_ind = np.arange(n_rows)[:, None] dist2 = dist2[row_ind, i] ind2 = ind2[row_ind, i]
self.X1_bool = self.X1.round(0) self.X2_bool = self.X2.round(0)
euclidean_pkl = pickle.loads(pickle.dumps(euclidean)) pyfunc_pkl = pickle.loads(pickle.dumps(pyfunc))
assert_array_almost_equal(dist1, dist2)
simultaneous_sort(dist, ind)
i = np.argsort(dist2, axis=1) row_ind = np.arange(n_rows)[:, None] dist2 = dist2[row_ind, i] ind2 = ind2[row_ind, i]
kde = KernelDensity(bandwidth, kernel=kernel).fit(X) samp = kde.sample(100) assert_equal(X.shape, samp.shape)
nbrs = NearestNeighbors(n_neighbors=1).fit(X) dist, ind = nbrs.kneighbors(X, return_distance=True)
assert np.all(dist < 5 * bandwidth)
rng = np.random.RandomState(0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
clf = NearestCentroid() clf.fit(X, y) assert_array_equal(clf.predict(T), true_result)
clf = NearestCentroid() clf.fit(X_csr, y) assert_array_equal(clf.predict(T_csr), true_result)
clf = NearestCentroid() clf.fit(X_csr, y) assert_array_equal(clf.predict(T), true_result)
clf = NearestCentroid() clf.fit(X, y) assert_array_equal(clf.predict(T_csr), true_result)
clf = NearestCentroid() clf.fit(X_csr.tocoo(), y) assert_array_equal(clf.predict(T_csr.tolil()), true_result)
obj = NearestCentroid() obj.fit(iris.data, iris.target) score = obj.score(iris.data, iris.target) s = pickle.dumps(obj)
self._validate_params(self.n_features, self.input_type) return self
if patches.shape[-1] == 1: return patches.reshape((n_patches, p_h, p_w)) else: return patches
img[i, j] /= float(min(i + 1, p_h, i_h - i) * min(j + 1, p_w, i_w - j))
if stop_words is not None: tokens = [w for w in tokens if w not in stop_words]
text_document = self._white_spaces.sub(" ", text_document)
text_document = self._white_spaces.sub(" ", text_document)
self._get_hasher().fit(X, y=y) return self
fit_transform = transform
vocabulary = defaultdict() vocabulary.default_factory = vocabulary.__len__
continue
vocabulary = dict(vocabulary) if not vocabulary: raise ValueError("empty vocabulary; perhaps the documents only" " contain stop words")
self._validate_vocabulary() max_df = self.max_df min_df = self.min_df max_features = self.max_features
_, X = self._count_vocab(raw_documents, fixed_vocab=True) if self.binary: X.data.fill(1) return X
X = X.tocsr()
X = np.asmatrix(X)
df += int(self.smooth_idf) n_samples += int(self.smooth_idf)
idf = np.log(float(n_samples) / df) + 1.0 self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, n=n_features)
X = sp.csr_matrix(X, copy=copy)
X = sp.csr_matrix(X, dtype=np.float64, copy=copy)
X = X * self._idf_diag
return self._tfidf.transform(X, copy=False)
hasher = FeatureHasher() hasher.set_params(n_features=np.inf) assert_raises(TypeError, hasher.fit)
X = FeatureHasher().transform([{'foo': 0}]) assert_equal(X.data.shape, (0,))
np.testing.assert_array_equal(grad_x.data[grad_x.data > 0], grad_y.data[grad_y.data > 0])
mask = np.ones((size, size), dtype=np.int16) A = grid_to_graph(n_x=size, n_y=size, n_z=size, mask=mask) assert_true(connected_components(A)[0] == 1)
from scipy import misc face = misc.face(gray=True)
from scipy import misc face = misc.face(gray=True)
from scipy import misc face = misc.face(gray=True)
images = np.zeros((3,) + face.shape) images[0] = face images[1] = face + 1 images[2] = face + 2 return images
assert_array_equal(X.A, v.transform(iter(D) if iterable else D).A)
v_1 = DictVectorizer().fit([d_sorted]) v_2 = DictVectorizer().fit([d_shuffled])
a = '\xe0\xe1\xe2\xe3\xe4\xe5\xe7\xe8\xe9\xea\xeb' expected = 'aaaaaaceeee' assert_equal(strip_accents_unicode(a), expected)
a = "this is \xe0 test" expected = 'this is a test' assert_equal(strip_accents_unicode(a), expected)
a = '\xe0\xe1\xe2\xe3\xe4\xe5\xe7\xe8\xe9\xea\xeb' expected = 'aaaaaaceeee' assert_equal(strip_accents_ascii(a), expected)
a = "this is \xe0 test" expected = 'this is a test' assert_equal(strip_accents_ascii(a), expected)
text = "J'ai mang\xe9 du kangourou ce midi, c'\xe9tait pas tr\xeas bon." text_bytes = text.encode('utf-8')
wa = CountVectorizer(ngram_range=(1, 2), encoding='ascii').build_analyzer() assert_raises(UnicodeDecodeError, wa, text_bytes)
v.fit(["to be or not to be", "and me too", "and so do you"]) assert False, "we shouldn't get here"
assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
X = [[1, 1, 0], [1, 1, 0], [1, 0, 0]] tr = TfidfTransformer(smooth_idf=False, norm='l2')
train_data = iter(ALL_FOOD_DOCS[:-1]) test_data = [ALL_FOOD_DOCS[-1]] n_train = len(ALL_FOOD_DOCS) - 1
v2 = CountVectorizer(vocabulary=v1.vocabulary_)
for v in (v1, v2): counts_test = v.transform(test_data) if hasattr(counts_test, 'tocsr'): counts_test = counts_test.tocsr()
assert_false("the" in vocabulary)
assert_false("copyright" in vocabulary)
tfidf_test = t1.transform(counts_test).toarray() assert_equal(tfidf_test.shape, (len(test_data), len(v1.vocabulary_)))
t2 = TfidfTransformer(norm='l1', use_idf=False) tf = t2.fit(counts_train).transform(counts_train).toarray() assert_equal(t2.idf_, None)
t3 = TfidfTransformer(use_idf=True) assert_raises(ValueError, t3.transform, counts_train)
X = [[1, 1, 5], [1, 1, 0]] t3.fit(X) X_incompt = [[1, 3], [1, 3]] assert_raises(ValueError, t3.transform, X_incompt)
assert_array_almost_equal(np.sum(tf, axis=1), [1.0] * n_train)
train_data = iter(ALL_FOOD_DOCS[:-1]) tv = TfidfVectorizer(norm='l1')
tfidf_test2 = tv.transform(test_data).toarray() assert_array_almost_equal(tfidf_test, tfidf_test2)
v3 = CountVectorizer(vocabulary=None) assert_raises(ValueError, v3.transform, train_data)
v3.set_params(strip_accents='ascii', lowercase=False) assert_equal(v3.build_preprocessor(), strip_accents_ascii)
v3.set_params(strip_accents='_gabbledegook_', preprocessor=None) assert_raises(ValueError, v3.build_preprocessor)
v3.set_params = '_invalid_analyzer_type_' assert_raises(ValueError, v3.build_analyzer)
for i in range(X.shape[0]): assert_almost_equal(np.linalg.norm(X[0].data, 2), 1.0)
ngrams_nnz = X.nnz assert_true(ngrams_nnz > token_nnz) assert_true(ngrams_nnz < 2 * token_nnz)
assert_true(np.min(X.data) > 0) assert_true(np.max(X.data) < 1)
for i in range(X.shape[0]): assert_almost_equal(np.linalg.norm(X[0].data, 1), 1.0)
assert_raises(ValueError, cv.get_feature_names)
vectorizer = vec_factory(max_df=0.6, max_features=4) vectorizer.fit(ALL_FOOD_DOCS) assert_equal(set(vectorizer.vocabulary_), expected_vocabulary) assert_equal(vectorizer.stop_words_, expected_stop_words)
assert_equal(7, counts_1.max()) assert_equal(7, counts_3.max()) assert_equal(7, counts_None.max())
vect = CountVectorizer(analyzer='char', max_df=1.0, binary=True, dtype=np.float32) X_sparse = vect.fit_transform(test_data) assert_equal(X_sparse.dtype, np.float32)
vect = HashingVectorizer(analyzer='char', non_negative=True, binary=True, norm=None, dtype=np.float64) X = vect.transform(test_data) assert_equal(X.dtype, np.float64)
data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
train_data, test_data, target_train, target_test = train_test_split( data, target, test_size=.2, random_state=0)
grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)
pred = grid_search.fit(train_data, target_train).predict(test_data) assert_array_equal(pred, target_test)
assert_equal(grid_search.best_score_, 1.0) best_vectorizer = grid_search.best_estimator_.named_steps['vect'] assert_equal(best_vectorizer.ngram_range, (1, 1))
data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
train_data, test_data, target_train, target_test = train_test_split( data, target, test_size=.1, random_state=0)
grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)
pred = grid_search.fit(train_data, target_train).predict(test_data) assert_array_equal(pred, target_test)
data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
assert_equal(X_counted.nnz, X_hashed.nnz)
assert_array_equal(np.sort(X_counted.data), np.sort(X_hashed.data))
message = "np.nan is an invalid document, expected byte or unicode string." exception = ValueError
v = TfidfVectorizer(binary=True, use_idf=False, norm=None) assert_true(v.binary)
X = [X] if isinstance(X, Mapping) else X
values = []
X = check_array(X, accept_sparse=['csr', 'csc']) n_samples = X.shape[0]
from .metrics import r2_score return r2_score(y, self.predict(X), sample_weight=sample_weight, multioutput='uniform_average')
self.steps = tosequence(steps) transforms = estimators[:-1] estimator = estimators[-1]
return getattr(self.steps[0][1], '_pairwise', False)
return transformer.transform(X) * transformer_weights[name]
n_samples, self.n_features_ = X.shape is_classification = isinstance(self, ClassifierMixin)
y = np.reshape(y, (-1, 1))
max_depth = ((2 ** 31) - 1 if self.max_depth is None else self.max_depth) max_leaf_nodes = (-1 if self.max_leaf_nodes is None else self.max_leaf_nodes)
if self.min_weight_fraction_leaf != 0. and sample_weight is not None: min_weight_leaf = (self.min_weight_fraction_leaf * np.sum(sample_weight)) else: min_weight_leaf = 0.
if self.presort == 'auto' and issparse(X): presort = False elif self.presort == 'auto': presort = True
if max_leaf_nodes < 0: builder = DepthFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth) else: builder = BestFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, max_leaf_nodes)
if isinstance(self, ClassifierMixin): if self.n_outputs_ == 1: return self.classes_.take(np.argmax(proba, axis=1), axis=0)
else: if self.n_outputs_ == 1: return proba[:, 0]
s, v = 0.75, 0.9 c = s * v m = v - c
if tree.n_outputs == 1: value = tree.value[node_id][0, :] else: value = tree.value[node_id]
labels = (label == 'root' and node_id == 0) or label == 'all'
if node_ids: if labels: node_string += 'node ' node_string += characters[0] + str(node_id) + characters[4]
if node_string[-2:] == '\\n': node_string = node_string[:-2] if node_string[-5:] == '<br/>': node_string = node_string[:-5]
if max_depth is None or depth <= max_depth:
out_file.write(', fillcolor="#C0C0C0"')
out_file.write('%d -> %d ;\n' % (parent, node_id))
ranks = {'leaves': []} colors = {'bounds': None}
if isinstance(decision_tree, _tree.Tree): recurse(decision_tree, 0, criterion="impurity") else: recurse(decision_tree.tree_, 0, criterion=decision_tree.criterion)
clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2, criterion="gini", random_state=2) clf.fit(X, y)
clf = DecisionTreeClassifier(max_depth=2, min_samples_split=2, criterion="gini", random_state=2) clf = clf.fit(X, y2, sample_weight=w)
clf = DecisionTreeRegressor(max_depth=3, min_samples_split=2, criterion="mse", random_state=2) clf.fit(X, y)
clf = DecisionTreeClassifier(max_depth=3) clf.fit(X, y_degraded)
clf = DecisionTreeClassifier(max_depth=3, min_samples_split=2) clf.fit(X, y)
out = StringIO() assert_raises(IndexError, export_graphviz, clf, out, feature_names=[])
out = StringIO() assert_raises(IndexError, export_graphviz, clf, out, class_names=[])
boston = datasets.load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
for name, Tree in CLF_TREES.items(): clf = Tree(random_state=0)
y = np.zeros((10, 10)) y[:5, :5] = 1 y[5:, 5:] = 1
X = np.arange(10000)[:, np.newaxis] y = np.arange(10000)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]] y = [1, 1, 1, 1, 1, 1]
X, y = datasets.make_classification(n_samples=2000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=0)
clf = DecisionTreeClassifier() clf.feature_importances_
est = TreeEstimator(max_features=10) assert_raises(ValueError, est.fit, X, y)
for name, TreeEstimator in CLF_TREES.items(): est = TreeEstimator() assert_raises(NotFittedError, est.predict_proba, X)
est = TreeEstimator() y2 = y[:-1] assert_raises(ValueError, est.fit, X, y2)
Xf = np.asfortranarray(X) est = TreeEstimator() est.fit(Xf, y) assert_almost_equal(est.predict(T), true_result)
est = TreeEstimator() assert_raises(NotFittedError, est.predict, T)
est.fit(X, y) t = np.asarray(T) assert_raises(ValueError, est.predict, t[:, 1:])
Xt = np.array(X).T
est = TreeEstimator() assert_raises(NotFittedError, est.apply, T)
for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()): TreeEstimator = ALL_TREES[name]
est = TreeEstimator(min_samples_split=10, max_leaf_nodes=max_leaf_nodes, random_state=0) est.fit(X, y) node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]
est = TreeEstimator(min_samples_split=0.2, max_leaf_nodes=max_leaf_nodes, random_state=0) est.fit(X, y) node_samples = est.tree_.n_node_samples[est.tree_.children_left != -1]
X = np.asfortranarray(iris.data.astype(tree._tree.DTYPE)) y = iris.target
for max_leaf_nodes, name in product((None, 1000), ALL_TREES.keys()): TreeEstimator = ALL_TREES[name]
leaf_weights = node_weights[node_weights != 0] assert_greater_equal( np.min(leaf_weights), total_weight * est.min_weight_fraction_leaf, "Failed with {0} " "min_weight_fraction_leaf={1}".format( name, est.min_weight_fraction_leaf))
for name in ALL_TREES: yield check_min_weight_fraction_leaf, name, "iris"
for name in SPARSE_TREES: yield check_min_weight_fraction_leaf, name, "multilabel", True
for name, TreeClassifier in CLF_TREES.items(): clf = TreeClassifier(random_state=0) clf.fit(X, y)
unbalanced_X = iris.data[:125] unbalanced_y = iris.target[:125] sample_weight = compute_sample_weight("balanced", unbalanced_y)
for (name, TreeEstimator), dtype in product(ALL_TREES.items(), [np.float64, np.float32]): est = TreeEstimator(random_state=0)
X = np.asarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = np.ascontiguousarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csr_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csc_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = np.arange(100)[:, np.newaxis] y = np.ones(100) y[:50] = 0.0
X = np.arange(200)[:, np.newaxis] y = np.zeros(200) y[50:100] = 1 y[100:200] = 2 X[100:200, 0] = 200
X = iris.data y = iris.target
X = np.arange(100)[:, np.newaxis] y = np.ones(100) y[:50] = 0.0
TreeClassifier = CLF_TREES[name] _y = np.vstack((y, np.array(y) * 2)).T
clf = TreeClassifier(class_weight='the larch', random_state=0) assert_raises(ValueError, clf.fit, X, y) assert_raises(ValueError, clf.fit, X, _y)
clf = TreeClassifier(class_weight=1, random_state=0) assert_raises(ValueError, clf.fit, X, _y)
clf = TreeClassifier(class_weight=[{-1: 0.5, 1: 1.}], random_state=0) assert_raises(ValueError, clf.fit, X, _y)
huge = 2 ** (n_bits + 1) clf = DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge) assert_raises(Exception, clf.fit, X, y)
huge = 2 ** (n_bits - 1) - 1 clf = DecisionTreeClassifier(splitter='best', max_leaf_nodes=huge) assert_raises(MemoryError, clf.fit, X, y)
if dataset in ["digits", "boston"]: n_samples = X.shape[0] // 5 X = X[:n_samples] X_sparse = X_sparse[:n_samples] y = y[:n_samples]
d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y) s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
for tree, dataset in product(REG_TREES, ["boston", "reg_small"]): if tree in SPARSE_TREES: yield (check_sparse_input, tree, dataset, 2)
n_samples = n_features samples = np.arange(n_samples)
X_sparse_test = X_sparse_test.copy()
assert_greater((X_sparse.data == 0.).sum(), 0) assert_greater((X_sparse_test.data == 0.).sum(), 0)
d = TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y) s = TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
est = TreeEstimator(random_state=0) est.fit(X, y, sample_weight=sample_weight) assert_equal(est.tree_.max_depth, 1)
leaves = est.apply(X) leave_indicator = [node_indicator[i, j] for i, j in enumerate(leaves)] assert_array_almost_equal(leave_indicator, np.ones(shape=n_samples))
all_leaves = est.tree_.children_left == TREE_LEAF assert_array_almost_equal(np.dot(node_indicator, all_leaves), np.ones(shape=n_samples))
max_depth = node_indicator.sum(axis=1).max() assert_less_equal(est.tree_.max_depth, max_depth)
for name in ALL_TREES: yield (check_no_sparse_y_support, name)
from abc import ABCMeta, abstractmethod
self.kernel = kernel self.gamma = gamma self.n_neighbors = n_neighbors
self.alpha = alpha
graph_matrix = self._build_graph()
classes = np.unique(y) classes = (classes[classes != -1]) self.classes_ = classes
self.label_distributions_ = np.zeros((n_samples, n_classes)) for label in classes: self.label_distributions_[y == label, classes == label] = 1
self.label_distributions_ = np.multiply( clamp_weights, self.label_distributions_) + y_static remaining_iter -= 1
transduction = self.classes_[np.argmax(self.label_distributions_, axis=1)] self.transduction_ = transduction.ravel() self.n_iter_ = self.max_iter - remaining_iter return self
super(LabelSpreading, self).__init__(kernel=kernel, gamma=gamma, n_neighbors=n_neighbors, alpha=alpha, max_iter=max_iter, tol=tol, n_jobs=n_jobs)
for i, file_path in enumerate(file_paths): if i % 1000 == 0: logger.info("Loading face #%05d / %05d", i + 1, n_faces)
img = imread(file_path) if img.ndim is 0: raise RuntimeError("Failed to read the image file %s, " "Please make sure that libjpeg is installed" % file_path)
face = face.mean(axis=2)
m = Memory(cachedir=lfw_home, compress=6, verbose=0) load_func = m.cache(_fetch_lfw_people)
faces, target, target_names = load_func( data_folder_path, resize=resize, min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)
return Bunch(data=faces.reshape(len(faces), -1), images=faces, target=target, target_names=target_names, DESCR="LFW faces dataset")
m = Memory(cachedir=lfw_home, compress=6, verbose=0) load_func = m.cache(_fetch_lfw_pairs)
pairs, target, target_names = load_func( index_file_path, data_folder_path, resize=resize, color=color, slice_=slice_)
return Bunch(data=pairs.reshape(len(pairs), -1), pairs=pairs, target=target, target_names=target_names, DESCR="'%s' segment of the LFW pairs dataset" % subset)
random_state = check_random_state(random_state) r = random_state.randint(0, n_samples_abnormal, 3377) abnormal_samples = abnormal_samples[r] abnormal_targets = abnormal_targets[r]
s = data[:, 11] == 1 data = np.c_[data[s, :11], data[s, 12:]] target = target[s]
dir_suffix = "-py3"
dir_suffix = ""
permutation = _find_permutation(sample_id_bis, sample_id) y = y[permutation, :]
categories = np.empty(N_CATEGORIES, dtype=object) for k in category_names.keys(): categories[category_names[k]] = k
order = np.argsort(categories) categories = categories[order] y = sp.csr_matrix(y[:, order])
from urllib2 import urlopen PY2 = True
from urllib.request import urlopen PY2 = False
names = F.readline().strip().split(',')
names = F.readline().decode('ascii').strip().split(',')
xgrid = np.arange(xmin, xmax, batch.grid_size) ygrid = np.arange(ymin, ymax, batch.grid_size)
extra_params = dict(x_left_lower_corner=-94.8, Nx=1212, y_left_lower_corner=-56.05, Ny=1592, grid_size=0.05) dtype = np.int16
from urllib2 import HTTPError from urllib2 import quote from urllib2 import urlopen
from urllib.error import HTTPError from urllib.parse import quote from urllib.request import urlopen
dataname = mldata_filename(dataname)
data_home = get_data_home(data_home=data_home) data_home = join(data_home, 'mldata') if not exists(data_home): os.makedirs(data_home)
with open(filename, 'rb') as matlab_file: matlab_dict = io.loadmat(matlab_file, struct_as_record=True)
col_names = [str(descr[0]) for descr in matlab_dict['mldata_descr_ordering'][0]]
if isinstance(target_name, numbers.Integral): target_name = col_names[target_name] if isinstance(data_name, numbers.Integral): data_name = col_names[data_name]
if len(col_names) == 1: data_name = col_names[0] dataset['data'] = matlab_dict[data_name] else: for name in col_names: dataset[name] = matlab_dict[name]
logger.warning("Download was incomplete, downloading again.") os.remove(archive_path)
data_lst = np.array(data.data, dtype=object) data_lst = data_lst[indices] data.data = data_lst.tolist()
data_train = fetch_20newsgroups(data_home=data_home, subset='train', categories=None, shuffle=True, random_state=12, remove=remove)
X_train = X_train.astype(np.float64) X_test = X_test.astype(np.float64) normalize(X_train, copy=False) normalize(X_test, copy=False)
else: with closing(_gen_open(f)) as f: actual_dtype, data, ind, indptr, labels, query = \ _load_svmlight_file(f, dtype, multilabel, zero_based, query_id)
if not multilabel: labels = frombuffer_empty(labels, np.float64) data = frombuffer_empty(data, actual_dtype) indices = frombuffer_empty(ind, np.intc)
import urllib2 urlopen = urllib2.urlopen
import urllib.request urlopen = urllib.request.urlopen
MODULE_DOCS = __doc__
filenames = np.array(filenames) target = np.array(target)
feature_names=feature_names[:-1], DESCR=descr_text)
X = np.zeros((n_samples, n_features)) y = np.zeros(n_samples, dtype=np.int)
X[:, :n_informative] = generator.randn(n_samples, n_informative)
stop = 0 for k, centroid in enumerate(centroids): start, stop = stop, stop + n_samples_per_cluster[k]
if n_useless > 0: X[:, -n_useless:] = generator.randn(n_samples, n_useless)
if flip_y >= 0.0: flip_mask = generator.rand(n_samples) < flip_y y[flip_mask] = generator.randint(n_classes, size=flip_mask.sum())
if shift is None: shift = (2 * generator.rand(n_features) - 1) * class_sep X += shift
X, y = util_shuffle(X, y, random_state=generator)
indices = np.arange(n_features) generator.shuffle(indices) X[:, :] = X[:, indices]
y_size = n_classes + 1 while (not allow_unlabeled and y_size == 0) or y_size > n_classes: y_size = generator.poisson(n_labels)
n_words = 0 while n_words == 0: n_words = generator.poisson(length)
if len(y) == 0: words = generator.randint(n_features, size=n_words) return words, y
X = generator.randn(n_samples, n_features)
X = make_low_rank_matrix(n_samples=n_samples, n_features=n_features, effective_rank=effective_rank, tail_strength=tail_strength, random_state=generator)
ground_truth = np.zeros((n_features, n_targets)) ground_truth[:n_informative, :] = 100 * generator.rand(n_informative, n_targets)
if noise > 0.0: y += generator.normal(scale=noise, size=y.shape)
if shuffle: X, y = util_shuffle(X, y, random_state=generator)
singular_ind = np.arange(n, dtype=np.float64)
D = generator.randn(n_features, n_components) D /= np.sqrt(np.sum((D ** 2), axis=0))
Y = np.dot(D, X)
permutation = random_state.permutation(dim) aux = aux[permutation].T[permutation] chol += aux prec = np.dot(chol.T, chol)
d = np.diag(prec).reshape(1, prec.shape[0]) d = 1. / np.sqrt(d)
X = generator.multivariate_normal(mean, cov * np.identity(n_features), (n_samples,))
idx = np.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1)) X = X[idx, :]
step = n_samples // n_classes
from urllib2 import urlopen
from urllib.request import urlopen
MODULE_DOCS = __doc__
columns_index = [8, 7, 2, 3, 4, 5, 6, 1, 0] cal_housing = cal_housing[:, columns_index] joblib.dump(cal_housing, filepath, compress=6)
data[:, 2] /= data[:, 5]
data[:, 3] /= data[:, 5]
data[:, 5] = data[:, 4] / data[:, 5]
target = target / 100000.0
data_home = get_data_home(data_home=DATA_HOME) assert_equal(data_home, DATA_HOME) assert_true(os.path.exists(data_home))
clear_data_home(data_home=data_home) assert_false(os.path.exists(data_home))
data_home = get_data_home(data_home=DATA_HOME) assert_true(os.path.exists(data_home))
class_sep = 1e6 make = partial(make_classification, class_sep=class_sep, n_redundant=0, n_repeated=0, flip_y=0, shift=0, scale=1, shuffle=False)
signs = np.sign(X) signs = signs.view(dtype='|S{0}'.format(signs.strides[0])) unique_signs, cluster_index = np.unique(signs, return_inverse=True)
X2, Y2, p_c, p_w_c = make_multilabel_classification( n_samples=25, n_features=20, n_classes=3, random_state=0, allow_unlabeled=allow_unlabeled, return_distributions=True)
assert_almost_equal(np.std(y - np.dot(X, c)), 1.0, decimal=1)
assert_almost_equal(np.std(y - np.dot(X, c)), 1.0, decimal=1)
assert_equal(len(data2cats.filenames), len(data2cats.target)) assert_equal(len(data2cats.filenames), len(data2cats.data))
raise SkipTest("Test too slow.")
global tmpdir tmpdir = tempfile.mkdtemp() os.makedirs(os.path.join(tmpdir, 'mldata'))
if tmpdir is not None: shutil.rmtree(tmpdir)
x = sp.arange(6).reshape(2, 3) datasets.mldata.urlopen = mock_mldata_urlopen({dataname: {'x': x}})
dset = fetch_mldata(dataname, transpose_data=False, data_home=tmpdir) assert_equal(dset.data.shape, (3, 2))
assert_true(sp.issparse(X1)) assert_true(sp.issparse(Y1)) assert_equal(60915113, X1.data.size) assert_equal(2606875, Y1.data.size)
data2 = fetch_rcv1(shuffle=True, subset='train', random_state=77, download_if_missing=False) X2, Y2 = data2.data, data2.target s2 = data2.sample_id
assert_array_equal(np.sort(s1[:23149]), np.sort(s2))
some_sample_ids = (2286, 3274, 14042) for sample_id in some_sample_ids: idx1 = s1.tolist().index(sample_id) idx2 = s2.tolist().index(sample_id)
assert_equal(X.indptr.shape[0], 7) assert_equal(X.shape[0], 6) assert_equal(X.shape[1], 21) assert_equal(y.shape[0], 6)
X[0, 2] *= 2 assert_equal(X[0, 2], 5)
assert_array_equal(y, [1, 2, 3, 4, 1, 2])
X1, y1 = load_svmlight_file(datafile)
assert_equal(X.indptr.shape[0], 7) assert_equal(X.shape[0], 6) assert_equal(X.shape[1], 22)
assert_raises(ValueError, load_svmlight_file, datafile, n_features=20)
os.remove(tmp.name)
os.remove(tmp.name)
load_svmlight_file(.42)
X_sliced = X_sparse[np.arange(X_sparse.shape[0])] y_sliced = y_sparse[np.arange(y_sparse.shape[0])]
y = y.T
assert_array_almost_equal( X_dense.astype(dtype), X2_dense, 4) assert_array_almost_equal( y_dense.astype(dtype), y2, 4)
assert_array_almost_equal( X_dense.astype(dtype), X2_dense, 15) assert_array_almost_equal( y_dense.astype(dtype), y2, 15)
utf8_comment = b("It is true that\n\xc2\xbd\xc2\xb2 = \xc2\xbc") f = BytesIO() assert_raises(UnicodeDecodeError, dump_svmlight_file, X, y, f, comment=utf8_comment)
assert_equal(lfw_people.images.shape, (10, 62, 47)) assert_equal(lfw_people.data.shape, (10, 2914))
assert_array_equal(lfw_people.target, [2, 0, 1, 0, 2, 0, 2, 1, 1, 2])
expected_classes = ['Abdelatif Smith', 'Abhati Kepler', 'Onur Lopez'] assert_array_equal(lfw_people.target_names, expected_classes)
lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False) assert_equal(lfw_people.images.shape, (17, 250, 250, 3))
assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 62, 47))
assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0])
expected_classes = ['Different persons', 'Same person'] assert_array_equal(lfw_pairs_train.target_names, expected_classes)
lfw_pairs_train = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False) assert_equal(lfw_pairs_train.pairs.shape, (10, 2, 250, 250, 3))
assert_array_equal(lfw_pairs_train.target, [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]) assert_array_equal(lfw_pairs_train.target_names, expected_classes)
lines = '\n'.join(l.rstrip(' ') for l in lines.split('\n')) return lines
init = getattr(cls.__init__, 'deprecated_original', cls.__init__) if init is object.__init__: return []
return self
self.fit(X) return self.labels_
if y is None: return self.fit(X, **fit_params).transform(X) else: return self.fit(X, y, **fit_params).transform(X)
y_type = y_type.pop()
if (y_type not in ["binary", "multiclass", "multilabel-indicator"]): raise ValueError("{0} is not supported".format(y_type))
ind = np.logical_and(y_pred < n_labels, y_true < n_labels) y_pred = y_pred[ind] y_true = y_true[ind] sample_weight = sample_weight[ind]
score[pred_or_true == 0.0] = 1.0
result[mask] = 0.0
axis0 = 'sample' axis1 = 'label' if average == 'samples': axis0, axis1 = axis1, axis0
return (0., 0., 0., 0)
true_and_pred = y_true.multiply(y_pred) tp_sum = count_nonzero(true_and_pred, axis=sum_axis, sample_weight=sample_weight) pred_sum = count_nonzero(y_pred, axis=sum_axis, sample_weight=sample_weight) true_sum = count_nonzero(y_true, axis=sum_axis, sample_weight=sample_weight)
tp = y_true == y_pred tp_bins = y_true[tp] if sample_weight is not None: tp_bins_weights = np.asarray(sample_weight)[tp] else: tp_bins_weights = None
true_sum = pred_sum = tp_sum = np.zeros(len(labels))
indices = np.searchsorted(sorted_labels, labels[:n_labels]) tp_sum = tp_sum[indices] true_sum = true_sum[indices] pred_sum = pred_sum[indices]
Y = np.clip(y_pred, eps, 1 - eps)
if not isinstance(Y, np.ndarray): raise ValueError("y_pred should be an array of floats.")
if Y.ndim == 1: Y = Y[:, np.newaxis] if Y.shape[1] == 1: Y = np.append(1 - Y, Y, axis=1)
Y /= Y.sum(axis=1)[:, np.newaxis] loss = -(T * np.log(Y)).sum(axis=1)
pred_decision = column_or_1d(pred_decision) pred_decision = np.ravel(pred_decision)
losses[losses <= 0] = 0 return np.average(losses, weights=sample_weight)
return comb(n, 2, exact=1)
sum_comb_c = sum(comb2(n_c) for n_c in contingency.sum(axis=1)) sum_comb_k = sum(comb2(n_k) for n_k in contingency.sum(axis=0))
return -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))
assert_almost_equal(s, 2.0/3.0)
dataset = datasets.load_iris() X = dataset.data
n_clusters_range = [2, 10, 50, 90] n_samples = 100 n_runs = 10
assert expected_mutual_information(np.array([[70000]]), 70000) <= 1
intra_clust_dists = np.ones(distances.shape[0], dtype=distances.dtype)
inter_clust_dists = np.inf * intra_clust_dists
mask = labels == curr_label current_distances = distances[mask]
n_samples_curr_lab = np.sum(mask) - 1 if n_samples_curr_lab != 0: intra_clust_dists[mask] = np.sum( current_distances[:, mask], axis=1) / n_samples_curr_lab
for other_label in unique_labels: if other_label != curr_label: other_mask = labels == other_label other_distances = np.mean( current_distances[:, other_mask], axis=1) inter_clust_dists[mask] = np.minimum( inter_clust_dists[mask], other_distances)
multioutput = None
multioutput = None
return output_scores
avg_weights = None
if isinstance(y_pred, list): y_pred = np.vstack(p for p in y_pred).T
r2_scorer = make_scorer(r2_score) mean_squared_error_scorer = make_scorer(mean_squared_error, greater_is_better=False) mean_absolute_error_scorer = make_scorer(mean_absolute_error, greater_is_better=False) median_absolute_error_scorer = make_scorer(median_absolute_error, greater_is_better=False)
accuracy_scorer = make_scorer(accuracy_score) f1_scorer = make_scorer(f1_score)
roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True) average_precision_scorer = make_scorer(average_precision_score, needs_threshold=True) precision_scorer = make_scorer(precision_score) recall_scorer = make_scorer(recall_score)
log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)
adjusted_rand_scorer = make_scorer(adjusted_rand_score)
order = np.lexsort((y, x)) x, y = x[order], y[order]
area = area.dtype.type(area)
y_true = (y_true == pos_label)
desc_score_indices = np.argsort(y_score, kind="mergesort")[::-1] y_score = y_score[desc_score_indices] y_true = y_true[desc_score_indices] if sample_weight is not None: weight = sample_weight[desc_score_indices] else: weight = 1.
distinct_value_indices = np.where(np.logical_not(isclose( np.diff(y_score), 0)))[0] threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]
last_ind = tps.searchsorted(tps[-1]) sl = slice(last_ind, None, -1) return np.r_[precision[sl], 1], np.r_[recall[sl], 0], thresholds[sl]
tps = np.r_[0, tps] fps = np.r_[0, fps] thresholds = np.r_[thresholds[0] + 1, thresholds]
0.416...
out += 1. continue
loss[i] = np.dot(true_at_reversed_rank.cumsum(), false_at_reversed_rank)
loss[np.logical_or(n_positives == 0, n_positives == n_labels)] = 0.
average_weight = score_weight score_weight = None not_average_axis = 0
if average is not None: return np.average(score, weights=average_weight) else: return score
distances.flat[::distances.shape[0] + 1] = 0.0
indices = np.empty(X.shape[0], dtype=np.intp) values = np.empty(X.shape[0]) values.fill(np.infty)
min_indices = d_chunk.argmin(axis=1) min_values = d_chunk[np.arange(chunk_x.stop - chunk_x.start), min_indices]
S = cosine_similarity(X, Y) S *= -1 S += 1 return S
PAIRWISE_DISTANCE_FUNCTIONS = { 'cityblock': manhattan_distances, 'cosine': cosine_distances, 'euclidean': euclidean_distances, 'l2': euclidean_distances, 'l1': manhattan_distances, 'manhattan': manhattan_distances,
return func(X, Y, **kwds)
out = out + out.T
for i in range(X.shape[0]): x = X[i] out[i, i] = metric(x, x, **kwds)
from ..gaussian_process.kernels import Kernel as GPKernel
assert_raises(ValueError, pairwise_distances, X, Y, metric="blah")
S = func(np.array([[1]], dtype='int'), metric='precomputed') assert_equal('f', S.dtype.kind)
S = func([[1]], metric='precomputed') assert_true(isinstance(S, np.ndarray))
if make_data is csr_matrix: assert_raises(type(exc), func, X, metric=metric, n_jobs=2, **kwds) continue else: raise
assert_equal(pairwise_distances([[1]], metric=lambda x, y: 5)[0, 0], 5)
K = rbf_kernel(np.atleast_2d(x), np.atleast_2d(y), **kwds) return K
K1 = pairwise_kernels(X, Y=X, metric=metric, **kwds) K2 = rbf_kernel(X, Y=X, **kwds) assert_array_almost_equal(K1, K2)
Y = rng.random_sample((3, 4)) assert_raises(ValueError, paired_distances, X, Y)
X = [[0], [1]] Y = [[-1], [2]]
D, E = pairwise_distances_argmin_min(X, Y, metric=minkowski, metric_kwargs={"p": 2}) assert_array_almost_equal(D, [0, 1]) assert_array_almost_equal(E, [1., 1.])
rng = np.random.RandomState(0) X = rng.randn(97, 149) Y = rng.randn(111, 149)
X = [[0]] Y = [[1], [2]] D = euclidean_distances(X, Y) assert_array_almost_equal(D, [[1., 2.]])
X = [[0], [0]] Y = [[1], [2]] D = paired_euclidean_distances(X, Y) assert_array_almost_equal(D, [1., 2.])
X = [[0], [0]] Y = [[1], [2]] D = paired_manhattan_distances(X, Y) assert_array_almost_equal(D, [1., 2.])
assert_raises(ValueError, chi2_kernel, [[0, -1]]) assert_raises(ValueError, chi2_kernel, [[0, -1]], [[-1, -1]]) assert_raises(ValueError, chi2_kernel, [[0, 1]], [[-1, -1]])
assert_raises(ValueError, chi2_kernel, [[0, 1]], [[.2, .2, .6]])
assert_raises(ValueError, chi2_kernel, csr_matrix(X), csr_matrix(Y)) assert_raises(ValueError, additive_chi2_kernel, csr_matrix(X), csr_matrix(Y))
assert_array_almost_equal(K.flat[::6], [linalg.norm(x) ** 2 for x in X])
assert_array_almost_equal(K.flat[::6], np.ones(5))
assert_array_almost_equal(np.diag(K), np.ones(5))
assert_true(np.all(K > 0)) assert_true(np.all(K - np.diag(np.diag(K)) < 1))
s = X.shape if len(s) > 1: return tuple(tuplify(row) for row in X) else: return tuple(r for r in X)
XA_checked, XB_checked = check_pairwise_arrays(XA, XB) assert_equal(XA_checked.dtype, np.float32) assert_equal(XB_checked.dtype, np.float32)
XA_checked, XB_checked = check_pairwise_arrays(XA.astype(np.float), XB) assert_equal(XA_checked.dtype, np.float) assert_equal(XB_checked.dtype, np.float)
XA_checked, XB_checked = check_pairwise_arrays(XA, XB.astype(np.float)) assert_equal(XA_checked.dtype, np.float) assert_equal(XB_checked.dtype, np.float)
METRIC_UNDEFINED_BINARY = [ "samples_f0.5_score", "samples_f1_score", "samples_f2_score", "samples_precision_score", "samples_recall_score", "coverage_error",
METRIC_UNDEFINED_MULTICLASS = [ "brier_score_loss", "matthews_corrcoef_score", ]
METRIC_UNDEFINED_BINARY_MULTICLASS = set(METRIC_UNDEFINED_BINARY).union( set(METRIC_UNDEFINED_MULTICLASS))
METRICS_WITH_AVERAGING = [ "precision_score", "recall_score", "f1_score", "f2_score", "f0.5_score" ]
THRESHOLDED_METRICS_WITH_AVERAGING = [ "roc_auc_score", "average_precision_score", ]
METRICS_WITH_POS_LABEL = [ "roc_curve",
"weighted_f0.5_score", "weighted_f1_score", "weighted_f2_score", "weighted_precision_score", "weighted_recall_score",
METRICS_WITH_NORMALIZE_OPTION = [ "accuracy_score", "jaccard_similarity_score", "zero_one_loss", ]
THRESHOLDED_MULTILABEL_METRICS = [ "log_loss", "unnormalized_log_loss",
MULTILABELS_METRICS = [ "accuracy_score", "unnormalized_accuracy_score", "hamming_loss", "jaccard_similarity_score", "unnormalized_jaccard_similarity_score", "zero_one_loss", "unnormalized_zero_one_loss",
MULTIOUTPUT_METRICS = [ "mean_absolute_error", "mean_squared_error", "r2_score", "explained_variance_score" ]
SYMMETRIC_METRICS = [ "accuracy_score", "unnormalized_accuracy_score", "hamming_loss", "jaccard_similarity_score", "unnormalized_jaccard_similarity_score", "zero_one_loss", "unnormalized_zero_one_loss",
NOT_SYMMETRIC_METRICS = [ "explained_variance_score", "r2_score", "confusion_matrix",
METRICS_WITHOUT_SAMPLE_WEIGHT = [ "cohen_kappa_score",
"median_absolute_error",
random_state = check_random_state(0) y_true = random_state.randint(0, 2, size=(20, )) y_pred = random_state.randint(0, 2, size=(20, ))
assert_equal(set(SYMMETRIC_METRICS).union( NOT_SYMMETRIC_METRICS, THRESHOLDED_METRICS, METRIC_UNDEFINED_BINARY_MULTICLASS), set(ALL_METRICS))
for name in SYMMETRIC_METRICS: metric = ALL_METRICS[name] assert_almost_equal(metric(y_true, y_pred), metric(y_pred, y_true), err_msg="%s is not symmetric" % name)
assert_almost_equal(metric(y1_1d, y2_list), measure, err_msg="%s is not representation invariant " "with mix np-array-1d and list" % name)
if (name not in (MULTIOUTPUT_METRICS + THRESHOLDED_MULTILABEL_METRICS + MULTILABELS_METRICS)): assert_raises(ValueError, metric, y1_row, y2_row)
random_state = check_random_state(0) y1 = random_state.randint(0, 2, size=(20, )) y2 = random_state.randint(0, 2, size=(20, ))
metric_str = metric if name in METRICS_WITH_POS_LABEL: metric_str = partial(metric_str, pos_label=pos_label_str)
metric_str = metric if name in METRICS_WITH_POS_LABEL: metric_str = partial(metric_str, pos_label=pos_label_str)
metric = ALL_METRICS[name]
for i, j in product([0, 1], repeat=2): metric([i], [j])
continue
n_classes = 4 n_samples = 50
y1 += [0]*n_classes y2 += [0]*n_classes
if isinstance(metric, partial): metric.__module__ = 'tmp' metric.__name__ = name
assert_almost_equal(metric(y1_sparse_indicator, y2_sparse_indicator), measure, err_msg="%s failed representation invariance " "between dense and sparse indicator " "formats." % name)
random_state = check_random_state(0) y_true = random_state.randint(0, 2, size=(n_samples, )) y_pred = random_state.randint(0, 2, size=(n_samples, ))
n_classes = 4 n_samples = 100
_, y_true = make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=0, allow_unlabeled=True, n_samples=n_samples) _, y_pred = make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=1, allow_unlabeled=True, n_samples=n_samples)
y_true += [0]*n_classes y_pred += [0]*n_classes
label_measure = metric(y_true, y_pred, average=None) assert_array_almost_equal(label_measure, [metric(y_true_binarize[:, i], y_pred_binarize[:, i]) for i in range(n_classes)])
micro_measure = metric(y_true, y_pred, average="micro") assert_almost_equal(micro_measure, metric(y_true_binarize.ravel(), y_pred_binarize.ravel()))
macro_measure = metric(y_true, y_pred, average="macro") assert_almost_equal(macro_measure, np.mean(label_measure))
weights = np.sum(y_true_binarize, axis=0, dtype=int)
binary_metric = (lambda y_true, y_score, average="macro": _average_binary_score( precision_score, y_true, y_score, average)) _check_averaging(binary_metric, y_true, y_pred, y_true_binarize, y_pred_binarize, is_multilabel=True)
for scaling in [2, 0.3]: assert_almost_equal( weighted_score, metric(y1, y2, sample_weight=sample_weight * scaling), err_msg="%s sample_weight is not invariant " "under scaling" % name)
assert_raises(Exception, metric, y1, y2, sample_weight=np.hstack([sample_weight, sample_weight]))
X_mm, y_mm, y_ml_mm, ESTIMATORS = None, None, None, None shutil.rmtree(TEMP_FOLDER)
for name, scorer in SCORERS.items(): repr(scorer)
scores = cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1], scoring=DummyScorer()) assert_array_equal(scores, 1)
f = lambda *args: 0 assert_raises(ValueError, make_scorer, f, needs_threshold=True, needs_proba=True)
scorer = make_scorer(fbeta_score, beta=2) score1 = scorer(clf, X_test, y_test) score2 = fbeta_score(y_test, clf.predict(X_test), beta=2) assert_almost_equal(score1, score2)
unpickled_scorer = pickle.loads(pickle.dumps(scorer)) score3 = unpickled_scorer(clf, X_test, y_test) assert_almost_equal(score1, score3)
repr(fbeta_score)
X, y = make_multilabel_classification(allow_unlabeled=False, random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
estimator = _make_estimators(X_train, y_train, y_ml_train)
for name in SCORERS.keys(): yield check_scorer_memmap, name
dataset = datasets.load_iris()
X, y = X[y < 2], y[y < 2]
rng = np.random.RandomState(0) X = np.c_[X, rng.randn(n_samples, 200 * n_features)]
clf = svm.SVC(kernel='linear', probability=True, random_state=0) probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])
probas_pred = probas_pred[:, 1]
y_true, _, probas_pred = make_prediction(binary=True) expected_auc = _auc(y_true, probas_pred)
y_true, _, probas_pred = make_prediction(binary=True) fpr, tpr, thresholds = roc_curve(y_true, probas_pred)
assert_array_almost_equal(tpr, tpr_correct, decimal=2) assert_equal(fpr.shape, tpr.shape) assert_equal(fpr.shape, thresholds.shape)
dataset = datasets.load_digits() X = dataset['data'] y = dataset['target']
clf = ensemble.RandomForestClassifier(n_estimators=100, random_state=0)
train, test = slice(None, None, 2), slice(1, None, 2) probas_pred = clf.fit(X[train], y[train]).predict_proba(X[test])
fpr, tpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=False) assert_equal(thresholds.size, np.unique(np.round(thresholds, 2)).size)
y_true, _, probas_pred = make_prediction(binary=False)
y_true, _, probas_pred = make_prediction(binary=True)
y_true, pred, probas_pred = make_prediction(binary=True)
assert_raises(ValueError, auc, [0.0, 0.5, 1.0], [0.1, 0.2])
assert_raises(ValueError, auc, [0.0], [0.1])
assert_raises(ValueError, auc, [1.0, 0.0, 0.5], [0.0, 0.0, 0.0])
y_true = rng.randint(0, 3, size=10) assert_raise_message(ValueError, "multiclass format is not supported", roc_auc_score, y_true, y_pred)
y_true[np.where(y_true == 0)] = -1 y_true_copy = y_true.copy() _test_precision_recall_curve(y_true, probas_pred) assert_array_equal(y_true_copy, y_true)
assert_raises(ValueError, precision_recall_curve, [0, 1, 2], [[0.0], [1.0], [1.0]])
y_true, _, probas_pred = make_prediction(binary=True)
y_true = np.zeros((1, n_labels)) assert_equal(lrap_score(y_true, y_score), 1.) assert_equal(lrap_score(y_true, y_score_ties), 1.)
y_true = np.ones((1, n_labels)) assert_equal(lrap_score(y_true, y_score), 1.) assert_equal(lrap_score(y_true, y_score_ties), 1.)
assert_almost_equal(lrap_score([[1], [0], [1], [0]], [[0.5], [0.5], [0.5], [0.5]]), 1.)
for n_labels in range(2, 10): y_score = np.ones((1, n_labels))
for n_labels in range(2, 10): y_score = n_labels - (np.arange(n_labels).reshape((1, n_labels)) + 1)
unique_rank, inv_rank = np.unique(y_score[i], return_inverse=True) n_ranks = unique_rank.size rank = n_ranks - inv_rank
corr_rank = np.bincount(rank, minlength=n_ranks + 1).cumsum() rank = corr_rank[rank]
n_ranked_above = sum(rank[r] <= rank[label] for r in relevant)
score[i] += n_ranked_above / rank[label]
y_score = sparse_random_matrix(n_components=y_true.shape[0], n_features=y_true.shape[1], random_state=random_state)
dataset = datasets.load_iris()
X, y = X[y < 2], y[y < 2]
rng = np.random.RandomState(0) X = np.c_[X, rng.randn(n_samples, 200 * n_features)]
clf = svm.SVC(kernel='linear', probability=True, random_state=0) probas_pred = clf.fit(X[:half], y[:half]).predict_proba(X[half:])
probas_pred = probas_pred[:, 1]
y1 = np.array([[0, 1, 1], [1, 0, 1]]) y2 = np.array([[0, 0, 1], [1, 0, 1]])
y_true, y_pred, _ = make_prediction(binary=True)
for average in ['macro', 'weighted', 'micro']: assert_not_equal(recall_13(average=average), recall_all(average=average))
rng = check_random_state(404) y_pred = rng.rand(10)
y_true = rng.randint(0, 3, size=10) assert_raise_message(ValueError, "multiclass format is not supported", average_precision_score, y_true, y_pred)
assert_raises(ValueError, precision_recall_fscore_support, y_true, y_pred, beta=0.0)
assert_raises(ValueError, precision_recall_fscore_support, y_true, y_pred, pos_label=2, average='macro')
assert_raises(ValueError, precision_recall_fscore_support, [0, 1, 2], [1, 2, 0], average='mega')
y_true, y_pred, _ = make_prediction(binary=True)
y1 = np.append(y1, [2] * 4) y2 = np.append(y2, [2] * 4) assert_equal(cohen_kappa_score(y1, y2, labels=[0, 1]), kappa)
assert_almost_equal(matthews_corrcoef(y_true, y_true), 1.0)
y_true_inv = ["b" if i == "a" else "a" for i in y_true]
mcc = assert_warns_message(RuntimeWarning, 'invalid value encountered', matthews_corrcoef, [0, 0, 0, 0], [0, 0, 0, 0])
assert_almost_equal(mcc, 0.)
mcc = assert_warns_message(RuntimeWarning, 'invalid value encountered', matthews_corrcoef, y_true, rng.randint(-100, 100) * np.ones(20, dtype=int))
assert_almost_equal(mcc, 0.)
mask = [1] * 10 + [0] * 10 assert_raises(AssertionError, assert_almost_equal, matthews_corrcoef(y_1, y_2, sample_weight=mask), 0.)
y_true, y_pred, _ = make_prediction(binary=False)
ps = precision_score(y_true, y_pred, pos_label=1, average='micro') assert_array_almost_equal(ps, 0.53, 2)
p, r, f, s = precision_recall_fscore_support(y_true, y_pred, pos_label=None, average='weighted')
y_true, y_pred, _ = make_prediction(binary=False)
cm = confusion_matrix(y_true, y_pred) assert_array_equal(cm, [[19, 4, 1], [4, 3, 24], [0, 2, 18]])
y_true, y_pred, _ = make_prediction(binary=False)
cm = confusion_matrix(y_true, y_pred, labels=[0, 1]) assert_array_equal(cm, [[19, 4], [4, 3]])
cm = confusion_matrix(y_true, y_pred, labels=[2, 1]) assert_array_equal(cm, [[18, 2], [24, 3]])
iris = datasets.load_iris() y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
iris = datasets.load_iris() y_true, y_pred, _ = make_prediction(dataset=iris, binary=False)
y1 = np.array([[0, 1, 1], [1, 0, 1]]) y2 = np.array([[0, 0, 1], [1, 0, 1]])
assert_equal(hamming_loss(y1[0], y2[0]), sp_hamming(y1[0], y2[0]))
y1 = np.array([[0, 1, 1], [1, 0, 1]]) y2 = np.array([[0, 0, 1], [1, 0, 1]])
y_true = [1, 2, 3, 3] y_pred = [1, 2, 3, 1] y_true_bin = [0, 1, 1] y_pred_bin = [0, 1, 0]
assert_no_warnings(metric, y_true_bin, y_pred_bin)
y_true *= 2 y_pred *= 2 loss = log_loss(y_true, y_pred, normalize=False) assert_almost_equal(loss, 0.6904911 * 6, decimal=6)
error = mean_absolute_error(y_true, y_pred) assert_almost_equal(error, (1. / 3 + 2. / 3 + 2. / 3) / 4.)
evecs /= np.apply_along_axis(np.linalg.norm, 0, evecs)
std = Xc.std(axis=0) std[std == 0] = 1. fac = 1. / (n_samples - n_classes)
X = np.sqrt(fac) * (Xc / std) U, S, V = linalg.svd(X, full_matrices=False)
scalings = (V[:rank] / std).T / S[:rank]
prob /= prob.sum(axis=1).reshape((prob.shape[0], -1)) return prob
if len(self.classes_) == 2: return dec_func[:, 1] - dec_func[:, 0] return dec_func
likelihood = np.exp(values - values.max(axis=1)[:, np.newaxis]) return likelihood / likelihood.sum(axis=1)[:, np.newaxis]
probas_ = self.predict_proba(X) return np.log(probas_)
kl_divergence = 2.0 * np.dot(P, np.log(P / Q))
if len(P.shape) == 2: P = squareform(P) kl_divergence = 2.0 * np.dot(P, np.log(P / Q))
degrees_of_freedom = max(self.n_components - 1.0, 1) n_samples = X.shape[0] k = min(n_samples - 1, int(3. * self.perplexity + 1))
neighbors_nn = np.argsort(distances, axis=1)[:, :k]
bt = BallTree(X) distances_nn, neighbors_nn = bt.query(X, k=k + 1) neighbors_nn = neighbors_nn[:, 1:]
X_embedded = 1e-4 * random_state.randn(n_samples, self.n_components)
opt_args['objective_error'] = objective_error opt_args['kwargs']['angle'] = self.angle opt_args['kwargs']['verbose'] = self.verbose
P *= self.early_exaggeration
self.n_iter_final = it
P /= self.early_exaggeration opt_args['n_iter'] = self.n_iter opt_args['it'] = it + 1 params, error, it = _gradient_descent(obj_func, params, **opt_args)
X = random_state.rand(n_samples * n_components) X = X.reshape((n_samples, n_components))
n_components = init.shape[1] if n_samples != init.shape[0]: raise ValueError("init matrix should be of shape (%d, %d)" % (n_samples, n_components)) X = init
dis = euclidean_distances(X)
dis_flat_w = dis_flat[sim_flat != 0]
stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2
graph = graph.tocsr()
n_connected_components, _ = connected_components(graph) return n_connected_components == 1
return _graph_connected_component(graph, 0).sum() == graph.shape[0]
if drop_first: n_components = n_components + 1
laplacian = _set_diag(laplacian, 1, norm_laplacian)
self.affinity_matrix_ = 0.5 * (self.affinity_matrix_ + self.affinity_matrix_.T) return self.affinity_matrix_
sim = np.array([[0, 5, 9, 4], [5, 0, 2, 2], [3, 2, 0, 1], [4, 2, 1, 0]])
sim = np.array([[0, 5, 9, 4], [5, 0, 2, 2], [4, 2, 1, 0]])
sim = np.array([[0, 5, 3, 4], [5, 0, 2, 2], [3, 2, 0, 1], [4, 2, 1, 0]])
N_per_side = 5 Npts = N_per_side ** 2 n_neighbors = Npts - 1
X = np.array(list(product(range(N_per_side), repeat=2)))
G = neighbors.kneighbors_graph(X, n_neighbors, mode='distance').toarray()
N_per_side = 5 Npts = N_per_side ** 2 n_neighbors = Npts - 1
X = np.array(list(product(range(N_per_side), repeat=2)))
rng = np.random.RandomState(0) noise = 0.1 * rng.randn(Npts, 1) X = np.concatenate((X, noise), 1)
G = neighbors.kneighbors_graph(X, n_neighbors, mode='distance').toarray()
G_iso = neighbors.kneighbors_graph(clf.embedding_, n_neighbors, mode='distance').toarray()
reconstruction_error = np.linalg.norm(K - K_iso) / Npts assert_almost_equal(reconstruction_error, clf.reconstruction_error())
X, y = datasets.samples_generator.make_s_curve(n_samples, random_state=0)
iso = manifold.Isomap(n_components, 2) X_iso = iso.fit_transform(X)
rng = np.random.RandomState(0) noise = noise_scale * rng.randn(*X.shape) X_iso2 = iso.transform(X + noise)
assert_less(np.sqrt(np.mean((X_iso - X_iso2) ** 2)), 2 * noise_scale)
def test_barycenter_kneighbors_graph(): X = np.array([[0, 1], [1.01, 1.], [2, 0]])
rng = np.random.RandomState(2)
noise = rng.randn(*X.shape) / 100 X_reembedded = clf.transform(X + noise) assert_less(linalg.norm(X_reembedded - clf.embedding_), tol)
def test_lle_init_parameters(): X = np.random.rand(5, 3)
class ObjectiveSmallGradient: def __init__(self): self.it = -1
for k in np.linspace(80, n_samples, 10): k = int(k)
random_state = check_random_state(0)
random_state = check_random_state(0)
X = random_state.randn(100, 2) assert_equal(trustworthiness(X, 5.0 + X / 10.0), 1.0)
tsne = TSNE(early_exaggeration=0.99) assert_raises_regexp(ValueError, "early_exaggeration .*", tsne.fit_transform, np.array([[0.0]]))
tsne = TSNE(n_iter=199) assert_raises_regexp(ValueError, "n_iter .*", tsne.fit_transform, np.array([[0.0]]))
tsne = TSNE(metric="precomputed") assert_raises_regexp(ValueError, ".* square distance matrix", tsne.fit_transform, np.array([[0.0], [1.0]]))
m = "'init' must be 'pca', 'random' or a NumPy array" assert_raises_regexp(ValueError, m, TSNE, init="not available")
random_state = check_random_state(0) tsne = TSNE(verbose=2) X = random_state.randn(5, 2)
random_state = check_random_state(0) tsne = TSNE(metric="chebyshev") X = random_state.randn(5, 2) tsne.fit_transform(X)
angle = 0.0 perplexity = 10 n_samples = 100 for n_components in [2, 3]: n_features = 5 degrees_of_freedom = float(n_components - 1.0)
Xs = []
Xs.append(np.array([[1, 0.0003817754041], [2, 0.0003817753750]], dtype=np.float32))
Xs.append(np.array([[0.0003817754041, 1.0], [0.0003817753750, 2.0]], dtype=np.float32))
assert_equal(_barnes_hut_tsne.test_index2offset(), 1) assert_equal(_barnes_hut_tsne.test_index_offset(), 1)
for i in range(len(group) - 1): connections.append((group[i], group[i + 1]))
component_2 = _graph_connected_component(affinity, p[stop - 1]) assert_equal(component_2.sum(), component_size) assert_array_equal(component_1, component_2)
affinity[0, n_sample + 1] = 1 affinity[n_sample + 1, 0] = 1 affinity.flat[::2 * n_sample + 1] = 0 affinity = 0.5 * (affinity + affinity.T)
try: from pyamg import smoothed_aggregation_solver except ImportError: raise SkipTest("pyamg not available.")
se = SpectralEmbedding(n_components=1, affinity="precomputed", random_state=np.random.RandomState(seed), eigen_solver="<unknown>") assert_raises(ValueError, se.fit, S)
se = SpectralEmbedding(n_components=1, affinity="<unknown>", random_state=np.random.RandomState(seed)) assert_raises(ValueError, se.fit, S)
laplacian, dd = graph_laplacian(sims, normed=False, return_diag=True) _, diffusion_map = eigh(laplacian) embedding_2 = diffusion_map.T[:n_components] * dd embedding_2 = _deterministic_vector_sign_flip(embedding_2).T
for i, A in enumerate(Z.transpose(0, 2, 1)):
if use_svd: U = svd(Gi, full_matrices=0)[0] else: Ci = np.dot(Gi, Gi.T) U = eigh(Ci)[1][:, ::-1]
V = np.zeros((N, n_neighbors, n_neighbors)) nev = min(d_in, n_neighbors) evals = np.zeros([N, nev])
use_svd = (n_neighbors > d_in)
reg = 1E-3 * evals.sum(1)
rho = evals[:, n_components:].sum(1) / evals[:, :n_components].sum(1) eta = np.median(rho)
M = np.zeros((N, N), dtype=np.float64) for i in range(N): s_i = s_range[i]
Vi = V[i, :, n_neighbors - s_i:] alpha_i = np.linalg.norm(Vi.sum(0)) / np.sqrt(s_i)
h = alpha_i * np.ones(s_i) - np.dot(Vi.T, np.ones(n_neighbors))
if use_svd: v = svd(Xi, full_matrices=True)[0] else: Ci = np.dot(Xi, Xi.T) v = eigh(Ci)[1][:, ::-1]
param_grid = [param_grid]
product = partial(reduce, operator.mul) return sum(product(len(v) for v in p.values()) if p else 1 for p in self.param_grid)
for sub_grid in self.param_grid: if not sub_grid: if ind == 0: return {} else: ind -= 1 continue
keys, values_lists = zip(*sorted(sub_grid.items())[::-1]) sizes = [len(v_list) for v_list in values_lists] total = np.product(sizes)
ind -= total
all_lists = np.all([not hasattr(v, "rvs") for v in self.param_distributions.values()]) rnd = check_random_state(self.random_state)
param_grid = ParameterGrid(self.param_distributions) grid_size = len(param_grid)
n_fits = len(out)
best = sorted(grid_scores, key=lambda x: x.mean_validation_score, reverse=True)[0] self.best_params_ = best.parameters self.best_score_ = best.mean_validation_score
pass
n_samples_per_label = np.bincount(labels)
indices = np.argsort(n_samples_per_label)[::-1] n_samples_per_label = n_samples_per_label[indices]
n_samples_per_fold = np.zeros(self.n_folds)
label_to_fold = np.zeros(len(unique_labels))
for label_index, weight in enumerate(n_samples_per_label): lightest_fold = np.argmin(n_samples_per_fold) n_samples_per_fold[lightest_fold] += weight label_to_fold[indices[label_index]] = lightest_fold
labels = np.array(labels, copy=True) unique_labels = np.unique(labels) for i in unique_labels: yield labels == i
permutation = rng.permutation(n_samples) ind_test = permutation[:n_test] ind_train = permutation[n_test:(n_test + n_train)] yield ind_train, ind_test
raise ValueError("Invalid value for test_size: %r" % test_size)
raise ValueError("Invalid value for train_size: %r" % train_size)
raise ValueError("Cannot use a custom kernel function. " "Precompute the kernel matrix instead.")
if self._is_training_data(X): return 2. - float(self.train_sizes) / self.n_max_train_sizes else: return float(self.train_sizes) / self.n_max_train_sizes
scores = cross_val_score(clf, X, y2) assert_array_equal(scores, clf.score(X, y2))
multioutput_y = np.column_stack([y2, y2[::-1]]) scores = cross_val_score(clf, X_sparse, multioutput_y) assert_array_equal(scores, clf.score(X_sparse, multioutput_y))
scores = cross_val_score(clf, X_sparse, multioutput_y) assert_array_equal(scores, clf.score(X_sparse, multioutput_y))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) scores = cross_val_score(clf, X.tolist(), y2.tolist())
X_3d = X[:, :, np.newaxis] clf = MockClassifier(allow_nd=True) scores = cross_val_score(clf, X_3d, y2)
X, y = make_classification(n_samples=20, n_classes=2, random_state=0)
svm = SVC(kernel="precomputed") assert_raises(ValueError, cross_val_score, svm, X, y)
assert_raises(ValueError, cross_val_score, svm, linear_kernel.tolist(), y)
scores = cross_val_score(reg, X, y, cv=5) assert_array_almost_equal(scores, [0.94, 0.97, 0.97, 0.99, 0.92], 2)
def custom_score(y_true, y_pred): return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
y = np.mod(np.arange(len(y)), 3)
preds2 = np.zeros_like(y) for train, test in cv.split(X, y): est.fit(X[train], y[train]) preds2[test] = est.predict(X[test])
predictions = cross_val_predict(clf, X, y) assert_equal(predictions.shape, (150,))
predictions = cross_val_predict(clf, X_sparse, multioutput_y) assert_equal(predictions.shape, (150, 2))
predictions = cross_val_predict(clf, X_sparse, multioutput_y) assert_array_equal(predictions.shape, (150, 2))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) predictions = cross_val_predict(clf, X.tolist(), y.tolist())
estimator = MockImprovingEstimator(1) assert_raises(ValueError, learning_curve, estimator, X, y, exploit_incremental_learning=True)
for train, test in kfold.split(X, y): est.fit(X[train], y[train]) expected_predictions[test] = func(X[test])
empty = ParameterGrid({}) assert_equal(len(empty), 1) assert_equal(list(empty), [{}]) assert_grid_iter_equals_getitem(empty) assert_raises(IndexError, lambda: empty[1])
grid_search.score(X, y) grid_search.predict_proba(X) grid_search.decision_function(X) grid_search.transform(X)
grid_search.scoring = 'sklearn' assert_raises(ValueError, grid_search.fit, X, y)
grid_search_no_score.fit(X, y)
assert_equal(grid_search_no_score.best_params_, grid_search.best_params_) assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))
grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs}) assert_raise_message(TypeError, "no scoring", grid_search_no_score.fit, [[1]])
assert_true(score_auc < 1.0) assert_true(score_accuracy < 1.0) assert_not_equal(score_auc, score_accuracy)
rng = np.random.RandomState(0)
gs.fit(X, y)
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1]}) grid_search.fit(X, y) assert_true(hasattr(grid_search, "grid_scores_"))
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
K_train = np.dot(X_[:180], X_[:180].T) y_train = y_[:180]
K_test = np.dot(X_[180:], X_[:180].T) y_test = y_[180:]
assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
types = [(MockDataFrame, MockDataFrame)] try: from pandas import Series, DataFrame types.append((DataFrame, Series)) except ImportError: pass
grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4])) grid_search.fit(X) assert_equal(grid_search.best_params_["n_clusters"], 4)
X, y = make_classification(n_samples=200, n_features=100, n_informative=3, random_state=0)
sorted_grid_scores = list(sorted(search.grid_scores_, key=lambda x: x.mean_validation_score)) best_score = sorted_grid_scores[-1].mean_validation_score assert_equal(search.best_score_, best_score)
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True) grid_search.fit(X, y)
assert all(np.all(this_point.cv_validation_scores == 0.0) for this_point in gs.grid_scores_ if this_point.parameters['parameter'] == FailingClassifier.FAILING_PARAMETER)
gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy', refit=False, error_score='raise')
assert_raises(ValueError, gs.fit, X, y)
assert_equal(n_splits[i], cv.get_n_splits(X, y, labels))
assert_equal(cv_repr, repr(cv))
train, test = set(train), set(test)
assert_equal(train.intersection(test), set())
assert_equal(train.union(test), set(range(n_samples)))
if expected_n_iter is not None: assert_equal(cv.get_n_splits(X, y, labels), expected_n_iter) else: expected_n_iter = cv.get_n_splits(X, y, labels)
assert_equal(iterations, expected_n_iter) if n_samples is not None: assert_equal(collected_test_samples, set(range(n_samples)))
assert_raises(ValueError, next, KFold(4).split(X1))
y = np.array([3, 3, -1, -1, 3])
with warnings.catch_warnings(): warnings.simplefilter("ignore") check_cv_coverage(skf_3, X2, y, labels=None, expected_n_iter=3)
y = np.array([3, 3, -1, -1, 2])
assert_raises(TypeError, KFold, n_folds=4, shuffle=None)
X1 = np.ones(18) kf = KFold(3) check_cv_coverage(kf, X1, y=None, labels=None, expected_n_iter=3)
X2 = np.ones(17) kf = KFold(3) check_cv_coverage(kf, X2, y=None, labels=None, expected_n_iter=3)
assert_equal(5, KFold(5).get_n_splits(X2))
X2 = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
assert_equal(5, StratifiedKFold(5).get_n_splits(X, y))
X = np.ones(17) y = [0] * 3 + [1] * 14
kf = KFold(3) kf2 = KFold(3, shuffle=True, random_state=0) kf3 = KFold(3, shuffle=True, random_state=1)
assert_not_equal(len(np.intersect1d(tr_a, tr_b)), len(tr1))
all_folds[te2] = 1
assert_equal(sum(all_folds), 300)
assert_raises(ValueError, next, StratifiedShuffleSplit(3, 0.2).split(X, y))
assert_raises(ValueError, next, StratifiedShuffleSplit(train_size=2).split(X, y)) assert_raises(ValueError, next, StratifiedShuffleSplit(test_size=2).split(X, y))
n_folds = 5 n_iter = 1000
y = [0, 1, 2, 3] * 3 + [4, 5] * 5 X = np.ones_like(y)
repr(slo)
assert_equal(slo.get_n_splits(X, y, labels=l), n_iter)
assert_equal(l[train].size + l[test].size, l.size)
assert_array_equal(np.intersect1d(train, test), [])
assert_equal(3, LeavePLabelOut(n_labels=2).get_n_splits(X, y, labels)) assert_equal(3, LeaveOneLabelOut().get_n_splits(X, y, labels))
np.testing.assert_equal(list(KFold(3).split(X)), list(cv.split(X)))
np.testing.assert_equal(list(cv), list(wrapped_old_skf.split()))
assert_equal(len(cv), wrapped_old_skf.get_n_splits())
n_labels = 15 n_samples = 1000 n_folds = 5
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
for label in np.unique(labels): assert_equal(len(np.unique(folds[labels == label])), 1)
folds = np.zeros(n_samples) for i, (_, test) in enumerate(lkf.split(X, y, labels)): folds[test] = i
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
rng = np.random.RandomState(0)
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
if not callable(getattr(estimator, method)): raise AttributeError('{} not implemented in estimator' .format(method))
predictions = [pred_block_i for pred_block_i, _ in prediction_blocks] test_indices = np.concatenate([indices_i for _, indices_i in prediction_blocks])
if sp.issparse(predictions[0]): predictions = sp.vstack(predictions, format=predictions[0].format) else: predictions = np.concatenate(predictions) return predictions[inv_test_indices]
fit_params = fit_params if fit_params is not None else {} fit_params = dict([(k, _index_param_value(X, v, train)) for k, v in fit_params.items()])
return v
cv_iter = list(cv_iter) scorer = check_scoring(estimator, scoring=scoring)
train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples) n_unique_ticks = train_sizes_abs.shape[0] if verbose > 0: print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
X_train, y_train, sw_train = \ X[:n_samples], y[:n_samples], sample_weight[:n_samples] X_test, y_test = X[n_samples:], y[n_samples:]
clf = MultinomialNB().fit(X_train, y_train, sample_weight=sw_train) prob_pos_clf = clf.predict_proba(X_test)[:, 1]
assert_greater(brier_score_loss(y_test, prob_pos_clf), brier_score_loss(y_test, prob_pos_pc_clf))
pc_clf.fit(this_X_train, y_train + 1, sample_weight=sw_train) prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1] assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)
pc_clf.fit(this_X_train, 2 * y_train - 1, sample_weight=sw_train) prob_pos_pc_clf_relabeled = pc_clf.predict_proba(this_X_test)[:, 1] assert_array_almost_equal(prob_pos_pc_clf, prob_pos_pc_clf_relabeled)
clf_base_regressor = CalibratedClassifierCV(Ridge()) clf_base_regressor.fit(X_train, y_train) clf_base_regressor.predict(X_test)
clf_invalid_method = CalibratedClassifierCV(clf, method="foo") assert_raises(ValueError, clf_invalid_method.fit, X_train, y_train)
clf_base_regressor = \ CalibratedClassifierCV(RandomForestRegressor(), method="sigmoid") assert_raises(RuntimeError, clf_base_regressor.fit, X_train, y_train)
msg = "LinearSVC does not support sample_weight." assert_warns_message( UserWarning, msg, calibrated_clf.fit, X_train, y_train, sample_weight=sw_train) probs_with_sw = calibrated_clf.predict_proba(X_test)
calibrated_clf.fit(X_train, y_train) probs_without_sw = calibrated_clf.predict_proba(X_test)
clf = LinearSVC() X, y_idx = make_blobs(n_samples=100, n_features=2, random_state=42, centers=3, cluster_std=3.0)
target_names = np.array(['a', 'b', 'c']) y = target_names[y_idx]
clf = MultinomialNB() clf.fit(X_train, y_train, sw_train) prob_pos_clf = clf.predict_proba(X_test)[:, 1]
assert_raises(ValueError, _SigmoidCalibration().fit, np.vstack((exF, exF)), exY)
assert_raises(ValueError, calibration_curve, [1.1], [-0.1], normalize=False)
X2 = rng.randint(5, size=(6, 100)) y2 = np.array([1, 1, 2, 2, 3, 3])
assert_raises(ValueError, GaussianNB().partial_fit, X, y, classes=[0, 1])
sw = np.ones(6) clf = GaussianNB().fit(X, y) clf_sw = GaussianNB().fit(X, y, sw)
ind = rng.randint(0, X.shape[0], 20) sample_weight = np.bincount(ind, minlength=X.shape[0])
clf.fit(X, y) assert_raises(ValueError, clf.partial_fit, np.hstack((X, X)), y)
clf = MultinomialNB() assert_raises(ValueError, clf.fit, -X, y2) y_pred = clf.fit(X, y2).predict(X)
y_pred_proba = clf.predict_proba(X) y_pred_log_proba = clf.predict_log_proba(X) assert_array_almost_equal(np.log(y_pred_proba), y_pred_log_proba, 8)
clf3 = MultinomialNB() clf3.partial_fit(X, y2, classes=np.unique(y2))
for cls in [BernoulliNB, MultinomialNB, GaussianNB]: assert_raises(ValueError, cls().fit, X2, y2[:-1])
clf = cls().fit(X2, y2) assert_raises(ValueError, clf.predict, X2[:, :-1])
assert_raises(ValueError, cls().partial_fit, X2, y2[:-1], classes=np.unique(y2))
assert_raises(ValueError, cls().partial_fit, X2, y2)
clf = cls() clf.partial_fit(X2, y2, classes=np.unique(y2)) assert_raises(ValueError, clf.partial_fit, X2, y2, classes=np.arange(42))
assert_raises(ValueError, clf.partial_fit, X2[:, :-1], y2)
assert_raises(ValueError, clf.predict, X2[:, :-1])
X_bernoulli = [[1, 100, 0], [0, 1, 0], [0, 100, 1]] X_multinomial = [[0, 1], [1, 3], [4, 0]]
yield check_sample_weight_multiclass, cls
X = [[1, 0, 0], [1, 1, 1]]
scores = cross_val_score(MultinomialNB(alpha=10), X, y, cv=10) assert_greater(scores.mean(), 0.86)
scores = cross_val_score(BernoulliNB(alpha=10), X > 4, y, cv=10) assert_greater(scores.mean(), 0.83)
scores = cross_val_score(GaussianNB(), X, y, cv=10) assert_greater(scores.mean(), 0.77)
clf = BernoulliNB(alpha=1.0) clf.fit(X, Y)
assert_array_equal(clf.feature_log_prob_, (num - denom))
Y = np.array([0, 0, 0, 1])
clf = BernoulliNB(alpha=1.0) clf.fit(X, Y)
class_prior = np.array([0.75, 0.25]) assert_array_almost_equal(np.exp(clf.class_log_prior_), class_prior)
X_test = np.array([[0, 1, 1, 0, 0, 1]])
unnorm_predict_proba = np.array([[0.005183999999999999, 0.02194787379972565]]) predict_proba = unnorm_predict_proba / np.sum(unnorm_predict_proba) assert_array_almost_equal(clf.predict_proba(X_test), predict_proba)
class MyEstimator(BaseEstimator):
buggy = Buggy() buggy.a = 2 assert_raises(RuntimeError, clone, buggy)
clf = MyEstimator(empty=np.array([])) clf2 = clone(clf) assert_array_equal(clf.empty, clf2.empty)
clf = MyEstimator(empty=np.nan) clf2 = clone(clf)
my_estimator = MyEstimator() str(my_estimator)
est = DeprecatedAttributeEstimator(a=1)
estimators = [DecisionTreeClassifier(max_depth=2), DecisionTreeRegressor(max_depth=2)] sets = [datasets.load_iris(), datasets.load_boston()]
assert_raises(Exception, getattr(delegator, method), delegator_data.fit_args[0])
getattr(delegator, method)(delegator_data.fit_args[0])
if self._is_training_data(X): return 2. - float(self.train_sizes) / self.n_max_train_sizes else: return float(self.train_sizes) / self.n_max_train_sizes
estimator = MockImprovingEstimator(1) assert_raises(ValueError, learning_curve, estimator, X, y, exploit_incremental_learning=True)
pipe.set_params(svc__a=0.1) assert_equal(clf.a, 0.1) assert_equal(clf.b, None) repr(pipe)
clf = SVC() filter1 = SelectKBest(f_classif) pipe = Pipeline([('anova', filter1), ('svc', clf)])
assert_raises(ValueError, Pipeline, [('svc', SVC()), ('svc', SVC())])
pipe.set_params(svc__C=0.1) assert_equal(clf.C, 0.1) repr(pipe)
assert_raises(ValueError, pipe.set_params, anova__C=0.1)
pipe2 = clone(pipe) assert_false(pipe.named_steps['svc'] is pipe2.named_steps['svc'])
params = pipe.get_params(deep=True) params2 = pipe2.get_params(deep=True)
params.pop('svc') params.pop('anova') params2.pop('svc') params2.pop('anova') assert_equal(params, params2)
pipe = Pipeline([('cls', LinearRegression())])
error_msg = ('Invalid parameter %s for estimator %s. ' 'Check the list of available parameters ' 'with `estimator.get_params().keys()`.')
assert_raise_message(ValueError, error_msg % ("fake", pipe), pipe.set_params, fake__estimator='nope')
predict = pipe.predict(X) assert_equal(predict.shape, (n_samples,))
iris = load_iris() scaler = StandardScaler() km = KMeans(random_state=0)
scaled = scaler.fit_transform(iris.data) separate_pred = km.fit_predict(scaled)
assert_array_almost_equal(X_transformed[:, :-1], svd.fit_transform(X)) assert_array_equal(X_transformed[:, -1], select.fit_transform(X, y).ravel())
fs.set_params(select__k=2) assert_equal(fs.fit_transform(X, y).shape, (X.shape[0], 4))
iris = load_iris() X = iris.data pca = PCA(n_components=2, svd_solver='full') pipeline = Pipeline([('pca', pca)])
iris = load_iris() X = iris.data y = iris.target transft = TransfT() pipeline = Pipeline([('mock', transft)])
X_trans = pipeline.fit_transform(X, y) X_trans2 = transft.fit(X, y).transform(X) assert_array_almost_equal(X_trans, X_trans2)
X = JUNK_FOOD_DOCS
X_transformed_parallel2 = fs_parallel2.fit_transform(X) assert_array_equal( X_transformed.toarray(), X_transformed_parallel2.toarray() )
X_transformed_parallel2 = fs_parallel2.transform(X) assert_array_equal( X_transformed.toarray(), X_transformed_parallel2.toarray() )
empty = ParameterGrid({}) assert_equal(len(empty), 1) assert_equal(list(empty), [{}]) assert_grid_iter_equals_getitem(empty) assert_raises(IndexError, lambda: empty[1])
grid_search.score(X, y) grid_search.predict_proba(X) grid_search.decision_function(X) grid_search.transform(X)
grid_search.scoring = 'sklearn' assert_raises(ValueError, grid_search.fit, X, y)
grid_search_no_score.fit(X, y)
assert_equal(grid_search_no_score.best_params_, grid_search.best_params_) assert_equal(grid_search.score(X, y), grid_search_no_score.score(X, y))
grid_search_no_score = GridSearchCV(clf_no_score, {'C': Cs}) assert_raise_message(TypeError, "no scoring", grid_search_no_score.fit, [[1]])
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1]}) grid_search.fit(X, y) assert_true(hasattr(grid_search, "grid_scores_"))
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
X_, y_ = make_classification(n_samples=200, n_features=100, random_state=0)
K_train = np.dot(X_[:180], X_[:180].T) y_train = y_[:180]
K_test = np.dot(X_[180:], X_[:180].T) y_test = y_[180:]
assert_raises(ValueError, cv.fit, K_train.tolist(), y_train)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
X = np.arange(100).reshape(10, 10) y = np.array([0] * 5 + [1] * 5)
types = [(MockDataFrame, MockDataFrame)] try: from pandas import Series, DataFrame types.append((DataFrame, Series)) except ImportError: pass
grid_search = GridSearchCV(km, param_grid=dict(n_clusters=[2, 3, 4])) grid_search.fit(X) assert_equal(grid_search.best_params_["n_clusters"], 4)
X, y = make_classification(n_samples=200, n_features=100, n_informative=3, random_state=0)
sorted_grid_scores = list(sorted(search.grid_scores_, key=lambda x: x.mean_validation_score)) best_score = sorted_grid_scores[-1].mean_validation_score assert_equal(search.best_score_, best_score)
clf = MockClassifier() grid_search = GridSearchCV(clf, {'foo_param': [1, 2, 3]}, refit=True) grid_search.fit(X, y)
assert all(np.all(this_point.cv_validation_scores == 0.0) for this_point in gs.grid_scores_ if this_point.parameters['parameter'] == FailingClassifier.FAILING_PARAMETER)
gs = GridSearchCV(clf, [{'parameter': [0, 1, 2]}], scoring='accuracy', refit=False, error_score='raise')
assert_raises(ValueError, gs.fit, X, y)
from __future__ import print_function
estimators = all_estimators(include_meta_estimators=True)
assert_greater(len(estimators), 0)
yield check_parameters_default_constructible, name, Estimator
elif (name in CROSS_DECOMPOSITION or name in ['LinearSVC', 'LogisticRegression']): continue
yield (check_non_transformer_estimators_n_iter, name, estimator, 'Multi' in name)
with ignore_warnings(): estimator = Estimator()
external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding', 'RandomizedLasso', 'LogisticRegressionCV']
with ignore_warnings(): yield check_transformer_n_iter, name, estimator
if issubclass(Estimator, ProjectedGradientNMF): with ignore_warnings(): yield check_get_params_invariance, name, Estimator else: yield check_get_params_invariance, name, Estimator
for random_matrix in all_random_matrix: yield check_input_size_random_matrix, random_matrix yield check_size_generated, random_matrix yield check_zero_mean_and_unit_norm, random_matrix
n_components = 100 n_features = 1000 A = gaussian_random_matrix(n_components, n_features, random_state=0)
n_components = 100 n_features = 500
def test_sparse_random_projection_transformer_invalid_density(): for RandomProjection in all_SparseRandomProjection: assert_raises(ValueError, RandomProjection(density=1.1).fit, data)
original_distances = original_distances[non_identical]
projected_distances = projected_distances[non_identical]
assert_less(distances_ratio.max(), 1 + eps) assert_less(1 - eps, distances_ratio.min())
rp = SparseRandomProjection(n_components=10, dense_output=True, random_state=0) rp.fit(data) assert isinstance(rp.transform(data), np.ndarray)
rp = SparseRandomProjection(n_components=10, dense_output=False, random_state=0) rp = rp.fit(data) assert isinstance(rp.transform(data), np.ndarray)
assert sp.issparse(rp.transform(sparse_data))
assert_equal(rp.n_components, 'auto') assert_equal(rp.n_components_, 110)
projected_2 = rp.transform(data) assert_array_equal(projected_1, projected_2)
rp2 = RandomProjection(random_state=0, eps=0.5) projected_3 = rp2.fit_transform(data) assert_array_equal(projected_1, projected_3)
assert_raises(ValueError, rp.transform, data[:, 1:5])
X, y = datasets.make_regression(n_targets=1) X_train, y_train = X[:50], y[:50] X_test, y_test = X[50:], y[50:]
rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0)) rgr.fit(X, y, w)
multi_target_forest.fit(X, y)
for i in range(3):
svc = LinearSVC(random_state=0) multi_class_svc = OneVsRestClassifier(svc) multi_target_svc = MultiOutputClassifier(multi_class_svc)
for i in range(3):
X_ = X[:, np.newaxis, :] Y_ = Y[np.newaxis, :, :]
kernel = (large_kernel.sum(axis=2))
transform = AdditiveChi2Sampler(sample_steps=3) X_trans = transform.fit_transform(X) Y_trans = transform.transform(Y)
Y_neg = Y.copy() Y_neg[0, 0] = -1 assert_raises(ValueError, transform.transform, Y_neg)
transform = AdditiveChi2Sampler(sample_steps=4) assert_raises(ValueError, transform.fit, X)
sample_steps_available = [1, 2, 3] for sample_steps in sample_steps_available:
transform = AdditiveChi2Sampler(sample_steps=sample_steps) assert_equal(transform.sample_interval, None)
transform.fit(X) assert_not_equal(transform.sample_interval_, None)
sample_interval = 0.3 transform = AdditiveChi2Sampler(sample_steps=4, sample_interval=sample_interval) assert_equal(transform.sample_interval, sample_interval) transform.fit(X) assert_equal(transform.sample_interval_, sample_interval)
c = 0.03 X_c = (X + c)[:, np.newaxis, :] Y_c = (Y + c)[np.newaxis, :, :]
transform = SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42) X_trans = transform.fit_transform(X) Y_trans = transform.transform(Y)
Y_neg = Y.copy() Y_neg[0, 0] = -1 assert_raises(ValueError, transform.transform, Y_neg)
gamma = 10. kernel = rbf_kernel(X, Y, gamma=gamma)
rbf_transform = RBFSampler(gamma=gamma, n_components=1000, random_state=42) X_trans = rbf_transform.fit_transform(X) Y_trans = rbf_transform.transform(Y) kernel_approx = np.dot(X_trans, Y_trans.T)
rnd = np.random.RandomState(0) X = rnd.uniform(size=(10, 4))
X_transformed = Nystroem(n_components=X.shape[0]).fit_transform(X) K = rbf_kernel(X) assert_array_almost_equal(np.dot(X_transformed, X_transformed.T), K)
rng = np.random.RandomState(0) X = rng.rand(10, 20)
rnd = np.random.RandomState(37) X = rnd.uniform(size=(10, 4))
rnd = np.random.RandomState(42) n_samples = 10 X = rnd.uniform(size=(n_samples, 4))
assert_equal(_top_import_error, None)
log_proba = clf.predict_log_proba(X)
assert_array_equal(np.log(proba[k]), log_proba[k])
est = DummyRegressor() est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy="median") est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy="quantile", quantile=0.5) est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy="quantile", quantile=0.8) est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
constants = random_state.randn(5)
est = DummyRegressor(strategy="constant", constant=constants) est.fit(X_learn, y_learn) y_pred_learn = est.predict(X_learn) y_pred_test = est.predict(X_test)
est = DummyRegressor(strategy='mean') est.fit(X, y)
version = sys.version_info if version[0] == 3: if version[1] == 3: reload = None else: from importlib import reload
X1 = np.array([[-2, ], [-1, ], [-1, ], [1, ], [1, ], [2, ]], dtype='f')
y4 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 2])
y_pred1 = clf.fit(X1, y).predict(X1) assert_array_equal(y_pred1, y, 'solver %s' % solver)
y_pred3 = clf.fit(X, y3).predict(X) assert_true(np.any(y_pred3 != y3), 'solver %s' % solver)
clf = LinearDiscriminantAnalysis(priors=[0.5, 0.5]) clf.fit(X, y)
n_features = 2 n_classes = 2 n_samples = 1000 X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=11)
assert_array_almost_equal(clf_lda_svd.explained_variance_ratio_, clf_lda_eigen.explained_variance_ratio_[:tested_length])
means = np.array([[0, 0, -1], [0, 2, 0], [0, -2, 0], [0, 0, 5]])
clf = LinearDiscriminantAnalysis(solver="svd").fit(X, y) means_transformed = clf.transform(means)
assert_almost_equal(np.cov(clf.transform(scatter).T), np.eye(2))
assert_almost_equal(np.abs(np.dot(d1[:2], [1, 0])), 1.0)
assert_almost_equal(np.abs(np.dot(d2[:2], [0, 1])), 1.0)
assert_equal(clf.fit(x, y).score(x, y), 1.0, 'using covariance: %s' % solver)
clf = QuadraticDiscriminantAnalysis() y_pred = clf.fit(X6, y6).predict(X6) assert_array_equal(y_pred, y6)
y_pred1 = clf.fit(X7, y6).predict(X7) assert_array_equal(y_pred1, y6)
assert_true(np.any(y_pred3 != y7))
assert_raises(ValueError, clf.fit, X6, y4)
clf = QuadraticDiscriminantAnalysis().fit(X6, y6) assert_true(not hasattr(clf, 'covariances_'))
clf = QuadraticDiscriminantAnalysis(store_covariances=True).fit(X6, y6) assert_true(hasattr(clf, 'covariances_'))
clf = QuadraticDiscriminantAnalysis() with ignore_warnings(): y_pred = clf.fit(X2, y6).predict(X2) assert_true(np.any(y_pred != y6))
clf = QuadraticDiscriminantAnalysis(reg_param=0.01) with ignore_warnings(): clf.fit(X2, y6) y_pred = clf.predict(X2) assert_array_equal(y_pred, y6)
clf = QuadraticDiscriminantAnalysis(reg_param=0.1) with ignore_warnings(): clf.fit(X5, y5) y_pred5 = clf.predict(X5) assert_array_equal(y_pred5, y5)
reload(sklearn.lda) return sklearn.lda
reload(sklearn.qda) return sklearn.qda
x = np.dot(x, np.arange(x.shape[1] ** 2).reshape(x.shape[1], x.shape[1]))
y = np.array([0.0, 1.1, 2.0, 3.0]) msg = type_of_target(y) assert_raise_message(ValueError, msg, check_classification_targets, y)
assert_greater(np.mean(pred == iris.target), .9)
Y_proba = clf_sprs.predict_proba(X_test)
pred = Y_proba > .5 assert_array_equal(pred, Y_pred_sprs.toarray())
X = np.ones((10, 2)) X[:5, :] = 0
y = np.zeros((10, 3)) y[5:, 0] = 1 y[:, 1] = 1 y[:, 2] = 1
y = np.zeros((10, 2))
clf = OneVsRestClassifier(base_clf).fit(X, Y) y_pred = clf.predict([[0, 0, 4]])[0] assert_array_equal(y_pred, [0, 0, 1])
clf = OneVsRestClassifier(base_clf).fit(X, Y) y_pred = clf.predict([[3, 0, 0]])[0] assert_equal(y_pred, 1)
decision_only = OneVsRestClassifier(svm.SVR()).fit(X_train, Y_train) assert_raises(AttributeError, decision_only.predict_proba, X_test)
decision_only = OneVsRestClassifier(svm.SVC(probability=False)) decision_only.fit(X_train, Y_train) assert_raises(AttributeError, decision_only.predict_proba, X_test)
pred = Y_proba > .5 assert_array_equal(pred, Y_pred)
decision_only = OneVsRestClassifier(svm.SVR()).fit(X_train, Y_train) assert_raises(AttributeError, decision_only.predict_proba, X_test)
pred = np.array([l.argmax() for l in Y_proba]) assert_false((pred - Y_pred).any())
ovr = OneVsRestClassifier(LinearSVC(random_state=0)) assert_raises(ValueError, lambda x: ovr.coef_, None)
ovr = OneVsRestClassifier(DecisionTreeClassifier()) ovr.fit(iris.data, iris.target) assert_raises(AttributeError, lambda x: ovr.coef_, None)
votes = np.zeros((n_samples, n_classes))
assert_array_equal(votes, np.round(decisions))
assert_true(set(votes[:, class_idx]).issubset(set([0., 1., 2.])))
votes = np.round(ovo_decision) normalized_confidences = ovo_decision - votes
assert_array_equal(votes[0, :], 1) assert_array_equal(np.argmax(votes[1:], axis=1), ovo_prediction[1:]) assert_equal(ovo_prediction[0], normalized_confidences[0].argmax())
X = np.array([[1, 2], [2, 1], [-2, 1], [-2, -1]]) y_ref = np.array([2, 0, 1, 2])
X = np.eye(4) y = np.array(['a', 'b', 'c', 'd'])
is_increasing = assert_no_warnings(check_increasing, x, y) assert_true(is_increasing)
is_increasing = assert_no_warnings(check_increasing, x, y) assert_true(is_increasing)
is_increasing = assert_no_warnings(check_increasing, x, y) assert_false(is_increasing)
is_increasing = assert_no_warnings(check_increasing, x, y) assert_false(is_increasing)
is_increasing = assert_warns_message(UserWarning, "interval", check_increasing, x, y)
ir = IsotonicRegression() assert_array_equal(ir.fit_transform(np.ones(len(x)), y), np.mean(y))
y = np.array([10, 9, 10, 7, 6, 6.1, 5]) x = np.arange(len(y))
is_increasing = y_[0] < y_[-1] assert_false(is_increasing)
y = np.array([5, 6.1, 6, 7, 10, 9, 10]) x = np.arange(len(y))
is_increasing = y_[0] < y_[-1] assert_true(is_increasing)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="raise") ir.fit(x, y)
assert_raises(ValueError, ir.predict, [min(x) - 10, max(x) + 10])
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="clip") ir.fit(x, y)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="nan") ir.fit(x, y)
y1 = ir.predict([min(x) - 10, max(x) + 10]) assert_equal(sum(np.isnan(y1)), 2)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="xyz")
assert_raises(ValueError, ir.fit, x, y)
y = np.array([3, 7, 5, 9, 8, 7, 10]) x = np.arange(len(y))
ir = IsotonicRegression(increasing='auto', out_of_bounds="raise")
ir.fit(x, y) ir.out_of_bounds = "xyz" assert_raises(ValueError, ir.transform, x)
ir = IsotonicRegression(increasing='auto', out_of_bounds="clip") ir.fit(x, y)
rng = np.random.RandomState(42)
regression = IsotonicRegression() n_samples = 50 x = np.linspace(-3, 3, n_samples) y = x + rng.uniform(size=n_samples)
w = rng.uniform(size=n_samples) w[5:8] = 0 regression.fit(x, y, sample_weight=w)
regression.fit(x, y, sample_weight=w)
weights[rng.rand(n_samples) < 0.1] = 0
X_train_fit, y_train_fit = slow_model._build_y(X_train, y_train, sample_weight=weights, trim_duplicates=False) slow_model._build_f(X_train_fit, y_train_fit)
fast_model.fit(X_train, y_train, sample_weight=weights)
ir = IsotonicRegression() copy.copy(ir)
y = np.arange(10) % 3
train, test = set(train), set(test)
assert_equal(train.intersection(test), set())
assert_equal(train.union(test), set(range(n_samples)))
if expected_n_iter is not None: assert_equal(len(cv), expected_n_iter) else: expected_n_iter = len(cv)
assert_equal(iterations, expected_n_iter) if n_samples is not None: assert_equal(collected_test_samples, set(range(n_samples)))
assert_raises(ValueError, cval.KFold, 3, 4)
y = [3, 3, -1, -1, 3]
check_cv_coverage(cv, expected_n_iter=3, n_samples=len(y))
y = [3, 3, -1, -1, 2]
assert_raises(ValueError, cval.KFold, 2.5, 2)
assert_raises(ValueError, cval.KFold, 5, 1.5) assert_raises(ValueError, cval.StratifiedKFold, y, 1.5)
kf = cval.KFold(300, 3) check_cv_coverage(kf, expected_n_iter=3, n_samples=300)
kf = cval.KFold(17, 3) check_cv_coverage(kf, expected_n_iter=3, n_samples=17)
splits = iter(cval.KFold(4, 2)) train, test = next(splits) assert_array_equal(test, [0, 1]) assert_array_equal(train, [2, 3])
kf = cval.KFold(300, 3, shuffle=True, random_state=0) ind = np.arange(300)
n_labels = 15 n_samples = 1000 n_folds = 5
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
for label in np.unique(labels): assert_equal(len(np.unique(folds[labels == label])), 1)
assert_equal(len(folds), len(labels)) for i in np.unique(folds): assert_greater_equal(tolerance, abs(sum(folds == i) - ideal_n_labels_per_fold))
for label in np.unique(labels): assert_equal(len(np.unique(folds[labels == label])), 1)
for train, test in cval.LabelKFold(labels, n_folds=n_folds): assert_equal(len(np.intersect1d(labels[train], labels[test])), 0)
labels = np.array([1, 1, 1, 2, 2]) assert_raises(ValueError, cval.LabelKFold, labels, n_folds=3)
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 0.2)
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 2) assert_raises(ValueError, cval.StratifiedShuffleSplit, y, 3, 3, 2)
assert_raises(ValueError, cval.StratifiedShuffleSplit, y, train_size=2) assert_raises(ValueError, cval.StratifiedShuffleSplit, y, test_size=2)
n_folds = 5 n_iter = 1000
labels = [0, 1, 2, 3] * 3 + [4, 5] * 5
repr(slo)
assert_equal(len(slo), n_iter)
assert_equal(y[train].size + y[test].size, y.size)
assert_array_equal(np.intersect1d(train, test), [])
scores = cval.cross_val_score(clf, X, y) assert_array_equal(scores, clf.score(X, y))
scores = cval.cross_val_score(clf, X_sparse, X) assert_array_equal(scores, clf.score(X_sparse, X))
scores = cval.cross_val_score(clf, X_sparse, X) assert_array_equal(scores, clf.score(X_sparse, X))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) scores = cval.cross_val_score(clf, X.tolist(), y.tolist())
X_3d = X[:, :, np.newaxis] clf = MockClassifier(allow_nd=True) scores = cval.cross_val_score(clf, X_3d, y)
svm = SVC(kernel="precomputed") assert_raises(ValueError, cval.cross_val_score, svm, X, y)
assert_raises(ValueError, cval.cross_val_score, svm, linear_kernel.tolist(), y)
X_df = MockDataFrame(X) X_train, X_test = cval.train_test_split(X_df) assert_true(isinstance(X_train, MockDataFrame)) assert_true(isinstance(X_test, MockDataFrame))
def custom_score(y_true, y_pred): return (((y_true == y_pred).sum() - (y_true != y_pred).sum()) / y_true.shape[0])
y = np.mod(np.arange(len(y)), 3)
ss = cval.ShuffleSplit(10, random_state=21) assert_array_equal(list(a for a, b in ss), list(a for a, b in ss))
preds2 = np.zeros_like(y) for train, test in cv: est.fit(X[train], y[train]) preds2[test] = est.predict(X[test])
predictions = cval.cross_val_predict(clf, X, y) assert_equal(predictions.shape, (10,))
predictions = cval.cross_val_predict(clf, X_sparse, X) assert_equal(predictions.shape, (10, 2))
predictions = cval.cross_val_predict(clf, X_sparse, X) assert_array_equal(predictions.shape, (10, 2))
list_check = lambda x: isinstance(x, list) clf = CheckingClassifier(check_X=list_check) predictions = cval.cross_val_predict(clf, X.tolist(), y.tolist())
cv = list(check_cv(cv, X, y, classifier=is_classifier(estimator))) scorer = check_scoring(estimator, scoring=scoring)
train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples) n_unique_ticks = train_sizes_abs.shape[0] if verbose > 0: print("[learning_curve] Training set sizes: " + str(train_sizes_abs))
assert_array_equal(X, Xdigits)
X = csr_matrix(Xdigits[:4])
rbm1.random_state = 42 d_score = rbm1.score_samples(X) rbm1.random_state = 42 s_score = rbm1.score_samples(lil_matrix(X)) assert_almost_equal(d_score, s_score)
with np.errstate(under='ignore'): rbm1.score_samples([np.arange(1000) * 100])
X = X_digits_binary[:100] y = y_digits_binary[:100]
mlp.n_iter_ = 0 mlp.learning_rate_ = 0.1
mlp.n_layers_ = 3
mlp._coef_grads = [0] * (mlp.n_layers_ - 1) mlp._intercept_grads = [0] * (mlp.n_layers_ - 1)
def loss_grad_fun(t): return mlp._loss_grad_lbfgs(t, X, Y, activations, deltas, coef_grads, intercept_grads)
for X, y in classification_datasets: X_train = X[:150] y_train = y[:150] X_test = X[150:]
for X, y in classification_datasets: X = X y = y mlp = MLPClassifier(algorithm='sgd', max_iter=100, random_state=1, tol=0, alpha=1e-5, learning_rate_init=0.2)
X = Xboston y = yboston
mlp.fit(X, y)
X = [[3, 2], [1, 6]] y = [1, 0]
assert_raises(ValueError, MLPClassifier( algorithm='sgd').partial_fit, X, y, classes=[2])
assert_false(hasattr(MLPClassifier(algorithm='l-bfgs'), 'partial_fit'))
X = [[3, 2], [1, 6]] y = [1, 0] clf = MLPClassifier
X = X_digits_binary[:50] y = y_digits_binary[:50]
X = X_digits_multi[:10] y = y_digits_multi[:10]
for i in range(self.n_layers_ - 1): activations[i + 1] = safe_sparse_dot(activations[i], self.coefs_[i]) activations[i + 1] += self.intercepts_[i]
if (i + 1) != (self.n_layers_ - 1): activations[i + 1] = hidden_activation(activations[i + 1])
if with_output_activation: output_activation = ACTIVATIONS[self.out_activation_] activations[i + 1] = output_activation(activations[i + 1])
activations = self._forward_pass(activations)
last = self.n_layers_ - 2
diff = y - activations[-1] deltas[last] = -diff
coef_grads, intercept_grads = self._compute_loss_grad( last, n_samples, activations, deltas, coef_grads, intercept_grads)
self.n_iter_ = 0 self.t_ = 0 self.n_outputs_ = y.shape[1]
self.n_layers_ = len(layer_units)
self.coefs_ = [] self.intercepts_ = []
init_bound = np.sqrt(2. / (fan_in + fan_out))
raise ValueError("Unknown activation function %s" % self.activation)
hidden_layer_sizes = self.hidden_layer_sizes if not hasattr(hidden_layer_sizes, "__iter__"): hidden_layer_sizes = [hidden_layer_sizes] hidden_layer_sizes = list(hidden_layer_sizes)
self._validate_hyperparameters() if np.any(np.array(hidden_layer_sizes) <= 0): raise ValueError("hidden_layer_sizes must be > 0, got %s." % hidden_layer_sizes)
if y.ndim == 1: y = y.reshape((-1, 1))
self._initialize(y, layer_units)
activations = [X] activations.extend(np.empty((batch_size, n_fan_out)) for n_fan_out in layer_units[1:]) deltas = [np.empty_like(a_layer) for a_layer in activations]
if self.algorithm in _STOCHASTIC_ALGOS: self._fit_stochastic(X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)
elif self.algorithm == 'l-bfgs': self._fit_lbfgs(X, y, activations, deltas, coef_grads, intercept_grads, layer_units) return self
self._coef_indptr = [] self._intercept_indptr = [] start = 0
for i in range(self.n_layers_ - 1): n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
for i in range(self.n_layers_ - 1): end = start + layer_units[i + 1] self._intercept_indptr.append((start, end)) start = end
packed_coef_inter = _pack(self.coefs_, self.intercepts_)
grads = coef_grads + intercept_grads self._optimizer.update_params(grads)
self._update_no_improvement_count(early_stopping, X_val, y_val)
self._optimizer.iteration_ends(self.t_)
self.coefs_ = self._best_coefs self.intercepts_ = self._best_intercepts
self.validation_scores_.append(self.score(X_val, y_val))
last_valid_score = self.validation_scores_[-1]
hidden_layer_sizes = self.hidden_layer_sizes if not hasattr(hidden_layer_sizes, "__iter__"): hidden_layer_sizes = [hidden_layer_sizes] hidden_layer_sizes = list(hidden_layer_sizes)
activations = [X]
self._forward_pass(activations, with_output_activation=False) y_pred = activations[-1]
self._check_params()
X, y = check_X_y(X, y, multi_output=True, y_numeric=True) self.y_ndim_ = y.ndim if y.ndim == 1: y = y[:, np.newaxis]
n_samples, n_features = X.shape _, n_targets = y.shape
self._check_params(n_samples)
X = check_array(X) n_eval, _ = X.shape n_samples, n_features = self.X.shape n_samples_y, n_targets = self.y.shape
self._check_params(n_samples)
X = (X - self.X_mean) / self.X_std
y = np.zeros(n_eval) if eval_MSE: MSE = np.zeros(n_eval)
dx = manhattan_distances(X, Y=self.X, sum_over_features=False) f = self.regr(X) r = self.corr(self.theta_, dx).reshape(n_eval, n_samples)
y_ = np.dot(f, self.beta) + np.dot(r, self.gamma)
y = (self.y_mean + self.y_std * y_).reshape(n_eval, n_targets)
u = linalg.solve_triangular(self.G.T, np.dot(self.Ft.T, rt) - f.T, lower=True)
u = np.zeros((n_targets, n_eval))
MSE[MSE < 0.] = 0.
theta = self.theta_
reduced_likelihood_function_value = - np.inf par = {}
n_samples = self.X.shape[0] D = self.D ij = self.ij F = self.F
try: C = linalg.cholesky(R, lower=True) except linalg.LinAlgError: return reduced_likelihood_function_value, par
beta = linalg.solve_triangular(G, np.dot(Q.T, Yt))
beta = np.array(self.beta0)
detR = (np.diag(C) ** (2. / n_samples)).prod()
best_optimal_theta = [] best_optimal_rlf_value = [] best_optimal_par = []
if self.optimizer == 'Welch' and self.theta0.size == 1: self.optimizer = 'fmin_cobyla'
theta0 = self.theta0
theta0, thetaL, thetaU = self.theta0, self.thetaL, self.thetaU corr = self.corr verbose = self.verbose
self.optimizer = 'fmin_cobyla' self.verbose = False
self.theta0, self.thetaL, self.thetaU = theta0, thetaL, thetaU self.corr = corr self.optimizer = 'Welch' self.verbose = verbose
if self.beta0 is not None: self.beta0 = np.atleast_2d(self.beta0) if self.beta0.shape[1] != 1: self.beta0 = self.beta0.T
self.theta0 = np.atleast_2d(self.theta0) lth = self.theta0.size
self.verbose = bool(self.verbose)
self.normalize = bool(self.normalize)
if self.optimizer not in self._optimizer_types: raise ValueError("optimizer should be one of %s" % self._optimizer_types)
self.random_start = int(self.random_start)
def obj_func(theta, eval_gradient=True): if eval_gradient: lml, grad = self.log_marginal_likelihood( theta, eval_gradient=True) return -lml, -grad else: return -self.log_marginal_likelihood(theta)
optima = [self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds)]
K = self.kernel_(self.X_train_)
var_f_star = self.kernel_.diag(X) - np.einsum("ij,ij->j", v, v)
Z, (pi, W_sr, L, b, a) = \ self._posterior_mode(K, return_temporaries=True)
d_Z = np.empty(theta.shape[0])
s_2 = -0.5 * (np.diag(K) - np.einsum('ij, ij -> j', C, C)) \
s_1 = .5 * a.T.dot(C).dot(a) - .5 * R.T.ravel().dot(C.ravel())
return np.mean( [estimator.log_marginal_likelihood( theta[n_dims * i:n_dims * (i + 1)]) for i, estimator in enumerate(estimators)])
return self
setattr(self, hyperparameter.name, np.exp(theta[i:i + hyperparameter.n_elements])) i += hyperparameter.n_elements
K = squareform(K) np.fill_diagonal(K, 1)
return K, np.empty((X.shape[0], X.shape[0], 0))
K = squareform(K) np.fill_diagonal(K, 1)
K_gradient = np.empty((X.shape[0], X.shape[0], 0)) return K, K_gradient
if not self.hyperparameter_length_scale.fixed: length_scale_gradient = \ dists * K / (self.length_scale ** 2 * base) length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
if not self.hyperparameter_length_scale.fixed: length_scale_gradient = \ 4 / self.length_scale**2 * sin_of_arg**2 * K length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
if not self.hyperparameter_periodicity.fixed: periodicity_gradient = \ 4 * arg / self.length_scale**2 * cos_of_arg \ * sin_of_arg * K periodicity_gradient = periodicity_gradient[:, :, np.newaxis]
return np.apply_along_axis(self, 1, X)[:, 0]
for i, hyperparameter in enumerate(kernel.hyperparameters): assert_equal(theta[i], np.log(getattr(kernel, hyperparameter.name)))
for i, hyperparameter in enumerate(kernel.hyperparameters): theta[i] = np.log(42) kernel.theta = theta assert_almost_equal(getattr(kernel, hyperparameter.name), 42)
assert_almost_equal((RBF(2.0) + 1.0)(X), (1.0 + RBF(2.0))(X))
assert_almost_equal((3.0 * RBF(2.0))(X), (RBF(2.0) * 3.0)(X))
assert_not_equal(id(attr_value), id(attr_value_cloned))
if kernel != kernel_white: K1 = kernel(X) K2 = pairwise_kernels(X, metric=kernel) assert_array_almost_equal(K1, K2)
K1 = kernel(X, Y) K2 = pairwise_kernels(X, Y, metric=kernel) assert_array_almost_equal(K1, K2)
index = 0 params = kernel.get_params() for hyperparameter in kernel.hyperparameters: if hyperparameter.bounds is "fixed": continue size = hyperparameter.n_elements
index = 0
all_corr = ['absolute_exponential', 'squared_exponential', 'cubic', 'linear']
gp = GaussianProcess(corr='absolute_exponential', theta0=1e-4, thetaL=1e-12, thetaU=1e-2, nugget=1e-2, optimizer='Welch', regr="linear", random_state=0)
assert_greater(gpc.log_marginal_likelihood(gpc.kernel_.theta), gpc.log_marginal_likelihood(kernel.theta))
assert_almost_equal(np.diag(y_cov), np.exp(kernel.theta[0]), 5)
gpr = GaussianProcessRegressor(kernel=kernel) gpr.fit(X, y_norm) gpr_norm = GaussianProcessRegressor(kernel=kernel, normalize_y=True) gpr_norm.fit(X, y)
y_pred, y_pred_std = gpr.predict(X2, return_std=True) y_pred = y_mean + y_pred y_pred_norm, y_pred_std_norm = gpr_norm.predict(X2, return_std=True)
kernel = RBF(length_scale=1.0)
assert_almost_equal(y_std_1d, y_std_2d) assert_almost_equal(y_cov_1d, y_cov_2d)
for kernel in kernels: gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True) gpr.fit(X, y)
assert_greater(gpr.log_marginal_likelihood(gpr.kernel_.theta), gpr.log_marginal_likelihood(gpr.kernel.theta))
if self.normalize_y: self.y_train_mean = np.mean(y, axis=0) y = y - self.y_train_mean else: self.y_train_mean = np.zeros(1)
def obj_func(theta, eval_gradient=True): if eval_gradient: lml, grad = self.log_marginal_likelihood( theta, eval_gradient=True) return -lml, -grad else: return -self.log_marginal_likelihood(theta)
optima = [(self._constrained_optimization(obj_func, self.kernel_.theta, self.kernel_.bounds))]
K = self.kernel_(self.X_train_) K[np.diag_indices_from(K)] += self.alpha
y_train = self.y_train_ if y_train.ndim == 1: y_train = y_train[:, np.newaxis]
log_likelihood_gradient_dims = \ 0.5 * np.einsum("ijl,ijk->kl", tmp, K_gradient) log_likelihood_gradient = log_likelihood_gradient_dims.sum(-1)
return True
return True
check_ortho(Wx, "x weights are not orthogonal") check_ortho(Wy, "y weights are not orthogonal")
check_ortho(T, "x scores are not orthogonal") check_ortho(U, "y scores are not orthogonal")
pls_ca = pls_.PLSCanonical(n_components=X.shape[1]) pls_ca.fit(X, Y)
x_weights_sign_flip = pls_ca.x_weights_ / x_weights
assert_array_almost_equal(x_rotations_sign_flip, x_weights_sign_flip) assert_array_almost_equal(np.abs(x_rotations_sign_flip), 1, 4) assert_array_almost_equal(np.abs(x_weights_sign_flip), 1, 4)
pls_2 = pls_.PLSRegression(n_components=X.shape[1]) pls_2.fit(X, Y)
assert_array_almost_equal(x_loadings_sign_flip, x_weights_sign_flip, 4) assert_array_almost_equal(np.abs(x_loadings_sign_flip), 1, 4) assert_array_almost_equal(np.abs(x_weights_sign_flip), 1, 4)
check_ortho(pls_ca.x_weights_, "x weights are not orthogonal") check_ortho(pls_ca.y_weights_, "y weights are not orthogonal")
check_ortho(pls_ca.x_scores_, "x scores are not orthogonal") check_ortho(pls_ca.y_scores_, "y scores are not orthogonal")
d = load_linnerud() X = d.data Y = d.target
model1 = clf.fit(X, Y[:, 0]).coef_ model2 = clf.fit(X, Y[:, :1]).coef_ assert_array_almost_equal(model1, model2)
X1[:, -1] = 1.0
clf.set_params(scale=True) X_score, Y_score = clf.fit_transform(X_s, Y_s) assert_array_almost_equal(X_s_score, X_score) assert_array_almost_equal(Y_s_score, Y_score)
from distutils.version import LooseVersion from sklearn.utils.extmath import svd_flip
pinv2_args = {'check_finite': False}
while True: if mode == "B": if X_pinv is None: X_pinv = linalg.pinv2(X, **pinv2_args) x_weights = np.dot(X_pinv, y_score)
x_weights = np.dot(X.T, y_score) / np.dot(y_score.T, y_score)
y_weights = np.dot(Y.T, x_score) / np.dot(x_score.T, x_score)
X -= self.x_mean_ X /= self.x_std_ Ypred = np.dot(X, self.coef_) return Ypred + self.y_mean_
axis = uniques
axis = np.linspace(emp_percentiles[0, col], emp_percentiles[1, col], num=grid_resolution, endpoint=True)
if feature_names is None: feature_names = [str(i) for i in range(gbrt.n_features)] elif isinstance(feature_names, np.ndarray): feature_names = feature_names.tolist()
for i in fxs: l.append(feature_names[i]) names.append(l)
pd_result = Parallel(n_jobs=n_jobs, verbose=verbose)( delayed(partial_dependence)(gbrt, fxs, X=X, grid_resolution=grid_resolution, percentiles=percentiles) for fxs in features)
if 2 in pdp_lim: Z_level = np.linspace(*pdp_lim[2], num=8)
ax.xaxis.set_major_locator(MaxNLocator(nbins=6, prune='lower')) tick_formatter = ScalarFormatter() tick_formatter.set_powerlimits((-3, 4)) ax.xaxis.set_major_formatter(tick_formatter)
self.n_classes = np.unique(y).shape[0] if self.n_classes == 2: self.n_classes = 1
self.n_classes = 1
terminal_regions = tree.apply(X)
masked_terminal_regions = terminal_regions.copy() masked_terminal_regions[~sample_mask] = -1
for leaf in np.where(tree.children_left == TREE_LEAF)[0]: self._update_terminal_region(tree, masked_terminal_regions, leaf, X, y, residual, y_pred[:, k], sample_weight)
y_pred[:, k] += (learning_rate * tree.value[:, 0, 0].take(terminal_regions, axis=0))
y_pred[:, k] += learning_rate * tree.predict(X).ravel()
super(BinomialDeviance, self).__init__(1)
Y = np.zeros((y.shape[0], self.K), dtype=np.float64) for k in range(self.K): Y[:, k] = y == k
super(ExponentialLoss, self).__init__(1)
print(('%10s ' + '%16s ' * (len(header_fields) - 1)) % tuple(header_fields))
self.verbose_mod = 1 self.start_time = time() self.begin_at_stage = begin_at_stage
self.verbose_mod *= 10
sample_weight = sample_weight * sample_mask.astype(np.float64)
self.estimators_[i, k] = tree
if self.n_classes_ > 1: max_features = max(1, int(np.sqrt(self.n_features))) else: max_features = self.n_features
if self.subsample < 1.0: self.oob_improvement_ = np.zeros((self.n_estimators), dtype=np.float64)
total_n_estimators = self.n_estimators if total_n_estimators < self.estimators_.shape[0]: raise ValueError('resize with smaller n_estimators %d < %d' % (total_n_estimators, self.estimators_[0]))
if hasattr(self, 'oob_improvement_'): self.oob_improvement_.resize(total_n_estimators) else: self.oob_improvement_ = np.zeros((total_n_estimators,), dtype=np.float64)
if not self.warm_start: self._clear_state()
self._init_state()
self.init_.fit(X, y, sample_weight)
y_pred = self.init_.predict(X) begin_at_stage = 0
if presort == 'auto' and issparse(X): presort = False elif presort == 'auto': presort = True
if self.min_weight_fraction_leaf != 0. and sample_weight is not None: min_weight_leaf = (self.min_weight_fraction_leaf * np.sum(sample_weight)) else: min_weight_leaf = 0.
i = begin_at_stage for i in range(begin_at_stage, self.n_estimators):
if do_oob: sample_mask = _random_sample_mask(n_samples, n_inbag, random_state) old_oob_score = loss_(y[~sample_mask], y_pred[~sample_mask], sample_weight[~sample_mask])
y_pred = self._fit_stage(i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)
raise NotImplementedError()
score = self._init_decision_function(X) predict_stages(self.estimators_, X, self.learning_rate, score) return score
yield dec
return y
n_estimators, n_classes = self.estimators_.shape leaves = np.zeros((X.shape[0], n_estimators, n_classes))
yield dec
n_samples, n_features = X.shape max_features = ensemble.max_features
estimators = [] estimators_samples = [] estimators_features = []
if bootstrap_features: features = random_state.randint(0, n_features, max_features) else: features = sample_without_replacement(n_features, max_features, random_state=random_state)
if support_sample_weight: if sample_weight is None: curr_sample_weight = np.ones((n_samples,)) else: curr_sample_weight = sample_weight.copy()
else: if bootstrap: indices = random_state.randint(0, n_samples, max_samples) else: indices = sample_without_replacement(n_samples, max_samples, random_state=random_state)
predictions = estimator.predict(X[:, features])
X, y = check_X_y(X, y, ['csr', 'csc'])
n_samples, self.n_features_ = X.shape y = self._validate_y(y)
self._validate_estimator()
if not isinstance(max_samples, (numbers.Integral, np.integer)): max_samples = int(max_samples * X.shape[0])
self.estimators_ = [] self.estimators_samples_ = [] self.estimators_features_ = []
n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators, self.n_jobs) total_n_estimators = sum(n_estimators)
if self.warm_start and len(self.estimators_) > 0: random_state.randint(MAX_INT, size=len(self.estimators_))
return column_or_1d(y, warn=True)
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators, self.n_jobs)
proba = sum(all_proba) / self.n_estimators
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators( self.n_estimators, self.n_jobs)
log_proba = all_log_proba[0]
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators, self.n_jobs)
decisions = sum(all_decisions) / self.n_estimators
X = check_array(X, accept_sparse=['csr', 'csc'])
n_jobs, n_estimators, starts = _partition_estimators(self.n_estimators, self.n_jobs)
y_hat = sum(all_y_hat) / self.n_estimators
X = check_array(X, accept_sparse="csc", dtype=DTYPE) y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None) if issparse(X): X.sort_indices()
n_samples, self.n_features_ = X.shape
y = np.reshape(y, (-1, 1))
self._validate_estimator()
self.estimators_ = []
random_state.randint(MAX_INT, size=len(self.estimators_))
self.estimators_.extend(trees)
if hasattr(self, "classes_") and self.n_outputs_ == 1: self.n_classes_ = self.n_classes_[0] self.classes_ = self.classes_[0]
return y, None
X = self._validate_X_predict(X)
n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
all_proba = Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")( delayed(parallel_helper)(e, 'predict_proba', X, check_input=False) for e in self.estimators_)
proba = all_proba[0]
X = self._validate_X_predict(X)
n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
all_y_hat = Parallel(n_jobs=n_jobs, verbose=self.verbose, backend="threading")( delayed(parallel_helper)(e, 'predict', X, check_input=False) for e in self.estimators_)
y_hat = sum(all_y_hat) / len(self.estimators_)
X = check_array(X, accept_sparse=['csc'], ensure_2d=False) if issparse(X): X.sort_indices()
self.base_estimator = base_estimator self.n_estimators = n_estimators self.estimator_params = estimator_params
self.estimators_ = []
n_jobs = min(_get_n_jobs(n_jobs), n_estimators)
n_estimators_per_job = (n_estimators // n_jobs) * np.ones(n_jobs, dtype=np.int) n_estimators_per_job[:n_estimators % n_jobs] += 1 starts = np.cumsum(n_estimators_per_job)
boston = datasets.load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
iris = datasets.load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
clf = GradientBoostingClassifier(loss=loss, n_estimators=10, random_state=1, presort=presort)
assert_raises(ValueError, lambda: GradientBoostingClassifier().feature_importances_)
assert_raises(ValueError, lambda X, y: GradientBoostingClassifier( loss='deviance').fit(X, y), X, [0, 0, 0, 0])
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
X, y = datasets.make_friedman1(n_samples=1200, random_state=random_state, noise=1.0) X_train, y_train = X[:200], y[:200] X_test, y_test = X[200:], y[200:]
X, y = datasets.make_friedman2(n_samples=1200, random_state=random_state) X_train, y_train = X[:200], y[:200] X_test, y_test = X[200:], y[200:]
X, y = datasets.make_friedman3(n_samples=1200, random_state=random_state) X_train, y_train = X[:200], y[:200] X_test, y_test = X[200:], y[200:]
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
y_proba = clf.predict_proba(T) assert_true(np.all(y_proba >= 0.0)) assert_true(np.all(y_proba <= 1.0))
y_pred = clf.classes_.take(y_proba.argmax(axis=1), axis=0) assert_array_equal(y_pred, true_result)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1) assert_raises(ValueError, clf.fit, X, y + [0, 1])
clf = GradientBoostingClassifier(n_estimators=100, random_state=1) clf.fit(X, y)
clf = GradientBoostingRegressor(n_estimators=100, random_state=1, max_features=0) assert_raises(ValueError, clf.fit, X, y)
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1) _, n_features = X.shape
for y in clf.staged_predict(X_test): assert_equal(y.shape, y_pred.shape)
for y_pred in clf.staged_predict(X_test): assert_equal(y_test.shape, y_pred.shape)
for staged_proba in clf.staged_predict_proba(X_test): assert_equal(y_test.shape[0], staged_proba.shape[0]) assert_equal(2, staged_proba.shape[1])
rng = np.random.RandomState(0) X = rng.uniform(size=(10, 3))
continue
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
assert_raises(ValueError, clf.fit, X, np.ones(len(X)))
clf_quantile = GradientBoostingRegressor(n_estimators=100, loss='quantile', max_depth=4, alpha=0.5, random_state=7)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
clf = GradientBoostingClassifier(n_estimators=100, random_state=1)
assert_warns(DataConversionWarning, clf.fit, X, y_) assert_array_equal(clf.predict(T), true_result) assert_equal(100, len(clf.estimators_))
clf = GradientBoostingClassifier(n_estimators=100, random_state=1, subsample=1.0) clf.fit(X, y) assert_raises(AttributeError, lambda: clf.oob_improvement_)
assert_equal(10 + 9, n_lines)
assert_equal(100, n_lines)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=200, max_depth=1) est.fit(X, y)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=300, max_depth=1) est.fit(X, y)
assert_equal(est.estimators_[0, 0].max_depth, 1) for i in range(1, 11): assert_equal(est.estimators_[-i, 0].max_depth, 2)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=100, max_depth=1) est.fit(X, y)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) for Cls in [GradientBoostingRegressor, GradientBoostingClassifier]: est = Cls(n_estimators=100, max_depth=1) est.fit(X, y)
assert_array_equal(est.oob_improvement_[-10:] == 0.0, np.zeros(10, dtype=np.bool))
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)
from sklearn.tree._tree import TREE_LEAF X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) k = 4
from sklearn.tree._tree import TREE_LEAF k = 4
X = iris.data y = np.array(iris.target) est = GradientBoostingClassifier(n_estimators=20, max_depth=1, random_state=1, init=ZeroEstimator()) est.fit(X, y)
X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1) all_estimators = [GradientBoostingRegressor, GradientBoostingClassifier]
clf = GradientBoostingClassifier(loss='exponential', n_estimators=100, random_state=1)
y_pred = clf.classes_.take(y_proba.argmax(axis=1), axis=0) assert_array_equal(y_pred, true_result)
ensemble = BaggingClassifier(base_estimator=Perceptron(), n_estimators=3)
iris = load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
boston = load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
sparse_classifier = BaggingClassifier( base_estimator=CustomSVC(decision_function_shape='ovr'), random_state=1, **params ).fit(X_train_sparse, y_train) sparse_results = getattr(sparse_classifier, f)(X_test_sparse)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data[:50], boston.target[:50], random_state=rng)
sparse_classifier = BaggingRegressor( base_estimator=CustomSVR(), random_state=1, **params ).fit(X_train_sparse, y_train) sparse_results = sparse_classifier.predict(X_test_sparse)
dense_results = BaggingRegressor( base_estimator=CustomSVR(), random_state=1, **params ).fit(X_train, y_train).predict(X_test)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_samples=1.0, bootstrap=False, random_state=rng).fit(X_train, y_train)
ensemble = BaggingRegressor(base_estimator=DecisionTreeRegressor(), max_samples=1.0, bootstrap=True, random_state=rng).fit(X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
ensemble = BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=rng).fit(X_train, y_train)
ensemble = BaggingClassifier(base_estimator=LogisticRegression(), random_state=rng, max_samples=5).fit(X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
assert_warns(UserWarning, BaggingClassifier(base_estimator=base_estimator, n_estimators=1, bootstrap=True, oob_score=True, random_state=rng).fit, X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
assert_warns(UserWarning, BaggingRegressor(base_estimator=DecisionTreeRegressor(), n_estimators=1, bootstrap=True, oob_score=True, random_state=rng).fit, X_train, y_train)
rng = check_random_state(0) X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
X, y = iris.data, iris.target base = DecisionTreeClassifier()
assert_false(hasattr(BaggingClassifier(base).fit(X, y), 'decision_function'))
rng = check_random_state(0)
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
ensemble.set_params(n_jobs=1) y1 = ensemble.predict_proba(X_test) ensemble.set_params(n_jobs=2) y2 = ensemble.predict_proba(X_test) assert_array_almost_equal(y1, y2)
ensemble = BaggingClassifier(SVC(decision_function_shape='ovr'), n_jobs=3, random_state=0).fit(X_train, y_train)
rng = check_random_state(0)
X, y = iris.data, iris.target y[y == 2] = 1
parameters = {'n_estimators': (1, 2), 'base_estimator__C': (1, 2)}
rng = check_random_state(0)
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=rng)
X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=rng)
X, y = make_hastie_10_2(n_samples=20, random_state=1)
X, y = make_hastie_10_2(n_samples=20, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)
X_train += 1.
X, y = make_hastie_10_2(n_samples=20, random_state=1) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)
X, y = make_hastie_10_2(n_samples=20, random_state=1) clf = BaggingClassifier(n_estimators=5, warm_start=True, oob_score=True) assert_raises(ValueError, clf.fit, X, y)
iris = datasets.load_iris() X, y = iris.data[:, 1:3], iris.target
rng = np.random.RandomState(0)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]
iris = datasets.load_iris() perm = rng.permutation(iris.target.size) iris.data, iris.target = shuffle(iris.data, iris.target, random_state=rng)
boston = datasets.load_boston() boston.data, boston.target = shuffle(boston.data, boston.target, random_state=rng)
class MockEstimator(object): def predict_proba(self, X): assert_array_equal(X.shape, probs.shape) return probs mock = MockEstimator()
assert_array_equal(np.argmin(samme_proba, axis=1), [2, 0, 0, 2]) assert_array_equal(np.argmax(samme_proba, axis=1), [0, 1, 1, 1])
clf = AdaBoostRegressor(random_state=0) clf.fit(X, y_regr) assert_array_equal(clf.predict(T), y_t_regr)
classes = np.unique(iris.target) clf_samme = prob_samme = None
clf_samme.algorithm = "SAMME.R" assert_array_less(0, np.abs(clf_samme.predict_proba(iris.data) - prob_samme))
clf = AdaBoostRegressor(random_state=0) clf.fit(boston.data, boston.target) score = clf.score(boston.data, boston.target) assert score > 0.85
rng = np.random.RandomState(0) iris_weights = rng.randint(10, size=iris.target.shape) boston_weights = rng.randint(10, size=boston.target.shape)
for alg in ['SAMME', 'SAMME.R']: clf = AdaBoostClassifier(algorithm=alg, n_estimators=10) clf.fit(iris.data, iris.target, sample_weight=iris_weights)
clf = AdaBoostRegressor(n_estimators=10, random_state=0) clf.fit(boston.data, boston.target, sample_weight=boston_weights)
import pickle
X, y = datasets.make_classification(n_samples=2000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=1)
assert_raises(ValueError, AdaBoostClassifier(learning_rate=-1).fit, X, y_class)
from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC
clf = AdaBoostClassifier(RandomForestClassifier()) clf.fit(X, y_regr)
y = np.ravel(y)
sparse_classifier = AdaBoostClassifier( base_estimator=CustomSVC(probability=True), random_state=1, algorithm="SAMME" ).fit(X_train_sparse, y_train)
dense_classifier = AdaBoostClassifier( base_estimator=CustomSVC(probability=True), random_state=1, algorithm="SAMME" ).fit(X_train, y_train)
sparse_results = sparse_classifier.predict(X_test_sparse) dense_results = dense_classifier.predict(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.decision_function(X_test_sparse) dense_results = dense_classifier.decision_function(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.predict_log_proba(X_test_sparse) dense_results = dense_classifier.predict_log_proba(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.predict_proba(X_test_sparse) dense_results = dense_classifier.predict_proba(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.score(X_test_sparse, y_test) dense_results = dense_classifier.score(X_test, y_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.staged_decision_function( X_test_sparse) dense_results = dense_classifier.staged_decision_function(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
sparse_results = sparse_classifier.staged_predict(X_test_sparse) dense_results = dense_classifier.staged_predict(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
sparse_results = sparse_classifier.staged_predict_proba(X_test_sparse) dense_results = dense_classifier.staged_predict_proba(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
sparse_results = sparse_classifier.staged_score(X_test_sparse, y_test) dense_results = dense_classifier.staged_score(X_test, y_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
types = [i.data_type_ for i in sparse_classifier.estimators_]
sparse_classifier = AdaBoostRegressor( base_estimator=CustomSVR(), random_state=1 ).fit(X_train_sparse, y_train)
dense_classifier = dense_results = AdaBoostRegressor( base_estimator=CustomSVR(), random_state=1 ).fit(X_train, y_train)
sparse_results = sparse_classifier.predict(X_test_sparse) dense_results = dense_classifier.predict(X_test) assert_array_equal(sparse_results, dense_results)
sparse_results = sparse_classifier.staged_predict(X_test_sparse) dense_results = dense_classifier.staged_predict(X_test) for sprase_res, dense_res in zip(sparse_results, dense_results): assert_array_equal(sprase_res, dense_res)
bd = BinomialDeviance(2)
est = LogOddsEstimator() assert_raises(ValueError, est.fit, None, np.array([1]))
rng = check_random_state(13) X = rng.rand(100, 2) sample_weight = np.ones(100) reg_y = rng.rand(100)
continue
assert_array_equal(out, sw_out)
p = np.zeros((y.shape[0], k), dtype=np.float64) for i in range(k): p[:, i] = y == i
iris = datasets.load_iris() rng = check_random_state(0) perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
boston = datasets.load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
hastie_X, hastie_y = datasets.make_hastie_10_2(n_samples=20, random_state=1) hastie_X = hastie_X.astype(np.float32)
leaf_indices = clf.apply(X) assert_equal(leaf_indices.shape, (len(X), clf.n_estimators))
ForestClassifier = FOREST_CLASSIFIERS[name]
ForestRegressor = FOREST_REGRESSORS[name]
r = FOREST_REGRESSORS[name](random_state=0) assert_false(hasattr(r, "classes_")) assert_false(hasattr(r, "n_classes_"))
X_new = assert_warns( DeprecationWarning, est.transform, X, threshold="mean") assert_less(0 < X_new.shape[1], X.shape[1])
importances = est.feature_importances_ est.set_params(n_jobs=2) importances_parrallel = est.feature_importances_ assert_array_almost_equal(importances, importances_parrallel)
coef = 1. / (binomial(k, n_features) * (n_features - k))
for B in combinations(features, k): for b in product(*[values[B[j]] for j in range(k)]): mask_b = np.ones(n_samples, dtype=np.bool)
true_importances = np.zeros(n_features)
clf = ExtraTreesClassifier(n_estimators=500, max_features=1, criterion="entropy", random_state=0).fit(X, y)
assert_almost_equal(entropy(y), sum(importances)) assert_less(np.abs(true_importances - importances).mean(), 0.01)
yield check_oob_score, name, csc_matrix(iris.data), iris.target
yield check_oob_score, name, iris.data, iris.target * 2 + 1
yield check_oob_score, name, csc_matrix(boston.data), boston.target, 50
assert_raises(ValueError, ForestEstimator(oob_score=True, bootstrap=False).fit, X, y)
for name in FOREST_CLASSIFIERS: yield check_gridsearch, name
ForestClassifier = FOREST_CLASSIFIERS[name]
clf = ForestClassifier(random_state=0).fit(X, y)
_y = np.vstack((y, np.array(y) * 2)).T clf = ForestClassifier(random_state=0).fit(X, _y)
hasher = RandomTreesEmbedding(n_estimators=10, sparse_output=False) X, y = datasets.make_circles(factor=0.5) X_transformed = hasher.fit_transform(X)
assert_equal(type(X_transformed), np.ndarray)
assert_array_equal(X_transformed_sparse.toarray(), X_transformed_dense)
@ignore_warnings def test_random_hasher(): hasher = RandomTreesEmbedding(n_estimators=30, random_state=1) X, y = datasets.make_circles(factor=0.5) X_transformed = hasher.fit_transform(X)
hasher = RandomTreesEmbedding(n_estimators=30, random_state=1) assert_array_equal(hasher.fit(X).transform(X).toarray(), X_transformed.toarray())
X = rng.randint(0, 4, size=(1000, 1)) y = rng.rand(1000) n_trees = 500
ForestEstimator = FOREST_ESTIMATORS[name] est = ForestEstimator(max_depth=1, max_leaf_nodes=4, n_estimators=1, random_state=0).fit(X, y) assert_greater(est.estimators_[0].tree_.max_depth, 1)
ForestEstimator = FOREST_ESTIMATORS[name]
assert_raises(ValueError, ForestEstimator(min_samples_leaf=-1).fit, X, y) assert_raises(ValueError, ForestEstimator(min_samples_leaf=0).fit, X, y)
leaf_count = node_counts[node_counts != 0] assert_greater(np.min(leaf_count), 4, "Failed with {0}".format(name))
leaf_count = node_counts[node_counts != 0] assert_greater(np.min(leaf_count), len(X) * 0.25 - 1, "Failed with {0}".format(name))
ForestEstimator = FOREST_ESTIMATORS[name] rng = np.random.RandomState(0) weights = rng.rand(X.shape[0]) total_weight = np.sum(weights)
for frac in np.linspace(0, 0.5, 6): est = ForestEstimator(min_weight_fraction_leaf=frac, n_estimators=1, random_state=0) if "RandomForest" in name: est.bootstrap = False
leaf_weights = node_weights[node_weights != 0] assert_greater_equal( np.min(leaf_weights), total_weight * est.min_weight_fraction_leaf, "Failed with {0} " "min_weight_fraction_leaf={1}".format( name, est.min_weight_fraction_leaf))
X = np.asarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = np.ascontiguousarray(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csr_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = csc_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
X = coo_matrix(iris.data, dtype=dtype) y = iris.target assert_array_equal(est.fit(X, y).predict(X), y)
ForestClassifier = FOREST_CLASSIFIERS[name]
ForestClassifier = FOREST_CLASSIFIERS[name] _y = np.vstack((y, np.array(y) * 2)).T
clf = ForestClassifier(class_weight='the larch', random_state=0) assert_raises(ValueError, clf.fit, X, y) assert_raises(ValueError, clf.fit, X, _y)
clf = ForestClassifier(class_weight='auto', warm_start=True, random_state=0) assert_warns(UserWarning, clf.fit, X, y) assert_warns(UserWarning, clf.fit, X, _y)
clf = ForestClassifier(class_weight=1, random_state=0) assert_raises(ValueError, clf.fit, X, _y)
clf = ForestClassifier(class_weight=[{-1: 0.5, 1: 1.}], random_state=0) assert_raises(ValueError, clf.fit, X, _y)
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=5, max_depth=1, warm_start=False, random_state=1) clf.fit(X, y)
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=5, max_depth=3, warm_start=True, random_state=1) clf.fit(X, y)
assert_array_equal(clf.apply(X), clf_2.apply(X))
X, y = hastie_X, hastie_y ForestEstimator = FOREST_ESTIMATORS[name] clf = ForestEstimator(n_estimators=15, max_depth=3, warm_start=False, random_state=1, bootstrap=True, oob_score=True) clf.fit(X, y)
clf_3 = ForestEstimator(n_estimators=15, max_depth=3, warm_start=True, random_state=1, bootstrap=True, oob_score=False) clf_3.fit(X, y) assert_true(not(hasattr(clf_3, 'oob_score_')))
iris = load_iris() perm = rng.permutation(iris.target.size) iris.data = iris.data[perm] iris.target = iris.target[perm]
boston = load_boston() perm = rng.permutation(boston.target.size) boston.data = boston.data[perm] boston.target = boston.target[perm]
sparse_classifier = IsolationForest( n_estimators=10, random_state=1, **params).fit(X_train_sparse) sparse_results = sparse_classifier.predict(X_test_sparse)
dense_classifier = IsolationForest( n_estimators=10, random_state=1, **params).fit(X_train) dense_results = dense_classifier.predict(X_test)
rng = check_random_state(2) X = 0.3 * rng.randn(120, 2) X_train = np.r_[X + 2, X - 2] X_train = X[:100]
clf = IsolationForest(max_samples=100, random_state=rng).fit(X_train)
y_pred = - clf.decision_function(X_test)
assert_greater(roc_auc_score(y_test, y_pred), 0.98)
X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]
clf = IsolationForest(random_state=rng, contamination=0.25) clf.fit(X) decision_func = - clf.decision_function(X) pred = clf.predict(X)
assert_greater(np.min(decision_func[-2:]), np.max(decision_func[:-2])) assert_array_equal(pred, 6 * [1] + 2 * [-1])
boston = datasets.load_boston()
iris = datasets.load_iris()
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(X, y)
assert pdp.shape == (1, 4) assert axes[0].shape[0] == 4
X_ = np.asarray(X) grid = np.unique(X_[:, 0]) pdp_2, axes = partial_dependence(clf, [0], grid=grid)
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target)
clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target)
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(X, y)
assert_raises(ValueError, partial_dependence, {}, [0], X=X)
assert_raises(ValueError, partial_dependence, GradientBoostingClassifier(), [0], X=X)
grid = np.random.rand(10, 2, 1) assert_raises(ValueError, partial_dependence, clf, [0], grid=grid)
clf = GradientBoostingRegressor(n_estimators=10, random_state=1) clf.fit(boston.data, boston.target)
fig, axs = plot_partial_dependence(clf, boston.data, ['CRIM', 'ZN', ('CRIM', 'ZN')], grid_resolution=grid_resolution, feature_names=boston.feature_names)
clf = GradientBoostingClassifier(n_estimators=10, random_state=1)
assert_raises(ValueError, plot_partial_dependence, clf, X, [0])
assert_raises(ValueError, plot_partial_dependence, {}, X, [0])
assert_raises(ValueError, plot_partial_dependence, clf, X, [-1])
assert_raises(ValueError, plot_partial_dependence, clf, X, [100])
assert_raises(ValueError, plot_partial_dependence, clf, X, ['foobar'])
assert_raises(ValueError, plot_partial_dependence, clf, X, [{'foo': 'bar'}])
clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, iris.target)
target = iris.target_names[iris.target] clf = GradientBoostingClassifier(n_estimators=10, random_state=1) clf.fit(iris.data, target)
assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], label='foobar', grid_resolution=grid_resolution)
assert_raises(ValueError, plot_partial_dependence, clf, iris.data, [0, 1], grid_resolution=grid_resolution)
bootstrap=bootstrap, bootstrap_features=False, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, n_jobs=n_jobs, random_state=random_state, verbose=verbose)
X = check_array(X, accept_sparse=['csc'], ensure_2d=False) if issparse(X): X.sort_indices()
n_samples = X.shape[0]
X = self.estimators_[0]._validate_X_predict(X, check_input=True) n_samples = X.shape[0]
return 0.5 - scores
if self.learning_rate <= 0: raise ValueError("learning_rate must be greater than zero")
sample_weight = np.empty(X.shape[0], dtype=np.float64) sample_weight[:] = 1. / X.shape[0]
sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)
if sample_weight.sum() <= 0: raise ValueError( "Attempting to fit with a non-positive " "weighted number of samples.")
self._validate_estimator()
self.estimators_ = [] self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64) self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)
sample_weight, estimator_weight, estimator_error = self._boost( iboost, X, y, sample_weight)
if sample_weight is None: break
if estimator_error == 0: break
if sample_weight_sum <= 0: break
sample_weight /= sample_weight_sum
proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps log_proba = np.log(proba)
if self.algorithm not in ('SAMME', 'SAMME.R'): raise ValueError("algorithm %s is not supported" % self.algorithm)
return super(AdaBoostClassifier, self).fit(X, y, sample_weight)
incorrect = y_predict != y
estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))
if estimator_error <= 0: return sample_weight, 1., 0.
estimator_weight = (-1. * self.learning_rate * (((n_classes - 1.) / n_classes) * inner1d(y_coding, np.log(y_predict_proba))))
if not iboost == self.n_estimators - 1: sample_weight *= np.exp(estimator_weight * ((sample_weight > 0) | (estimator_weight < 0)))
incorrect = y_predict != y
estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))
if estimator_error <= 0: return sample_weight, 1., 0.
estimator_weight = self.learning_rate * ( np.log((1. - estimator_error) / estimator_error) + np.log(n_classes - 1.))
if not iboost == self.n_estimators - 1: sample_weight *= np.exp(estimator_weight * incorrect * ((sample_weight > 0) | (estimator_weight < 0)))
pred = sum(_samme_proba(estimator, n_classes, X) for estimator in self.estimators_)
current_pred = _samme_proba(estimator, n_classes, X)
proba = sum(_samme_proba(estimator, n_classes, X) for estimator in self.estimators_)
current_proba = _samme_proba(estimator, n_classes, X)
return super(AdaBoostRegressor, self).fit(X, y, sample_weight)
estimator.fit(X[bootstrap_idx], y[bootstrap_idx]) y_predict = estimator.predict(X)
estimator_error = (sample_weight * error_vect).sum()
return sample_weight, 1., 0.
if len(self.estimators_) > 1: self.estimators_.pop(-1) return None, None, None
estimator_weight = self.learning_rate * np.log(1. / beta)
predictions = np.array([ est.predict(X) for est in self.estimators_[:limit]]).T
sorted_idx = np.argsort(predictions, axis=1)
weight_cdf = self.estimator_weights_[sorted_idx].cumsum(axis=1) median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis] median_idx = median_or_above.argmax(axis=1)
return predictions[np.arange(X.shape[0]), median_estimators]
import subprocess
import sklearn
config.set_options(ignore_setup_xxx_py=True, assume_default_configuration=True, delegate_options_to_subpackages=True, quiet=True)
try: from setuptools import setup except ImportError: from distutils.core import setup
generate_cython()
try: WindowsError except NameError: WindowsError = None
current_hash = get_hash_tuple(full_header_path, full_cython_path, full_gen_file_path)
hashes[clean_path(full_cython_path)] = current_hash
save_hashes(hashes, HASH_FILE)
pr_url = os.environ.get('CI_PULL_REQUEST') if not pr_url: exit("not a pull request")
msg = "no doc impacting files detected:\n" + u"\n".join(filenames) exit(msg, skip=True)
global custom_data_home custom_data_home = tempfile.mkdtemp() makedirs(join(custom_data_home, 'mldata')) globs['custom_data_home'] = custom_data_home return globs
greet = Word( alphas ) + "," + Word( alphas ) + "!"
return str(obj)
#~ asList = False
if not formatted: indent = "" nextLevelIndent = "" nl = ""
if f.__call__.func_code.co_flags & STAR_ARGS: return f numargs = f.__call__.func_code.co_argcount if hasattr(f.__call__,"im_self"): numargs -= 1
pass
pass
pass
_exprArgCache = {} def resetCache(): ParserElement._exprArgCache.clear() resetCache = staticmethod(resetCache)
self.returnString = matchString self.name = "'%s'" % self.returnString self.errmsg = "Expected " + self.name #self.myException.msg = self.errmsg
quoteChar = quoteChar.strip() if len(quoteChar) == 0: warnings.warn("quoteChar cannot be the empty string",SyntaxWarning,stacklevel=2) raise SyntaxError()
ret = ret[self.quoteCharLen:-self.endQuoteCharLen]
if self.escChar: ret = re.sub(self.escCharReplacePattern,"\g<1>",ret)
if self.escQuote: ret = ret.replace(self.escQuote, self.endQuoteChar)
else: if self.exprs: raise maxException else: raise ParseException(instring, loc, "no defined alternatives to match", self)
if adjacent: self.leaveWhitespace() self.adjacent = adjacent self.skipWhitespace = True self.joinString = joinString
tflat = _flatten(t.asList()) rep << And( [ Literal(tt) for tt in tflat ] )
return MatchFirst( [ parseElementClass(sym) for sym in symbols ] )
if not isinstance(opExpr, Optional): opExpr = Optional(opExpr) matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )
cStyleComment = Regex(r"/\*(?:[^*]*\*+)+?/").setName("C style comment")
from urllib2 import Request, build_opener
from urllib.request import Request, build_opener
}
request.add_header('User-Agent', 'OpenAnything/1.0') html_content = opener.open(request).read() open(html_filename, 'wb').write(html_content)
languages_data_folder = sys.argv[1] dataset = load_files(languages_data_folder)
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.5)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
movie_reviews_data_folder = sys.argv[1] dataset = load_files(movie_reviews_data_folder, shuffle=False) print("n_samples: %d" % len(dataset.data))
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.25, random_state=None)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
languages_data_folder = sys.argv[1] dataset = load_files(languages_data_folder)
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.5)
vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char', use_idf=False)
clf = Pipeline([ ('vec', vectorizer), ('clf', Perceptron()), ])
clf.fit(docs_train, y_train)
y_predicted = clf.predict(docs_test)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
movie_reviews_data_folder = sys.argv[1] dataset = load_files(movie_reviews_data_folder, shuffle=False) print("n_samples: %d" % len(dataset.data))
docs_train, docs_test, y_train, y_test = train_test_split( dataset.data, dataset.target, test_size=0.25, random_state=None)
pipeline = Pipeline([ ('vect', TfidfVectorizer(min_df=3, max_df=0.95)), ('clf', LinearSVC(C=1000)), ])
print(grid_search.grid_scores_)
y_predicted = grid_search.predict(docs_test)
print(metrics.classification_report(y_test, y_predicted, target_names=dataset.target_names))
cm = metrics.confusion_matrix(y_test, y_predicted) print(cm)
try: import gen_rst except: pass
templates_path = ['templates']
autosummary_generate = True
source_suffix = '.rst'
#source_encoding = 'utf-8'
plot_gallery = True
master_doc = 'index'
project = u('scikit-learn') copyright = u('2010 - 2016, scikit-learn developers (BSD License)')
import sklearn version = sklearn.__version__ release = sklearn.__version__
#today = '' #today_fmt = '%B %d, %Y'
#unused_docs = []
exclude_trees = ['_build', 'templates', 'includes']
#default_role = None
add_function_parentheses = False
#add_module_names = True
#show_authors = False
pygments_style = 'sphinx'
#modindex_common_prefix = []
html_theme = 'scikit-learn'
html_theme_options = {'oldversion': False, 'collapsiblesidebar': True, 'google_analytics': True, 'surveybanner': False, 'sprintbanner': True}
html_theme_path = ['themes']
#html_title = None
html_short_title = 'scikit-learn'
html_logo = 'logos/scikit-learn-logo-small.png'
html_favicon = 'logos/favicon.ico'
html_static_path = ['images']
#html_last_updated_fmt = '%b %d, %Y'
#html_use_smartypants = True
#html_sidebars = {}
#html_additional_pages = {}
html_domain_indices = False
html_use_index = False
#html_split_index = False
#html_show_sourcelink = True
#html_use_opensearch = ''
#html_file_suffix = ''
htmlhelp_basename = 'scikit-learndoc'
#latex_paper_size = 'letter'
#latex_font_size = '10pt'
latex_documents = [('index', 'user_guide.tex', u('scikit-learn user guide'), u('scikit-learn developers'), 'manual'), ]
latex_logo = "logos/scikit-learn-logo.png"
#latex_use_parts = False
#latex_appendices = []
latex_domain_indices = False
app.add_javascript('js/copybutton.js') app.connect('autodoc-process-docstring', generate_example_rst)
linkcode_resolve = make_linkcode_resolve('sklearn', u'https://github.com/scikit-learn/' 'scikit-learn/blob/{revision}/' '{package}/{path}#L{lineno}')
execfile
import matplotlib matplotlib.use('Agg')
pass
subdict_str = _select_block(dict_str[pos:], '{', '}') value = _parse_dict_recursive(subdict_str) pos_tmp = pos + len(subdict_str)
if hasattr(searchindex, 'decode'): searchindex = searchindex.decode('UTF-8')
query = 'objects:' pos = searchindex.find(query) if pos < 0: raise ValueError('"objects:" not found in search index')
sindex = get_data(searchindex_url) filenames, objects = parse_sphinx_searchindex(sindex)
comb_name = comb_name.decode('utf-8', 'replace')
link = self._get_link(cobj) self._link_cache[full_name] = link
return None
link = link.replace('\\', '/')
link = link[3:]
print(file=ex_file) print('Examples using ``%s``' % backref, file=ex_file) print('%s--' % ('-' * len(backref)), file=ex_file) print(file=ex_file)
DOCMODULES = ['sklearn', 'matplotlib', 'numpy', 'scipy']
img.thumbnail((width_sc, height_sc), Image.ANTIALIAS)
short_name = '.'.join(parts[:(i + 1)]) break
attrs.append(node.id) self.accessed_names.add('.'.join(reversed(attrs)))
self.visit(node)
full_name = self.imported_names[local_name] + remainder yield name, full_name
figure_list = []
my_stdout = my_stdout.replace( my_globals['__doc__'], '')
make_thumbnail('images/no_image.png', thumb_file, 200, 140)
return
doc_resolvers = {} doc_resolvers['sklearn'] = SphinxDocLinkResolver(app.builder.outdir, relative=True)
link_pattern = '<a href="%s">%s</a>' orig_pattern = '<span class="n">%s</span>' period = '<span class="o">.</span>'
names = sorted(str_repl, key=len, reverse=True)
app.connect('build-finished', embed_code_links)
pass
def _str_header(self, name, symbol='`'): return ['.. rubric:: ' + name, '']
out += ['.. autosummary::', ''] out += autosum
app.add_domain(NumpyPythonDomain) app.add_domain(NumpyCDomain)
from docutils.statemachine import ViewList self.content = ViewList(lines, self.content.parent)
try: from StringIO import StringIO except: from io import StringIO
class_name = class_name.encode('utf-8')
n_samples_range = np.logspace(1, 9, 9)
eps_range = np.linspace(0.01, 0.99, 100)
n_samples_range = np.logspace(2, 6, 5) colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))
data = fetch_20newsgroups_vectorized().data[:500]
nonzero = dists != 0 dists = dists[nonzero]
predicted = cross_val_predict(lr, boston.data, y, cv=10)
weight = X[rows][:, cols].sum() cut = (X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()) return cut / weight
from __future__ import print_function
('subjectbody', SubjectBodyExtractor()),
('union', FeatureUnion( transformer_list=[
('subject', Pipeline([ ('selector', ItemSelector(key='subject')), ('tfidf', TfidfVectorizer(min_df=50)), ])),
('body_stats', Pipeline([ ('selector', ItemSelector(key='body')),
transformer_weights={ 'subject': 0.8, 'body_bow': 0.5, 'body_stats': 1.0, },
('svc', SVC(kernel='linear')),
X = 5 * rng.rand(10000, 1) y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))
plt.figure()
plt.figure()
X = X[:, np.newaxis]
rng = np.random.RandomState(0) X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0, bias=100.0)
colors = ['r-', 'b-', 'y-', 'm-']
plt.figure(figsize=(20, 6))
iris = datasets.load_iris()
y = iris.target colors = "bry"
idx = np.arange(X.shape[0]) np.random.seed(13) np.random.shuffle(idx) X = X[idx] y = y[idx]
mean = X.mean(axis=0) std = X.std(axis=0) X = (X - mean) / std
xmin, xmax = plt.xlim() ymin, ymax = plt.ylim() coef = clf.coef_ intercept = clf.intercept_
print("--- Dense matrices")
print("--- Sparse matrices")
diabetes = datasets.load_diabetes()
diabetes_X = diabetes.data[:, np.newaxis, 2]
diabetes_X_train = diabetes_X[:-20] diabetes_X_test = diabetes_X[-20:]
diabetes_y_train = diabetes.target[:-20] diabetes_y_test = diabetes.target[-20:]
regr = linear_model.LinearRegression()
regr.fit(diabetes_X_train, diabetes_y_train)
plt.scatter(diabetes_X_test, diabetes_y_test, color='black') plt.plot(diabetes_X_test, regr.predict(diabetes_X_test), color='blue', linewidth=3)
clf = ARDRegression(compute_score=True) clf.fit(X, y)
X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)
clf = SGDClassifier(loss="hinge", alpha=0.01, n_iter=200, fit_intercept=True) clf.fit(X, Y)
xx = np.linspace(-1, 5, 10) yy = np.linspace(-1, 5, 10)
X /= np.sqrt(np.sum(X ** 2, axis=0))
m_log_alphas = -np.log10(model.alphas_)
m_log_alphas = -np.log10(model.cv_alphas_)
np.random.seed(0) n_samples, n_features = 100, 100
clf = BayesianRidge(compute_score=True) clf.fit(X, y)
n_features = 501 n_relevant_features = 3 noise_level = .2 coef_min = .2 n_samples = 25 block_size = n_relevant_features
coef = np.zeros(n_features) coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)
alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42, eps=0.05)
plt.xlim(0, 100) plt.legend(loc='best') plt.title('Feature selection scores - Mutual incoherence: %.1f' % mi)
clf = linear_model.LogisticRegression(C=1e5) clf.fit(X, y)
y = (y > 4).astype(np.int)
print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))
xmin, xmax = plt.xlim() ymin, ymax = plt.ylim() coef = clf.coef_ intercept = clf.intercept_
X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis]) y = np.ones(10)
y_noisy = y + 0.05 * np.random.randn(len(y))
np.random.seed(42)
y += 0.01 * np.random.normal((n_samples,))
n_samples = X.shape[0] X_train, y_train = X[:n_samples / 2], y[:n_samples / 2] X_test, y_test = X[n_samples / 2:], y[n_samples / 2:]
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
model = linear_model.LinearRegression() model.fit(X, y)
model_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression()) model_ransac.fit(X, y) inlier_mask = model_ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask)
line_X = np.arange(-5, 5) line_y = model.predict(line_X[:, np.newaxis]) line_y_ransac = model_ransac.predict(line_X[:, np.newaxis])
print("Estimated coefficients (true, normal, RANSAC):") print(coef, model.coef_, model_ransac.estimator_.coef_)
x_plot = np.linspace(0, 10, 100)
X = x[:, np.newaxis] X_plot = x_plot[:, np.newaxis]
iris = datasets.load_iris()
logreg.fit(X, Y)
Z = Z.reshape(xx.shape) plt.figure(1, figsize=(4, 3)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
n_samples = 200
clf = GridSearchCV(clf, {'anova__percentile': [5, 10, 20]}, cv=cv)
shutil.rmtree(cachedir, ignore_errors=True)
fig = plt.figure(fignum, figsize=(4, 3)) plt.clf() ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)
X_varied, y_varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state) y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)
X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10])) y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)
n_runs = 5
n_init_range = np.array([1, 5, 10, 15, 20])
n_samples_per_center = 100 grid_size = 3 scale = 0.1 n_clusters = grid_size ** 2
n_samples = 1500 noise = 0.05 X, _ = make_swiss_roll(n_samples, noise) X[:, 1] *= .5
from sklearn.neighbors import kneighbors_graph connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)
try: face = sp.face(gray=True) except AttributeError: from scipy import misc face = misc.face(gray=True)
face = sp.misc.imresize(face, 0.10) / 255.
connectivity = grid_to_graph(*face.shape)
print("Compute structured hierarchical clustering...") st = time.time()
img = circle1 + circle2 + circle3 + circle4
mask = img.astype(bool)
graph = image.img_to_graph(img, mask=mask)
graph.data = np.exp(-graph.data / graph.data.std())
labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack') label_im = -np.ones(mask.shape) label_im[mask] = labels
img = circle1 + circle2 mask = img.astype(bool) img = img.astype(float)
china = load_sample_image("china.jpg")
china = np.array(china, dtype=np.float64) / 255
w, h, d = original_shape = tuple(china.shape) assert d == 3 image_array = np.reshape(china, (w * h, d))
centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)
af = AffinityPropagation(preference=-50).fit(X) cluster_centers_indices = af.cluster_centers_indices_ labels = af.labels_
import matplotlib.pyplot as plt from itertools import cycle
pca = PCA(n_components=n_digits).fit(data) bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1), name="PCA-based", data=data) print(79 * '_')
Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
centers = [[1, 1], [-1, -1], [1, -1]] X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4, random_state=0)
db = DBSCAN(eps=0.3, min_samples=10).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
import matplotlib.pyplot as plt
n_features = 2000 t = np.pi * np.linspace(0, 1, n_features)
additional_noise[np.abs(additional_noise) < .997] = 0
try: face = sp.face(gray=True) except AttributeError: from scipy import misc face = misc.face(gray=True)
face = sp.misc.imresize(face, 0.10) / 255.
graph = image.img_to_graph(face)
beta = 5 eps = 1e-6 graph.data = np.exp(-beta * graph.data / graph.data.std()) + eps
N_REGIONS = 25
knn_graph = kneighbors_graph(X, 30, include_self=False)
X = StandardScaler().fit_transform(X)
bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)
connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False) connectivity = 0.5 * (connectivity + connectivity.T)
from scipy import misc face = misc.face(gray=True)
face_compressed = np.choose(labels, values) face_compressed.shape = face.shape
plt.figure(1, figsize=(3, 2.2)) plt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)
plt.figure(2, figsize=(3, 2.2)) plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)
regular_values = np.linspace(0, 256, n_clusters + 1) regular_labels = np.searchsorted(regular_values, face) - 1
centers = [[1, 1], [-1, -1], [1, -1]] X, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)
import matplotlib.pyplot as plt from itertools import cycle
print("Computing embedding") X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X) print("Done.")
X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True,
fig, (ax1, ax2) = plt.subplots(1, 2) fig.set_size_inches(18, 7)
clusterer = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = clusterer.fit_predict(X)
silhouette_avg = silhouette_score(X, cluster_labels) print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg)
sample_silhouette_values = silhouette_samples(X, cluster_labels)
ith_cluster_silhouette_values = \ sample_silhouette_values[cluster_labels == i]
ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
centers = clusterer.cluster_centers_ ax2.scatter(centers[:, 0], centers[:, 1], marker='o', c="white", alpha=1, s=200)
np.random.seed(0)
different = (mbk_means_labels == 4) ax = fig.add_subplot(1, 3, 3)
colors_ = cycle(colors.cnames.keys())
birch_models = [Birch(threshold=1.7, n_clusters=None), Birch(threshold=1.7, n_clusters=100)] final_step = ['without global clustering', 'with global clustering']
labels = birch_model.labels_ centroids = birch_model.subcluster_centers_ n_clusters = np.unique(labels).size print("n_clusters : %d" % n_clusters)
n_samples = 200 outliers_fraction = 0.25 clusters_separation = [0, 1, 2]
n_samples = 60 n_features = 20
emp_cov = np.dot(X.T, X) / n_samples
plt.figure(figsize=(10, 6)) plt.subplots_adjust(left=0.02, right=0.98)
robust_cov = MinCovDet().fit(X)
emp_cov = EmpiricalCovariance().fit(X)
fig = plt.figure() plt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)
coloring_matrix = np.random.normal(size=(n_features, n_features)) X_train = np.dot(base_X_train, coloring_matrix) X_test = np.dot(base_X_test, coloring_matrix)
shrinkages = np.logspace(-2, 0, 30) negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages]
real_cov = np.dot(coloring_matrix.T, coloring_matrix) emp_cov = empirical_covariance(X_train) loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))
tuned_parameters = [{'shrinkage': shrinkages}] cv = GridSearchCV(ShrunkCovariance(), tuned_parameters) cv.fit(X_train)
lw = LedoitWolf() loglik_lw = lw.fit(X_train).score(X_test)
oa = OAS() loglik_oa = oa.fit(X_train).score(X_test)
n_samples = 80 n_features = 5 repeat = 10
for i, n_outliers in enumerate(range_n_outliers): for j in range(repeat):
mcd = MinCovDet().fit(X) err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2) err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))
err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2) err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm( np.eye(n_features))
r = 0.1 real_cov = toeplitz(r ** np.arange(n_features)) coloring_matrix = cholesky(real_cov)
plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)
std_error = scores_std / np.sqrt(n_folds)
plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)
standard_scaler = StandardScaler() Xtr_s = standard_scaler.fit_transform(X_train) Xte_s = standard_scaler.transform(X_test)
from sklearn.neighbors import KNeighborsClassifier
w = clf.coef_[0] a = -w[0] / w[1]
if n_features > 1: X = np.hstack([X, np.random.randn(n_samples, n_features - 1)]) return X, y
import matplotlib.pyplot as plt
from sklearn import datasets, svm, metrics
digits = datasets.load_digits()
n_samples = len(digits.images) data = digits.images.reshape((n_samples, -1))
classifier = svm.SVC(gamma=0.001)
classifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])
expected = digits.target[n_samples / 2:] predicted = classifier.predict(data[n_samples / 2:])
Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
fignum = 1
for kernel in ('linear', 'poly', 'rbf'): clf = svm.SVC(kernel=kernel, gamma=2) clf.fit(X, Y)
plt.figure(fignum, figsize=(4, 3)) plt.clf()
clf = svm.SVC(kernel='linear') clf.fit(X, Y)
w = clf.coef_[0] a = -w[0] / w[1] xx = np.linspace(-5, 5) yy = a * xx - (clf.intercept_[0]) / w[1]
b = clf.support_vectors_[0] yy_down = a * xx + (b[1] - a * b[0]) b = clf.support_vectors_[-1] yy_up = a * xx + (b[1] - a * b[0])
plt.plot(xx, yy, 'k-') plt.plot(xx, yy_down, 'k--') plt.plot(xx, yy_up, 'k--')
iris = datasets.load_iris()
y = iris.target
titles = ['SVC with linear kernel', 'LinearSVC (linear kernel)', 'SVC with RBF kernel', 'SVC with polynomial (degree 3) kernel']
plt.subplot(2, 2, i + 1) plt.subplots_adjust(wspace=0.4, hspace=0.4)
Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
n_samples = 100 n_features = 300
X_1, y_1 = datasets.make_classification(n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1)
plt.figure(fignum, figsize=(9, 10))
grid = GridSearchCV(clf, refit=False, param_grid=param_grid, cv=ShuffleSplit(train_size=train_size, n_iter=250, random_state=1)) grid.fit(X, y) scores = [x[1] for x in grid.grid_scores_]
xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))
clf_weights = svm.SVC() clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)
X = np.sort(5 * np.random.rand(40, 1), axis=0) y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - np.random.rand(8))
fignum = 1
for name, penalty in (('unreg', 1), ('reg', 0.05)):
w = clf.coef_[0] a = -w[0] / w[1] xx = np.linspace(-5, 5) yy = a * xx - (clf.intercept_[0]) / w[1]
margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2)) yy_down = yy + a * margin yy_up = yy - a * margin
Z = Z.reshape(XX.shape) plt.figure(fignum, figsize=(4, 3)) plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)
clf = svm.SVC(kernel='linear', C=1.0) clf.fit(X, y)
wclf = svm.SVC(kernel='linear', class_weight={1: 10}) wclf.fit(X, y)
iris = datasets.load_iris()
Y = iris.target
clf = svm.SVC(kernel=my_kernel) clf.fit(X, Y)
Z = Z.reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
clf = svm.NuSVC() clf.fit(X, Y)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)
scores = [x[1] for x in grid.grid_scores_] scores = np.array(scores).reshape(len(C_range), len(gamma_range))
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape)
score_means = list() score_stds = list() percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)
this_scores = cross_val_score(clf, X, y, n_jobs=1) score_means.append(this_scores.mean()) score_stds.append(this_scores.std())
skf = StratifiedKFold(n_folds=4) train_index, test_index = next(iter(skf.split(iris.data, iris.target)))
estimators = dict((cov_type, GaussianMixture(n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0)) for cov_type in ['spherical', 'diag', 'tied', 'full'])
estimator.means_init = np.array([X_train[y_train == i].mean(axis=0) for i in range(n_classes)])
estimator.fit(X_train)
for n, color in enumerate(colors): data = X_test[y_test == n] plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)
np.random.seed(0)
shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])
C = np.array([[0., -0.7], [3.5, .7]]) stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)
X_train = np.vstack([shifted_gaussian, stretched_gaussian])
clf = mixture.GaussianMixture(n_components=2, covariance_type='full') clf.fit(X_train)
if not np.any(Y_ == i): continue plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
angle = np.arctan(u[1] / u[0])
n_samples = 500
n_samples = 500
gmm = mixture.GaussianMixture(n_components=n_components, covariance_type=cv_type) gmm.fit(X) bic.append(gmm.bic(X)) if bic[-1] < lowest_bic: lowest_bic = bic[-1] best_gmm = gmm
angle = np.arctan2(w[0][1], w[0][0])
if not np.any(Y_ == i): continue plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
angle = np.arctan(u[1] / u[0])
n_samples = 100
np.random.seed(0) X = np.zeros((n_samples, 2)) step = 4. * np.pi / n_samples
X, y = samples_generator.make_classification( n_features=20, n_informative=3, n_redundant=0, n_classes=4, n_clusters_per_class=2)
anova_filter = SelectKBest(f_regression, k=3) clf = svm.SVC(kernel='linear')
digits = load_digits() X = digits.images.reshape((len(digits.images), -1)) y = digits.target
plt.matshow(ranking, cmap=plt.cm.Blues) plt.colorbar() plt.title("Ranking of pixels with RFE") plt.show()
iris = datasets.load_iris()
E = np.random.uniform(0, 0.1, size=(len(iris.data), 20))
X = np.hstack((iris.data, E)) y = iris.target
clf = svm.SVC(kernel='linear') clf.fit(X, y)
iris = datasets.load_iris() X = iris.data y = iris.target n_classes = np.unique(y).size
random = np.random.RandomState(seed=0) E = random.normal(size=(len(X), 2200))
X = np.c_[X, E]
X, y = make_classification(n_samples=1000, n_features=25, n_informative=3, n_redundant=2, n_repeated=0, n_classes=8, n_clusters_per_class=1, random_state=0)
svc = SVC(kernel="linear") rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2), scoring='accuracy') rfecv.fit(X, y)
boston = load_boston() X, y = boston['data'], boston['target']
clf = LassoCV()
sfm = SelectFromModel(clf, threshold=0.25) sfm.fit(X, y) n_features = sfm.transform(X).shape[1]
while n_features > 2: sfm.threshold += 0.1 X_transform = sfm.transform(X) n_features = X_transform.shape[1]
rng = np.random.RandomState(42) S = rng.standard_t(1.5, size=(20000, 2)) S[:, 0] *= 2.
face = sp.face(gray=True)
face = face / 255
dataset = fetch_olivetti_faces(shuffle=True, random_state=rng) faces = dataset.data
faces_centered = faces - faces.mean(axis=0)
faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)
estimators = [ ('Eigenfaces - RandomizedPCA', decomposition.RandomizedPCA(n_components=n_components, whiten=True), True),
X_homo = X + sigma * rng.randn(n_samples, n_features)
sigmas = sigma * rng.rand(n_features) + sigma / 2. X_hetero = X + rng.randn(n_samples, n_features) * sigmas
np.random.seed(0) n_samples = 2000 time = np.linspace(0, 8, n_samples)
ica = FastICA(n_components=3)
assert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)
pca = PCA(n_components=3)
y = np.linspace(0, resolution - 1, resolution) first_quarter = y < resolution / 4 y[first_quarter] = 3. y[np.logical_not(first_quarter)] = -1.
print('explained variance ratio (first two components): %s' % str(pca.explained_variance_ratio_))
import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm from sklearn.neighbors import KernelDensity
ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True) ax[0, 0].text(-3.5, 0.31, "Histogram")
X_plot = np.linspace(-6, 6, 1000)[:, None] X_src = np.zeros((1, 1))
iris = datasets.load_iris()
y = iris.target
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X, y)
Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
import numpy as np import matplotlib.pyplot as plt from sklearn import neighbors
y[::5] += 1 * (0.5 - np.random.rand(8))
n_neighbors = 5
n_samples_min = int(1e3) n_samples_max = int(1e5) n_features = 100 n_centers = 100 n_queries = 100 n_steps = 6 n_iter = 5
n_samples_values = np.logspace(np.log10(n_samples_min), np.log10(n_samples_max), n_steps).astype(np.int)
rng = np.random.RandomState(42) all_data, _ = make_blobs(n_samples=n_samples_max + n_queries, n_features=n_features, centers=n_centers, shuffle=True, random_state=0) queries = all_data[:n_queries] index_data = all_data[n_queries:]
average_times_exact = [] average_times_approx = [] std_times_approx = [] accuracies = [] std_accuracies = [] average_speedups = [] std_speedups = []
query = queries[[rng.randint(0, n_queries)]]
iris = datasets.load_iris()
y = iris.target
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
try: from mpl_toolkits.basemap import Basemap basemap = True except ImportError: basemap = False
data = fetch_species_distributions() species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']
fig = plt.figure() fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)
Z = -9999 + np.zeros(land_mask.shape[0]) Z[land_mask] = np.exp(kde.score_samples(xy)) Z = Z.reshape(X.shape)
levels = np.linspace(0, Z.max(), 25) plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)
n_samples = 10000 n_features = 100 n_queries = 30 rng = np.random.RandomState(42)
n_estimators_values = [1, 5, 10, 20, 30, 40, 50] accuracies_trees = np.zeros(len(n_estimators_values), dtype=float)
digits = load_digits() data = digits.data
pca = PCA(n_components=15, whiten=False) data = pca.fit_transform(digits.data)
params = {'bandwidth': np.logspace(-1, 1, 20)} grid = GridSearchCV(KernelDensity(), params) grid.fit(data)
kde = grid.best_estimator_
new_data = kde.sample(44, random_state=0) new_data = pca.inverse_transform(new_data)
new_data = new_data.reshape((4, 11, -1)) real_data = digits.data[:44].reshape((4, 11, -1))
n_classes = 3 plot_colors = "bry" plot_step = 0.02
iris = load_iris()
X = iris.data[:, pair] y = iris.target
clf = DecisionTreeClassifier().fit(X, y)
plt.subplot(2, 3, pairidx + 1)
node_depth = np.zeros(shape=n_nodes) is_leaves = np.zeros(shape=n_nodes, dtype=bool)
if (children_left[node_id] != children_right[node_id]): stack.append((children_left[node_id], parent_depth + 1)) stack.append((children_right[node_id], parent_depth + 1)) else: is_leaves[node_id] = True
sample_ids = [0, 1] common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) == len(sample_ids))
import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt
regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=5) regr_1.fit(X, y) regr_2.fit(X, y)
X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test)
pred_entropies = stats.distributions.entropy( lp_model.label_distributions_.T)
uncertainty_index = uncertainty_index = np.argsort(pred_entropies)[-5:]
delete_indices = np.array([])
delete_index, = np.where(unlabeled_indices == image_index) delete_indices = np.concatenate((delete_indices, delete_index))
n_samples = 200 X, y = make_circles(n_samples=n_samples, shuffle=False) outer, inner = 0, 1 labels = -np.ones(n_samples) labels[0] = outer labels[-1] = inner
label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0) label_spread.fit(X, labels)
y_train = np.copy(y) y_train[unlabeled_set] = -1
lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5) lp_model.fit(X, y_train) predicted_labels = lp_model.transduction_[unlabeled_set] true_labels = y[unlabeled_set]
pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)
uncertainty_index = np.argsort(pred_entropies)[-10:]
f = plt.figure(figsize=(7, 5)) for index, image_index in enumerate(uncertainty_index): image = images[image_index]
h = .02
titles = ['Label Spreading 30% data', 'Label Spreading 50% data', 'Label Spreading 100% data', 'SVC with rbf kernel']
plt.subplot(2, 2, i + 1) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired) plt.axis('off')
colors = [color_map[y] for y in y_train] plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)
iris = datasets.load_iris()
RANDOM_SEED = np.random.randint(2 ** 10)
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
n_samples, h, w = lfw_people.images.shape
X = lfw_people.data n_features = X.shape[1]
y = lfw_people.target target_names = lfw_people.target_names n_classes = target_names.shape[0]
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=42)
n_components = 150
return '__file__' in globals()
throughputs = benchmark_throughputs(configuration) plot_benchmark_throughput(throughputs, configuration)
np.random.seed(0)
from matplotlib.finance import quotes_historical_yahoo as quotes_historical_yahoo_ochl
d1 = datetime.datetime(2003, 1, 1) d2 = datetime.datetime(2008, 1, 1)
variation = close - open
edge_model = covariance.GraphLassoCV()
X = variation.copy().T X /= X.std(axis=0) edge_model.fit(X)
node_position_model = manifold.LocallyLinearEmbedding( n_components=2, eigen_solver='dense', n_neighbors=6)
plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels, cmap=plt.cm.spectral)
for index, (name, label, (x, y)) in enumerate( zip(names, labels, embedding.T)):
rgr_ridge = Ridge(alpha=0.2) rgr_ridge.fit(proj_operator, proj.ravel()) rec_l2 = rgr_ridge.coef_.reshape(l, l)
rgr_lasso = Lasso(alpha=0.001) rgr_lasso.fit(proj_operator, proj.ravel()) rec_l1 = rgr_lasso.coef_.reshape(l, l)
import Tkinter as Tk
self.fitted = False
self.refit()
return '__file__' in globals()
vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18, non_negative=True)
data_stream = stream_reuters_documents()
test_stats = {'n_test': 0, 'n_test_pos': 0}
minibatch_size = 1000
minibatch_iterators = iter_minibatches(data_stream, minibatch_size) total_vect_time = 0.0
for i, (X_train_text, y_train) in enumerate(minibatch_iterators):
cls.partial_fit(X_train, y_train, classes=all_classes)
plt.figure() fig = plt.gcf() cls_runtime = [] for cls_name, stats in sorted(cls_stats.items()): cls_runtime.append(stats['total_fit_time'])
try: from mpl_toolkits.basemap import Basemap basemap = True except ImportError: basemap = False
pts = pts[pts['species'] == species_name] bunch['pts_%s' % label] = pts
data = fetch_species_distributions()
xgrid, ygrid = construct_grids(data)
X, Y = np.meshgrid(xgrid, ygrid[::-1])
land_reference = data.coverages[6]
for i, species in enumerate([BV_bunch, MM_bunch]): print("_" * 80) print("Modeling distribution of species '%s'" % species.name)
mean = species.cov_train.mean(axis=0) std = species.cov_train.std(axis=0) train_cover_std = (species.cov_train - mean) / std
Z = np.ones((data.Ny, data.Nx), dtype=np.float64)
idx = np.where(land_reference > -9999) coverages_land = data.coverages[:, idx[0], idx[1]].T
plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds) plt.colorbar(format='%.2f')
X, redirects, index_map = get_adjacency_matrix( redirects_filename, page_links_filename, limit=5000000) names = dict((i, name) for name, i in iteritems(index_map))
data = fetch_olivetti_faces() targets = data.target
n_faces = 5 rng = check_random_state(4) face_ids = rng.randint(test.shape[0], size=(n_faces, )) test = test[face_ids, :]
image_shape = (64, 64)
estimator = RandomForestRegressor(random_state=0, n_estimators=100) score = cross_val_score(estimator, X_full, y_full).mean() print("Score with the complete dataset = %.2f" % score)
import matplotlib.pyplot as plt import numpy as np from time import time
from sklearn import datasets, svm, pipeline from sklearn.kernel_approximation import (RBFSampler, Nystroem) from sklearn.decomposition import PCA
digits = datasets.load_digits(n_class=9)
n_samples = len(digits.data) data = digits.data / 16. data -= data.mean(axis=0)
data_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]
data_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:] #data_test = scaler.transform(data_test)
kernel_svm = svm.SVC(gamma=.2) linear_svm = svm.LinearSVC()
plt.figure(figsize=(8, 8)) accuracy = plt.subplot(211) timescale = plt.subplot(212)
accuracy.plot([64, 64], [0.7, 1], label="n_features")
pca = PCA(n_components=8).fit(data_train)
for i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)): plt.subplot(1, 3, i + 1) Z = clf.predict(flat_grid)
Z = Z.reshape(grid.shape[:-1]) plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired) plt.axis('off')
plt.scatter(X[:, 0], X[:, 1], c=targets_train, cmap=plt.cm.Paired)
pca.fit(X_digits)
for X, y in datasets: X = StandardScaler().fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
digits = datasets.load_digits() X = np.asarray(digits.data, 'float32') X, Y = nudge_dataset(X, digits.target)
logistic = linear_model.LogisticRegression() rbm = BernoulliRBM(random_state=0, verbose=True)
rbm.learning_rate = 0.06 rbm.n_iter = 20 rbm.n_components = 100 logistic.C = 6000.0
classifier.fit(X_train, Y_train)
logistic_classifier = linear_model.LogisticRegression(C=100.0) logistic_classifier.fit(X_train, Y_train)
X, y = mnist.data / 255., mnist.target X_train, X_test = X[:60000], X[60000:] y_train, y_test = y[:60000], y[60000:]
mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4, algorithm='sgd', verbose=10, tol=1e-4, random_state=1, learning_rate_init=.1)
Axes3D
n_neighbors = 10 n_samples = 1000
random_state = check_random_state(0) p = random_state.rand(n_samples) * (2 * np.pi - 0.55) t = random_state.rand(n_samples) * np.pi
fig = plt.figure(figsize=(15, 8)) plt.suptitle("Manifold Learning with %i points, %i neighbors" % (1000, n_neighbors), fontsize=14)
ax.view_init(40, -10)
methods = ['standard', 'ltsa', 'hessian', 'modified'] labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
continue
print("Computing random projection") rp = random_projection.SparseRandomProjection(n_components=2, random_state=42) X_projected = rp.fit_transform(X) plot_embedding(X_projected, "Random Projection of the digits")
print("Computing Spectral embedding") embedder = manifold.SpectralEmbedding(n_components=2, random_state=0, eigen_solver="arpack") t0 = time() X_se = embedder.fit_transform(X)
print("Computing t-SNE embedding") tsne = manifold.TSNE(n_components=2, init='pca', random_state=0) t0 = time() X_tsne = tsne.fit_transform(X)
X_true -= X_true.mean()
clf = PCA(n_components=2) X_true = clf.fit_transform(X_true)
Axes3D
from mpl_toolkits.mplot3d import Axes3D Axes3D
lr = LogisticRegression() gnb = GaussianNB() svc = LinearSVC(C=1.0) rfc = RandomForestClassifier(n_estimators=100)
clf = RandomForestClassifier(n_estimators=25) clf.fit(X_train_valid, y_train_valid) clf_probs = clf.predict_proba(X_test) score = log_loss(y_test, clf_probs)
X_train, X_test, y_train, y_test, sw_train, sw_test = \ train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)
clf = GaussianNB()
clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic') clf_isotonic.fit(X_train, y_train, sw_train) prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]
clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid') clf_sigmoid.fit(X_train, y_train, sw_train) prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]
X, y = datasets.make_classification(n_samples=100000, n_features=20, n_informative=2, n_redundant=10, random_state=42)
isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')
sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')
lr = LogisticRegression(C=1., solver='lbfgs')
plot_calibration_curve(GaussianNB(), "Naive Bayes", 1)
plot_calibration_curve(LinearSVC(), "SVC", 2)
scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring="mean_squared_error", cv=10)
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
pipeline = Pipeline([ ('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier()), ])
parameters = { 'vect__max_df': (0.5, 0.75, 1.0), #'vect__max_features': (None, 5000, 10000, 50000),
grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)
n_samples_train, n_samples_test, n_features = 75, 150, 500 np.random.seed(0) coef = np.random.randn(n_features)
X_train, X_test = X[:n_samples_train], X[n_samples_train:] y_train, y_test = y[:n_samples_train], y[n_samples_train:]
enet.set_params(alpha=alpha_optim) coef_ = enet.fit(X, y).coef_
iris = datasets.load_iris() X = iris.data y = iris.target
colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal']) lw = 2
y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1]
random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=random_state)
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state)) y_score = classifier.fit(X_train, y_train).decision_function(X_test)
digits = load_digits() X, y = digits.data, digits.target
clf = RandomForestClassifier(n_estimators=20)
n_iter_search = 20 random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search)
grid_search = GridSearchCV(clf, param_grid=param_grid) start = time() grid_search.fit(X, y)
iris = datasets.load_iris() X = iris.data y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
classifier = svm.SVC(kernel='linear', C=0.01) y_pred = classifier.fit(X_train, y_train).predict(X_test)
cm = confusion_matrix(y_test, y_pred) np.set_printoptions(precision=2) print('Confusion matrix, without normalization') print(cm) plt.figure() plot_confusion_matrix(cm)
iris = datasets.load_iris() X = iris.data y = iris.target X, y = X[y != 2], y[y != 2] n_samples, n_features = X.shape
random_state = np.random.RandomState(0) X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
cv = StratifiedKFold(n_folds=6) classifier = svm.SVC(kernel='linear', probability=True, random_state=random_state)
digits = datasets.load_digits()
n_samples = len(digits.images) X = digits.images.reshape((n_samples, -1)) y = digits.target
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=0)
iris = datasets.load_iris() X = iris.data y = iris.target
y = label_binarize(y, classes=[0, 1, 2]) n_classes = y.shape[1]
random_state = np.random.RandomState(0) n_samples, n_features = X.shape X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state)) y_score = classifier.fit(X_train, y_train).decision_function(X_test)
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr) for i in range(n_classes): mean_tpr += interp(all_fpr, fpr[i], tpr[i])
mean_tpr /= n_classes
cv = ShuffleSplit(n_iter=100, test_size=0.2, random_state=0)
pca = PCA(n_components=2)
selection = SelectKBest(k=1)
X_features = combined_features.fit(X, y).transform(X)
lim = 8
y = np.array(g(X) > 0, dtype=int)
k3 = 0.66**2 \ * RationalQuadratic(length_scale=1.2, alpha=0.78) k4 = 0.18**2 * RBF(length_scale=0.134) \
k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0) k4 = 0.1**2 * RBF(length_scale=0.1) \ + WhiteKernel(noise_level=0.1**2,
iris = datasets.load_iris()
plt.subplot(1, 2, i + 1)
X = 15 * rng.rand(100, 1) y = np.sin(X).ravel()
gp = GaussianProcessRegressor(kernel=kernel)
gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), optimizer=None) gp_fix.fit(X[:train_size], y[:train_size])
X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
y = f(X).ravel()
x = np.atleast_2d(np.linspace(0, 10, 1000)).T
kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
gp.fit(X, y)
y_pred, sigma = gp.predict(x, return_std=True)
X = np.linspace(0.1, 9.9, 20) X = np.atleast_2d(X).T
gp = GaussianProcessRegressor(kernel=kernel, alpha=(dy / y) ** 2, n_restarts_optimizer=10)
gp.fit(X, y)
y_pred, sigma = gp.predict(x, return_std=True)
l1 = np.random.normal(size=n) l2 = np.random.normal(size=n)
plsca = PLSCanonical(n_components=2) plsca.fit(X_train, Y_train) X_train_r, Y_train_r = plsca.transform(X_train, Y_train) X_test_r, Y_test_r = plsca.transform(X_test, Y_test)
Y = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5
print("Estimated B") print(np.round(pls2.coef_, 1)) pls2.predict(X)
print("Estimated betas") print(np.round(pls1.coef_, 1))
bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm="SAMME", n_estimators=200)
import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor
regr_1 = DecisionTreeRegressor(max_depth=4)
y_1 = regr_1.predict(X) y_2 = regr_2.predict(X)
probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]
class1_1 = [pr[0, 0] for pr in probas] class2_1 = [pr[0, 1] for pr in probas]
n_jobs = 1
data = fetch_olivetti_faces() X = data.images.reshape((len(data.images), -1)) y = data.target
print("Fitting ExtraTreesClassifier on faces data with %d cores..." % n_jobs) t0 = time() forest = ExtraTreesClassifier(n_estimators=1000, max_features=128, n_jobs=n_jobs, random_state=0)
plt.matshow(importances, cmap=plt.cm.hot) plt.title("Pixel importances with forests of trees") plt.show()
test_score = np.zeros((params['n_estimators'],), dtype=np.float64)
learning_rate = 1.
discrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete] real_estimator_errors = bdt_real.estimator_errors_[:n_trees_real] discrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]
plt.subplots_adjust(wspace=0.25) plt.show()
X = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T X = X.astype(np.float32)
y = f(X).ravel()
xx = np.atleast_2d(np.linspace(0, 10, 1000)).T xx = xx.astype(np.float32)
y_upper = clf.predict(xx)
y_lower = clf.predict(xx)
y_pred = clf.predict(xx)
iris = datasets.load_iris() X = iris.data[:, [0, 2]] y = iris.target
cv_score = cv_estimate(3)
test_score = heldout_score(clf, X_test, y_test)
cumsum = -np.cumsum(clf.oob_improvement_)
oob_best_iter = x[np.argmin(cumsum)]
test_score -= test_score[0] test_best_iter = x[np.argmin(test_score)]
cv_score -= cv_score[0] cv_best_iter = x[np.argmin(cv_score)]
y_multirf = regr_multirf.predict(X_test) y_rf = regr_rf.predict(X_test)
X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size=0.5)
rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator, random_state=0)
y_pred_grd = grd.predict_proba(X_test)[:, 1] fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)
y_pred_rf = rf.predict_proba(X_test)[:, 1] fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)
n_classes = 3 n_estimators = 30 plot_colors = "ryb" cmap = plt.cm.RdYlBu
iris = load_iris()
X = iris.data[:, pair] y = iris.target
idx = np.arange(X.shape[0]) np.random.seed(RANDOM_SEED) np.random.shuffle(idx) X = X[idx] y = y[idx]
mean = X.mean(axis=0) std = X.std(axis=0) X = (X - mean) / std
clf = clone(model) clf = model.fit(X, y)
plt.title(model_title)
X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False)
forest = ExtraTreesClassifier(n_estimators=250, random_state=0)
print("Feature ranking:")
X, y = make_circles(factor=0.5, random_state=0, noise=0.05)
hasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3) X_transformed = hasher.fit_transform(X)
pca = TruncatedSVD(n_components=2) X_reduced = pca.fit_transform(X_transformed)
nb = BernoulliNB() nb.fit(X_transformed, y)
trees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0) trees.fit(X, y)
fig = plt.figure(figsize=(9, 8))
transformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()]) y_grid_pred = nb.predict_proba(transformed_grid)[:, 1]
y_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
X_train, X_test, y_train, y_test = train_test_split(cal_housing.data, cal_housing.target, test_size=0.2, random_state=1) names = cal_housing.feature_names
if __name__ == '__main__': main()
labels, y = np.unique(y, return_inverse=True)
test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)
test_deviance[i] = clf.loss_(y_test, y_pred)
clf = IsolationForest(max_samples=100, random_state=rng) clf.fit(X_train) y_pred_train = clf.predict(X_train) y_pred_test = clf.predict(X_test) y_pred_outliers = clf.predict(X_outliers)
estimators = [("Tree", DecisionTreeRegressor()), ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))]
def f(x): x = x.ravel()
for n, (name, estimator) in enumerate(estimators): y_predict = np.zeros((n_test, n_repeat))
y_error = np.zeros(n_test)
X, y = make_classification(n_samples=500, n_features=25, n_clusters_per_class=1, n_informative=15, random_state=RANDOM_STATE)
error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)
min_estimators = 15 max_estimators = 175
oob_error = 1 - clf.oob_score_ error_rate[label].append((i, oob_error))
for label, clf_err in error_rate.items(): xs, ys = zip(*clf_err) plt.plot(xs, ys, label=label)
#categories = None
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
hasher = HashingVectorizer(n_features=opts.n_features, stop_words='english', non_negative=True, norm=None, binary=False) vectorizer = make_pipeline(hasher, TfidfTransformer())
svd = TruncatedSVD(opts.n_components) normalizer = Normalizer(copy=False) lsa = make_pipeline(svd, normalizer)
plt.matshow(cm) plt.title('Confusion matrix of the %s classifier' % name) plt.colorbar()
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
if opts.all_categories: categories = None else: categories = [ 'alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space', ]
y_train, y_test = data_train.target, data_test.target
if opts.use_hashing: feature_names = None else: feature_names = vectorizer.get_feature_names()
feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]
results.append(benchmark(LinearSVC(loss='l2', penalty=penalty, dual=False, tol=1e-3)))
results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50, penalty=penalty)))
print('=' * 80) print("Elastic-Net penalty") results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50, penalty="elasticnet")))
print('=' * 80) print("NearestCentroid (aka Rocchio classifier)") results.append(benchmark(NearestCentroid()))
C = 1. fit_intercept = True tol = 1.0e-14
max_squared_sum = get_max_squared_sum(X) step_size = get_auto_step_size(max_squared_sum, alpha, "log", fit_intercept)
n = 23149 X_test = X[:n, :] y_test = y[:n] X = X[n:, :] y = y[n:]
scikit_classifier_results = [] scikit_regressor_results = []
tstart = datetime.now() clf = DecisionTreeClassifier() clf.fit(X, Y).predict(X) delta = (datetime.now() - tstart)
tstart = datetime.now() clf = DecisionTreeRegressor() clf.fit(X, Y).predict(X) delta = (datetime.now() - tstart)
memory = Memory(os.path.join(get_data_home(), 'covertype_benchmark_data'), mmap_mode='r')
eps = 1e-5 n, m = V.shape W, H = _initialize_nmf(V, r, init, random_state=0)
ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3, color=c) ax.plot([1], [1], [1], color=c, label=label)
t_start = datetime.now() clf.fit(X) delta = (datetime.now() - t_start) time_to_fit = compute_time(t_start, delta)
t_start = datetime.now() clf.transform(X) delta = (datetime.now() - t_start) time_to_transform = compute_time(t_start, delta)
op = optparse.OptionParser() op.add_option("--n-times", dest="n_times", default=5, type=int, help="Benchmark results are average over n_times experiments")
n_nonzeros = int(opts.ratio_nonzeros * opts.n_features)
transformers = {}
gaussian_matrix_params = { "n_components": opts.n_components, "random_state": opts.random_seed } transformers["GaussianRandomProjection"] = \ GaussianRandomProjection(**gaussian_matrix_params)
sparse_matrix_params = { "n_components": opts.n_components, "random_state": opts.random_seed, "density": opts.density, "eps": opts.eps, }
time_fit = collections.defaultdict(list) time_transform = collections.defaultdict(list)
idx = np.arange(n_train) np.random.seed(13) np.random.shuffle(idx) X_train = X_train[idx] y_train = y_train[idx]
y += 0.01 * np.random.normal((n_samples,))
return X[:n_samples], X[n_samples:]
n_samples = [int(1e3), int(1e4), int(1e5), int(1e6)] n_features = int(1e2) n_queries = 100 n_neighbors = 10
plt.figure() plt.legend(legend_rects, legend_labels, loc='upper left')
plt.figure() plt.legend(legend_rects, legend_labels, loc='upper left')
enable_spectral_norm = False
MAX_MEMORY = np.int(2e9)
CIFAR_FOLDER = "./cifar-10-batches-py/" SVHN_FOLDER = "./SVHN/"
U, mu, V = fbpca.pca(X, n_comps, raw=True, n_iter=n_iter, l=n_oversamples+n_comps) call_time = time() - t0
value = sp.sparse.linalg.svds(A, k=1, return_singular_vectors=False)
if not args.show_plot: print(n, np.mean(time_per_iteration))
ax.plot_surface(X, Y, Z.T, cstride=1, rstride=1, color=c, alpha=0.8)
#ax.plot([1], [1], [1], color=c, label=label)
plot_batch_errors(all_errors, n_components, batch_sizes, data)
X = faces.data[:5000] n_samples, h, w = faces.images.shape n_features = X.shape[1]
memory = Memory(os.path.join(get_data_home(), 'mnist_benchmark_data'), mmap_mode='r')
X = X / 255
tstart = time() clf = factory(alpha=alpha).fit(X, Y) delta = (time() - tstart)
import matplotlib.pyplot as plt
ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3, color=c) ax.plot([1], [1], [1], color=c, label=label)
import time
s = (y != 4) X = X[s, :] y = y[s] y = (y != 1).astype(int)
s = (y == 2) + (y == 4) X = X[s, :] y = y[s] y = (y != 2).astype(int)
t_start = datetime.now() sampling(n_population, n_samples) delta = (datetime.now() - t_start) time = compute_time(t_start, delta) return time
op = optparse.OptionParser() op.add_option("--n-times", dest="n_times", default=5, type=int, help="Benchmark results are average over n_times experiments")
sampling_algorithm = {}
sampling_algorithm["python-core-sample"] = \ lambda n_population, n_sample: \ random.sample(xrange(n_population), n_sample)
sampling_algorithm["custom-auto"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="auto", random_state=random_state)
sampling_algorithm["custom-tracking-selection"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="tracking_selection", random_state=random_state)
sampling_algorithm["custom-reservoir-sampling"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="reservoir_sampling", random_state=random_state)
sampling_algorithm["custom-pool"] = \ lambda n_population, n_samples, random_state=None: \ sample_without_replacement(n_population, n_samples, method="pool", random_state=random_state)
sampling_algorithm["numpy-permutation"] = \ lambda n_population, n_sample: \ np.random.permutation(n_population)[:n_sample]
sampling_algorithm = dict((key, value) for key, value in sampling_algorithm.items() if key in selected_algorithm)
time = {} n_samples = np.linspace(start=0, stop=opts.n_population, num=opts.n_steps).astype(np.int)